{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-batch model for hpc run with input edge weights in csv file\n",
    "\n",
    "Comments:\n",
    "\n",
    "By using the read weighted edge list from a csv file, it saves much space on self.graph\n",
    "\n",
    "This will for specific batch number and hop-layer number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import csv\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pickle\n",
    "import shutil\n",
    "\n",
    "from utils import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import torch\n",
    "from torch_geometric.utils import scatter_\n",
    "\n",
    "import math\n",
    "import random\n",
    "\n",
    "from torch.nn import Parameter\n",
    "from torch_scatter import scatter_add\n",
    "# from torch_geometric.nn import MessagePassing\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import add_remaining_self_loops\n",
    "\n",
    "### ====================== Establish a GCN based model ========================\n",
    "class ListModule(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Abstract list layer class.\n",
    "    \"\"\"\n",
    "    def __init__(self, *args):\n",
    "        \"\"\"\n",
    "        Module initializing.\n",
    "        \"\"\"\n",
    "        super(ListModule, self).__init__()\n",
    "        idx = 0\n",
    "        for module in args:\n",
    "            self.add_module(str(idx), module)\n",
    "            idx += 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Getting the indexed layer.\n",
    "        \"\"\"\n",
    "        if idx < 0 or idx >= len(self._modules):\n",
    "            raise IndexError('index {} is out of range'.format(idx))\n",
    "        it = iter(self._modules.values())\n",
    "        for i in range(idx):\n",
    "            next(it)\n",
    "        return next(it)\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Iterating on the layers.\n",
    "        \"\"\"\n",
    "        return iter(self._modules.values())\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Number of layers.\n",
    "        \"\"\"\n",
    "        return len(self._modules)\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, input_layers = [16, 16], dropout=0.3):\n",
    "        \"\"\"\n",
    "        input layers: list of integers\n",
    "        dropout: probability of droping out \n",
    "        \"\"\"\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.input_layers = input_layers\n",
    "        self.dropout = dropout\n",
    "        self.setup_layers()\n",
    "\n",
    "    def setup_layers(self):\n",
    "        \"\"\"\n",
    "        Creating the layes based on the args.\n",
    "        \"\"\"\n",
    "        self.layers = []\n",
    "        self.input_layers = [self.in_channels] + self.input_layers + [self.out_channels]\n",
    "        for i, _ in enumerate(self.input_layers[:-1]):\n",
    "            self.layers.append(GCNConv(self.input_layers[i],self.input_layers[i+1]))\n",
    "        self.layers = ListModule(*self.layers)\n",
    "\n",
    "    # change the dropout positions: \n",
    "    def forward(self, edge_index, features):\n",
    "        if len(self.layers) > 1:\n",
    "            for i in range(len(self.layers)-1):\n",
    "                features = F.relu(self.layers[i](features, edge_index))\n",
    "#                 if i>0:\n",
    "                features = F.dropout(features, p = self.dropout, training = self.training)\n",
    "                    \n",
    "            features = self.layers[len(self.layers)-1](features, edge_index)\n",
    "        else:\n",
    "            features = self.layers[0](features, edge_index)    # for a single layer case\n",
    "\n",
    "        predictions = F.log_softmax(features, dim=1)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partition GCN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Partition method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import metis\n",
    "import random\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import chain\n",
    "\n",
    "class ClusteringMachine(object):\n",
    "    \"\"\"\n",
    "    Clustering the graph, feature set and label. Performed on the CPU side\n",
    "    \"\"\"\n",
    "    def __init__(self, edge_index, features, label, tmp_folder = './tmp/'):\n",
    "        \"\"\"\n",
    "        :param edge_index: COO format of the edge indices.\n",
    "        :param features: Feature matrix (ndarray).\n",
    "        :param label: label vector (ndarray).\n",
    "        :tmp_folder(string): the path of the folder to contain all the clustering information files\n",
    "        \"\"\"\n",
    "        self.features = features\n",
    "        self.label = label\n",
    "        self._set_sizes()\n",
    "        self.edge_index = edge_index\n",
    "        \n",
    "        tmp = edge_index.t().numpy().tolist()\n",
    "        self.graph = nx.from_edgelist(tmp)\n",
    "    def _set_sizes(self):\n",
    "        \"\"\"\n",
    "        Setting the feature and class count.\n",
    "        \"\"\"\n",
    "        self.node_count = self.features.shape[0]\n",
    "        self.feature_count = self.features.shape[1]    # features all always in the columns\n",
    "        self.label_count = len(np.unique(self.label.numpy()) )\n",
    "    \n",
    "    # 1) first use different clustering method, then split each cluster into train, test and validation nodes, split edges\n",
    "    def split_cluster_nodes_edges(self, test_ratio, validation_ratio, partition_num = 2):\n",
    "        \"\"\"\n",
    "        Decomposing the graph, partitioning the features and label, creating Torch arrays.\n",
    "        \"\"\"\n",
    "        self.sg_nodes_global = self.metis_clustering(self.graph, partition_num)\n",
    "        # to always assign the same cluster of validation and train is an extra restriction\n",
    "        self.train_clusters = list(self.sg_nodes_global.keys())\n",
    "#         self.train_clusters, self.sg_nodes_global = self.random_clustering(list(self.graph.nodes()), partition_num)\n",
    "        self.validation_clusters = self.train_clusters\n",
    "        \n",
    "        relative_test_ratio = (test_ratio) / (1 - validation_ratio)\n",
    "        \n",
    "        self.sg_validation_nodes_global = {}\n",
    "        self.sg_train_nodes_global = {}\n",
    "        self.sg_test_nodes_global = {}\n",
    "        \n",
    "        # keep the info of each cluster:\n",
    "        self.info_isolate_cluster_size = {}\n",
    "        self.info_validation_cluster_size = {}\n",
    "        self.info_train_cluster_size = {}\n",
    "        self.info_test_cluster_size = {}\n",
    "        \n",
    "        for cluster in self.train_clusters:\n",
    "            sg_model_nodes_global, self.sg_validation_nodes_global[cluster] = train_test_split(self.sg_nodes_global[cluster], test_size = validation_ratio)\n",
    "            self.sg_train_nodes_global[cluster], self.sg_test_nodes_global[cluster] = train_test_split(sg_model_nodes_global, test_size = relative_test_ratio)\n",
    "            \n",
    "            # record the information of each cluster:\n",
    "            self.info_isolate_cluster_size[cluster] = len(self.sg_nodes_global[cluster])\n",
    "            self.info_validation_cluster_size[cluster] = len(self.sg_validation_nodes_global[cluster])\n",
    "            self.info_train_cluster_size[cluster] = len(self.sg_train_nodes_global[cluster])\n",
    "            self.info_test_cluster_size[cluster] = len(self.sg_test_nodes_global[cluster])\n",
    "    \n",
    "    # just allocate each node to arandom cluster, store the membership inside each dict\n",
    "    def random_clustering(self, target_nodes, partition_num):\n",
    "        \"\"\"\n",
    "            Random clustering the nodes.\n",
    "            Input: \n",
    "                1) target_nodes: list of node \n",
    "                2) partition_num: number of partition to be generated\n",
    "            Output: \n",
    "                1) membership of each node\n",
    "        \"\"\"\n",
    "        # randomly divide into two clusters\n",
    "        nodes_order = [node for node in target_nodes]\n",
    "        random.shuffle(nodes_order)\n",
    "        n = (len(nodes_order) + partition_num - 1) // partition_num\n",
    "        partition_list = [nodes_order[i * n:(i + 1) * n] for i in range(partition_num)]\n",
    "#         cluster_membership = {node : i for i, node_list in enumerate(partition_list) for node in node_list}\n",
    "        cluster_nodes_global = {i : node_list for i, node_list in enumerate(partition_list)}\n",
    "        \n",
    "        return cluster_nodes_global\n",
    "\n",
    "    def metis_clustering(self, target_graph, partition_num):\n",
    "        \"\"\"\n",
    "            Random clustering the nodes.\n",
    "            Input: \n",
    "                1) target_nodes: list of node \n",
    "                2) partition_num: number of partition to be generated\n",
    "            Output: \n",
    "                1) membership of each node\n",
    "        \"\"\"\n",
    "        (st, parts) = metis.part_graph(target_graph, partition_num)\n",
    "        clusters = list(set(parts))\n",
    "        cluster_nodes_global = defaultdict(list)\n",
    "        for node, cluster_id in enumerate(parts):\n",
    "            cluster_nodes_global[cluster_id].append(node)\n",
    "        return cluster_nodes_global\n",
    "        \n",
    "    # select the training nodes as the mini-batch for each cluster\n",
    "    def mini_batch_sample(self, target_seed, k, frac = 1):\n",
    "        \"\"\"\n",
    "            This function is to generate the neighbors of the seed (either train nodes or validation nodes)\n",
    "            params: cluster index, number of layer k, fraction of sampling from each neighbor layer\n",
    "            input: \n",
    "                1) target_seed: this is the 0 layer inside self.neighbor\n",
    "                2) target_graph: generate the mini-batch within the target_graph\n",
    "            output:\n",
    "                1) neighbor: nodes global idx inside each layer of the batch\n",
    "                2) accum_neighbor: accumulating neighbors , i.e. the final batch nodes\n",
    "        \"\"\"\n",
    "        \n",
    "        accum_neighbor = defaultdict(set)\n",
    "        for cluster in target_seed.keys():\n",
    "            # make sure the train batch won't exceed each isolated clustering, cannot cover the boundaries between clusters\n",
    "            target_graph = self.graph.subgraph(self.sg_nodes_global[cluster])\n",
    "            neighbor = set(target_seed[cluster])  # first layer of the neighbor nodes of each cluster\n",
    "            for layer in range(k):\n",
    "                # first accumulate last layer\n",
    "                accum_neighbor[cluster] |= neighbor\n",
    "                tmp_level = set()\n",
    "                for node in neighbor:\n",
    "                    tmp_level |= set(target_graph.neighbors(node))  # the key here we are using self.graph, extract neighbor from the whole graph\n",
    "                # add the new layer of neighbors\n",
    "                tmp_level -= accum_neighbor[cluster]\n",
    "                # each layer will only contains partial nodes from the previous layer\n",
    "                neighbor = set(random.sample(tmp_level, int(len(tmp_level) * frac) ) ) if 0 < frac < 1 else tmp_level\n",
    "    #                 print('layer ' + str(layer + 1) + ' : ', self.neighbor[cluster][layer+1])\n",
    "            # the most outside layer: kth layer will be added:\n",
    "            accum_neighbor[cluster] |= neighbor\n",
    "        return accum_neighbor\n",
    "    \n",
    "    def mini_batch_generate(self, batch_file_folder, target_seed, k, fraction = 1.0):\n",
    "        \"\"\"\n",
    "            create the mini-batch focused on the train nodes only, include a total of k layers of neighbors of the original training nodes\n",
    "            k: number of layers of neighbors for each training node\n",
    "            fraction: fraction of neighbor nodes in each layer to be considered\n",
    "            Input:\n",
    "                1) target_seed: global ids of the nodes for seed to generate the batch\n",
    "                    usually one of (train_global, test_global_, validation_global)\n",
    "            Output: all tensors which are gonna be used in the train, forward procedure\n",
    "                local:\n",
    "                    1) sg_mini_edges_local\n",
    "                    2) self.sg_mini_train_edge_weight_local\n",
    "                    3) self.sg_mini_train_nodes_local\n",
    "                    4) self.sg_mini_train_features\n",
    "                    5) self.sg_mini_train_labels\n",
    "            \n",
    "        \"\"\"\n",
    "        info_batch_size = {}\n",
    "                \n",
    "#         accum_neighbor = self.mini_batch_sample(target_seed, k, frac = fraction)\n",
    "        \n",
    "        for cluster in target_seed.keys():\n",
    "            batch_subgraph = self.graph.subgraph(self.sg_nodes_global[cluster])\n",
    "            \n",
    "             # first select all the overlapping nodes of the train nodes\n",
    "            mini_nodes_global = sorted(node for node in batch_subgraph.nodes())\n",
    "            \n",
    "            # store the global edges\n",
    "            mini_edges_global = {edge for edge in batch_subgraph.edges()}\n",
    "            \n",
    "            # map nodes from global index to local index\n",
    "            mini_mapper = {node: i for i, node in enumerate(mini_nodes_global)}\n",
    "            \n",
    "            # store local index of batch nodes\n",
    "            mini_nodes_local = [ mini_mapper[global_idx] for global_idx in target_seed[cluster] ]\n",
    "            \n",
    "            # store local index of batch edges\n",
    "            mini_edges_local = \\\n",
    "                           [ [ mini_mapper[edge[0]], mini_mapper[edge[1]] ] for edge in mini_edges_global ] + \\\n",
    "                           [ [ mini_mapper[edge[1]], mini_mapper[edge[0]] ] for edge in mini_edges_global ]\n",
    "            \n",
    "            # store local features and lables\n",
    "            mini_features = self.features[mini_nodes_global,:]\n",
    "            mini_labels = self.label[mini_nodes_global]\n",
    "            \n",
    "            # record information \n",
    "            info_batch_size[cluster] = len(mini_nodes_global)\n",
    "            \n",
    "            mini_nodes_local = torch.LongTensor(mini_nodes_local)\n",
    "            mini_edges_local = torch.LongTensor(mini_edges_local).t()\n",
    "            mini_features = torch.FloatTensor(mini_features)\n",
    "            mini_labels = torch.LongTensor(mini_labels)\n",
    "            \n",
    "            minibatch_data = [mini_nodes_local, mini_edges_local, mini_features, mini_labels]\n",
    "            \n",
    "            batch_file_name = batch_file_folder + 'batch_' + str(cluster)\n",
    "            \n",
    "            # store the batch files\n",
    "            t0 = time.time()\n",
    "            with open(batch_file_name, \"wb\") as fp:\n",
    "                pickle.dump(minibatch_data, fp)\n",
    "            store_time = ((time.time() - t0) * 1000)\n",
    "#             print('*** Generate batch file for # {0:3d} batch, writing the batch file costed {1:.2f} ms ***'.format(cluster, store_time) )\n",
    "#             print('writing to the path: ', batch_file_name)\n",
    "            \n",
    "        return info_batch_size\n",
    "    \n",
    "    def mini_batch_train_clustering(self, batch_folder, k, fraction = 1.0, train_batch_num = 2):\n",
    "        data_type = 'train'\n",
    "        batch_file_folder = batch_folder + data_type + '/'\n",
    "        check_folder_exist(batch_file_folder)\n",
    "        os.makedirs(os.path.dirname(batch_file_folder), exist_ok=True)\n",
    "        self.info_train_batch_size = self.mini_batch_generate(batch_file_folder, self.sg_train_nodes_global, k, fraction = 1.0)\n",
    "        self.info_train_cluster_size = {key : len(val) for key, val in self.sg_train_nodes_global.items()}\n",
    "        \n",
    "    def mini_batch_validation_clustering(self, batch_folder, k, fraction = 1.0, valid_batch_num = 2):\n",
    "        data_type = 'validation'\n",
    "        batch_file_folder = batch_folder + data_type + '/'\n",
    "        check_folder_exist(batch_file_folder)\n",
    "        os.makedirs(os.path.dirname(batch_file_folder), exist_ok=True)\n",
    "        self.info_validation_batch_size = self.mini_batch_generate(batch_file_folder, self.sg_validation_nodes_global, k, fraction = 1.0)\n",
    "        self.info_validation_cluster_size = {key : len(val) for key, val in self.sg_validation_nodes_global.items()}\n",
    "    \n",
    "    def mini_batch_test_clustering(self, batch_folder, k, fraction = 1.0, test_batch_num = 2):\n",
    "        data_type = 'test'\n",
    "        batch_file_folder = batch_folder + data_type + '/'\n",
    "        check_folder_exist(batch_file_folder)\n",
    "        os.makedirs(os.path.dirname(batch_file_folder), exist_ok=True)\n",
    "        self.info_test_batch_size = self.mini_batch_generate(batch_file_folder, self.sg_test_nodes_global, k, fraction = 1.0)\n",
    "        self.info_test_cluster_size = {key : len(val) for key, val in self.sg_test_nodes_global.items()}\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Partition Graph with trainiing and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Custom_GCN_layer import Net\n",
    "import time\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class ClusterGCNTrainer_mini_Train(object):\n",
    "    \"\"\"\n",
    "    Training a ClusterGCN.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_folder, in_channels, out_channels, input_layers = [32, 16], dropout=0.3):\n",
    "        \"\"\"\n",
    "        :param in_channels, out_channels: input and output feature dimension\n",
    "        :param clustering_machine:\n",
    "        \"\"\"  \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.data_folder = data_folder\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.input_layers = input_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.create_model()\n",
    "\n",
    "    def create_model(self):\n",
    "        \"\"\"\n",
    "        Creating a StackedGCN and transferring to CPU/GPU.\n",
    "        \"\"\"\n",
    "#         print('used layers are: ', str(self.input_layers))\n",
    "        self.model = Net(self.in_channels, self.out_channels, input_layers = self.input_layers, dropout = self.dropout)\n",
    "        self.model = self.model.to(self.device)\n",
    "    \n",
    "    # call the forward function batch by batch\n",
    "    def do_forward_pass(self, tr_train_nodes, tr_edges, tr_features, tr_target):\n",
    "        \"\"\"\n",
    "        Making a forward pass with data from a given partition.\n",
    "        :param cluster: Cluster index.\n",
    "        :return average_loss: Average loss on the cluster.\n",
    "        :return node_count: Number of nodes.\n",
    "        \"\"\"\n",
    "        \n",
    "        '''Target and features are one-one mapping'''\n",
    "        # calculate the probabilites from log_sofmax\n",
    "        predictions = self.model(tr_edges, tr_features)\n",
    "        \n",
    "        ave_loss = torch.nn.functional.nll_loss(predictions[tr_train_nodes], tr_target[tr_train_nodes])\n",
    "        node_count = tr_train_nodes.shape[0]\n",
    "\n",
    "        # for each cluster keep track of the counts of the nodes\n",
    "        return ave_loss, node_count\n",
    "\n",
    "    def update_average_loss(self, batch_average_loss, node_count, isolate = True):\n",
    "        \"\"\"\n",
    "        Updating the average loss in the epoch.\n",
    "        :param batch_average_loss: Loss of the cluster. \n",
    "        :param node_count: Number of nodes in currently processed cluster.\n",
    "        :return average_loss: Average loss in the epoch.\n",
    "        \"\"\"\n",
    "        self.accumulated_training_loss = self.accumulated_training_loss + batch_average_loss.item() * node_count\n",
    "        if isolate:\n",
    "            self.node_count_seen = self.node_count_seen + node_count\n",
    "        average_loss = self.accumulated_training_loss / self.node_count_seen\n",
    "        return average_loss\n",
    "\n",
    "    # iterate through epoch and also the clusters\n",
    "    def train_investigate_F1(self, epoch_num=10, learning_rate=0.01, weight_decay = 0.01, mini_epoch_num = 1, output_period = 10, train_batch_num = 2, valid_batch_num = 2):\n",
    "        \"\"\"\n",
    "            *** Periodically output the F1 score during training. After certain number of epochs ***\n",
    "            epoch_num:  number of total training epoch number\n",
    "            learning rate: learning rate during training\n",
    "            weight_decay:  decay coefficients for the regularization\n",
    "            mini_epoch_num:  number of epochs of repeating training after loading data on the GPU\n",
    "            output_period:  number of epochs after which output the F1 and accuray to investigate the model refining process\n",
    "        \"\"\"\n",
    "        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        self.model.train()   #   set into train mode, only effective for certain modules such as dropout and batchNorm\n",
    "        self.record_ave_training_loss = []\n",
    "        self.time_train_load_data = 0\n",
    "        \n",
    "        epoch_partition = epoch_num // mini_epoch_num\n",
    "        investigate_f1 = {}\n",
    "        investigate_accuracy = {}\n",
    "        \n",
    "        t0 = time.time()\n",
    "        train_clusters = list(range(train_batch_num))\n",
    "        for epoch_part in range(epoch_partition):\n",
    "#             For test purpose, we let the clusters to follow specific order\n",
    "            random.shuffle(train_clusters)\n",
    "            self.node_count_seen = 0\n",
    "            self.accumulated_training_loss = 0\n",
    "            for cluster in train_clusters:\n",
    "                # for each batch, we load once and train it for multiple epochs:\n",
    "                # read in the train data from the pickle files\n",
    "                batch_file_name = self.data_folder + 'train/batch_' + str(cluster)\n",
    "                \n",
    "                t2 = time.time()\n",
    "                with open(batch_file_name, \"rb\") as fp:\n",
    "                    minibatch_data_train = pickle.load(fp)\n",
    "                read_time = (time.time() - t2) * 1000\n",
    "#                 print('*** During training for # {0:3d} batch, reading batch file costed {1:.2f} ms ***'.format(cluster, read_time) )\n",
    "                \n",
    "                tr_train_nodes, tr_edges, tr_features, tr_target = minibatch_data_train\n",
    "                \n",
    "                t1 = time.time()\n",
    "                tr_train_nodes = tr_train_nodes.to(self.device)\n",
    "                tr_edges = tr_edges.to(self.device)\n",
    "                tr_features = tr_features.to(self.device)\n",
    "                tr_target = tr_target.to(self.device)\n",
    "                \n",
    "                self.time_train_load_data += (time.time() - t1) * 1000\n",
    "                # train each batch for multiple epochs\n",
    "                for mini_epoch in range(mini_epoch_num):\n",
    "                    # record the current overall epoch index:\n",
    "                    real_epoch_num = 1 + mini_epoch + mini_epoch_num * epoch_part # real_epoch_num starts from 0, therefore we add 1\n",
    "                    \n",
    "                    self.optimizer.zero_grad()\n",
    "                    batch_ave_loss, node_count = self.do_forward_pass(tr_train_nodes, tr_edges, tr_features, tr_target)\n",
    "                    batch_ave_loss.backward()\n",
    "                    self.optimizer.step()\n",
    "                    ave_loss = self.update_average_loss(batch_ave_loss, node_count)\n",
    "                    \n",
    "                    # at this point finish a single train duration: update the parameter and calcualte the loss function\n",
    "                    # periodically output the F1-score in the middle of the training process\n",
    "                    if real_epoch_num % output_period == 0:\n",
    "                        investigate_f1[real_epoch_num], investigate_accuracy[real_epoch_num] = self.batch_validate(valid_batch_num = valid_batch_num)\n",
    "                        self.model.train()    # reset to the train mode\n",
    "            \n",
    "            self.record_ave_training_loss.append(ave_loss)\n",
    "        # convert to ms\n",
    "        self.time_train_total = ((time.time() - t0) * 1000)\n",
    "        return investigate_f1, investigate_accuracy\n",
    "\n",
    "    # iterate through epoch and also the clusters\n",
    "    def train(self, epoch_num=10, learning_rate=0.01, weight_decay = 0.01, mini_epoch_num = 1, train_batch_num = 2):\n",
    "        \"\"\"\n",
    "            *** Training a model. ***\n",
    "            epoch_num:  number of total training epoch number\n",
    "            learning rate: learning rate during training\n",
    "            weight_decay:  decay coefficients for the regularization\n",
    "            mini_epoch_num:  number of epochs of repeating training after loading data on the GPU\n",
    "        \"\"\"\n",
    "        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        self.model.train()\n",
    "        self.record_ave_training_loss = []\n",
    "        self.time_train_load_data = 0\n",
    "        \n",
    "        epoch_partition = epoch_num // mini_epoch_num\n",
    "        t0 = time.time()\n",
    "        train_clusters = list(range(train_batch_num))\n",
    "        for epoch in range(epoch_partition):\n",
    "#             For test purpose, we let the clusters to follow specific order\n",
    "            random.shuffle(train_clusters)\n",
    "            self.node_count_seen = 0\n",
    "            self.accumulated_training_loss = 0\n",
    "            \n",
    "            for cluster in train_clusters:\n",
    "                # read in the train data from the pickle files\n",
    "                batch_file_name = self.data_folder + 'train/batch_' + str(cluster)\n",
    "                \n",
    "                t2 = time.time()\n",
    "                with open(batch_file_name, \"rb\") as fp:\n",
    "                    minibatch_data_train = pickle.load(fp)\n",
    "                read_time = (time.time() - t2) * 1000\n",
    "#                 print('*** During training for # {0:3d} batch, reading batch file costed {1:.2f} ms ***'.format(cluster, read_time) )\n",
    "                \n",
    "                tr_train_nodes, tr_edges, tr_features, tr_target = minibatch_data_train\n",
    "                \n",
    "                # for each cluster, we load once and train it for multiple epochs:\n",
    "                t1 = time.time()\n",
    "                tr_train_nodes = tr_train_nodes.to(self.device)\n",
    "                tr_edges = tr_edges.to(self.device)\n",
    "                tr_features = tr_features.to(self.device)\n",
    "                tr_target = tr_target.to(self.device)\n",
    "                \n",
    "                self.time_train_load_data += (time.time() - t1) * 1000\n",
    "                # train each batch for multiple epochs\n",
    "                for mini_epoch in range(mini_epoch_num):\n",
    "                    self.optimizer.zero_grad()\n",
    "                    batch_ave_loss, node_count = self.do_forward_pass(tr_train_nodes, tr_edges, tr_features, tr_target)\n",
    "                    batch_ave_loss.backward()\n",
    "                    self.optimizer.step()\n",
    "                    ave_loss = self.update_average_loss(batch_ave_loss, node_count)\n",
    "            \n",
    "            self.record_ave_training_loss.append(ave_loss)\n",
    "        # convert to ms\n",
    "        self.time_train_total = ((time.time() - t0) * 1000)\n",
    "    \n",
    "\n",
    "    def do_batch_validation_prediction(self, valid_validation_nodes, valid_edges, valid_features, valid_target):\n",
    "        \"\"\"\n",
    "        Scoring a cluster.\n",
    "        :param cluster: Cluster index.\n",
    "        :return prediction: Prediction matrix with probabilities.\n",
    "        :return target: Target vector.\n",
    "        \"\"\"\n",
    "        predictions = self.model(valid_edges, valid_features)\n",
    "        return predictions[valid_validation_nodes], valid_target[valid_validation_nodes]\n",
    "\n",
    "    def batch_validate(self, valid_batch_num = 2):\n",
    "        \"\"\"\n",
    "        Scoring the test and printing the F-1 score.\n",
    "        \"\"\"\n",
    "        self.model.eval()   # set into test mode, only effective for certain modules such as dropout and batchNorm\n",
    "        \n",
    "        predictions = []\n",
    "        targets = []\n",
    "        valid_clusters = list(range(valid_batch_num))\n",
    "        for cluster in valid_clusters:\n",
    "            # read in the train data from the pickle files\n",
    "            batch_file_name = self.data_folder + 'validation/batch_' + str(cluster)\n",
    "            \n",
    "            t2 = time.time()\n",
    "            with open(batch_file_name, \"rb\") as fp:\n",
    "                minibatch_data_validation = pickle.load(fp)\n",
    "            read_time = (time.time() - t2) * 1000\n",
    "#             print('*** During validation for # {0:3d} batch, reading batch file costed {1:.2f} ms ***'.format(cluster, read_time) )\n",
    "\n",
    "            valid_validation_nodes, valid_edges, valid_features, valid_target = minibatch_data_validation\n",
    "            \n",
    "            valid_validation_nodes = valid_validation_nodes.to(self.device)\n",
    "            valid_edges = valid_edges.to(self.device)\n",
    "            valid_features = valid_features.to(self.device)\n",
    "            valid_target = valid_target.to(self.device)\n",
    "            \n",
    "            \n",
    "            prediction, target = self.do_batch_validation_prediction(valid_validation_nodes, valid_edges, valid_features, valid_target)\n",
    "\n",
    "            predictions.append(prediction.cpu().detach().numpy())\n",
    "            targets.append(target.cpu().detach().numpy())\n",
    "        \n",
    "        # concatenate all the ndarrays inside this list\n",
    "        targets = np.concatenate(targets)\n",
    "        # along axis:    axis == 1\n",
    "        predictions = np.concatenate(predictions).argmax(1)  # return the indices of maximum probability \n",
    "#         print('shape of the targets and predictions are: ', self.targets.shape, self.predictions.shape)\n",
    "        \n",
    "        f1 = f1_score(targets, predictions, average=\"micro\")\n",
    "        accuracy = accuracy_score(targets, predictions)\n",
    "#         print(\"\\nTest F-1 score: {:.4f}\".format(score))\n",
    "        return (f1, accuracy)\n",
    "\n",
    "# for cross-validation purpose: \n",
    "    def do_prediction(self, cluster):\n",
    "        \"\"\"\n",
    "        Scoring a cluster.\n",
    "        :param cluster: Cluster index.\n",
    "        :return prediction: Prediction matrix with probabilities.\n",
    "        :return target: Target vector.\n",
    "        \"\"\"\n",
    "        test_nodes = self.clustering_machine.sg_test_nodes_global[cluster].to(self.device)\n",
    "        prediction = self.model(self.edges, self.features, self.edge_weights)\n",
    "        \n",
    "        return prediction[test_nodes], self.label[test_nodes]\n",
    "    \n",
    "    def test(self):\n",
    "        \"\"\"\n",
    "        Scoring the test and printing the F-1 score.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        self.predictions = []\n",
    "        self.targets = []\n",
    "        \n",
    "        self.edges = self.clustering_machine.edge_index_global_self_loops.to(self.device)\n",
    "        self.features = self.clustering_machine.features.to(self.device)\n",
    "        self.edge_weights = self.clustering_machine.edge_weight_global.to(self.device)\n",
    "        self.label = self.clustering_machine.label.to(self.device)\n",
    "        \n",
    "        for cluster in self.clustering_machine.test_clusters:\n",
    "            prediction, target = self.do_prediction(cluster)\n",
    "\n",
    "            self.predictions.append(prediction.cpu().detach().numpy())\n",
    "            self.targets.append(target.cpu().detach().numpy())\n",
    "        \n",
    "        # concatenate all the ndarrays inside this list\n",
    "        self.targets = np.concatenate(self.targets)\n",
    "        # along axis:    axis == 1\n",
    "        self.predictions = np.concatenate(self.predictions).argmax(1)  # return the indices of maximum probability \n",
    "#         print('shape of the targets and predictions are: ', self.targets.shape, self.predictions.shape)\n",
    "        \n",
    "        f1 = f1_score(self.targets, self.predictions, average=\"micro\")\n",
    "        accuracy = accuracy_score(self.targets, self.predictions)\n",
    "#         print(\"\\nTest F-1 score: {:.4f}\".format(score))\n",
    "        return (f1, accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Trivial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 2.],\n",
      "        [0., 3.],\n",
      "        [0., 4.],\n",
      "        [0., 5.],\n",
      "        [0., 6.],\n",
      "        [0., 7.],\n",
      "        [0., 8.],\n",
      "        [0., 9.]]) torch.Size([10, 2])\n"
     ]
    }
   ],
   "source": [
    "'''Trivial data'''\n",
    "edge_index = torch.tensor([[0, 1, 1, 3, 1, 2, 4, 2, 4, 6, 6, 7, 7, 9, 2, 5, 9, 8], \n",
    "                           [1, 0, 3, 1, 2, 1, 2, 4, 6, 4, 7, 6, 9, 7, 5, 2, 8, 9]])\n",
    "# features = torch.rand(10, 3)\n",
    "features = torch.tensor([[0, 0], [0, 1], [0, 2], [0, 3], [0, 4],  \n",
    "                           [0, 5], [0, 6], [0, 7], [0, 8], [0, 9]], dtype = torch.float)\n",
    "# label = torch.tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "label = torch.tensor([0, 1, 1, 0, 1, 1, 1, 0, 0, 0])\n",
    "print(features, features.shape)\n",
    "\n",
    "# set the tmp folder\n",
    "# tmp_folder = './tmp/'\n",
    "# check_folder_exist(tmp_folder)\n",
    "# os.makedirs(os.path.dirname(tmp_folder), exist_ok=True)\n",
    "# set the store clustering path\n",
    "clustering_folder = './res_save_batch/clustering/'\n",
    "check_folder_exist(clustering_folder)\n",
    "clustering_file_name = clustering_folder + 'check_clustering_machine.txt'\n",
    "os.makedirs(os.path.dirname(clustering_file_name), exist_ok=True)\n",
    "\n",
    "node_count = features.shape[0]\n",
    "clustering_machine = ClusteringMachine(edge_index, features, label)\n",
    "clustering_machine.split_cluster_nodes_edges(0.4, 0.4, partition_num = 2)\n",
    "\n",
    "with open(clustering_file_name, \"wb\") as fp:\n",
    "    pickle.dump(clustering_machine, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### minibatch train nodes and batch validatioin "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.75, 0.75)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "mini_batch_folder = './res_save_batch/mini_batch_files/'\n",
    "check_folder_exist(mini_batch_folder)\n",
    "\n",
    "with open(clustering_file_name, \"rb\") as fp:\n",
    "    clustering_machine = pickle.load(fp)\n",
    "\n",
    "# generate the batches for train and validation\n",
    "clustering_machine.mini_batch_train_clustering(mini_batch_folder, 1, train_batch_num = 2) # include number of layers\n",
    "\n",
    "clustering_machine.mini_batch_validation_clustering(mini_batch_folder, 1, valid_batch_num = 2)\n",
    "\n",
    "# construct the batch trainer\n",
    "gcn_trainer_batch = ClusterGCNTrainer_mini_Train(mini_batch_folder, 2, 2, input_layers = [16], dropout=0.3)\n",
    "\n",
    "gcn_trainer_batch.train(1, 0.0001, 0.1, train_batch_num = 2)\n",
    "gcn_trainer_batch.batch_validate(valid_batch_num = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the train loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_train_loss_converge(mini_batch_folder, data_name, dataset, image_path,  comments, input_layer = [32, 16], epoch_num = 300, \\\n",
    "                              dropout = 0.3, lr = 0.0001, weight_decay = 0.01, mini_epoch_num = 5, \\\n",
    "                               valid_part_num = 2, train_part_num = 2, test_part_num = 1):\n",
    "    # mini-batch, but valid also in batches\n",
    "    Trainer_folder = mini_batch_folder + 'GCNtrainer/'\n",
    "    check_folder_exist(Trainer_folder)\n",
    "    os.makedirs(os.path.dirname(Trainer_folder), exist_ok=True)\n",
    "    \n",
    "    trainer_id = 0\n",
    "    Cluster_train_batch_run(trainer_id, mini_batch_folder, data_name, dataset, image_path, input_layer = input_layer, epochs=epoch_num, \\\n",
    "                                                                               dropout = dropout, lr = lr, weight_decay = weight_decay, mini_epoch_num = mini_epoch_num, \\\n",
    "                                                                               train_part_num = train_part_num, test_part_num = test_part_num)\n",
    "    trainer_file_name = mini_batch_folder + 'GCNtrainer/GCN_trainer_' + str(trainer_id)\n",
    "    with open(trainer_file_name, \"rb\") as fp:\n",
    "        Cluster_train_batch_trainer = pickle.load(fp)\n",
    "    \n",
    "    draw_Cluster_train_valid_batch = draw_trainer_info(data_name, Cluster_train_batch_trainer, image_path, 'train_valid_batch_' + comments)\n",
    "    draw_Cluster_train_valid_batch.draw_ave_loss_per_node()\n",
    "    \n",
    "\n",
    "''' Draw the information about the GCN calculating batch size '''\n",
    "def draw_cluster_info(clustering_machine, data_name, img_path, comments = '_cluster_node_distr'):\n",
    "    \"\"\"\n",
    "        Won't call this for mini-batch with no clustering \n",
    "    \"\"\"\n",
    "    cluster_id = clustering_machine.train_clusters    # a list of cluster indices\n",
    "    cluster_datapoints = {'cluster_id': cluster_id,  \\\n",
    "                          'train_batch' : [clustering_machine.info_train_batch_size[idx] for idx in cluster_id], \\\n",
    "                          'cluster_size' : [clustering_machine.info_isolate_cluster_size[idx] for idx in cluster_id], \\\n",
    "                         }\n",
    "                         \n",
    "    df = pd.DataFrame(data=cluster_datapoints, dtype=np.int32)\n",
    "    # print(df)\n",
    "    df_reshape = df.melt('cluster_id', var_name = 'clusters', value_name = 'node_num')\n",
    "    \n",
    "    plt.clf()\n",
    "    plt.figure()\n",
    "    sns.set(style='whitegrid')\n",
    "    g = sns.catplot(x=\"cluster_id\", y=\"node_num\", hue='clusters', kind='bar', data=df_reshape)\n",
    "    g.despine(left=True)\n",
    "    g.fig.suptitle(data_name + comments)\n",
    "    g.set_xlabels(\"Cluster ID\")\n",
    "    g.set_ylabels(\"Number of nodes\")\n",
    "    \n",
    "    img_name = img_path + data_name + comments\n",
    "    os.makedirs(os.path.dirname(img_name), exist_ok=True)\n",
    "    g.savefig(img_name, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specific model run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Execute the testing program '''\n",
    "def set_clustering_machine(data, intermediate_data_folder, test_ratio = 0.05, validation_ratio = 0.85, train_part_num = 2):\n",
    "    \"\"\"\n",
    "        Set the batch machine plus generate the training batches\n",
    "            1) data: the target dataset data\n",
    "            2) intermediate_data_folder: path to store the intermediate generated data\n",
    "            3) test_ratio, validation_ratio: data split ratio\n",
    "            4) neigh_layer: number of hops (layers) for the neighbor nodes \n",
    "            5) train_frac: each time including fraction of the neigbor nodes in each layer\n",
    "            6) valid_part_num, train_part_num, test_part_num :  batch number for validation, train and test data correspondingly\n",
    "    \"\"\"\n",
    "    # set the tmp file for garbage tmp files, just collect the info:\n",
    "    tmp_folder = './tmp/'\n",
    "    check_folder_exist(tmp_folder)\n",
    "    os.makedirs(os.path.dirname(tmp_folder), exist_ok=True)\n",
    "    \n",
    "    # Set the clustering information storing path\n",
    "    clustering_file_folder = intermediate_data_folder + 'clustering/'\n",
    "    check_folder_exist(clustering_file_folder)  # if exist then delete\n",
    "    clustering_file_name = clustering_file_folder + 'clustering_machine.txt'\n",
    "    os.makedirs(os.path.dirname(clustering_file_folder), exist_ok=True)\n",
    "    \n",
    "    # if we use the random assignment of the code, then filtering out the isolated data may not be necessary\n",
    "#     connect_edge_index, connect_features, connect_label = filter_out_isolate(data.edge_index, data.x, data.y)\n",
    "#     clustering_machine = ClusteringMachine(connect_edge_index, connect_features, connect_label)\n",
    "    print('\\n' + '=' * 100)\n",
    "    print('Start to generate the clustering machine:')\n",
    "    t0 = time.time()\n",
    "    clustering_machine = ClusteringMachine(data.edge_index, data.x, data.y, tmp_folder)\n",
    "    batch_machine_create = time.time() - t0\n",
    "    print('Batch machine creation costs a total of {0:.4f} seconds!'.format(batch_machine_create))\n",
    "    \n",
    "    # at last output the information inside the folder:\n",
    "    print_dir_content_info(tmp_folder)\n",
    "    \n",
    "#     clustering_machine.split_cluster_nodes_edges(test_ratio, validation_ratio, partition_num = train_part_num)\n",
    "    # mini-batch only: split to train test valid before clustering\n",
    "    print('Start to split data into train, test, validation:')\n",
    "    t1 = time.time()\n",
    "    clustering_machine.split_cluster_nodes_edges(test_ratio, validation_ratio, train_part_num)\n",
    "    data_split_time = time.time() - t1\n",
    "    print('Data splitting costs a total of {0:.4f} seconds!'.format(data_split_time))\n",
    "    \n",
    "    print('Start to store the batch machine file:')\n",
    "    t3 = time.time()\n",
    "    with open(clustering_file_name, \"wb\") as fp:\n",
    "        pickle.dump(clustering_machine, fp)\n",
    "    batch_machine_store_time = time.time() - t3\n",
    "    print('Storing batch machine after training batches generation costs a total of {0:.4f} seconds!'.format(batch_machine_store_time))\n",
    "    print('\\n' + '=' * 100)\n",
    "    \n",
    "def set_clustering_machine_train_batch(intermediate_data_folder, neigh_layer = 1, train_frac = 1.0, train_part_num = 2):\n",
    "    \"\"\"\n",
    "        Generate the train batches\n",
    "    \"\"\"\n",
    "    clustering_file_folder = intermediate_data_folder + 'clustering/'\n",
    "    clustering_file_name = clustering_file_folder + 'clustering_machine.txt'\n",
    "    print('\\n' + '=' * 100)\n",
    "    \n",
    "    t0 = time.time()\n",
    "    with open(clustering_file_name, \"rb\") as fp:\n",
    "        clustering_machine = pickle.load(fp)\n",
    "    batch_machine_read = time.time() - t0\n",
    "    print('Batch machine reading costs a total of {0:.4f} seconds!'.format(batch_machine_read))\n",
    "    \n",
    "    mini_batch_folder = intermediate_data_folder\n",
    "#     check_folder_exist(mini_batch_folder)  # if exist then delete\n",
    "    print('Start to generate the training batches:')\n",
    "    t2 = time.time()\n",
    "    clustering_machine.mini_batch_train_clustering(mini_batch_folder, neigh_layer, fraction = train_frac, train_batch_num = train_part_num)\n",
    "    train_batch_production_time = time.time() - t2\n",
    "    print('Train batches production costs a total of {0:.4f} seconds!'.format(train_batch_production_time))\n",
    "    print_dir_content_info(mini_batch_folder + 'train/')\n",
    "    print('=' * 100)\n",
    "\n",
    "def set_clustering_machine_validation_batch(intermediate_data_folder, neigh_layer = 1, validation_frac = 1.0, valid_part_num = 2):\n",
    "    \"\"\"\n",
    "        Generate the validation batches\n",
    "    \"\"\"\n",
    "    clustering_file_folder = intermediate_data_folder + 'clustering/'\n",
    "    clustering_file_name = clustering_file_folder + 'clustering_machine.txt'\n",
    "    print('\\n' + '=' * 100)\n",
    "    \n",
    "    t0 = time.time()\n",
    "    with open(clustering_file_name, \"rb\") as fp:\n",
    "        clustering_machine = pickle.load(fp)\n",
    "    batch_machine_read = time.time() - t0\n",
    "    print('Batch machine reading costs a total of {0:.4f} seconds!'.format(batch_machine_read))\n",
    "    \n",
    "    print('Start to generate the validation batches:')\n",
    "    mini_batch_folder = intermediate_data_folder\n",
    "    t1 = time.time()\n",
    "    # for validation , fraction has to be 1.0 so that to include the information form original graph\n",
    "    clustering_machine.mini_batch_validation_clustering(mini_batch_folder, neigh_layer, fraction = validation_frac, valid_batch_num = valid_part_num)\n",
    "    validation_batch_production_time = time.time() - t1\n",
    "    print('Validation batches production costs a total of {0:.4f} seconds!'.format(validation_batch_production_time))\n",
    "    print_dir_content_info(mini_batch_folder + 'validation/')\n",
    "    print('=' * 100)\n",
    "    \n",
    "    # can off-line load the clustering model with train-batch generated\n",
    "#     with open(clustering_file_name, \"rb\") as fp:\n",
    "#         clustering_machine = pickle.load(fp)\n",
    "\n",
    "def Cluster_train_batch_run(trainer_id, mini_batch_folder, data_name, dataset, image_path, input_layer = [16, 16], epochs=300, \\\n",
    "                           dropout = 0.3, lr = 0.01, weight_decay = 0.01, mini_epoch_num = 5, \\\n",
    "                                 train_part_num = 2, test_part_num = 1):\n",
    "    \"\"\"\n",
    "    # Run the mini-batch model (train and validate both in batches)\n",
    "    Tuning parameters:  dropout, lr (learning rate), weight_decay: l2 regularization\n",
    "    return: validation accuracy value, validation F-1 value, time_training (ms), time_data_load (ms)\n",
    "    \"\"\"\n",
    "#     print('\\n' + '=' * 100)\n",
    "#     print('Start generate the trainer:')\n",
    "    t0 = time.time()\n",
    "    gcn_trainer = ClusterGCNTrainer_mini_Train(mini_batch_folder, dataset.num_node_features, dataset.num_classes, input_layers = input_layer, dropout = dropout)\n",
    "    train_create = time.time() - t0\n",
    "#     print('Trainer creation costs a total of {0:.4f} seconds!'.format(train_create))\n",
    "    \n",
    "#     print('Start train the model:')\n",
    "    t1 = time.time()\n",
    "    gcn_trainer.train(epoch_num=epochs, learning_rate=lr, weight_decay=weight_decay, mini_epoch_num = mini_epoch_num, train_batch_num = train_part_num)\n",
    "    train_period = time.time() - t1\n",
    "#     print('Training costs a total of {0:.4f} seconds!'.format(train_period))\n",
    "    \n",
    "#     print('Start to save the GCN trainer model (parameters: weights, bias):')\n",
    "    trainer_file_name = mini_batch_folder + 'GCNtrainer/GCN_trainer_' + str(trainer_id)\n",
    "    t2 = time.time()\n",
    "    with open(trainer_file_name, \"wb\") as fp:\n",
    "        pickle.dump(gcn_trainer, fp)\n",
    "    store_trainer = time.time() - t2\n",
    "#     print('Storing the trainer costs a total of {0:.4f} seconds!'.format(store_trainer))\n",
    "#     print('-' * 80)\n",
    "\n",
    "def Cluster_valid_batch_run(trainer_id, mini_batch_folder, data_name, dataset, image_path, input_layer = [16, 16], epochs=300, \\\n",
    "                           dropout = 0.3, lr = 0.01, weight_decay = 0.01, mini_epoch_num = 5, \\\n",
    "                                 valid_part_num = 2):\n",
    "#     print('Start to read the GCN trainer model (parameters: weights, bias):')\n",
    "    trainer_file_name = mini_batch_folder + 'GCNtrainer/GCN_trainer_' + str(trainer_id)\n",
    "    t1 = time.time()\n",
    "    with open(trainer_file_name, \"rb\") as fp:\n",
    "        gcn_trainer = pickle.load(fp)\n",
    "    read_trainer = (time.time() - t1) * 1000\n",
    "#     print('Reading the trainer costs a total of {0:.4f} seconds!'.format(read_trainer))\n",
    "    \n",
    "#     print('Start validate the model:')\n",
    "    t2 = time.time()\n",
    "    validation_F1, validation_accuracy = gcn_trainer.batch_validate(valid_batch_num = valid_part_num)\n",
    "    validation_period = time.time() - t2\n",
    "#     print('Validatoin costs a total of {0:.4f} seconds!'.format(validation_period))\n",
    "#     print('Finish train and validate the model:')\n",
    "#     print('=' * 100)\n",
    "    time_train_total = gcn_trainer.time_train_total\n",
    "    time_data_load = gcn_trainer.time_train_load_data\n",
    "    return validation_accuracy, validation_F1, time_train_total, time_data_load\n",
    "\n",
    "\n",
    "def Cluster_train_valid_batch_investigate(mini_batch_folder, data_name, dataset, image_path, input_layer = [16, 16], epochs=300, \\\n",
    "                           dropout = 0.3, lr = 0.01, weight_decay = 0.01, mini_epoch_num = 5, output_period = 10, \n",
    "                                         valid_part_num = 2, train_part_num = 2, test_part_num = 1):\n",
    "    \"\"\"\n",
    "        *** dynamically investigate the F1 score in the middle of the training after certain period ***\n",
    "        output: two dict containing F1-score and accuracy of a certain epoch index\n",
    "    \"\"\"\n",
    "#     print('\\n' + '=' * 100)\n",
    "#     print('Start generate the trainer:')\n",
    "    t0 = time.time()\n",
    "    gcn_trainer = ClusterGCNTrainer_mini_Train(mini_batch_folder, dataset.num_node_features, dataset.num_classes, input_layers = input_layer, dropout = dropout)\n",
    "    train_create = time.time() - t0\n",
    "#     print('Trainer creation costs a total of {0:.4f} seconds!'.format(train_create))\n",
    "    \n",
    "#     print('Start train the model:')\n",
    "    t1 = time.time()\n",
    "    Train_period_F1, Train_period_accuracy = gcn_trainer.train_investigate_F1(epoch_num=epochs, learning_rate=lr, weight_decay=weight_decay, mini_epoch_num = mini_epoch_num, \\\n",
    "                                                            output_period = output_period, train_batch_num = train_part_num, valid_batch_num = valid_part_num)\n",
    "    train_period = time.time() - t1\n",
    "    print('In-process Training costs a total of {0:.4f} seconds!'.format(train_period))\n",
    "#     print('=' * 100)\n",
    "    \n",
    "    return Train_period_F1, Train_period_accuracy\n",
    "\n",
    "# for the purpose for tuning \n",
    "def Cluster_train_valid_batch_run(mini_batch_folder, data_name, dataset, image_path, input_layer = [16, 16], epochs=300, \\\n",
    "                           dropout = 0.3, lr = 0.01, weight_decay = 0.01, mini_epoch_num = 5, \\\n",
    "                                 valid_part_num = 2, train_part_num = 2, test_part_num = 1):\n",
    "    \"\"\"\n",
    "    # Run the mini-batch model (train and validate both in batches)\n",
    "    Tuning parameters:  dropout, lr (learning rate), weight_decay: l2 regularization\n",
    "    return: validation accuracy value, validation F-1 value, time_training (ms), time_data_load (ms)\n",
    "    \"\"\"\n",
    "#     gcn_trainer_batch = ClusterGCNTrainer_mini_Train(mini_batch_folder, 2, 2, 2, 2, 2, input_layers = [16], dropout=0.3)\n",
    "#     print('\\n' + '=' * 100)\n",
    "#     print('Start generate the trainer:')\n",
    "    t0 = time.time()\n",
    "    gcn_trainer = ClusterGCNTrainer_mini_Train(mini_batch_folder, dataset.num_node_features, dataset.num_classes, input_layers = input_layer, dropout = dropout)\n",
    "    train_create = time.time() - t0\n",
    "#     print('Trainer creation costs a total of {0:.4f} seconds!'.format(train_create))\n",
    "    \n",
    "#     print('Start train the model:')\n",
    "    t1 = time.time()\n",
    "    gcn_trainer.train(epoch_num=epochs, learning_rate=lr, weight_decay=weight_decay, mini_epoch_num = mini_epoch_num, train_batch_num = train_part_num)\n",
    "    train_period = time.time() - t1\n",
    "#     print('Training costs a total of {0:.4f} seconds!'.format(train_period))\n",
    "#     print('-' * 80)\n",
    "    \n",
    "#     print('Start validate the model:')\n",
    "    t2 = time.time()\n",
    "    validation_F1, validation_accuracy = gcn_trainer.batch_validate(valid_batch_num = valid_part_num)\n",
    "    validation_period = time.time() - t2\n",
    "#     print('Validatoin costs a total of {0:.4f} seconds!'.format(validation_period))\n",
    "#     print('=' * 100)\n",
    "    time_train_total = gcn_trainer.time_train_total\n",
    "    time_data_load = gcn_trainer.time_train_load_data\n",
    "    return validation_accuracy, validation_F1, time_train_total, time_data_load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test and compare different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_one_train(mini_batch_folder, image_path, repeate_time = 5, input_layer = [32], epoch_num = 300, \\\n",
    "                dropout = 0.3, lr = 0.0001, weight_decay = 0.01, mini_epoch_num = 5, \\\n",
    "                train_part_num = 2, test_part_num = 1):\n",
    "    \"\"\"\n",
    "        Perform one train and store the results for all trainer\n",
    "    \"\"\"\n",
    "    Trainer_folder = mini_batch_folder + 'GCNtrainer/'\n",
    "    check_folder_exist(Trainer_folder)\n",
    "    os.makedirs(os.path.dirname(Trainer_folder), exist_ok=True)\n",
    "#     graph_model = ['batch_valid', 'train_batch', 'whole_graph', 'isolate']\n",
    "    for trainer_id in range(repeate_time):\n",
    "        model_res = []\n",
    "        \n",
    "        Cluster_train_batch_run(trainer_id, mini_batch_folder, data_name, dataset, image_path, input_layer = input_layer, epochs=epoch_num, \\\n",
    "                                                         dropout = dropout, lr = lr, weight_decay = weight_decay, mini_epoch_num = mini_epoch_num, \\\n",
    "                                                         train_part_num = train_part_num, test_part_num = test_part_num)\n",
    "        \n",
    "def execute_one_validation(mini_batch_folder, image_path, repeate_time = 5, input_layer = [32], epoch_num = 300, \\\n",
    "                dropout = 0.3, lr = 0.0001, weight_decay = 0.01, mini_epoch_num = 5, \\\n",
    "                valid_part_num = 2):\n",
    "    \"\"\"\n",
    "        return all test-F1 and validation-F1 for all four models\n",
    "    \"\"\"\n",
    "    validation_accuracy = {}\n",
    "    validation_f1 = {}\n",
    "    time_total_train = {}\n",
    "    time_data_load = {}\n",
    "    \n",
    "    # Each graph model corresponds to one function below\n",
    "#     graph_model = ['batch_valid', 'train_batch', 'whole_graph', 'isolate']\n",
    "    graph_model = ['batch_valid']\n",
    "    for trainer_id in range(repeate_time):\n",
    "        model_res = []\n",
    "        model_res.append(Cluster_valid_batch_run(trainer_id, mini_batch_folder, data_name, dataset, image_path, input_layer = input_layer, epochs=epoch_num, \\\n",
    "                                                         dropout = dropout, lr = lr, weight_decay = weight_decay, mini_epoch_num = mini_epoch_num, \\\n",
    "                                                      valid_part_num = valid_part_num)[:4])\n",
    "        \n",
    "        validation_accuracy[trainer_id], validation_f1[trainer_id], time_total_train[trainer_id], time_data_load[trainer_id] = zip(*model_res)\n",
    "    return graph_model, validation_accuracy, validation_f1, time_total_train, time_data_load\n",
    "\n",
    "def store_data_multi_tests(f1_data, data_name, graph_model, img_path, comments):\n",
    "    run_id = sorted(f1_data.keys())\n",
    "    run_data = {'run_id': run_id}\n",
    "    \n",
    "    run_data.update({model_name : [f1_data[key][idx] for key in run_id] for idx, model_name in enumerate(graph_model)})\n",
    "    \n",
    "    pickle_filename = img_path + data_name + '_' + comments + '.pkl'\n",
    "    os.makedirs(os.path.dirname(pickle_filename), exist_ok=True)\n",
    "    df = pd.DataFrame(data=run_data, dtype=np.int32)\n",
    "    df.to_pickle(pickle_filename)\n",
    "    return pickle_filename\n",
    "\n",
    "def draw_data_multi_tests(pickle_filename, data_name, comments, xlabel, ylabel):\n",
    "    df = pd.read_pickle(pickle_filename)\n",
    "    df_reshape = df.melt('run_id', var_name = 'model', value_name = ylabel)\n",
    "\n",
    "    plt.clf()\n",
    "    plt.figure()\n",
    "    sns.set(style='whitegrid')\n",
    "    g = sns.catplot(x=\"model\", y=ylabel, kind='box', data=df_reshape)\n",
    "    g.despine(left=True)\n",
    "    g.fig.suptitle(data_name + ' ' + ylabel + ' ' + comments)\n",
    "    g.set_xlabels(xlabel)\n",
    "    g.set_ylabels(ylabel)\n",
    "\n",
    "    img_name = pickle_filename[:-4] + '_img'\n",
    "    os.makedirs(os.path.dirname(img_name), exist_ok=True)\n",
    "    plt.savefig(img_name, bbox_inches='tight')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate performance in the middle of the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_investigate(mini_batch_folder, image_path, repeate_time = 5, input_layer = [32], epoch_num = 300, \\\n",
    "                        dropout = 0.3, lr = 0.0001, weight_decay = 0.01, mini_epoch_num = 5, output_period = 10, \\\n",
    "                         valid_part_num = 2, train_part_num = 2, test_part_num = 1):\n",
    "    \"\"\"\n",
    "        return all test-F1 and validation-F1 for all four models\n",
    "    \"\"\"\n",
    "    \n",
    "    Train_peroid_f1 = {}\n",
    "    Train_peroid_accuracy = {}\n",
    "    \n",
    "    for i in range(repeate_time):\n",
    "        Train_peroid_f1[i], Train_peroid_accuracy[i] = Cluster_train_valid_batch_investigate(mini_batch_folder, data_name, dataset, image_path, input_layer = input_layer, epochs=epoch_num, \\\n",
    "                                            dropout = dropout, lr = lr, weight_decay = weight_decay, mini_epoch_num = mini_epoch_num, output_period = output_period, \\\n",
    "                                                                    valid_part_num = valid_part_num, train_part_num = train_part_num, test_part_num = test_part_num)\n",
    "        \n",
    "    return Train_peroid_f1, Train_peroid_accuracy\n",
    "\n",
    "def store_data_multi_investigate(investigate_res, data_name, res_name, img_path, comments):\n",
    "    \"\"\"\n",
    "        investigate_res: currently either F1-score or accuracy a dict {epoch num : value}\n",
    "    \"\"\"\n",
    "    run_id = sorted(investigate_res.keys())\n",
    "    run_data = {'run_id': run_id}\n",
    "    \n",
    "    epoch_num_range = sorted(investigate_res[0].keys())  # at least one entry exists inside the dictionary and the epoch range is fixed\n",
    "    run_data.update({epoch_num : [investigate_res[key][epoch_num] for key in run_id] for epoch_num in epoch_num_range})\n",
    "    \n",
    "    pickle_filename = img_path + data_name + '_' + res_name + '_' + comments + '.pkl'\n",
    "    os.makedirs(os.path.dirname(pickle_filename), exist_ok=True)\n",
    "    df = pd.DataFrame(data=run_data, dtype=np.int32)\n",
    "    df.to_pickle(pickle_filename)\n",
    "    return pickle_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"To test one single model for different parameter values\"\"\"\n",
    "def execute_tuning(tune_params, mini_batch_folder, image_path, repeate_time = 7, input_layer = [32], epoch_num = 400, \\\n",
    "                  dropout = 0.1, lr = 0.0001, weight_decay = 0.1, mini_epoch_num = 20, \\\n",
    "                  valid_part_num = 2, train_part_num = 2, test_part_num = 1):\n",
    "    \"\"\"\n",
    "        Tune all the hyperparameters\n",
    "        1) learning rate\n",
    "        2) dropout\n",
    "        3) layer unit number\n",
    "        4) weight decay\n",
    "    \"\"\"\n",
    "    validation_accuracy = {}\n",
    "    validation_f1 = {}\n",
    "    time_total_train = {}\n",
    "    time_data_load = {}\n",
    "    \n",
    "    res = [{tune_val : Cluster_train_valid_batch_run(mini_batch_folder, data_name, dataset, image_path, input_layer = input_layer, epochs=epoch_num, \\\n",
    "            dropout = dropout, lr = lr, weight_decay = weight_decay, mini_epoch_num = tune_val, \\\n",
    "            valid_part_num = valid_part_num, train_part_num = train_part_num, test_part_num = test_part_num) for tune_val in tune_params} for i in range(repeate_time)]\n",
    "    \n",
    "    for i, ref in enumerate(res):\n",
    "        validation_accuracy[i] = {tune_val : res_lst[0] for tune_val, res_lst in ref.items()}\n",
    "        validation_f1[i] = {tune_val : res_lst[1] for tune_val, res_lst in ref.items()}\n",
    "        time_total_train[i] = {tune_val : res_lst[2] for tune_val, res_lst in ref.items()}\n",
    "        time_data_load[i] = {tune_val : res_lst[3] for tune_val, res_lst in ref.items()}\n",
    "        \n",
    "    return validation_accuracy, validation_f1, time_total_train, time_data_load\n",
    "\n",
    "def store_data_multi_tuning(tune_params, target, data_name, img_path, comments):\n",
    "    \"\"\"\n",
    "        tune_params: is the tuning parameter list\n",
    "        target: is the result, here should be F1-score, accuraycy, load time, train time\n",
    "    \"\"\"\n",
    "    run_ids = sorted(target.keys())   # key is the run_id\n",
    "    run_data = {'run_id': run_ids}\n",
    "    # the key can be converted to string or not: i.e. str(tune_val)\n",
    "    # here we keep it as integer such that we want it to follow order\n",
    "    tmp = {tune_val : [target[run_id][tune_val] for run_id in run_ids] for tune_val in tune_params}  # the value is list\n",
    "    run_data.update(tmp)\n",
    "    \n",
    "    pickle_filename = img_path + data_name + '_' + comments + '.pkl'\n",
    "    os.makedirs(os.path.dirname(pickle_filename), exist_ok=True)\n",
    "    df = pd.DataFrame(data=run_data, dtype=np.int32)\n",
    "    df.to_pickle(pickle_filename)\n",
    "    return pickle_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-test execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_train_loss(data, data_name, dataset, image_data_path, intermediate_data_path, partition_nums, layers, \\\n",
    "                      dropout = 0.1, lr = 0.0001, weight_decay = 0.1, mini_epoch_num = 20, valid_part_num = 2):\n",
    "    for partn in partition_nums:\n",
    "        for GCN_layer in layers:\n",
    "            net_layer = len(GCN_layer) + 1\n",
    "            hop_layer = net_layer\n",
    "            print('Start checking train loss for partition num: ' + str(partn) + ' hop layer: ' + str(hop_layer))\n",
    "            img_path = image_data_path + 'cluster_num_' + str(partn) + '/' + 'net_layer_' + str(net_layer) + '_hop_layer_' + str(hop_layer) + '/'\n",
    "            intermediate_data_folder = intermediate_data_path\n",
    "            \n",
    "            # set the batch for validation and train\n",
    "            mini_batch_folder = intermediate_data_folder\n",
    "            set_clustering_machine(data, intermediate_data_folder, test_ratio = 0.05, validation_ratio = 0.85, train_part_num = partn)\n",
    "            set_clustering_machine_train_batch(intermediate_data_folder, neigh_layer = hop_layer, train_frac = 1.0, train_part_num = partn)\n",
    "            set_clustering_machine_validation_batch(intermediate_data_folder, neigh_layer = hop_layer, validation_frac = 1.0, valid_part_num = valid_part_num)\n",
    "            \n",
    "            check_train_loss_converge(mini_batch_folder, data_name, dataset, img_path, 'part_num_' + str(partn), input_layer = GCN_layer, epoch_num = 400, \\\n",
    "                                     dropout = dropout, lr = lr, weight_decay = weight_decay, mini_epoch_num = mini_epoch_num, \n",
    "                                     valid_part_num = partn, train_part_num = partn, test_part_num = 1)\n",
    "            \n",
    "#             # for the large dataset and split first case, the cluster info cannot be generated\n",
    "#             clustering_machine.mini_batch_train_clustering(hop_layer)\n",
    "#             draw_cluster_info(clustering_machine, data_name, img_path, comments = '_cluster_node_distr_' + str(hop_layer) + '_hops')\n",
    "            \n",
    "def output_F1_score(data, data_name, dataset, image_data_path, intermediate_data_path, partition_nums, layers, \\\n",
    "                    dropout = 0.1, lr = 0.0001, weight_decay = 0.1, mini_epoch_num = 20, valid_part_num = 2):            \n",
    "    for partn in partition_nums:\n",
    "        for GCN_layer in layers:\n",
    "            net_layer = len(GCN_layer) + 1\n",
    "            hop_layer = net_layer\n",
    "            \n",
    "            # set the save path\n",
    "            print('Start running for partition num: ' + str(partn) + ' hop layer ' + str(hop_layer))\n",
    "            img_path = image_data_path + 'cluster_num_' + str(partn) + '/' + 'net_layer_' + str(net_layer) + '_hop_layer_' + str(hop_layer) + '/'\n",
    "            intermediate_data_folder = intermediate_data_path\n",
    "            \n",
    "            # set the batch for validation and train\n",
    "            mini_batch_folder = intermediate_data_folder\n",
    "            set_clustering_machine(data, intermediate_data_folder, test_ratio = 0.05, validation_ratio = 0.85, train_part_num = partn)\n",
    "            set_clustering_machine_train_batch(intermediate_data_folder, neigh_layer = hop_layer, train_frac = 1.0, train_part_num = partn)\n",
    "            set_clustering_machine_validation_batch(intermediate_data_folder, neigh_layer = hop_layer, validation_frac = 1.0, valid_part_num = valid_part_num)\n",
    "            \n",
    "            # start to run the model, train and validation \n",
    "            execute_one_train(mini_batch_folder, img_path, repeate_time = 7, input_layer = GCN_layer, epoch_num = 400, \n",
    "                                            dropout = dropout, lr = lr, weight_decay = weight_decay, mini_epoch_num = mini_epoch_num, \\\n",
    "                                             train_part_num = partn, test_part_num = 1)\n",
    "            \n",
    "            graph_model, validation_accuracy, validation_f1, time_total_train, time_data_load = \\\n",
    "                execute_one_validation(mini_batch_folder, img_path, repeate_time = 7, input_layer = GCN_layer, epoch_num = 400, \n",
    "                                            dropout = dropout, lr = lr, weight_decay = weight_decay, mini_epoch_num = mini_epoch_num, \\\n",
    "                                             valid_part_num = partn)\n",
    "            \n",
    "            \n",
    "            validation_accuracy = store_data_multi_tests(validation_accuracy, data_name, graph_model, img_path, 'test_cluster_num_' + str(partn) + '_hops_' + str(hop_layer))\n",
    "            draw_data_multi_tests(validation_accuracy, data_name, 'vali_cluster_num_' + str(partn) + '_hop_' + str(hop_layer), 'models', 'Accuracy')\n",
    "\n",
    "            validation_f1 = store_data_multi_tests(validation_f1, data_name, graph_model, img_path, 'validation_cluster_num_' + str(partn) + '_hops_' + str(hop_layer))\n",
    "            draw_data_multi_tests(validation_f1, data_name, 'vali_cluster_num_' + str(partn) + '_hop_' + str(hop_layer), 'models', 'F1 score')\n",
    "\n",
    "            time_train = store_data_multi_tests(time_total_train, data_name, graph_model, img_path, 'train_time_cluster_num_' + str(partn) + '_hops_' + str(hop_layer))\n",
    "            draw_data_multi_tests(time_train, data_name, 'train_time_cluster_num_' + str(partn) + '_hop_' + str(hop_layer), 'models', 'Train Time (ms)')\n",
    "\n",
    "            time_load = store_data_multi_tests(time_data_load, data_name, graph_model, img_path, 'load_time_cluster_num_' + str(partn) + '_hops_' + str(hop_layer))\n",
    "            draw_data_multi_tests(time_load, data_name, 'load_time_cluster_num_' + str(partn) + '_hop_' + str(hop_layer), 'models', 'Load Time (ms)')\n",
    "\n",
    "def output_train_investigate(data, data_name, dataset, image_data_path, intermediate_data_path, partition_nums, layers, \\\n",
    "                             dropout = 0.1, lr = 0.0001, weight_decay = 0.1, mini_epoch_num = 20, output_period = 40, valid_part_num = 2):            \n",
    "    for partn in partition_nums:\n",
    "        for GCN_layer in layers:\n",
    "            net_layer = len(GCN_layer) + 1\n",
    "            hop_layer = net_layer\n",
    "            # set the save path\n",
    "            print('Start running for partition num: ' + str(partn) + ' hop layer ' + str(hop_layer))\n",
    "            img_path = image_data_path + 'cluster_num_' + str(partn) + '/' + 'net_layer_' + str(net_layer) + '_hop_layer_' + str(hop_layer) + '/'\n",
    "            intermediate_data_folder = intermediate_data_path\n",
    "            \n",
    "            # set the batch for validation and train\n",
    "            mini_batch_folder = intermediate_data_folder\n",
    "            set_clustering_machine(data, intermediate_data_folder, test_ratio = 0.05, validation_ratio = 0.85, train_part_num = partn)\n",
    "            set_clustering_machine_train_batch(intermediate_data_folder, neigh_layer = hop_layer, train_frac = 1.0, train_part_num = partn)\n",
    "            set_clustering_machine_validation_batch(intermediate_data_folder, neigh_layer = hop_layer, validation_frac = 1.0, valid_part_num = valid_part_num)\n",
    "\n",
    "            Train_peroid_f1, Train_peroid_accuracy = execute_investigate(mini_batch_folder, img_path, repeate_time = 7, input_layer = GCN_layer, epoch_num = 400, \\\n",
    "                                            dropout = dropout, lr = lr, weight_decay = weight_decay, mini_epoch_num = mini_epoch_num, output_period = output_period, \\\n",
    "                                            valid_part_num = partn, train_part_num = partn, test_part_num = 1)\n",
    "            \n",
    "            Train_peroid_f1 = store_data_multi_investigate(Train_peroid_f1, data_name, 'F1_score', img_path, 'invest_batch_num_' + str(partn) + '_hops_' + str(hop_layer))\n",
    "            draw_data_multi_tests(Train_peroid_f1, data_name, 'Train_process_batch_num_' + str(partn) + '_hop_' + str(hop_layer), 'epoch number', 'F1 score')\n",
    "\n",
    "            Train_peroid_accuracy = store_data_multi_investigate(Train_peroid_accuracy, data_name, 'Accuracy', img_path, 'invest_batch_num_' + str(partn) + '_hops_' + str(hop_layer))\n",
    "            draw_data_multi_tests(Train_peroid_accuracy, data_name, 'Train_process_batch_num_' + str(partn) + '_hop_' + str(hop_layer), 'epoch number', 'Accuracy')\n",
    "            \n",
    "            \n",
    "            \n",
    "def output_tune_param(data, data_name, dataset, image_data_path, intermediate_data_path, partition_nums, layers, dropout = 0.1, lr = 0.0001, weight_decay = 0.1, mini_epoch_num = 20, valid_part_num = 2):\n",
    "    for partn in partition_nums:\n",
    "        for GCN_layer in layers:\n",
    "            net_layer = len(GCN_layer) + 1\n",
    "            hop_layer = net_layer\n",
    "            # Set the tune parameters and name\n",
    "            tune_name = 'batch_epoch_num'\n",
    "            tune_params = [400, 200, 100, 50, 20, 10, 5, 1]\n",
    "#             tune_name = 'weight_decay'\n",
    "#             tune_params = [0.01, 0.1, 0.3, 0.5]\n",
    "\n",
    "            img_path = image_data_path + 'cluster_num_' + str(partn) + '/' + 'net_layer_' + str(net_layer) + '_hop_layer_' + str(hop_layer) + '/' + 'tune_' + tune_name + '/'\n",
    "            intermediate_data_folder = intermediate_data_path\n",
    "            print('Start tuning for tuning param: ' + tune_name + ' partition num: ' + str(partn) + ' hop layer ' + str(hop_layer))\n",
    "            \n",
    "            # set the batch for validation and train\n",
    "            mini_batch_folder = intermediate_data_folder\n",
    "            set_clustering_machine(data, intermediate_data_folder, test_ratio = 0.05, validation_ratio = 0.85, train_part_num = partn)\n",
    "            set_clustering_machine_train_batch(intermediate_data_folder, neigh_layer = hop_layer, train_frac = 1.0, train_part_num = partn)\n",
    "            set_clustering_machine_validation_batch(intermediate_data_folder, neigh_layer = hop_layer, validation_frac = 1.0, valid_part_num = valid_part_num)\n",
    "\n",
    "            validation_accuracy, validation_f1, time_total_train, time_data_load = execute_tuning(tune_params, mini_batch_folder, img_path, repeate_time = 7, \\\n",
    "                                                input_layer = GCN_layer, epoch_num = 400, dropout = dropout, lr = lr, weight_decay = weight_decay, mini_epoch_num = mini_epoch_num, \\\n",
    "                                                valid_part_num = partn, train_part_num = partn, test_part_num = 1)\n",
    "\n",
    "            validation_accuracy = store_data_multi_tuning(tune_params,validation_accuracy, data_name, img_path, 'accuracy_cluster_num_' + str(partn) + '_hops_' + str(hop_layer))\n",
    "            draw_data_multi_tests(validation_accuracy, data_name, 'vali_cluster_num_' + str(partn) + '_hop_' + str(hop_layer), 'epochs_per_batch', 'Accuracy')\n",
    "\n",
    "            validation_f1 = store_data_multi_tuning(tune_params, validation_f1, data_name, img_path, 'validation_cluster_num_' + str(partn) + '_hops_' + str(hop_layer))\n",
    "            draw_data_multi_tests(validation_f1, data_name, 'vali_cluster_num_' + str(partn) + '_hop_' + str(hop_layer), 'epochs_per_batch', 'F1 score')\n",
    "\n",
    "            time_train = store_data_multi_tuning(tune_params, time_total_train, data_name, img_path, 'train_time_cluster_num_' + str(partn) + '_hops_' + str(hop_layer))\n",
    "            draw_data_multi_tests(time_train, data_name, 'train_time_cluster_num_' + str(partn) + '_hop_' + str(hop_layer), 'epochs_per_batch', 'Train Time (ms)')\n",
    "\n",
    "            time_load = store_data_multi_tuning(tune_params, time_data_load, data_name, img_path, 'load_time_cluster_num_' + str(partn) + '_hops_' + str(hop_layer))\n",
    "            draw_data_multi_tests(time_load, data_name, 'load_time_cluster_num_' + str(partn) + '_hop_' + str(hop_layer), 'epochs_per_batch', 'Load Time (ms)')\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use data from pytorch geometric datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_data_root = '/media/xiangli/storage1/projects/tmpdata/'\n",
    "test_folder_name = 'isolate_cluster/train_10%_full_neigh_random/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cora dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "data_name = 'Cora'\n",
    "dataset = Planetoid(root = local_data_root + 'Planetoid/Cora', name=data_name)\n",
    "data = dataset[0]\n",
    "image_data_path = './results/' + data_name + '/' + test_folder_name\n",
    "# set the current folder as the intermediate data folder so that we can easily copy either clustering \n",
    "intermediate_data_folder = './'\n",
    "\n",
    "partition_nums = [2, 4, 8]\n",
    "layers = [[], [32], [32,32]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PubMed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "data_name = 'PubMed'\n",
    "dataset = Planetoid(root = local_data_root + 'Planetoid/PubMed', name=data_name)\n",
    "data = dataset[0]\n",
    "image_data_path = './results/' + data_name + '/' + test_folder_name\n",
    "intermediate_data_folder = './'\n",
    "partition_nums = [2, 4, 8]\n",
    "layers = [[], [64], [64, 64]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tune hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start tuning for tuning param: batch_epoch_num partition num: 2 hop layer 1\n",
      "\n",
      "====================================================================================================\n",
      "Start to generate the clustering machine:\n",
      "Batch machine creation costs a total of 0.0101 seconds!\n",
      "\n",
      " Information about the content of ./tmp/\n",
      "\n",
      "Start to split data into train, test, validation:\n",
      "Data splitting costs a total of 0.0193 seconds!\n",
      "Start to store the batch machine file:\n",
      "Storing batch machine after training batches generation costs a total of 0.0102 seconds!\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0069 seconds!\n",
      "Start to generate the training batches:\n",
      "Train batches production costs a total of 0.0211 seconds!\n",
      "\n",
      " Information about the content of ./train/\n",
      "File name: [ batch_1 ]; with size: 7851.3994140625 KB\n",
      "File name: [ batch_0 ]; with size: 7415.2978515625 KB\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0465 seconds!\n",
      "Start to generate the validation batches:\n",
      "Validation batches production costs a total of 0.0214 seconds!\n",
      "\n",
      " Information about the content of ./validation/\n",
      "File name: [ batch_1 ]; with size: 7859.5732421875 KB\n",
      "File name: [ batch_0 ]; with size: 7423.0185546875 KB\n",
      "\n",
      "====================================================================================================\n",
      "Start tuning for tuning param: batch_epoch_num partition num: 2 hop layer 2\n",
      "\n",
      "====================================================================================================\n",
      "Start to generate the clustering machine:\n",
      "Batch machine creation costs a total of 0.0104 seconds!\n",
      "\n",
      " Information about the content of ./tmp/\n",
      "\n",
      "Start to split data into train, test, validation:\n",
      "Data splitting costs a total of 0.0176 seconds!\n",
      "Start to store the batch machine file:\n",
      "Storing batch machine after training batches generation costs a total of 0.0089 seconds!\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0065 seconds!\n",
      "Start to generate the training batches:\n",
      "Train batches production costs a total of 0.0216 seconds!\n",
      "\n",
      " Information about the content of ./train/\n",
      "File name: [ batch_1 ]; with size: 7851.3994140625 KB\n",
      "File name: [ batch_0 ]; with size: 7415.2978515625 KB\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0065 seconds!\n",
      "Start to generate the validation batches:\n",
      "Validation batches production costs a total of 0.0589 seconds!\n",
      "\n",
      " Information about the content of ./validation/\n",
      "File name: [ batch_1 ]; with size: 7859.5732421875 KB\n",
      "File name: [ batch_0 ]; with size: 7423.0185546875 KB\n",
      "\n",
      "====================================================================================================\n",
      "Start tuning for tuning param: batch_epoch_num partition num: 2 hop layer 3\n",
      "\n",
      "====================================================================================================\n",
      "Start to generate the clustering machine:\n",
      "Batch machine creation costs a total of 0.0107 seconds!\n",
      "\n",
      " Information about the content of ./tmp/\n",
      "\n",
      "Start to split data into train, test, validation:\n",
      "Data splitting costs a total of 0.0179 seconds!\n",
      "Start to store the batch machine file:\n",
      "Storing batch machine after training batches generation costs a total of 0.0089 seconds!\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0066 seconds!\n",
      "Start to generate the training batches:\n",
      "Train batches production costs a total of 0.0217 seconds!\n",
      "\n",
      " Information about the content of ./train/\n",
      "File name: [ batch_1 ]; with size: 7851.3994140625 KB\n",
      "File name: [ batch_0 ]; with size: 7415.2978515625 KB\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0458 seconds!\n",
      "Start to generate the validation batches:\n",
      "Validation batches production costs a total of 0.0214 seconds!\n",
      "\n",
      " Information about the content of ./validation/\n",
      "File name: [ batch_1 ]; with size: 7859.5732421875 KB\n",
      "File name: [ batch_0 ]; with size: 7423.0185546875 KB\n",
      "\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:58: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:58: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start tuning for tuning param: batch_epoch_num partition num: 4 hop layer 1\n",
      "\n",
      "====================================================================================================\n",
      "Start to generate the clustering machine:\n",
      "Batch machine creation costs a total of 0.0103 seconds!\n",
      "\n",
      " Information about the content of ./tmp/\n",
      "\n",
      "Start to split data into train, test, validation:\n",
      "Data splitting costs a total of 0.0187 seconds!\n",
      "Start to store the batch machine file:\n",
      "Storing batch machine after training batches generation costs a total of 0.0094 seconds!\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0067 seconds!\n",
      "Start to generate the training batches:\n",
      "Train batches production costs a total of 0.0199 seconds!\n",
      "\n",
      " Information about the content of ./train/\n",
      "File name: [ batch_1 ]; with size: 3854.3173828125 KB\n",
      "File name: [ batch_3 ]; with size: 3835.0634765625 KB\n",
      "File name: [ batch_0 ]; with size: 3823.1025390625 KB\n",
      "File name: [ batch_2 ]; with size: 3716.3642578125 KB\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0457 seconds!\n",
      "Start to generate the validation batches:\n",
      "Validation batches production costs a total of 0.0201 seconds!\n",
      "\n",
      " Information about the content of ./validation/\n",
      "File name: [ batch_1 ]; with size: 3858.3427734375 KB\n",
      "File name: [ batch_3 ]; with size: 3839.0654296875 KB\n",
      "File name: [ batch_0 ]; with size: 3827.0888671875 KB\n",
      "File name: [ batch_2 ]; with size: 3720.2412109375 KB\n",
      "\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:58: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:58: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:58: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:58: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start tuning for tuning param: batch_epoch_num partition num: 4 hop layer 2\n",
      "\n",
      "====================================================================================================\n",
      "Start to generate the clustering machine:\n",
      "Batch machine creation costs a total of 0.0101 seconds!\n",
      "\n",
      " Information about the content of ./tmp/\n",
      "\n",
      "Start to split data into train, test, validation:\n",
      "Data splitting costs a total of 0.0188 seconds!\n",
      "Start to store the batch machine file:\n",
      "Storing batch machine after training batches generation costs a total of 0.0089 seconds!\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0067 seconds!\n",
      "Start to generate the training batches:\n",
      "Train batches production costs a total of 0.0199 seconds!\n",
      "\n",
      " Information about the content of ./train/\n",
      "File name: [ batch_1 ]; with size: 3854.3173828125 KB\n",
      "File name: [ batch_3 ]; with size: 3835.0634765625 KB\n",
      "File name: [ batch_0 ]; with size: 3823.1025390625 KB\n",
      "File name: [ batch_2 ]; with size: 3716.3642578125 KB\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0446 seconds!\n",
      "Start to generate the validation batches:\n",
      "Validation batches production costs a total of 0.0202 seconds!\n",
      "\n",
      " Information about the content of ./validation/\n",
      "File name: [ batch_1 ]; with size: 3858.3427734375 KB\n",
      "File name: [ batch_3 ]; with size: 3839.0654296875 KB\n",
      "File name: [ batch_0 ]; with size: 3827.0888671875 KB\n",
      "File name: [ batch_2 ]; with size: 3720.2412109375 KB\n",
      "\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:58: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:58: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:58: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:58: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start tuning for tuning param: batch_epoch_num partition num: 4 hop layer 3\n",
      "\n",
      "====================================================================================================\n",
      "Start to generate the clustering machine:\n",
      "Batch machine creation costs a total of 0.0106 seconds!\n",
      "\n",
      " Information about the content of ./tmp/\n",
      "\n",
      "Start to split data into train, test, validation:\n",
      "Data splitting costs a total of 0.0187 seconds!\n",
      "Start to store the batch machine file:\n",
      "Storing batch machine after training batches generation costs a total of 0.0091 seconds!\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0066 seconds!\n",
      "Start to generate the training batches:\n",
      "Train batches production costs a total of 0.0199 seconds!\n",
      "\n",
      " Information about the content of ./train/\n",
      "File name: [ batch_1 ]; with size: 3854.3173828125 KB\n",
      "File name: [ batch_3 ]; with size: 3835.0634765625 KB\n",
      "File name: [ batch_0 ]; with size: 3823.1025390625 KB\n",
      "File name: [ batch_2 ]; with size: 3716.3642578125 KB\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0462 seconds!\n",
      "Start to generate the validation batches:\n",
      "Validation batches production costs a total of 0.0201 seconds!\n",
      "\n",
      " Information about the content of ./validation/\n",
      "File name: [ batch_1 ]; with size: 3858.3427734375 KB\n",
      "File name: [ batch_3 ]; with size: 3839.0654296875 KB\n",
      "File name: [ batch_0 ]; with size: 3827.0888671875 KB\n",
      "File name: [ batch_2 ]; with size: 3720.2412109375 KB\n",
      "\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:58: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:58: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:58: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:58: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start tuning for tuning param: batch_epoch_num partition num: 8 hop layer 1\n",
      "\n",
      "====================================================================================================\n",
      "Start to generate the clustering machine:\n",
      "Batch machine creation costs a total of 0.0104 seconds!\n",
      "\n",
      " Information about the content of ./tmp/\n",
      "\n",
      "Start to split data into train, test, validation:\n",
      "Data splitting costs a total of 0.0213 seconds!\n",
      "Start to store the batch machine file:\n",
      "Storing batch machine after training batches generation costs a total of 0.0092 seconds!\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0067 seconds!\n",
      "Start to generate the training batches:\n",
      "Train batches production costs a total of 0.0201 seconds!\n",
      "\n",
      " Information about the content of ./train/\n",
      "File name: [ batch_1 ]; with size: 1943.734375 KB\n",
      "File name: [ batch_7 ]; with size: 1898.4765625 KB\n",
      "File name: [ batch_3 ]; with size: 1843.1328125 KB\n",
      "File name: [ batch_4 ]; with size: 1955.5390625 KB\n",
      "File name: [ batch_6 ]; with size: 1938.19140625 KB\n",
      "File name: [ batch_0 ]; with size: 1842.6015625 KB\n",
      "File name: [ batch_5 ]; with size: 1855.15625 KB\n",
      "File name: [ batch_2 ]; with size: 1937.97265625 KB\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0069 seconds!\n",
      "Start to generate the validation batches:\n",
      "Validation batches production costs a total of 0.0198 seconds!\n",
      "\n",
      " Information about the content of ./validation/\n",
      "File name: [ batch_1 ]; with size: 1945.775390625 KB\n",
      "File name: [ batch_7 ]; with size: 1900.470703125 KB\n",
      "File name: [ batch_3 ]; with size: 1845.064453125 KB\n",
      "File name: [ batch_4 ]; with size: 1957.587890625 KB\n",
      "File name: [ batch_6 ]; with size: 1940.224609375 KB\n",
      "File name: [ batch_0 ]; with size: 1844.533203125 KB\n",
      "File name: [ batch_5 ]; with size: 1857.103515625 KB\n",
      "File name: [ batch_2 ]; with size: 1940.005859375 KB\n",
      "\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:58: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:58: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:58: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:58: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start tuning for tuning param: batch_epoch_num partition num: 8 hop layer 2\n",
      "\n",
      "====================================================================================================\n",
      "Start to generate the clustering machine:\n",
      "Batch machine creation costs a total of 0.0108 seconds!\n",
      "\n",
      " Information about the content of ./tmp/\n",
      "\n",
      "Start to split data into train, test, validation:\n",
      "Data splitting costs a total of 0.0215 seconds!\n",
      "Start to store the batch machine file:\n",
      "Storing batch machine after training batches generation costs a total of 0.0092 seconds!\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0067 seconds!\n",
      "Start to generate the training batches:\n",
      "Train batches production costs a total of 0.0197 seconds!\n",
      "\n",
      " Information about the content of ./train/\n",
      "File name: [ batch_1 ]; with size: 1943.734375 KB\n",
      "File name: [ batch_7 ]; with size: 1898.4765625 KB\n",
      "File name: [ batch_3 ]; with size: 1843.1328125 KB\n",
      "File name: [ batch_4 ]; with size: 1955.5390625 KB\n",
      "File name: [ batch_6 ]; with size: 1938.19140625 KB\n",
      "File name: [ batch_0 ]; with size: 1842.6015625 KB\n",
      "File name: [ batch_5 ]; with size: 1855.15625 KB\n",
      "File name: [ batch_2 ]; with size: 1937.97265625 KB\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0067 seconds!\n",
      "Start to generate the validation batches:\n",
      "Validation batches production costs a total of 0.0195 seconds!\n",
      "\n",
      " Information about the content of ./validation/\n",
      "File name: [ batch_1 ]; with size: 1945.775390625 KB\n",
      "File name: [ batch_7 ]; with size: 1900.470703125 KB\n",
      "File name: [ batch_3 ]; with size: 1845.064453125 KB\n",
      "File name: [ batch_4 ]; with size: 1957.587890625 KB\n",
      "File name: [ batch_6 ]; with size: 1940.224609375 KB\n",
      "File name: [ batch_0 ]; with size: 1844.533203125 KB\n",
      "File name: [ batch_5 ]; with size: 1857.103515625 KB\n",
      "File name: [ batch_2 ]; with size: 1940.005859375 KB\n",
      "\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:58: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:58: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:58: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:58: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start tuning for tuning param: batch_epoch_num partition num: 8 hop layer 3\n",
      "\n",
      "====================================================================================================\n",
      "Start to generate the clustering machine:\n",
      "Batch machine creation costs a total of 0.0103 seconds!\n",
      "\n",
      " Information about the content of ./tmp/\n",
      "\n",
      "Start to split data into train, test, validation:\n",
      "Data splitting costs a total of 0.0213 seconds!\n",
      "Start to store the batch machine file:\n",
      "Storing batch machine after training batches generation costs a total of 0.0086 seconds!\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0066 seconds!\n",
      "Start to generate the training batches:\n",
      "Train batches production costs a total of 0.0208 seconds!\n",
      "\n",
      " Information about the content of ./train/\n",
      "File name: [ batch_1 ]; with size: 1943.734375 KB\n",
      "File name: [ batch_7 ]; with size: 1898.4765625 KB\n",
      "File name: [ batch_3 ]; with size: 1843.1328125 KB\n",
      "File name: [ batch_4 ]; with size: 1955.5390625 KB\n",
      "File name: [ batch_6 ]; with size: 1938.19140625 KB\n",
      "File name: [ batch_0 ]; with size: 1842.6015625 KB\n",
      "File name: [ batch_5 ]; with size: 1855.15625 KB\n",
      "File name: [ batch_2 ]; with size: 1937.97265625 KB\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0068 seconds!\n",
      "Start to generate the validation batches:\n",
      "Validation batches production costs a total of 0.0197 seconds!\n",
      "\n",
      " Information about the content of ./validation/\n",
      "File name: [ batch_1 ]; with size: 1945.775390625 KB\n",
      "File name: [ batch_7 ]; with size: 1900.470703125 KB\n",
      "File name: [ batch_3 ]; with size: 1845.064453125 KB\n",
      "File name: [ batch_4 ]; with size: 1957.587890625 KB\n",
      "File name: [ batch_6 ]; with size: 1940.224609375 KB\n",
      "File name: [ batch_0 ]; with size: 1844.533203125 KB\n",
      "File name: [ batch_5 ]; with size: 1857.103515625 KB\n",
      "File name: [ batch_2 ]; with size: 1940.005859375 KB\n",
      "\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:58: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:58: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:58: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:58: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tuning the mini_epoch\n",
    "output_tune_param(data, data_name, dataset, image_data_path, intermediate_data_folder, partition_nums, layers, \\\n",
    "                  dropout = 0.3, lr = 0.0001, weight_decay = 0.001, mini_epoch_num = 20, valid_part_num = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start checking train loss for partition num: 2 hop layer: 1\n",
      "\n",
      "====================================================================================================\n",
      "Start to generate the clustering machine:\n",
      "Batch machine creation costs a total of 0.1689 seconds!\n",
      "\n",
      " Information about the content of ./tmp/\n",
      "\n",
      "Start to split data into train, test, validation:\n",
      "Data splitting costs a total of 0.1564 seconds!\n",
      "Start to store the batch machine file:\n",
      "Storing batch machine after training batches generation costs a total of 0.0417 seconds!\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0514 seconds!\n",
      "Start to generate the training batches:\n",
      "Train batches production costs a total of 0.1532 seconds!\n",
      "\n",
      " Information about the content of ./train/\n",
      "File name: [ batch_1 ]; with size: 19704.5185546875 KB\n",
      "File name: [ batch_0 ]; with size: 19672.4560546875 KB\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0382 seconds!\n",
      "Start to generate the validation batches:\n",
      "Validation batches production costs a total of 0.1540 seconds!\n",
      "\n",
      " Information about the content of ./validation/\n",
      "File name: [ batch_1 ]; with size: 19762.2998046875 KB\n",
      "File name: [ batch_0 ]; with size: 19730.2138671875 KB\n",
      "\n",
      "====================================================================================================\n",
      "Start checking train loss for partition num: 2 hop layer: 2\n",
      "\n",
      "====================================================================================================\n",
      "Start to generate the clustering machine:\n",
      "Batch machine creation costs a total of 0.1741 seconds!\n",
      "\n",
      " Information about the content of ./tmp/\n",
      "\n",
      "Start to split data into train, test, validation:\n",
      "Data splitting costs a total of 0.1565 seconds!\n",
      "Start to store the batch machine file:\n",
      "Storing batch machine after training batches generation costs a total of 0.0408 seconds!\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0522 seconds!\n",
      "Start to generate the training batches:\n",
      "Train batches production costs a total of 0.1574 seconds!\n",
      "\n",
      " Information about the content of ./train/\n",
      "File name: [ batch_1 ]; with size: 19704.5185546875 KB\n",
      "File name: [ batch_0 ]; with size: 19672.4560546875 KB\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0391 seconds!\n",
      "Start to generate the validation batches:\n",
      "Validation batches production costs a total of 0.1572 seconds!\n",
      "\n",
      " Information about the content of ./validation/\n",
      "File name: [ batch_1 ]; with size: 19762.2998046875 KB\n",
      "File name: [ batch_0 ]; with size: 19730.2138671875 KB\n",
      "\n",
      "====================================================================================================\n",
      "Start checking train loss for partition num: 2 hop layer: 3\n",
      "\n",
      "====================================================================================================\n",
      "Start to generate the clustering machine:\n",
      "Batch machine creation costs a total of 0.1303 seconds!\n",
      "\n",
      " Information about the content of ./tmp/\n",
      "\n",
      "Start to split data into train, test, validation:\n",
      "Data splitting costs a total of 0.1614 seconds!\n",
      "Start to store the batch machine file:\n",
      "Storing batch machine after training batches generation costs a total of 0.0437 seconds!\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0545 seconds!\n",
      "Start to generate the training batches:\n",
      "Train batches production costs a total of 0.1583 seconds!\n",
      "\n",
      " Information about the content of ./train/\n",
      "File name: [ batch_1 ]; with size: 19704.5185546875 KB\n",
      "File name: [ batch_0 ]; with size: 19672.4560546875 KB\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0386 seconds!\n",
      "Start to generate the validation batches:\n",
      "Validation batches production costs a total of 0.1629 seconds!\n",
      "\n",
      " Information about the content of ./validation/\n",
      "File name: [ batch_1 ]; with size: 19762.2998046875 KB\n",
      "File name: [ batch_0 ]; with size: 19730.2138671875 KB\n",
      "\n",
      "====================================================================================================\n",
      "Start checking train loss for partition num: 2 hop layer: 4\n",
      "\n",
      "====================================================================================================\n",
      "Start to generate the clustering machine:\n",
      "Batch machine creation costs a total of 0.1810 seconds!\n",
      "\n",
      " Information about the content of ./tmp/\n",
      "\n",
      "Start to split data into train, test, validation:\n",
      "Data splitting costs a total of 0.1605 seconds!\n",
      "Start to store the batch machine file:\n",
      "Storing batch machine after training batches generation costs a total of 0.0402 seconds!\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0510 seconds!\n",
      "Start to generate the training batches:\n",
      "Train batches production costs a total of 0.1212 seconds!\n",
      "\n",
      " Information about the content of ./train/\n",
      "File name: [ batch_1 ]; with size: 19704.5185546875 KB\n",
      "File name: [ batch_0 ]; with size: 19672.4560546875 KB\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0693 seconds!\n",
      "Start to generate the validation batches:\n",
      "Validation batches production costs a total of 0.1639 seconds!\n",
      "\n",
      " Information about the content of ./validation/\n",
      "File name: [ batch_1 ]; with size: 19762.2998046875 KB\n",
      "File name: [ batch_0 ]; with size: 19730.2138671875 KB\n",
      "\n",
      "====================================================================================================\n",
      "Start checking train loss for partition num: 4 hop layer: 1\n",
      "\n",
      "====================================================================================================\n",
      "Start to generate the clustering machine:\n",
      "Batch machine creation costs a total of 0.1269 seconds!\n",
      "\n",
      " Information about the content of ./tmp/\n",
      "\n",
      "Start to split data into train, test, validation:\n",
      "Data splitting costs a total of 0.1596 seconds!\n",
      "Start to store the batch machine file:\n",
      "Storing batch machine after training batches generation costs a total of 0.0413 seconds!\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0515 seconds!\n",
      "Start to generate the training batches:\n",
      "Train batches production costs a total of 0.1429 seconds!\n",
      "\n",
      " Information about the content of ./train/\n",
      "File name: [ batch_1 ]; with size: 9752.6357421875 KB\n",
      "File name: [ batch_3 ]; with size: 9755.3544921875 KB\n",
      "File name: [ batch_0 ]; with size: 9760.2060546875 KB\n",
      "File name: [ batch_2 ]; with size: 9762.5888671875 KB\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0532 seconds!\n",
      "Start to generate the validation batches:\n",
      "Validation batches production costs a total of 0.1005 seconds!\n",
      "\n",
      " Information about the content of ./validation/\n",
      "File name: [ batch_1 ]; with size: 9781.5185546875 KB\n",
      "File name: [ batch_3 ]; with size: 9784.2373046875 KB\n",
      "File name: [ batch_0 ]; with size: 9789.1123046875 KB\n",
      "File name: [ batch_2 ]; with size: 9791.4873046875 KB\n",
      "\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start checking train loss for partition num: 4 hop layer: 2\n",
      "\n",
      "====================================================================================================\n",
      "Start to generate the clustering machine:\n",
      "Batch machine creation costs a total of 0.1815 seconds!\n",
      "\n",
      " Information about the content of ./tmp/\n",
      "\n",
      "Start to split data into train, test, validation:\n",
      "Data splitting costs a total of 0.1667 seconds!\n",
      "Start to store the batch machine file:\n",
      "Storing batch machine after training batches generation costs a total of 0.0333 seconds!\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0377 seconds!\n",
      "Start to generate the training batches:\n",
      "Train batches production costs a total of 0.1014 seconds!\n",
      "\n",
      " Information about the content of ./train/\n",
      "File name: [ batch_1 ]; with size: 9752.6357421875 KB\n",
      "File name: [ batch_3 ]; with size: 9755.3544921875 KB\n",
      "File name: [ batch_0 ]; with size: 9760.2060546875 KB\n",
      "File name: [ batch_2 ]; with size: 9762.5888671875 KB\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0752 seconds!\n",
      "Start to generate the validation batches:\n",
      "Validation batches production costs a total of 0.1024 seconds!\n",
      "\n",
      " Information about the content of ./validation/\n",
      "File name: [ batch_1 ]; with size: 9781.5185546875 KB\n",
      "File name: [ batch_3 ]; with size: 9784.2373046875 KB\n",
      "File name: [ batch_0 ]; with size: 9789.1123046875 KB\n",
      "File name: [ batch_2 ]; with size: 9791.4873046875 KB\n",
      "\n",
      "====================================================================================================\n",
      "Start checking train loss for partition num: 4 hop layer: 3\n",
      "\n",
      "====================================================================================================\n",
      "Start to generate the clustering machine:\n",
      "Batch machine creation costs a total of 0.1343 seconds!\n",
      "\n",
      " Information about the content of ./tmp/\n",
      "\n",
      "Start to split data into train, test, validation:\n",
      "Data splitting costs a total of 0.2031 seconds!\n",
      "Start to store the batch machine file:\n",
      "Storing batch machine after training batches generation costs a total of 0.0423 seconds!\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0510 seconds!\n",
      "Start to generate the training batches:\n",
      "Train batches production costs a total of 0.1039 seconds!\n",
      "\n",
      " Information about the content of ./train/\n",
      "File name: [ batch_1 ]; with size: 9752.6357421875 KB\n",
      "File name: [ batch_3 ]; with size: 9755.3544921875 KB\n",
      "File name: [ batch_0 ]; with size: 9760.2060546875 KB\n",
      "File name: [ batch_2 ]; with size: 9762.5888671875 KB\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0521 seconds!\n",
      "Start to generate the validation batches:\n",
      "Validation batches production costs a total of 0.1369 seconds!\n",
      "\n",
      " Information about the content of ./validation/\n",
      "File name: [ batch_1 ]; with size: 9781.5185546875 KB\n",
      "File name: [ batch_3 ]; with size: 9784.2373046875 KB\n",
      "File name: [ batch_0 ]; with size: 9789.1123046875 KB\n",
      "File name: [ batch_2 ]; with size: 9791.4873046875 KB\n",
      "\n",
      "====================================================================================================\n",
      "Start checking train loss for partition num: 4 hop layer: 4\n",
      "\n",
      "====================================================================================================\n",
      "Start to generate the clustering machine:\n",
      "Batch machine creation costs a total of 0.1322 seconds!\n",
      "\n",
      " Information about the content of ./tmp/\n",
      "\n",
      "Start to split data into train, test, validation:\n",
      "Data splitting costs a total of 0.1595 seconds!\n",
      "Start to store the batch machine file:\n",
      "Storing batch machine after training batches generation costs a total of 0.0325 seconds!\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0706 seconds!\n",
      "Start to generate the training batches:\n",
      "Train batches production costs a total of 0.1042 seconds!\n",
      "\n",
      " Information about the content of ./train/\n",
      "File name: [ batch_1 ]; with size: 9752.6357421875 KB\n",
      "File name: [ batch_3 ]; with size: 9755.3544921875 KB\n",
      "File name: [ batch_0 ]; with size: 9760.2060546875 KB\n",
      "File name: [ batch_2 ]; with size: 9762.5888671875 KB\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0375 seconds!\n",
      "Start to generate the validation batches:\n",
      "Validation batches production costs a total of 0.1377 seconds!\n",
      "\n",
      " Information about the content of ./validation/\n",
      "File name: [ batch_1 ]; with size: 9781.5185546875 KB\n",
      "File name: [ batch_3 ]; with size: 9784.2373046875 KB\n",
      "File name: [ batch_0 ]; with size: 9789.1123046875 KB\n",
      "File name: [ batch_2 ]; with size: 9791.4873046875 KB\n",
      "\n",
      "====================================================================================================\n",
      "Start checking train loss for partition num: 8 hop layer: 1\n",
      "\n",
      "====================================================================================================\n",
      "Start to generate the clustering machine:\n",
      "Batch machine creation costs a total of 0.1314 seconds!\n",
      "\n",
      " Information about the content of ./tmp/\n",
      "\n",
      "Start to split data into train, test, validation:\n",
      "Data splitting costs a total of 0.1701 seconds!\n",
      "Start to store the batch machine file:\n",
      "Storing batch machine after training batches generation costs a total of 0.0392 seconds!\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0396 seconds!\n",
      "Start to generate the training batches:\n",
      "Train batches production costs a total of 0.1045 seconds!\n",
      "\n",
      " Information about the content of ./train/\n",
      "File name: [ batch_1 ]; with size: 4998.1259765625 KB\n",
      "File name: [ batch_7 ]; with size: 4756.1259765625 KB\n",
      "File name: [ batch_3 ]; with size: 4785.5166015625 KB\n",
      "File name: [ batch_4 ]; with size: 4952.5244140625 KB\n",
      "File name: [ batch_6 ]; with size: 5000.9384765625 KB\n",
      "File name: [ batch_0 ]; with size: 4787.7041015625 KB\n",
      "File name: [ batch_5 ]; with size: 4788.5166015625 KB\n",
      "File name: [ batch_2 ]; with size: 4795.0166015625 KB\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0729 seconds!\n",
      "Start to generate the validation batches:\n",
      "Validation batches production costs a total of 0.1035 seconds!\n",
      "\n",
      " Information about the content of ./validation/\n",
      "File name: [ batch_1 ]; with size: 5012.9794921875 KB\n",
      "File name: [ batch_7 ]; with size: 4770.2841796875 KB\n",
      "File name: [ batch_3 ]; with size: 4799.7607421875 KB\n",
      "File name: [ batch_4 ]; with size: 4967.2607421875 KB\n",
      "File name: [ batch_6 ]; with size: 5015.8232421875 KB\n",
      "File name: [ batch_0 ]; with size: 4801.9482421875 KB\n",
      "File name: [ batch_5 ]; with size: 4802.7607421875 KB\n",
      "File name: [ batch_2 ]; with size: 4809.2919921875 KB\n",
      "\n",
      "====================================================================================================\n",
      "Start checking train loss for partition num: 8 hop layer: 2\n",
      "\n",
      "====================================================================================================\n",
      "Start to generate the clustering machine:\n",
      "Batch machine creation costs a total of 0.1674 seconds!\n",
      "\n",
      " Information about the content of ./tmp/\n",
      "\n",
      "Start to split data into train, test, validation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data splitting costs a total of 0.1641 seconds!\n",
      "Start to store the batch machine file:\n",
      "Storing batch machine after training batches generation costs a total of 0.0325 seconds!\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0372 seconds!\n",
      "Start to generate the training batches:\n",
      "Train batches production costs a total of 0.0957 seconds!\n",
      "\n",
      " Information about the content of ./train/\n",
      "File name: [ batch_1 ]; with size: 4998.1259765625 KB\n",
      "File name: [ batch_7 ]; with size: 4756.1259765625 KB\n",
      "File name: [ batch_3 ]; with size: 4785.5166015625 KB\n",
      "File name: [ batch_4 ]; with size: 4952.5244140625 KB\n",
      "File name: [ batch_6 ]; with size: 5000.9384765625 KB\n",
      "File name: [ batch_0 ]; with size: 4787.7041015625 KB\n",
      "File name: [ batch_5 ]; with size: 4788.5166015625 KB\n",
      "File name: [ batch_2 ]; with size: 4795.0166015625 KB\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0397 seconds!\n",
      "Start to generate the validation batches:\n",
      "Validation batches production costs a total of 0.1056 seconds!\n",
      "\n",
      " Information about the content of ./validation/\n",
      "File name: [ batch_1 ]; with size: 5012.9794921875 KB\n",
      "File name: [ batch_7 ]; with size: 4770.2841796875 KB\n",
      "File name: [ batch_3 ]; with size: 4799.7607421875 KB\n",
      "File name: [ batch_4 ]; with size: 4967.2607421875 KB\n",
      "File name: [ batch_6 ]; with size: 5015.8232421875 KB\n",
      "File name: [ batch_0 ]; with size: 4801.9482421875 KB\n",
      "File name: [ batch_5 ]; with size: 4802.7607421875 KB\n",
      "File name: [ batch_2 ]; with size: 4809.2919921875 KB\n",
      "\n",
      "====================================================================================================\n",
      "Start checking train loss for partition num: 8 hop layer: 3\n",
      "\n",
      "====================================================================================================\n",
      "Start to generate the clustering machine:\n",
      "Batch machine creation costs a total of 0.1288 seconds!\n",
      "\n",
      " Information about the content of ./tmp/\n",
      "\n",
      "Start to split data into train, test, validation:\n",
      "Data splitting costs a total of 0.1728 seconds!\n",
      "Start to store the batch machine file:\n",
      "Storing batch machine after training batches generation costs a total of 0.0326 seconds!\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0362 seconds!\n",
      "Start to generate the training batches:\n",
      "Train batches production costs a total of 0.0958 seconds!\n",
      "\n",
      " Information about the content of ./train/\n",
      "File name: [ batch_1 ]; with size: 4998.1259765625 KB\n",
      "File name: [ batch_7 ]; with size: 4756.1259765625 KB\n",
      "File name: [ batch_3 ]; with size: 4785.5166015625 KB\n",
      "File name: [ batch_4 ]; with size: 4952.5244140625 KB\n",
      "File name: [ batch_6 ]; with size: 5000.9384765625 KB\n",
      "File name: [ batch_0 ]; with size: 4787.7041015625 KB\n",
      "File name: [ batch_5 ]; with size: 4788.5166015625 KB\n",
      "File name: [ batch_2 ]; with size: 4795.0166015625 KB\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0779 seconds!\n",
      "Start to generate the validation batches:\n",
      "Validation batches production costs a total of 0.0961 seconds!\n",
      "\n",
      " Information about the content of ./validation/\n",
      "File name: [ batch_1 ]; with size: 5012.9794921875 KB\n",
      "File name: [ batch_7 ]; with size: 4770.2841796875 KB\n",
      "File name: [ batch_3 ]; with size: 4799.7607421875 KB\n",
      "File name: [ batch_4 ]; with size: 4967.2607421875 KB\n",
      "File name: [ batch_6 ]; with size: 5015.8232421875 KB\n",
      "File name: [ batch_0 ]; with size: 4801.9482421875 KB\n",
      "File name: [ batch_5 ]; with size: 4802.7607421875 KB\n",
      "File name: [ batch_2 ]; with size: 4809.2919921875 KB\n",
      "\n",
      "====================================================================================================\n",
      "Start checking train loss for partition num: 8 hop layer: 4\n",
      "\n",
      "====================================================================================================\n",
      "Start to generate the clustering machine:\n",
      "Batch machine creation costs a total of 0.1371 seconds!\n",
      "\n",
      " Information about the content of ./tmp/\n",
      "\n",
      "Start to split data into train, test, validation:\n",
      "Data splitting costs a total of 0.1689 seconds!\n",
      "Start to store the batch machine file:\n",
      "Storing batch machine after training batches generation costs a total of 0.0310 seconds!\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0783 seconds!\n",
      "Start to generate the training batches:\n",
      "Train batches production costs a total of 0.1019 seconds!\n",
      "\n",
      " Information about the content of ./train/\n",
      "File name: [ batch_1 ]; with size: 4998.1259765625 KB\n",
      "File name: [ batch_7 ]; with size: 4756.1259765625 KB\n",
      "File name: [ batch_3 ]; with size: 4785.5166015625 KB\n",
      "File name: [ batch_4 ]; with size: 4952.5244140625 KB\n",
      "File name: [ batch_6 ]; with size: 5000.9384765625 KB\n",
      "File name: [ batch_0 ]; with size: 4787.7041015625 KB\n",
      "File name: [ batch_5 ]; with size: 4788.5166015625 KB\n",
      "File name: [ batch_2 ]; with size: 4795.0166015625 KB\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0379 seconds!\n",
      "Start to generate the validation batches:\n",
      "Validation batches production costs a total of 0.0993 seconds!\n",
      "\n",
      " Information about the content of ./validation/\n",
      "File name: [ batch_1 ]; with size: 5012.9794921875 KB\n",
      "File name: [ batch_7 ]; with size: 4770.2841796875 KB\n",
      "File name: [ batch_3 ]; with size: 4799.7607421875 KB\n",
      "File name: [ batch_4 ]; with size: 4967.2607421875 KB\n",
      "File name: [ batch_6 ]; with size: 5015.8232421875 KB\n",
      "File name: [ batch_0 ]; with size: 4801.9482421875 KB\n",
      "File name: [ batch_5 ]; with size: 4802.7607421875 KB\n",
      "File name: [ batch_2 ]; with size: 4809.2919921875 KB\n",
      "\n",
      "====================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check training loss\n",
    "output_train_loss(data, data_name, dataset, image_data_path, intermediate_data_folder, partition_nums, layers, \\\n",
    "                  dropout = 0.3, lr = 0.0001, weight_decay = 0.001, mini_epoch_num = 20, valid_part_num = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Check in_train performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start running for partition num: 2 hop layer 1\n",
      "\n",
      "====================================================================================================\n",
      "Start to generate the clustering machine:\n",
      "Batch machine creation costs a total of 0.1662 seconds!\n",
      "\n",
      " Information about the content of ./tmp/\n",
      "\n",
      "Start to split data into train, test, validation:\n",
      "Data splitting costs a total of 0.1548 seconds!\n",
      "Start to store the batch machine file:\n",
      "Storing batch machine after training batches generation costs a total of 0.0400 seconds!\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0505 seconds!\n",
      "Start to generate the training batches:\n",
      "Train batches production costs a total of 0.1552 seconds!\n",
      "\n",
      " Information about the content of ./train/\n",
      "File name: [ batch_1 ]; with size: 19704.5185546875 KB\n",
      "File name: [ batch_0 ]; with size: 19672.4560546875 KB\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0372 seconds!\n",
      "Start to generate the validation batches:\n",
      "Validation batches production costs a total of 0.1571 seconds!\n",
      "\n",
      " Information about the content of ./validation/\n",
      "File name: [ batch_1 ]; with size: 19762.2998046875 KB\n",
      "File name: [ batch_0 ]; with size: 19730.2138671875 KB\n",
      "\n",
      "====================================================================================================\n",
      "In-process Training costs a total of 2.1447 seconds!\n",
      "In-process Training costs a total of 1.9946 seconds!\n",
      "In-process Training costs a total of 2.0882 seconds!\n",
      "In-process Training costs a total of 2.0899 seconds!\n",
      "In-process Training costs a total of 2.1446 seconds!\n",
      "In-process Training costs a total of 1.9983 seconds!\n",
      "In-process Training costs a total of 1.9710 seconds!\n",
      "Start running for partition num: 2 hop layer 2\n",
      "\n",
      "====================================================================================================\n",
      "Start to generate the clustering machine:\n",
      "Batch machine creation costs a total of 0.1305 seconds!\n",
      "\n",
      " Information about the content of ./tmp/\n",
      "\n",
      "Start to split data into train, test, validation:\n",
      "Data splitting costs a total of 0.1494 seconds!\n",
      "Start to store the batch machine file:\n",
      "Storing batch machine after training batches generation costs a total of 0.0408 seconds!\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0844 seconds!\n",
      "Start to generate the training batches:\n",
      "Train batches production costs a total of 0.1624 seconds!\n",
      "\n",
      " Information about the content of ./train/\n",
      "File name: [ batch_1 ]; with size: 19704.5185546875 KB\n",
      "File name: [ batch_0 ]; with size: 19672.4560546875 KB\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0545 seconds!\n",
      "Start to generate the validation batches:\n",
      "Validation batches production costs a total of 0.1252 seconds!\n",
      "\n",
      " Information about the content of ./validation/\n",
      "File name: [ batch_1 ]; with size: 19762.2998046875 KB\n",
      "File name: [ batch_0 ]; with size: 19730.2138671875 KB\n",
      "\n",
      "====================================================================================================\n",
      "In-process Training costs a total of 3.0026 seconds!\n",
      "In-process Training costs a total of 2.9865 seconds!\n",
      "In-process Training costs a total of 3.0061 seconds!\n",
      "In-process Training costs a total of 2.9724 seconds!\n",
      "In-process Training costs a total of 2.9712 seconds!\n",
      "In-process Training costs a total of 2.9716 seconds!\n",
      "In-process Training costs a total of 2.9951 seconds!\n",
      "Start running for partition num: 2 hop layer 3\n",
      "\n",
      "====================================================================================================\n",
      "Start to generate the clustering machine:\n",
      "Batch machine creation costs a total of 0.1334 seconds!\n",
      "\n",
      " Information about the content of ./tmp/\n",
      "\n",
      "Start to split data into train, test, validation:\n",
      "Data splitting costs a total of 0.1574 seconds!\n",
      "Start to store the batch machine file:\n",
      "Storing batch machine after training batches generation costs a total of 0.0410 seconds!\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0513 seconds!\n",
      "Start to generate the training batches:\n",
      "Train batches production costs a total of 0.1591 seconds!\n",
      "\n",
      " Information about the content of ./train/\n",
      "File name: [ batch_1 ]; with size: 19704.5185546875 KB\n",
      "File name: [ batch_0 ]; with size: 19672.4560546875 KB\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0377 seconds!\n",
      "Start to generate the validation batches:\n",
      "Validation batches production costs a total of 0.1609 seconds!\n",
      "\n",
      " Information about the content of ./validation/\n",
      "File name: [ batch_1 ]; with size: 19762.2998046875 KB\n",
      "File name: [ batch_0 ]; with size: 19730.2138671875 KB\n",
      "\n",
      "====================================================================================================\n",
      "In-process Training costs a total of 4.0502 seconds!\n",
      "In-process Training costs a total of 4.0377 seconds!\n",
      "In-process Training costs a total of 4.0291 seconds!\n",
      "In-process Training costs a total of 3.9694 seconds!\n",
      "In-process Training costs a total of 4.0622 seconds!\n",
      "In-process Training costs a total of 4.0808 seconds!\n",
      "In-process Training costs a total of 4.3316 seconds!\n",
      "Start running for partition num: 2 hop layer 4\n",
      "\n",
      "====================================================================================================\n",
      "Start to generate the clustering machine:\n",
      "Batch machine creation costs a total of 0.1812 seconds!\n",
      "\n",
      " Information about the content of ./tmp/\n",
      "\n",
      "Start to split data into train, test, validation:\n",
      "Data splitting costs a total of 0.1629 seconds!\n",
      "Start to store the batch machine file:\n",
      "Storing batch machine after training batches generation costs a total of 0.0426 seconds!\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0538 seconds!\n",
      "Start to generate the training batches:\n",
      "Train batches production costs a total of 0.1621 seconds!\n",
      "\n",
      " Information about the content of ./train/\n",
      "File name: [ batch_1 ]; with size: 19704.5185546875 KB\n",
      "File name: [ batch_0 ]; with size: 19672.4560546875 KB\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0383 seconds!\n",
      "Start to generate the validation batches:\n",
      "Validation batches production costs a total of 0.1602 seconds!\n",
      "\n",
      " Information about the content of ./validation/\n",
      "File name: [ batch_1 ]; with size: 19762.2998046875 KB\n",
      "File name: [ batch_0 ]; with size: 19730.2138671875 KB\n",
      "\n",
      "====================================================================================================\n",
      "In-process Training costs a total of 5.2180 seconds!\n",
      "In-process Training costs a total of 5.4777 seconds!\n",
      "In-process Training costs a total of 5.0863 seconds!\n",
      "In-process Training costs a total of 5.0695 seconds!\n",
      "In-process Training costs a total of 5.0010 seconds!\n",
      "In-process Training costs a total of 5.1284 seconds!\n",
      "In-process Training costs a total of 5.0816 seconds!\n",
      "Start running for partition num: 4 hop layer 1\n",
      "\n",
      "====================================================================================================\n",
      "Start to generate the clustering machine:\n",
      "Batch machine creation costs a total of 0.1408 seconds!\n",
      "\n",
      " Information about the content of ./tmp/\n",
      "\n",
      "Start to split data into train, test, validation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data splitting costs a total of 0.1617 seconds!\n",
      "Start to store the batch machine file:\n",
      "Storing batch machine after training batches generation costs a total of 0.0429 seconds!\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0872 seconds!\n",
      "Start to generate the training batches:\n",
      "Train batches production costs a total of 0.1046 seconds!\n",
      "\n",
      " Information about the content of ./train/\n",
      "File name: [ batch_1 ]; with size: 9752.6357421875 KB\n",
      "File name: [ batch_3 ]; with size: 9755.3544921875 KB\n",
      "File name: [ batch_0 ]; with size: 9760.2060546875 KB\n",
      "File name: [ batch_2 ]; with size: 9762.5888671875 KB\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0532 seconds!\n",
      "Start to generate the validation batches:\n",
      "Validation batches production costs a total of 0.1011 seconds!\n",
      "\n",
      " Information about the content of ./validation/\n",
      "File name: [ batch_1 ]; with size: 9781.5185546875 KB\n",
      "File name: [ batch_3 ]; with size: 9784.2373046875 KB\n",
      "File name: [ batch_0 ]; with size: 9789.1123046875 KB\n",
      "File name: [ batch_2 ]; with size: 9791.4873046875 KB\n",
      "\n",
      "====================================================================================================\n",
      "In-process Training costs a total of 3.4917 seconds!\n",
      "In-process Training costs a total of 3.4784 seconds!\n",
      "In-process Training costs a total of 3.4726 seconds!\n",
      "In-process Training costs a total of 3.5315 seconds!\n",
      "In-process Training costs a total of 3.7587 seconds!\n",
      "In-process Training costs a total of 3.7111 seconds!\n",
      "In-process Training costs a total of 3.3831 seconds!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start running for partition num: 4 hop layer 2\n",
      "\n",
      "====================================================================================================\n",
      "Start to generate the clustering machine:\n",
      "Batch machine creation costs a total of 0.1316 seconds!\n",
      "\n",
      " Information about the content of ./tmp/\n",
      "\n",
      "Start to split data into train, test, validation:\n",
      "Data splitting costs a total of 0.1565 seconds!\n",
      "Start to store the batch machine file:\n",
      "Storing batch machine after training batches generation costs a total of 0.0302 seconds!\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0355 seconds!\n",
      "Start to generate the training batches:\n",
      "Train batches production costs a total of 0.1408 seconds!\n",
      "\n",
      " Information about the content of ./train/\n",
      "File name: [ batch_1 ]; with size: 9752.6357421875 KB\n",
      "File name: [ batch_3 ]; with size: 9755.3544921875 KB\n",
      "File name: [ batch_0 ]; with size: 9760.2060546875 KB\n",
      "File name: [ batch_2 ]; with size: 9762.5888671875 KB\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0365 seconds!\n",
      "Start to generate the validation batches:\n",
      "Validation batches production costs a total of 0.0992 seconds!\n",
      "\n",
      " Information about the content of ./validation/\n",
      "File name: [ batch_1 ]; with size: 9781.5185546875 KB\n",
      "File name: [ batch_3 ]; with size: 9784.2373046875 KB\n",
      "File name: [ batch_0 ]; with size: 9789.1123046875 KB\n",
      "File name: [ batch_2 ]; with size: 9791.4873046875 KB\n",
      "\n",
      "====================================================================================================\n",
      "In-process Training costs a total of 4.7869 seconds!\n",
      "In-process Training costs a total of 4.8224 seconds!\n",
      "In-process Training costs a total of 4.9439 seconds!\n",
      "In-process Training costs a total of 4.7973 seconds!\n",
      "In-process Training costs a total of 4.8713 seconds!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-6ce2a5ded794>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m output_train_investigate(data, data_name, dataset, image_data_path, intermediate_data_folder, partition_nums, layers, \\\n\u001b[0;32m----> 2\u001b[0;31m                          dropout = 0.1, lr = 0.0001, weight_decay = 0.1, mini_epoch_num = 20, output_period = 40, valid_part_num = 2)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-cc5f058b9b4b>\u001b[0m in \u001b[0;36moutput_train_investigate\u001b[0;34m(data, data_name, dataset, image_data_path, intermediate_data_path, partition_nums, layers, dropout, lr, weight_decay, mini_epoch_num, output_period, valid_part_num)\u001b[0m\n\u001b[1;32m     83\u001b[0m             Train_peroid_f1, Train_peroid_accuracy = execute_investigate(mini_batch_folder, img_path, repeate_time = 7, input_layer = GCN_layer, epoch_num = 400, \\\n\u001b[1;32m     84\u001b[0m                                             \u001b[0mdropout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_epoch_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmini_epoch_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_period\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_period\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m                                             valid_part_num = partn, train_part_num = partn, test_part_num = 1)\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mTrain_peroid_f1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstore_data_multi_investigate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrain_peroid_f1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'F1_score'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'invest_batch_num_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_hops_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhop_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-a82f41f7289c>\u001b[0m in \u001b[0;36mexecute_investigate\u001b[0;34m(mini_batch_folder, image_path, repeate_time, input_layer, epoch_num, dropout, lr, weight_decay, mini_epoch_num, output_period, valid_part_num, train_part_num, test_part_num)\u001b[0m\n\u001b[1;32m     12\u001b[0m         Train_peroid_f1[i], Train_peroid_accuracy[i] = Cluster_train_valid_batch_investigate(mini_batch_folder, data_name, dataset, image_path, input_layer = input_layer, epochs=epoch_num, \\\n\u001b[1;32m     13\u001b[0m                                             \u001b[0mdropout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_epoch_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmini_epoch_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_period\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_period\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                                                                     valid_part_num = valid_part_num, train_part_num = train_part_num, test_part_num = test_part_num)\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mTrain_peroid_f1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrain_peroid_accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-8f50aeeb96f6>\u001b[0m in \u001b[0;36mCluster_train_valid_batch_investigate\u001b[0;34m(mini_batch_folder, data_name, dataset, image_path, input_layer, epochs, dropout, lr, weight_decay, mini_epoch_num, output_period, valid_part_num, train_part_num, test_part_num)\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m     Train_period_F1, Train_period_accuracy = gcn_trainer.train_investigate_F1(epoch_num=epochs, learning_rate=lr, weight_decay=weight_decay, mini_epoch_num = mini_epoch_num, \\\n\u001b[0;32m--> 174\u001b[0;31m                                                             output_period = output_period, train_batch_num = train_part_num, valid_batch_num = valid_part_num)\n\u001b[0m\u001b[1;32m    175\u001b[0m     \u001b[0mtrain_period\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'In-process Training costs a total of {0:.4f} seconds!'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_period\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-db4678168961>\u001b[0m in \u001b[0;36mtrain_investigate_F1\u001b[0;34m(self, epoch_num, learning_rate, weight_decay, mini_epoch_num, output_period, train_batch_num, valid_batch_num)\u001b[0m\n\u001b[1;32m    116\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                     \u001b[0mbatch_ave_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_forward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_train_nodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_edges\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m                     \u001b[0mbatch_ave_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m                     \u001b[0mave_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_average_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ave_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "output_train_investigate(data, data_name, dataset, image_data_path, intermediate_data_folder, partition_nums, layers, \\\n",
    "                         dropout = 0.1, lr = 0.0001, weight_decay = 0.001, mini_epoch_num = 20, output_period = 40, valid_part_num = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output_F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check F1-score\n",
    "output_F1_score(data, data_name, dataset, image_data_path, intermediate_data_folder, partition_nums, layers, \\\n",
    "                dropout = 0.1, lr = tune_lr, weight_decay = 0.1, mini_epoch_num = check_mini_epoch, valid_part_num = 2)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CoraFull dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import CoraFull\n",
    "data_name = 'CoraFull'\n",
    "dataset = CoraFull(root = local_data_root + 'CoralFull')\n",
    "print('number of data: ', len(dataset))\n",
    "data = dataset[0]\n",
    "\n",
    "image_data_path = './results/' + data_name + '/' + test_folder_name\n",
    "intermediate_data_folder = './intermediate_data/' + data_name + '/' + test_folder_name\n",
    "\n",
    "partition_nums = [4]\n",
    "layers = [[128, 128]]\n",
    "tune_lr = 0.0001\n",
    "check_mini_epoch = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune the parameter\n",
    "\n",
    "output_tune_param(data, data_name, dataset, image_data_path, intermediate_data_folder, partition_nums, layers, \\\n",
    "                  dropout = 0.5, lr = 0.0001, weight_decay = 0.001, mini_epoch_num = 20, valid_part_num = 2)\n",
    "\n",
    "# in-train process\n",
    "# output_train_investigate(data, data_name, dataset, image_data_path, intermediate_data_folder, partition_nums, layers, \\\n",
    "#                          dropout = 0.5, lr = 0.0001, weight_decay = 0.001, mini_epoch_num = 20, output_period = 40, valid_part_num = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check F1-score\n",
    "# output_F1_score(data, data_name, dataset, image_data_path, intermediate_data_folder, partition_nums, layers, \\\n",
    "#                 dropout = 0.1, lr = 0.0001, weight_decay = 0.1, mini_epoch_num = 20, valid_part_num = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CiteSeer Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "data_name = 'CiteSeer'\n",
    "dataset = Planetoid(root = local_data_root + 'Planetoid/CiteSeer', name=data_name)\n",
    "data = dataset[0]\n",
    "image_data_path = './results/' + data_name + '/' + test_folder_name\n",
    "\n",
    "partition_nums = [2, 4, 8]\n",
    "layers = [[], [16], [16, 16]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning the epoch number per batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking train loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PubMed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "data_name = 'PubMed'\n",
    "dataset = Planetoid(root = local_data_root + 'Planetoid/PubMed', name=data_name)\n",
    "data = dataset[0]\n",
    "image_data_path = './results/' + data_name + '/' + test_folder_name\n",
    "\n",
    "partition_nums = [2, 4, 8]\n",
    "layers = [[], [64], [64, 64], [64, 64, 64]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tune epoch number per batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the train error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free GPU memory\n",
    "# !(nvidia-smi | grep 'python' | awk '{ print $3 }' | xargs -n1 kill -9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_geometric]",
   "language": "python",
   "name": "conda-env-pytorch_geometric-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
