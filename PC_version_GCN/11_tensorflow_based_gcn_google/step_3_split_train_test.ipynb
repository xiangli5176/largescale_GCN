{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# package from the pytorch side\n",
    "import copy\n",
    "import csv\n",
    "import sys\n",
    "import torch\n",
    "from pytorch_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "\n",
    "import networkx as nx\n",
    "import metis\n",
    "import sklearn\n",
    "import time\n",
    "# import models\n",
    "import numpy as np\n",
    "import partition_utils\n",
    "import tensorflow as tf\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n",
      "Num CPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "# tf.compat.v1.logging.ERROR\n",
    "# verbose information: tf.logging.INFO  \n",
    "\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)  # Sets the threshold for what messages will be logged.\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Num CPUs Available: \", len(tf.config.experimental.list_physical_devices('CPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCN model used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inits\n",
    "import tensorflow as tf\n",
    "import metrics\n",
    "\n",
    "def sparse_dropout(x, keep_prob, noise_shape):\n",
    "    \"\"\"Dropout for sparse tensors.\"\"\"\n",
    "    random_tensor = keep_prob\n",
    "    random_tensor += tf.compat.v1.random_uniform(noise_shape)\n",
    "    dropout_mask = tf.compat.v1.cast(tf.compat.v1.floor(random_tensor), dtype=tf.compat.v1.bool)\n",
    "    pre_out = tf.compat.v1.sparse_retain(x, dropout_mask)\n",
    "    return pre_out * (1. / keep_prob)\n",
    "\n",
    "\n",
    "def dot(x, y, sparse=False):\n",
    "    \"\"\"Wrapper for tf.compat.v1.matmul (sparse vs dense).\"\"\"\n",
    "    if sparse:\n",
    "        res = tf.compat.v1.sparse_tensor_dense_matmul(x, y)\n",
    "    else:\n",
    "        res = tf.compat.v1.matmul(x, y)\n",
    "    return res\n",
    "\n",
    "\n",
    "def layernorm(x, offset, scale):\n",
    "    mean, variance = tf.compat.v1.nn.moments(x, axes=[1], keep_dims=True)\n",
    "    return tf.compat.v1.nn.batch_normalization(x, mean, variance, offset, scale, 1e-9)\n",
    "\n",
    "\n",
    "class Layer(object):\n",
    "    \"\"\"Base layer class.\n",
    "\n",
    "    Defines basic API for all layer objects.\n",
    "    Implementation inspired by keras (http://keras.io).\n",
    "\n",
    "    # Properties\n",
    "        name: String, defines the variable scope of the layer.\n",
    "        logging: Boolean, switches Tensorflow histogram logging on/off\n",
    "\n",
    "    # Methods\n",
    "        _call(inputs): Defines computation graph of layer\n",
    "            (i.e. takes input, returns output)\n",
    "        __call__(inputs): Wrapper for _call()\n",
    "        _log_vars(): Log all variables\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name = None, logging = False):\n",
    "        \n",
    "        if not name:\n",
    "            layer = self.__class__.__name__.lower()\n",
    "            name = layer + '_' \n",
    "\n",
    "        self.name = name\n",
    "        \n",
    "        self.vars = {}\n",
    "        self.logging = logging\n",
    "        \n",
    "        self.sparse_inputs = False\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        return inputs\n",
    "\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        with tf.compat.v1.name_scope(self.name):\n",
    "            if self.logging and not self.sparse_inputs:\n",
    "                tf.compat.v1.summary.histogram(self.name + '/inputs', inputs)\n",
    "\n",
    "        outputs = self._call(inputs)\n",
    "\n",
    "        if self.logging:\n",
    "            tf.compat.v1.summary.histogram(self.name + '/outputs', outputs)\n",
    "        return outputs\n",
    "\n",
    "    def _log_vars(self):\n",
    "        for var in self.vars:\n",
    "            tf.compat.v1.summary.histogram(self.name + '/vars/' + var, self.vars[var])\n",
    "\n",
    "            \n",
    "class GraphConvolution(Layer):\n",
    "    \"\"\"Graph convolution layer.\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, placeholders,\n",
    "               dropout=0., sparse_inputs=False, act=tf.nn.relu, bias=False, featureless=False, norm=False, precalc=False,\n",
    "               name = None, logging = False):\n",
    "        super(GraphConvolution, self).__init__(name = name, logging = logging)\n",
    "#         print('During the constructor of GCN layer, input dim: {} ; output dim: {}'.format(input_dim, output_dim))\n",
    "\n",
    "        if dropout:\n",
    "            self.dropout = placeholders['dropout']\n",
    "        else:\n",
    "            self.dropout = 0.\n",
    "\n",
    "        self.act = act\n",
    "        self.support = placeholders['support']\n",
    "\n",
    "        self.sparse_inputs = sparse_inputs\n",
    "        self.featureless = featureless\n",
    "        self.bias = bias\n",
    "        self.norm = norm\n",
    "        self.precalc = precalc\n",
    "\n",
    "        # helper variable for sparse dropout\n",
    "        # self.num_features_nonzero = placeholders['num_features_nonzero']\n",
    "\n",
    "        with tf.variable_scope(self.name + '_vars'):\n",
    "            self.vars['weights'] = inits.glorot([input_dim, output_dim], name='weights')\n",
    "            if self.bias:\n",
    "                self.vars['bias'] = inits.zeros([output_dim], name='bias')\n",
    "\n",
    "            if self.norm:\n",
    "                self.vars['offset'] = inits.zeros([1, output_dim], name='offset')\n",
    "                self.vars['scale'] = inits.ones([1, output_dim], name='scale')\n",
    "\n",
    "        if self.logging:\n",
    "            self._log_vars()\n",
    "    \n",
    "\n",
    "    def _call(self, inputs):\n",
    "        x = inputs\n",
    "\n",
    "        # convolve\n",
    "        if self.precalc:\n",
    "            support = x\n",
    "        else:\n",
    "            support = dot(self.support, x, sparse=True)\n",
    "            support = tf.concat((support, x), axis=1)\n",
    "\n",
    "        # dropout\n",
    "        support = tf.nn.dropout(support, rate = self.dropout)\n",
    "\n",
    "#         tf.Print(support, [support], \"During the call of GCN layer, final input to be multiplied with weights: \")\n",
    "\n",
    "#         print('\\n inside the call of convolutiongraph layer: ')\n",
    "#         print('support vecotr dimension is : {} ;'.format(support.shape), 'weight matrix dimension is : {} ;'.format(self.vars['weights'].shape))\n",
    "\n",
    "        output = dot(support, self.vars['weights'], sparse=self.sparse_inputs)\n",
    "\n",
    "        # bias\n",
    "        if self.bias:\n",
    "            output += self.vars['bias']\n",
    "\n",
    "        with tf.variable_scope(self.name + '_vars'):\n",
    "            if self.norm:\n",
    "                output = layernorm(output, self.vars['offset'], self.vars['scale'])\n",
    "\n",
    "        return self.act(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct GCN layer nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Collections of different Models.\"\"\"\n",
    "\n",
    "\n",
    "class Model(object):\n",
    "    \"\"\"Model class to be inherited.\"\"\"\n",
    "\n",
    "    def __init__(self, weight_decay = 0, num_layers = 2, name = None, logging = False, multilabel = False, norm = False, precalc = False):\n",
    "        self.weight_decay = weight_decay\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        if not name:\n",
    "            name = self.__class__.__name__.lower()\n",
    "        self.name = name\n",
    "\n",
    "        self.logging = logging\n",
    "        self.multilabel = multilabel\n",
    "        self.norm = norm\n",
    "        self.precalc = precalc\n",
    "\n",
    "        self.vars = {}\n",
    "        self.placeholders = {}\n",
    "\n",
    "        self.layers = []\n",
    "        self.activations = []\n",
    "\n",
    "        self.inputs = None\n",
    "        self.outputs = None\n",
    "\n",
    "        self.loss = 0\n",
    "        self.accuracy = 0\n",
    "        self.pred = 0\n",
    "        self.optimizer = None\n",
    "        self.opt_op = None\n",
    "\n",
    "    def _build(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def build(self):\n",
    "        \"\"\"Wrapper for _build().\"\"\"\n",
    "        with tf.variable_scope(self.name):\n",
    "            self._build()\n",
    "\n",
    "        # Build sequential layer model\n",
    "        self.activations.append(self.inputs)\n",
    "        \n",
    "        # debug to output the embedding:\n",
    "#         self.hidden1 = layer(self.activations[-1])\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            hidden = layer(self.activations[-1])\n",
    "        \n",
    "            if isinstance(hidden, tuple):\n",
    "                tf.logging.info('{} shape = {}'.format(layer.name, hidden[0].get_shape()))\n",
    "            else:\n",
    "                tf.logging.info('{} shape = {}'.format(layer.name, hidden.get_shape()))\n",
    "\n",
    "            self.activations.append(hidden)\n",
    "            \n",
    "        self.outputs = self.activations[-1]\n",
    "\n",
    "        # Store model variables for easy access\n",
    "        variables = tf.get_collection( tf.GraphKeys.GLOBAL_VARIABLES, scope=self.name)\n",
    "        # GLOBAL_VARIABLES: the default collection of Variable objects, shared across distributed environment (model variables are subset of these).\n",
    "        self.vars = variables\n",
    "        for k in self.vars:\n",
    "            tf.logging.info((k.name, k.get_shape()))\n",
    "\n",
    "        # Build metrics\n",
    "        self._loss()\n",
    "        self._accuracy()\n",
    "        self._predict()\n",
    "\n",
    "        self.opt_op = self.optimizer.minimize(self.loss)\n",
    "\n",
    "    def _loss(self):\n",
    "        \"\"\"Construct the loss function.\"\"\"\n",
    "        # Weight decay loss\n",
    "        if self.weight_decay > 0.0:\n",
    "            for var in self.layers[0].vars.values():\n",
    "                self.loss += self.weight_decay * tf.nn.l2_loss(var)\n",
    "\n",
    "        # Cross entropy error\n",
    "        if self.multilabel:\n",
    "            self.loss += metrics.masked_sigmoid_cross_entropy(self.outputs, self.placeholders['labels'], self.placeholders['labels_mask'])\n",
    "        else:\n",
    "            self.loss += metrics.masked_softmax_cross_entropy(self.outputs, self.placeholders['labels'], self.placeholders['labels_mask'])\n",
    "\n",
    "    def _accuracy(self):\n",
    "        if self.multilabel:\n",
    "            # use the outputs directly, without any softmax or sigmoid\n",
    "            self.accuracy = metrics.masked_accuracy_multilabel(self.outputs, self.placeholders['labels'], self.placeholders['labels_mask'])\n",
    "        else:\n",
    "            self.accuracy = metrics.masked_accuracy(self.outputs, self.placeholders['labels'], self.placeholders['labels_mask'])\n",
    "\n",
    "    def _predict(self):\n",
    "        # this prediction to generate the possibility distribution is not used\n",
    "        if self.multilabel:\n",
    "            self.pred = tf.nn.sigmoid(self.outputs)\n",
    "        else:\n",
    "            self.pred = tf.nn.softmax(self.outputs)\n",
    "\n",
    "    def save(self, sess=None):\n",
    "        if not sess:\n",
    "            raise AttributeError('TensorFlow session not provided.')\n",
    "        saver = tf.train.Saver(self.vars)\n",
    "        save_path = saver.save(sess, './tmp/%s.ckpt' % self.name)\n",
    "        tf.logging.info('Model saved in file:', save_path)\n",
    "\n",
    "    def load(self, sess=None):\n",
    "        if not sess:\n",
    "            raise AttributeError('TensorFlow session not provided.')\n",
    "        saver = tf.train.Saver(self.vars)\n",
    "        save_path = './tmp/%s.ckpt' % self.name\n",
    "        saver.restore(sess, save_path)\n",
    "        tf.logging.info('Model restored from file:', save_path)\n",
    "\n",
    "\n",
    "class GCN(Model):\n",
    "    \"\"\"Implementation of GCN model.\"\"\"\n",
    "\n",
    "    def __init__(self, placeholders, input_dim, output_dim, hidden_neuron_num, learning_rate = 0.01, \n",
    "                 num_layers = 3, name = None, logging = False, multilabel = False, norm = False, precalc = False):\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.hidden1 = hidden_neuron_num\n",
    "        \n",
    "        super(GCN, self).__init__(weight_decay = 0, num_layers = num_layers, name = name, logging = logging, \\\n",
    "                                  multilabel = multilabel, norm = norm, precalc = precalc)\n",
    "\n",
    "        self.inputs = placeholders['features']\n",
    "        self.input_dim = input_dim\n",
    "#         self.output_dim = placeholders['labels'].get_shape().as_list()[1]\n",
    "        self.output_dim = output_dim\n",
    "    \n",
    "        self.placeholders = placeholders\n",
    "\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate = self.learning_rate)\n",
    "\n",
    "        self.build()\n",
    "\n",
    "    def _build(self):\n",
    "        self.layers.append(\n",
    "            GraphConvolution(\n",
    "                input_dim = self.input_dim if self.precalc else self.input_dim * 2,\n",
    "                output_dim = self.hidden1,\n",
    "                placeholders = self.placeholders,\n",
    "                act=tf.nn.relu,\n",
    "                dropout = True,\n",
    "                sparse_inputs = False,\n",
    "                logging = self.logging,\n",
    "                norm = self.norm,\n",
    "                precalc = self.precalc))\n",
    "        \n",
    "        for _ in range(self.num_layers - 2):\n",
    "            self.layers.append(\n",
    "              GraphConvolution(\n",
    "                  input_dim = self.hidden1 * 2,\n",
    "                  output_dim = self.hidden1,\n",
    "                  placeholders = self.placeholders,\n",
    "                  act=tf.nn.relu,\n",
    "                  dropout = True,\n",
    "                  sparse_inputs = False,\n",
    "                  logging = self.logging,\n",
    "                  norm = self.norm,\n",
    "                  precalc = False))\n",
    "            \n",
    "        self.layers.append(\n",
    "            GraphConvolution(\n",
    "                input_dim = self.hidden1 * 2,\n",
    "                output_dim = self.output_dim,\n",
    "                placeholders = self.placeholders,\n",
    "                act = lambda x: x,    # for the last layer, no relu\n",
    "                dropout = True,\n",
    "                logging = self.logging,\n",
    "                norm = False,\n",
    "                precalc = False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_scatter import scatter_add\n",
    "from torch_geometric.utils import add_remaining_self_loops\n",
    "\n",
    "def get_edge_weight(edge_index, num_nodes, edge_weight = None, improved = False, dtype=None, store_path='./tmp/'):\n",
    "    \"\"\"\n",
    "        edge_index(ndarray): undirected edge index (two-directions both included)\n",
    "        num_nodes(int):  number of nodes inside the graph\n",
    "        edge_weight(ndarray): if any weights already assigned, otherwise will be generated \n",
    "        improved(boolean):   may assign 2 to the self loop weight if true\n",
    "        store_path(string): the path of the folder to contain all the clustering information files\n",
    "    \"\"\"\n",
    "    # calculate the global graph properties, global edge weights\n",
    "    if edge_weight is None:\n",
    "        edge_weight = torch.ones((edge_index.size(1), ), dtype=dtype, device=edge_index.device)\n",
    "\n",
    "    fill_value = 1 if not improved else 2\n",
    "    # there are num_nodes self-loop edges added after the edge_index\n",
    "    edge_index, edge_weight = add_remaining_self_loops(edge_index, edge_weight, fill_value, num_nodes)\n",
    "\n",
    "    row, col = edge_index   \n",
    "    # row includes the starting points of the edges  (first row of edge_index)\n",
    "    # col includes the ending points of the edges   (second row of edge_index)\n",
    "\n",
    "    deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n",
    "    # row records the source nodes, which is the index we are trying to add\n",
    "    # deg will record the out-degree of each node of x_i in all edges (x_i, x_j) including self_loops\n",
    "\n",
    "    deg_inv_sqrt = deg.pow(-0.5)\n",
    "    deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "    # normalize the edge weight\n",
    "    normalized_edge_weight = deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n",
    "\n",
    "    # transfer from tensor to the numpy to construct the dict for the edge_weights\n",
    "    edge_index = edge_index.t().numpy()\n",
    "    normalized_edge_weight = normalized_edge_weight.numpy()\n",
    "\n",
    "    num_edge = edge_index.shape[0]\n",
    "\n",
    "    output = ([edge_index[i][0], edge_index[i][1], normalized_edge_weight[i]] for i in range(num_edge))\n",
    "\n",
    "    # output the edge weights as the csv file\n",
    "    input_edge_weight_txt_file = store_path + 'input_edge_weight_list.csv'\n",
    "    os.makedirs(os.path.dirname(input_edge_weight_txt_file), exist_ok=True)\n",
    "    with open(input_edge_weight_txt_file, 'w', newline='\\n') as fp:\n",
    "        wr = csv.writer(fp, delimiter = ' ')\n",
    "        for line in output:\n",
    "            wr.writerow(line)\n",
    "    return input_edge_weight_txt_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import metis\n",
    "import random\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import chain\n",
    "import scipy.sparse as sp\n",
    "\n",
    "def construct_adj(edges, nodes_count):\n",
    "    # Compressed Sparse Row matrix\n",
    "    # csr_matrix((data, ij), [shape=(M, N)])\n",
    "    # where data and ij satisfy the relationship a[ij[0, k], ij[1, k]] = data[k]\n",
    "    adj = sp.csr_matrix( ( np.ones((edges.shape[0]), dtype=np.float32), (edges[:, 0], edges[:, 1]) ), shape=(nodes_count, nodes_count) )\n",
    "    adj += adj.transpose()   # double the weight of each edge if it is two direction\n",
    "    return adj\n",
    "\n",
    "class ClusteringMachine(object):\n",
    "    \"\"\"\n",
    "    Clustering the graph, feature set and label. Performed on the CPU side\n",
    "    \"\"\"\n",
    "    def __init__(self, edge_index, features, label, tmp_folder = './tmp/', info_folder = './info/'):\n",
    "        \"\"\"\n",
    "        :param edge_index: COO format of the edge indices.\n",
    "        :param features: Feature matrix (ndarray).\n",
    "        :param label: label vector (ndarray).\n",
    "        :tmp_folder(string): the path of the folder to contain all the clustering information files\n",
    "        \"\"\"\n",
    "        self.features = features\n",
    "        self.label = label\n",
    "        self._set_sizes()\n",
    "        self.edge_index = edge_index\n",
    "        # store the information folder for memory tracing\n",
    "        self.tmp_folder = tmp_folder\n",
    "        self.info_folder = info_folder\n",
    "        \n",
    "        # first, use the get edge weights func to construct the two-sided self-loop added graph\n",
    "        edge_weight_file = self.tmp_folder + 'input_edge_weight_list.csv'\n",
    "        self.graph = nx.read_weighted_edgelist(edge_weight_file, create_using = nx.Graph, nodetype = int)\n",
    "        \n",
    "#         # second construct teh graph directly from the edge_list, here we are lacking the self-loop age\n",
    "#         tmp = edge_index.t().numpy().tolist()\n",
    "#         self.graph = nx.from_edgelist(tmp)\n",
    "        \n",
    "    def _set_sizes(self):\n",
    "        \"\"\"\n",
    "        Setting the feature and class count.\n",
    "        \"\"\"\n",
    "        self.node_count = self.features.shape[0]\n",
    "        self.feature_count = self.features.shape[1]    # features all always in the columns\n",
    "        self.label_count = len(np.unique(self.label.numpy()) )\n",
    "    \n",
    "    # 2) first assign train, test, validation nodes, split edges; this is based on the assumption that the clustering is no longer that important\n",
    "    def split_whole_nodes_edges_then_cluster(self, validation_ratio, test_ratio, normalize = False, precalc = True):\n",
    "        \"\"\"\n",
    "            Only split nodes\n",
    "            First create train-test splits, then split train and validation into different batch seeds\n",
    "            Input:  \n",
    "                1) ratio of test, validation\n",
    "                2) partition number of train nodes, test nodes, validation nodes\n",
    "            Output:\n",
    "                1) sg_validation_nodes_global, sg_train_nodes_global, sg_test_nodes_global\n",
    "        \"\"\"\n",
    "        relative_validation_ratio = (validation_ratio) / (1 - test_ratio)\n",
    "        \n",
    "        # first divide the nodes for the whole graph, result will always be a list of lists \n",
    "        model_nodes_global, self.test_nodes_global = train_test_split(list(self.graph.nodes()), test_size = test_ratio)\n",
    "        self.train_nodes_global, self.valid_nodes_global = train_test_split(model_nodes_global, test_size = relative_validation_ratio)\n",
    "        \n",
    "        edges = np.array(self.graph.edges(), dtype=np.int32)\n",
    "        self.feats = np.array(self.features.numpy(), dtype=np.float32)\n",
    "        \n",
    "        \n",
    "#         # normalize all the features\n",
    "#         if normalize:\n",
    "#             scaler = sklearn.preprocessing.StandardScaler()\n",
    "#             scaler.fit(feats)\n",
    "#             feats = scaler.transform(feats)\n",
    "        \n",
    "        classes = np.array(self.label.numpy(), dtype=np.int32)\n",
    "\n",
    "        # construct the 1-hot labels:\n",
    "        self.labels = np.zeros((self.node_count, self.label_count), dtype=np.float32)\n",
    "        for i, label in enumerate(classes):\n",
    "            self.labels[i, label] = 1\n",
    "            \n",
    "        self.full_adj = construct_adj(edges, self.node_count)\n",
    "    \n",
    "    def train_batch_generation(self, train_batch_num = 2, diag_lambda = -1, precalc = True, mini_cluster_num = 1):\n",
    "        # all the properties needed for the training\n",
    "        train_subgraph = self.graph.subgraph(self.train_nodes_global)\n",
    "        train_edges = np.array(train_subgraph.edges(), dtype=np.int32)\n",
    "        # check the double direction inside this adjacent matrix\n",
    "        train_adj = construct_adj(train_edges, self.node_count)\n",
    "        train_feats = self.feats[self.train_nodes_global]\n",
    "        y_train = np.zeros(self.labels.shape)\n",
    "        y_train[self.train_nodes_global, :] = self.labels[self.train_nodes_global, :]\n",
    "        train_mask = utils.sample_mask(self.train_nodes_global, self.labels.shape[0])\n",
    "        \n",
    "        if precalc:\n",
    "            train_feats = train_adj.dot(self.feats)   # calculate the feature matrix weighted by the edge weights\n",
    "            # numpy.hstack: Stack arrays in sequence horizontally (column wise).\n",
    "            train_feats = np.hstack((train_feats, self.feats))\n",
    "        visible_data = self.train_nodes_global\n",
    "        \n",
    "        # Partition graph and do preprocessing\n",
    "        if mini_cluster_num > 1:\n",
    "            _, parts = partition_utils.partition_graph(train_adj, visible_data, train_batch_num)\n",
    "            parts = [np.array(pt) for pt in parts]\n",
    "            return train_adj, parts, train_feats, y_train, train_mask\n",
    "        else:\n",
    "            (parts, features_batches, support_batches, y_train_batches, train_mask_batches) = \\\n",
    "            utils.preprocess(train_adj, train_feats, y_train, train_mask, visible_data, train_batch_num, diag_lambda)\n",
    "            return parts, features_batches, support_batches, y_train_batches, train_mask_batches\n",
    "    \n",
    "    \n",
    "    \n",
    "    def test_batch_generation(self, test_batch_num = 2, diag_lambda = -1, precalc = True):\n",
    "        # all the properties needed for the test\n",
    "        self.test_feats = self.feats   # why test gives the full feature of the whole graph\n",
    "        y_test = np.zeros(self.labels.shape)\n",
    "        y_test[self.test_nodes_global, :] = self.labels[self.test_nodes_global, :]\n",
    "        test_mask = utils.sample_mask(self.test_nodes_global, self.labels.shape[0])\n",
    "        if precalc:\n",
    "            self.test_feats = self.full_adj.dot(self.feats)\n",
    "            self.test_feats = np.hstack((self.test_feats, self.feats))\n",
    "            \n",
    "        (_, test_features_batches, test_support_batches, y_test_batches, test_mask_batches) = \\\n",
    "            utils.preprocess(self.full_adj, self.test_feats, y_test, test_mask, np.arange(self.node_count), test_batch_num, diag_lambda)\n",
    "        return test_features_batches, test_support_batches, y_test_batches, test_mask_batches, self.test_nodes_global\n",
    "        \n",
    "    def validation_batch_generation(self, valid_batch_num = 2, diag_lambda = -1):\n",
    "        y_val = np.zeros(self.labels.shape)\n",
    "        y_val[self.valid_nodes_global, :] = self.labels[self.valid_nodes_global, :]\n",
    "        val_mask = utils.sample_mask(self.valid_nodes_global, self.labels.shape[0])\n",
    "\n",
    "        (_, val_features_batches, val_support_batches, y_val_batches, val_mask_batches) = \\\n",
    "            utils.preprocess(self.full_adj, self.test_feats, y_val, val_mask, np.arange(self.node_count), valid_batch_num, diag_lambda)\n",
    "        return val_features_batches, val_support_batches, y_val_batches, val_mask_batches, self.valid_nodes_global\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the Graph_sage format data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy_from_metric(y_pred, y_true, labels_mask, multilabel):\n",
    "    \"\"\" \n",
    "        call the implemented metric masked accuracy inside the framework\n",
    "        For the multi-label: still use the 1-hot \n",
    "        y_pred: whole outputs, did not use any softmax or sigmoid activation, just use the direct final layer output\n",
    "        y_label: whole labels\n",
    "    \"\"\"\n",
    "    if multilabel:\n",
    "        # use the outputs directly, without any softmax or sigmoid\n",
    "        accuracy = metrics.masked_accuracy_multilabel(y_pred, y_true, labels_mask)\n",
    "    else:\n",
    "        accuracy = metrics.masked_accuracy(y_pred, y_true, labels_mask)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def test_customized_accuracy(y_pred, y_true, multilabel):\n",
    "    \"\"\" \n",
    "        Implement a customized version using the same logic\n",
    "        For the multi-label: still use the 1-hot \n",
    "        y_pred : the masked outputs, final layer of the GCN neural nets\n",
    "        y_label: the masked labels\n",
    "    \"\"\"\n",
    "    if multilabel:\n",
    "        y_pred[y_pred > 0] = 1\n",
    "        y_pred[y_pred <= 0] = 0\n",
    "        correct_prediction = tf.equal(y_pred, y_true)\n",
    "    else:\n",
    "        y_true = np.argmax(y_true, axis=1)\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "        correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.argmax(y_true, 1))\n",
    "    \n",
    "    accuracy_all = tf.cast(correct_prediction, tf.float32)\n",
    "#     sklearn.metrics.accuracy_score(y_true, y_pred, normalize=True)\n",
    "\n",
    "    return tf.reduce_mean(accuracy_all)\n",
    "\n",
    "\n",
    "def test_sklearn_accuracy(y_pred, y_true, multilabel):\n",
    "    \"\"\" \n",
    "        Implement a customized version using the same logic\n",
    "        For the multi-label: still use the 1-hot \n",
    "        y_pred : the masked outputs, final layer of the GCN neural nets\n",
    "        y_label: the masked labels\n",
    "    \"\"\"\n",
    "    if multilabel:\n",
    "        y_pred[y_pred > 0] = 1\n",
    "        y_pred[y_pred <= 0] = 0\n",
    "    else:\n",
    "        y_true = np.argmax(y_true, axis=1)\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    return sklearn.metrics.accuracy_score(y_true, y_pred, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model evaluation function\n",
    "def evaluate(sess, model, val_features_batches, val_support_batches,\n",
    "             y_val_batches, val_mask_batches, val_data, placeholders, multilabel = True):\n",
    "    \"\"\"evaluate GCN model.\n",
    "        sess:  graph session\n",
    "        model: \n",
    "    \n",
    "    \"\"\"\n",
    "#     print('inside the evaluation func:')\n",
    "    total_pred = []   # predicted classes from the trained model\n",
    "    total_lab = []    # true labels\n",
    "    total_loss = 0    # miss prediction \n",
    "    total_acc = 0     # accurately predicting the labels\n",
    "\n",
    "    num_batches = len(val_features_batches)\n",
    "    for i in range(num_batches):\n",
    "        features_b = val_features_batches[i]\n",
    "        support_b = val_support_batches[i]\n",
    "        y_val_b = y_val_batches[i]\n",
    "        val_mask_b = val_mask_batches[i]\n",
    "        num_data_b = np.sum(val_mask_b)\n",
    "        if num_data_b == 0:\n",
    "            # there is no validation data:\n",
    "            continue\n",
    "        else:\n",
    "            feed_dict = utils.construct_feed_dict(features_b, support_b, y_val_b, val_mask_b, placeholders)\n",
    "            outs = sess.run([model.loss, model.accuracy, model.outputs], feed_dict = feed_dict)\n",
    "\n",
    "        total_pred.append(outs[2][val_mask_b])\n",
    "        total_lab.append(y_val_b[val_mask_b])\n",
    "        \n",
    "#         # ============ debug purpose to test the accuracy metrics ====================\n",
    "#         print('For the test batch number # ', i)\n",
    "#         # debug here: to calculate each batch accuracy:\n",
    "#         # 1) the reference as from the tensorflow trained model : outs[1]\n",
    "#         print('current accuracy from model.accuracy: ', outs[1])\n",
    "        \n",
    "#         # 2) use the metric accuracy for masked data implemented in this framework\n",
    "#         calc_metric_accuracy = test_accuracy_from_metric(outs[2], y_val_b, val_mask_b, multilabel)\n",
    "#         # for each tensor operation, use the session.run to fetch the value\n",
    "#         calc_metric_accuracy = sess.run(calc_metric_accuracy, feed_dict = feed_dict)\n",
    "#         print('calculated accuracy from model.outputs: ', calc_metric_accuracy)\n",
    "        \n",
    "#         # 3) based on the logic we learn, build our own accuracy calcuation function: extrac all the mask value first\n",
    "#         local_pred = outs[2][val_mask_b]\n",
    "#         local_lab = y_val_b[val_mask_b]\n",
    "        \n",
    "#         custom_accuracy = test_customized_accuracy(local_pred, local_lab, multilabel)\n",
    "#         custom_accuracy = sess.run(custom_accuracy, feed_dict = feed_dict)\n",
    "#         print('customized accuracy from model.outputs: ', custom_accuracy)\n",
    "#         # call the customized metrics to calculate the accray with the masked data\n",
    "        \n",
    "#         # 4) Using the suspicious sklearn logic\n",
    "#         sklearn_accuracy = test_sklearn_accuracy(local_pred, local_lab, multilabel)\n",
    "#         print('sklearn_accuracy from model.outputs: ', sklearn_accuracy)\n",
    "#         # ============ End of debugging ======================\n",
    "        \n",
    "        total_loss += outs[0] * num_data_b\n",
    "        total_acc += outs[1] * num_data_b\n",
    "\n",
    "    total_pred = np.vstack(total_pred)  # \n",
    "    total_lab = np.vstack(total_lab)\n",
    "    loss = total_loss / len(val_data)\n",
    "    # this acc1 is calculated by hand\n",
    "    acc = total_acc / len(val_data)\n",
    "    # this acc is calculated from the sklearn\n",
    "#     print('total accumuated accuracy :')\n",
    "    micro, macro, acc_sklearn = utils.calc_f1(total_pred, total_lab, multilabel)\n",
    "#     print('\\nhand calculated loss rate is: ', loss)\n",
    "#     print('\\nhand calculated accuracy is: ', acc)\n",
    "#     print('sklearn calculated accuracy is: ', acc_sklearn)\n",
    "    return loss, acc, micro, macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aritificial convertion of single label accuracy to multi-label accuracy in 1-hot case\n",
    "def obtain_multi_label_accuracy(label_count, node_count, single_label_accuracy):\n",
    "    accurate_node = int(node_count * single_label_accuracy)\n",
    "    total_label = label_count * node_count\n",
    "    actual_accurate_label = accurate_node * label_count + (node_count - accurate_node) * (label_count - 2)\n",
    "    return actual_accurate_label / total_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusterGCNTrainer(object):\n",
    "    \"\"\"\n",
    "    Training a ClusterGCN.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim, num_layers = 3, hidden_neuron_num = 16, learning_rate = 0.001, \\\n",
    "                 logging = False, multilabel = True, norm = True, precalc = True):\n",
    "        \n",
    "        # Define self.placeholders\n",
    "        self.placeholders = {\n",
    "          'support':\n",
    "              tf.sparse_placeholder(tf.float32),\n",
    "          'features':\n",
    "              tf.placeholder(tf.float32),\n",
    "          'labels':\n",
    "              tf.placeholder(tf.float32),\n",
    "          'labels_mask':\n",
    "              tf.placeholder(tf.int32),\n",
    "          'dropout':\n",
    "              tf.placeholder_with_default(0., shape=()),\n",
    "    #       'num_features_nonzero':\n",
    "    #           tf.placeholder(tf.int32)  # helper variable for sparse dropout\n",
    "        }\n",
    "        \n",
    "        now_input_dim = 2 * input_dim if precalc else input_dim\n",
    "        self.model = GCN(self.placeholders,\n",
    "              input_dim = now_input_dim,\n",
    "              output_dim = output_dim,\n",
    "              hidden_neuron_num = hidden_neuron_num,\n",
    "              learning_rate = learning_rate,\n",
    "              logging = logging,\n",
    "              multilabel = multilabel,\n",
    "              norm = norm,\n",
    "              precalc = precalc,\n",
    "              num_layers = num_layers)\n",
    "        \n",
    "        self.multilabel = multilabel\n",
    "        self.precalc = precalc\n",
    "        \n",
    "    def train(self, clustering_machine, seed = 6, diag_lambda = 1, mini_cluster_num = 1, \\\n",
    "                               train_batch_num = 2, valid_batch_num = 2, \\\n",
    "                               epoch_num = 400, mini_epoch_num = 20, dropout = 0.3, train_save_name = './tmp/saver.txt'):\n",
    "        # load the needed data\n",
    "        # data for training:\n",
    "        if mini_cluster_num > 1:\n",
    "            train_adj, parts, train_feats, y_train, train_mask = \\\n",
    "                    clustering_machine.train_batch_generation(train_batch_num = train_batch_num,\n",
    "                                                diag_lambda = diag_lambda, precalc = self.precalc, mini_cluster_num = mini_cluster_num)\n",
    "        else:\n",
    "            parts, features_batches, support_batches, y_train_batches, train_mask_batches = \\\n",
    "                    clustering_machine.train_batch_generation(train_batch_num = train_batch_num, \\\n",
    "                                                diag_lambda = diag_lambda, precalc = self.precalc, mini_cluster_num = mini_cluster_num)\n",
    "        \n",
    "        \n",
    "        \n",
    "#         val_features_batches, val_support_batches, y_val_batches, val_mask_batches, val_data = \\\n",
    "#                     clustering_machine.validation_batch_generation(valid_batch_num = valid_batch_num, diag_lambda = diag_lambda)\n",
    "        \n",
    "        idx_parts = list(range(len(parts)))\n",
    "        \n",
    "        # Initialize session\n",
    "        sess = tf.Session()\n",
    "        tf.set_random_seed(seed)\n",
    "\n",
    "        # Init variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "        cost_val = []\n",
    "        self.time_train_total = 0.0\n",
    "        # Train model:  epoch_num\n",
    "        epoch_partition = epoch_num // mini_epoch_num\n",
    "        for epoch in range(epoch_partition):\n",
    "            t0 = time.time()\n",
    "            np.random.shuffle(idx_parts)\n",
    "\n",
    "            # recombine mini-clusters to form larger batches\n",
    "            if mini_cluster_num > 1:\n",
    "                (features_batches, support_batches, y_train_batches,\n",
    "                train_mask_batches) = utils.preprocess_multicluster(\n",
    "                   train_adj, parts, train_feats, y_train, train_mask,\n",
    "                   train_batch_num, mini_cluster_num, diag_lambda)\n",
    "\n",
    "                for pid in range(len(features_batches)):\n",
    "                    # Use preprocessed batch data\n",
    "                    features_b = features_batches[pid]\n",
    "                    support_b = support_batches[pid]\n",
    "                    y_train_b = y_train_batches[pid]\n",
    "                    train_mask_b = train_mask_batches[pid]\n",
    "                    # Construct feed dictionary\n",
    "                    feed_dict = utils.construct_feed_dict(features_b, support_b, y_train_b, train_mask_b, self.placeholders)\n",
    "\n",
    "                    feed_dict.update({self.placeholders['dropout']: dropout})\n",
    "                    for _ in range(mini_epoch_num):\n",
    "                        # Training step\n",
    "                        outs = sess.run([self.model.opt_op, self.model.loss, self.model.accuracy], feed_dict=feed_dict)\n",
    "            else:\n",
    "                np.random.shuffle(idx_parts)\n",
    "\n",
    "                for pid in idx_parts:\n",
    "                    # Use preprocessed batch data\n",
    "                    features_b = features_batches[pid]\n",
    "                    support_b = support_batches[pid]\n",
    "                    y_train_b = y_train_batches[pid]\n",
    "                    train_mask_b = train_mask_batches[pid]\n",
    "\n",
    "                    # Construct feed dictionary\n",
    "                    feed_dict = utils.construct_feed_dict(features_b, support_b, y_train_b, train_mask_b, self.placeholders)\n",
    "\n",
    "                    # investigate the constructed matrix for inputs:\n",
    "                    feed_dict.update({self.placeholders['dropout'] : dropout})\n",
    "                    for _ in range(mini_epoch_num):\n",
    "                        # Training step:\n",
    "                        outs = sess.run([self.model.opt_op, self.model.loss, self.model.accuracy], feed_dict=feed_dict)\n",
    "\n",
    "            self.time_train_total += time.time() - t0\n",
    "            \n",
    "            print_str = 'Epoch: %04d ' % (epoch + 1) + 'training time: {:.5f} '.format(\n",
    "                self.time_train_total) + 'train_acc= {:.5f} '.format(outs[2])\n",
    "\n",
    "#             # Validation\n",
    "#             cost, acc, micro, macro = evaluate(sess, self.model, val_features_batches, val_support_batches, y_val_batches,\n",
    "#                                              val_mask_batches, val_data, self.placeholders)\n",
    "#             cost_val.append(cost)\n",
    "#             print_str += 'val_acc= {:.5f} '.format(acc) + 'mi F1= {:.5f} ma F1= {:.5f} '.format(micro, macro)\n",
    "\n",
    "#             tf.logging.info(print_str)\n",
    "\n",
    "#             if epoch > early_stopping_epoch_thresh and cost_val[-1] > np.mean(\n",
    "#                 cost_val[-(early_stopping_epoch_thresh + 1):-1]):\n",
    "#                 tf.logging.info('Early stopping...')\n",
    "#                 break\n",
    "\n",
    "        tf.logging.info('Optimization Finished!')\n",
    "\n",
    "        # Save model\n",
    "        saver.save(sess, train_save_name)\n",
    "        \n",
    "    def test(self, clustering_machine, seed = 6, diag_lambda = 1, \\\n",
    "                               test_batch_num = 2, train_save_name = './tmp/saver.txt'):\n",
    "        \n",
    "        test_features_batches, test_support_batches, y_test_batches, test_mask_batches, test_data = \\\n",
    "                    clustering_machine.test_batch_generation(test_batch_num = test_batch_num, diag_lambda = diag_lambda, precalc = self.precalc)\n",
    "        \n",
    "        with tf.device('/cpu:0'):\n",
    "            sess_cpu = tf.Session(config=tf.ConfigProto(device_count={'GPU': 0}))  # set the usable GPU count as 0, therefore only use CPU\n",
    "            sess_cpu.run(tf.global_variables_initializer())\n",
    "            saver = tf.train.Saver()\n",
    "            # load the trained model into the current session through the saved file: train_save_name which contains the trained model saver\n",
    "            saver.restore(sess_cpu, train_save_name)   \n",
    "            # The Saver class adds ops to save and restore variables to and from checkpoints. It also provides convenience methods to run these ops.\n",
    "\n",
    "            # Testing\n",
    "            test_cost, test_acc, micro, macro = evaluate(\n",
    "                sess_cpu, self.model, test_features_batches, test_support_batches, y_test_batches, test_mask_batches, test_data, self.placeholders, multilabel = self.multilabel)\n",
    "\n",
    "            print_str = 'Test set results: ' + 'cost= {:.5f} '.format(test_cost) + 'accuracy= {:.5f} '.format(test_acc) + 'mi F1= {:.5f} ma F1= {:.5f}'.format(micro, macro)\n",
    "            tf.logging.info(print_str)\n",
    "#             print(print_str)\n",
    "        \n",
    "        converted_acc = obtain_multi_label_accuracy(clustering_machine.label_count, clustering_machine.node_count, micro)\n",
    "        \n",
    "        print(\"micro-F1 is : {} and the accuracy is : {}\".format(micro, test_acc))\n",
    "        print(\"artificially converted multi-label accuracy is : {}\".format(converted_acc))\n",
    "        \n",
    "        return micro, test_acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use trivial dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 2.],\n",
      "        [0., 3.],\n",
      "        [0., 4.],\n",
      "        [0., 5.],\n",
      "        [0., 6.],\n",
      "        [0., 7.],\n",
      "        [0., 8.],\n",
      "        [0., 9.]]) torch.Size([10, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./res_save_batch/clustering/input_edge_weight_list.csv'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_index = torch.tensor([[0, 1, 1, 3, 1, 2, 4, 2, 4, 6, 6, 7, 7, 9, 2, 5, 9, 8], \n",
    "                           [1, 0, 3, 1, 2, 1, 2, 4, 6, 4, 7, 6, 9, 7, 5, 2, 8, 9]])\n",
    "# features = torch.rand(10, 3)\n",
    "features = torch.tensor([[0, 0], [0, 1], [0, 2], [0, 3], [0, 4],  \n",
    "                           [0, 5], [0, 6], [0, 7], [0, 8], [0, 9]], dtype = torch.float)\n",
    "# label = torch.tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "label = torch.tensor([0, 1, 1, 0, 1, 1, 1, 0, 0, 0])\n",
    "print(features, features.shape)\n",
    "\n",
    "clustering_folder = './res_save_batch/clustering/'\n",
    "check_folder_exist(clustering_folder)\n",
    "clustering_file_name = clustering_folder + 'check_clustering_machine.txt'\n",
    "os.makedirs(os.path.dirname(clustering_folder), exist_ok=True)\n",
    "info_folder = './res_save_batch/info/'\n",
    "check_folder_exist(info_folder)\n",
    "os.makedirs(os.path.dirname(info_folder), exist_ok=True)\n",
    "\n",
    "node_count = features.shape[0]\n",
    "get_edge_weight(edge_index, node_count, store_path = clustering_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'sparse_placeholder'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-caf4d3051b5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m gcn_trainer = ClusterGCNTrainer(clustering_machine.feature_count, clustering_machine.label_count, num_layers = 3, hidden_neuron_num = 16, learning_rate = 0.001, \\\n\u001b[0;32m----> 5\u001b[0;31m                  logging = False, multilabel = False, norm = True, precalc = True)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-e7744cad4a1e>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dim, output_dim, num_layers, hidden_neuron_num, learning_rate, logging, multilabel, norm, precalc)\u001b[0m\n\u001b[1;32m      9\u001b[0m         self.placeholders = {\n\u001b[1;32m     10\u001b[0m           \u001b[0;34m'support'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m               \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_placeholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m           \u001b[0;34m'features'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m               \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'sparse_placeholder'"
     ]
    }
   ],
   "source": [
    "clustering_machine = ClusteringMachine(edge_index, features, label, clustering_folder, info_folder = clustering_folder)\n",
    "clustering_machine.split_whole_nodes_edges_then_cluster(0.25, 0.25, normalize = False, precalc = True)\n",
    "\n",
    "gcn_trainer = ClusterGCNTrainer(clustering_machine.feature_count, clustering_machine.label_count, num_layers = 3, hidden_neuron_num = 16, learning_rate = 0.001, \\\n",
    "                 logging = False, multilabel = False, norm = True, precalc = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start the training of the model:\n",
    "gcn_trainer.train(clustering_machine, seed = 6, diag_lambda = -1, mini_cluster_num = 4, \\\n",
    "                               train_batch_num = 2, epoch_num = 400, dropout = 0.3, train_save_name = './tmp/saver.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro-F1 is : 0.3333333333333333 and the accuracy is : 0.3333333333333333\n",
      "artificially converted multi-label accuracy is : 0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangli/anaconda3/envs/tensorflow_1_pytorch_geometric/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.3333333333333333, 0.3333333333333333)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start to test the trained model:\n",
    "gcn_trainer.test(clustering_machine, seed = 6, diag_lambda = -1, test_batch_num = 2, train_save_name = './tmp/saver.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cluster_train_valid_batch_run(clustering_machine, num_layers = 3, hidden_neuron_num = 32, learning_rate = 0.001, \\\n",
    "                 logging = False, multilabel = True, norm = True, precalc = True, diag_lambda = -1, \\\n",
    "                 mini_cluster_num = 1, train_batch_num = 2, test_batch_num = 2, \\\n",
    "                 epoch_num = 400, mini_epoch_num = 20, dropout = 0.3, train_save_name = './tmp/saver.txt'):\n",
    "    \"\"\"\n",
    "        # Run the mini-batch model (train and validate both in batches)\n",
    "        Tuning parameters:  dropout, lr (learning rate), weight_decay: l2 regularization\n",
    "        return: validation accuracy value, validation F-1 value, time_training (ms), time_data_load (ms)\n",
    "    \"\"\"\n",
    "    \n",
    "    gcn_trainer = ClusterGCNTrainer(clustering_machine.feature_count, clustering_machine.label_count, \\\n",
    "                num_layers = num_layers, hidden_neuron_num = hidden_neuron_num, learning_rate = learning_rate, \\\n",
    "                 logging = logging, multilabel = multilabel, norm = norm, precalc = precalc)\n",
    "    \n",
    "    # start to train the model\n",
    "    gcn_trainer.train(clustering_machine, seed = 6, diag_lambda = diag_lambda, mini_cluster_num = mini_cluster_num, \\\n",
    "                train_batch_num = train_batch_num, \\\n",
    "                epoch_num = epoch_num, mini_epoch_num = mini_epoch_num, dropout = dropout, train_save_name = train_save_name)\n",
    "    \n",
    "    # start to test the model\n",
    "    validation_F1, validation_accuracy = gcn_trainer.test(clustering_machine, seed = 6, \\\n",
    "                diag_lambda = diag_lambda, test_batch_num = test_batch_num, train_save_name = train_save_name)\n",
    "    \n",
    "    time_train_total = gcn_trainer.time_train_total\n",
    "#     time_data_load = gcn_trainer.time_train_load_data\n",
    "    \n",
    "    return validation_accuracy, validation_F1, time_train_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"To test one single model for different parameter values\"\"\"\n",
    "def execute_tuning(tune_params, clustering_machine, repeate_time = 7, num_layers = 3, hidden_neuron_num = 32, learning_rate = 0.001, \\\n",
    "                 logging = False, multilabel = True, norm = True, precalc = True, diag_lambda = -1, \\\n",
    "                 mini_cluster_num = 1, train_batch_num = 2, test_batch_num = 2, \\\n",
    "                 epoch_num = 400, mini_epoch_num = 20, dropout = 0.3, train_save_name = './tmp/saver.txt'):\n",
    "    \"\"\"\n",
    "        Tune all the hyperparameters\n",
    "        1) learning rate\n",
    "        2) dropout\n",
    "        3) layer unit number\n",
    "        4) weight decay\n",
    "    \"\"\"\n",
    "    validation_accuracy = {}\n",
    "    validation_f1 = {}\n",
    "    time_total_train = {}\n",
    "#     time_data_load = {}\n",
    "    \n",
    "    res = [{tune_val : Cluster_train_valid_batch_run(clustering_machine, num_layers = num_layers, hidden_neuron_num = hidden_neuron_num, learning_rate = learning_rate, \\\n",
    "                 logging = logging, multilabel = multilabel, norm = norm, precalc = precalc, diag_lambda = diag_lambda, \\\n",
    "                 mini_cluster_num = mini_cluster_num, train_batch_num = train_batch_num, test_batch_num = test_batch_num, \\\n",
    "                 epoch_num = epoch_num, mini_epoch_num = tune_val, dropout = dropout, train_save_name = train_save_name) for tune_val in tune_params} for i in range(repeate_time)]\n",
    "    \n",
    "    for i, ref in enumerate(res):\n",
    "        validation_accuracy[i] = {tune_val : res_lst[0] for tune_val, res_lst in ref.items()}\n",
    "        validation_f1[i] = {tune_val : res_lst[1] for tune_val, res_lst in ref.items()}\n",
    "        time_total_train[i] = {tune_val : res_lst[2] for tune_val, res_lst in ref.items()}\n",
    "#         time_data_load[i] = {tune_val : res_lst[3] for tune_val, res_lst in ref.items()}\n",
    "        \n",
    "    return validation_accuracy, validation_f1, time_total_train\n",
    "\n",
    "def store_data_multi_tuning(tune_params, target, data_name, img_path, comments):\n",
    "    \"\"\"\n",
    "        tune_params: is the tuning parameter list\n",
    "        target: is the result, here should be F1-score, accuraycy, load time, train time\n",
    "    \"\"\"\n",
    "    run_ids = sorted(target.keys())   # key is the run_id\n",
    "    run_data = {'run_id': run_ids}\n",
    "    # the key can be converted to string or not: i.e. str(tune_val)\n",
    "    # here we keep it as integer such that we want it to follow order\n",
    "    tmp = {tune_val : [target[run_id][tune_val] for run_id in run_ids] for tune_val in tune_params}  # the value is list\n",
    "    run_data.update(tmp)\n",
    "    \n",
    "    pickle_filename = img_path + data_name + '_' + comments + '.pkl'\n",
    "    os.makedirs(os.path.dirname(pickle_filename), exist_ok=True)\n",
    "    df = pd.DataFrame(data=run_data, dtype=np.int32)\n",
    "    df.to_pickle(pickle_filename)\n",
    "    return pickle_filename\n",
    "\n",
    "def draw_data_multi_tests(pickle_filename, data_name, comments, xlabel, ylabel):\n",
    "    df = pd.read_pickle(pickle_filename)\n",
    "    df_reshape = df.melt('run_id', var_name = 'model', value_name = ylabel)\n",
    "\n",
    "    plt.clf()\n",
    "    plt.figure()\n",
    "    sns.set(style='whitegrid')\n",
    "    g = sns.catplot(x=\"model\", y=ylabel, kind='box', data=df_reshape)\n",
    "    g.despine(left=True)\n",
    "    g.fig.suptitle(data_name + ' ' + ylabel + ' ' + comments)\n",
    "    g.set_xlabels(xlabel)\n",
    "    g.set_ylabels(ylabel)\n",
    "\n",
    "    img_name = pickle_filename[:-4] + '_img'\n",
    "    os.makedirs(os.path.dirname(img_name), exist_ok=True)\n",
    "    plt.savefig(img_name, bbox_inches='tight')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_tune_param(data, data_name, image_data_path, tmp_folder, partition_nums, layers, hidden_neuron_num = 32, test_batch_num = 2):\n",
    "    node_count = data.x.shape[0]\n",
    "    get_edge_weight(data.edge_index, node_count, store_path = tmp_folder)\n",
    "    \n",
    "    for partn in partition_nums:\n",
    "        for layer_num in layers:\n",
    "            net_layer = layer_num - 1\n",
    "            # Set the tune parameters and name\n",
    "            tune_name = 'batch_epoch_num'\n",
    "            tune_params = [400, 200, 100, 50, 20, 10, 5, 1]\n",
    "#             tune_name = 'weight_decay'\n",
    "#             tune_params = [0.0001, 0.001, 0.01, 0.1]\n",
    "\n",
    "            img_path = image_data_path + 'cluster_num_' + str(partn) + '/' + 'net_layer_' + str(net_layer) + '/'\n",
    "            img_path += 'tune_' + tune_name + '/'  # further subfolder for different task\n",
    "            print('Start tuning for tuning param: ' + tune_name + ' partition num: ' + str(partn) + 'net_layer_' + str(net_layer) )  \n",
    "            \n",
    "            clustering_machine = ClusteringMachine(data.edge_index, data.x, data.y, tmp_folder, info_folder = image_data_path)\n",
    "            clustering_machine.split_whole_nodes_edges_then_cluster(0.05, 0.85, normalize = False, precalc = True)\n",
    "            \n",
    "            validation_accuracy, validation_f1, time_total_train = \\\n",
    "                execute_tuning(tune_params, clustering_machine, repeate_time = 7, num_layers = layer_num, hidden_neuron_num = hidden_neuron_num, learning_rate = 0.001, \\\n",
    "                 logging = False, multilabel = True, norm = True, precalc = True, diag_lambda = 1, \\\n",
    "                 mini_cluster_num = 32, train_batch_num = partn, test_batch_num = test_batch_num, \\\n",
    "                 epoch_num = 400, mini_epoch_num = 20, dropout = 0.3, train_save_name = tmp_folder + 'saver.txt')\n",
    "            \n",
    "\n",
    "            validation_accuracy = store_data_multi_tuning(tune_params,validation_accuracy, data_name, img_path, 'accuracy_cluster_num_' + str(partn))\n",
    "            draw_data_multi_tests(validation_accuracy, data_name, 'Train_cluster_num_' + str(partn) + '_net_layer_' + str(net_layer), tune_name, 'Accuracy')\n",
    "\n",
    "            validation_f1 = store_data_multi_tuning(tune_params, validation_f1, data_name, img_path, 'validation_cluster_num_' + str(partn) )\n",
    "            draw_data_multi_tests(validation_f1, data_name, 'Train_cluster_num_' + str(partn) + '_net_layer_' + str(net_layer), tune_name, 'F1 score')\n",
    "\n",
    "            time_train = store_data_multi_tuning(tune_params, time_total_train, data_name, img_path, 'train_time_cluster_num_' + str(partn) )\n",
    "            draw_data_multi_tests(time_train, data_name, 'train_time_cluster_num_' + str(partn) + '_net_layer_' + str(net_layer), tune_name, 'Train Time (ms)')\n",
    "\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Pytorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_data_root = '/media/xiangli/storage1/projects/tmpdata/'\n",
    "test_folder_name = 'test_multi_label/mini_cluster/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch_geometric.datasets import Planetoid\n",
    "data_name = 'Cora'\n",
    "dataset = Planetoid(root = local_data_root + 'Planetoid/Cora', name=data_name)\n",
    "data = dataset[0]\n",
    "image_data_path = './results/' + data_name + '/' + test_folder_name\n",
    "# set the current folder as the intermediate data folder so that we can easily copy either clustering \n",
    "intermediate_data_folder = './'\n",
    "\n",
    "partition_nums = [4]\n",
    "layers = [3]\n",
    "tmp_folder = './tmp/'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_count = data.x.shape[0]\n",
    "get_edge_weight(data.edge_index, node_count, store_path = tmp_folder)\n",
    "\n",
    "clustering_machine = ClusteringMachine(data.edge_index, data.x, data.y, tmp_folder, info_folder = image_data_path)\n",
    "\n",
    "clustering_machine.split_whole_nodes_edges_then_cluster(0.05, 0.85, normalize = False, precalc = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NON multi-label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro-F1 is : 0.7936576889661164 and the accuracy is : 0.7936578462633436\n",
      "artificially converted multi-label accuracy is : 0.9410213125131884\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7936576889661164, 0.7936578462633436)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gcn_trainer = ClusterGCNTrainer(clustering_machine.feature_count, clustering_machine.label_count, num_layers = 3, hidden_neuron_num = 32, learning_rate = 0.001, \\\n",
    "                 logging = False, multilabel = False, norm = True, precalc = True)\n",
    "\n",
    "gcn_trainer.train(clustering_machine, seed = 6, diag_lambda = 1, mini_cluster_num = 32, \\\n",
    "                               train_batch_num = 2, epoch_num = 400, mini_epoch_num = 20, dropout = 0.3, train_save_name = './tmp/saver.txt')\n",
    "\n",
    "# print(node_count)\n",
    "# start to test the trained model:\n",
    "gcn_trainer.test(clustering_machine, seed = 6, diag_lambda = 1, test_batch_num = 4, train_save_name = './tmp/saver.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro-F1 is : 0.7878787878787878 and the accuracy is : 0.9435286698757722\n",
      "artificially converted multi-label accuracy is : 0.9393331926566786\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7878787878787878, 0.9435286698757722)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gcn_trainer = ClusterGCNTrainer(clustering_machine.feature_count, clustering_machine.label_count, num_layers = 3, hidden_neuron_num = 32, learning_rate = 0.001, \\\n",
    "                 logging = False, multilabel = True, norm = True, precalc = True)\n",
    "\n",
    "gcn_trainer.train(clustering_machine, seed = 6, diag_lambda = 1, mini_cluster_num = 32, \\\n",
    "                               train_batch_num = 2, epoch_num = 400, mini_epoch_num = 20, dropout = 0.3, train_save_name = './tmp/saver.txt')\n",
    "\n",
    "# print(node_count)\n",
    "# start to test the trained model:\n",
    "gcn_trainer.test(clustering_machine, seed = 6, diag_lambda = 1, test_batch_num = 4, train_save_name = './tmp/saver.txt')\n",
    "\"\"\"\n",
    "    This artificial convertion does not make much sense, since multi-label is using a different loss-function for prediction\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start tuning for tuning param: batch_epoch_num partition num: 4net_layer_3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_tune_param(data, data_name, image_data_path, tmp_folder, partition_nums, layers, hidden_neuron_num = 32, test_batch_num = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free GPU memory\n",
    "# !(nvidia-smi | grep 'python' | awk '{ print $3 }' | xargs -n1 kill -9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the torch.tensor\n",
    "import torch\n",
    "import numpy as np\n",
    "# a = torch.tensor([[1, 2], [2, 3]])\n",
    "# b = torch.tensor([[4, 2], [2, 3]])\n",
    "\n",
    "a = np.array([[1, 2], [2, 3]])\n",
    "b = np.array([[4, 2], [2, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n"
     ]
    }
   ],
   "source": [
    "def binary_acc(y_test, y_pred):\n",
    "    \"\"\"\n",
    "        y_test (np.array) : the true label for the nodes\n",
    "        y_pred (np.array) : predicted tags for the nodes\n",
    "    \"\"\"\n",
    "    ave_loss = (y_test == y_pred).mean(dtype=np.float).item() \n",
    "    return ave_loss\n",
    "\n",
    "print(binary_acc(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(binary_acc(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,) (4,)\n",
      "0.75\n"
     ]
    }
   ],
   "source": [
    "d = a.flatten()\n",
    "e = b.flatten()\n",
    "print(d.shape, e.shape)\n",
    "print(accuracy_score(d, e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_geom_tensorflow_2]",
   "language": "python",
   "name": "conda-env-pytorch_geom_tensorflow_2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
