{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only apply support in the hidden embeddings. For feature matrix, concatenate the previous layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# package from the pytorch side\n",
    "import copy\n",
    "import csv\n",
    "import sys\n",
    "import torch\n",
    "from pytorch_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/xiangli/anaconda3/envs/pytorch_geom_tensorflow_2/lib/python3.7/site-packages/tensorflow_core/python/compat/v2_compat.py:88: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import os\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "\n",
    "import networkx as nx\n",
    "import metis\n",
    "import sklearn\n",
    "import time\n",
    "# import models\n",
    "import numpy as np\n",
    "import partition_utils\n",
    "import utils\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n",
      "Num CPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "# tf.compat.v1.logging.ERROR\n",
    "# verbose information: tf.logging.INFO  \n",
    "\n",
    "tf.logging.set_verbosity(tf.compat.v1.logging.ERROR)  # Sets the threshold for what messages will be logged.\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Num CPUs Available: \", len(tf.config.experimental.list_physical_devices('CPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCN model used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inits\n",
    "import metrics\n",
    "\n",
    "def sparse_dropout(x, keep_prob, noise_shape):\n",
    "    \"\"\"Dropout for sparse tensors.\"\"\"\n",
    "    random_tensor = keep_prob\n",
    "    random_tensor += tf.compat.v1.random_uniform(noise_shape)\n",
    "    dropout_mask = tf.compat.v1.cast(tf.compat.v1.floor(random_tensor), dtype=tf.compat.v1.bool)\n",
    "    pre_out = tf.compat.v1.sparse_retain(x, dropout_mask)\n",
    "    return pre_out * (1. / keep_prob)\n",
    "\n",
    "\n",
    "def dot(x, y, sparse=False):\n",
    "    \"\"\"Wrapper for tf.compat.v1.matmul (sparse vs dense).\"\"\"\n",
    "    if sparse:\n",
    "        res = tf.compat.v1.sparse_tensor_dense_matmul(x, y)\n",
    "        # Please note that one and only one of the inputs MUST be a SparseTensor and the other MUST be a dense matrix.\n",
    "    else:\n",
    "        res = tf.compat.v1.matmul(x, y)\n",
    "    return res\n",
    "\n",
    "\n",
    "def layernorm(x, offset, scale):\n",
    "    \"\"\"\n",
    "        tf.nn.moments(x, axes, shift=None, keepdims=False, name=None)\n",
    "        The mean and variance are calculated by aggregating the contents of x across axes\n",
    "        keepdims:  \tproduce moments with the same dimensionality as the input.\n",
    "    \"\"\"\n",
    "    mean, variance = tf.compat.v1.nn.moments(x, axes=[1], keep_dims=True)\n",
    "    return tf.compat.v1.nn.batch_normalization(x, mean, variance, offset, scale, 1e-9)\n",
    "\n",
    "\n",
    "class Layer(object):\n",
    "    \"\"\"Base layer class.\n",
    "\n",
    "    Defines basic API for all layer objects.\n",
    "    Implementation inspired by keras (http://keras.io).\n",
    "\n",
    "    # Properties\n",
    "        name: String, defines the variable scope of the layer.\n",
    "        logging: Boolean, switches Tensorflow histogram logging on/off\n",
    "\n",
    "    # Methods\n",
    "        _call(inputs): Defines computation graph of layer\n",
    "            (i.e. takes input, returns output)\n",
    "        __call__(inputs): Wrapper for _call()\n",
    "        _log_vars(): Log all variables\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name = None, logging = False):\n",
    "        \n",
    "        if not name:\n",
    "            layer = self.__class__.__name__.lower()\n",
    "            name = layer + '_' \n",
    "\n",
    "        self.name = name\n",
    "        \n",
    "        self.vars = {}\n",
    "        self.logging = logging\n",
    "        \n",
    "        self.sparse_inputs = False\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        return inputs\n",
    "\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        with tf.compat.v1.name_scope(self.name):\n",
    "            if self.logging and not self.sparse_inputs:\n",
    "                tf.compat.v1.summary.histogram(self.name + '/inputs', inputs)\n",
    "\n",
    "        outputs = self._call(inputs)\n",
    "\n",
    "        if self.logging:\n",
    "            tf.compat.v1.summary.histogram(self.name + '/outputs', outputs)\n",
    "        return outputs\n",
    "\n",
    "    def _log_vars(self):\n",
    "        for var in self.vars:\n",
    "            tf.compat.v1.summary.histogram(self.name + '/vars/' + var, self.vars[var])\n",
    "\n",
    "            \n",
    "class GraphConvolution(Layer):\n",
    "    \"\"\"\n",
    "        Graph convolution layer.\n",
    "        Here each layer concatenate the embedding alongside the inputs\n",
    "    \"\"\"\n",
    "    # input_dim is usually 2 times of the original column number of the previous layer embedding matrix\n",
    "    def __init__(self, input_dim, output_dim, placeholders,\n",
    "               dropout = 0.0, \n",
    "                sparse_inputs = False, \n",
    "                act = tf.nn.relu, \n",
    "                bias = False, \n",
    "                featureless=False, \n",
    "                norm = False,    # whether to do the batch_normalization after each layer\n",
    "                precalc = False,\n",
    "                name = None, \n",
    "                logging = False):\n",
    "        super(GraphConvolution, self).__init__(name = name, logging = logging)\n",
    "#         print('During the constructor of GCN layer, input dim: {} ; output dim: {}'.format(input_dim, output_dim))\n",
    "\n",
    "        if dropout:\n",
    "            self.dropout = placeholders['dropout']\n",
    "        else:\n",
    "            self.dropout = 0.\n",
    "\n",
    "        self.act = act\n",
    "        self.support = placeholders['support']  # this is the edge weight for the subgraph\n",
    "        # in the original Tensorflow version, we use the edge weight generated from the whole graph\n",
    "        # for now we use the edge weights generated from each subgraph temporarily\n",
    "        self.sparse_inputs = sparse_inputs\n",
    "        self.featureless = featureless\n",
    "        self.bias = bias\n",
    "        self.norm = norm\n",
    "        self.precalc = precalc\n",
    "\n",
    "        # helper variable for sparse dropout\n",
    "        # self.num_features_nonzero = placeholders['num_features_nonzero']\n",
    "\n",
    "        with tf.variable_scope(self.name + '_vars'):\n",
    "            self.vars['weights'] = inits.glorot([input_dim, output_dim], name='weights')\n",
    "            if self.bias:\n",
    "                self.vars['bias'] = inits.zeros([output_dim], name='bias')\n",
    "\n",
    "            if self.norm:\n",
    "                self.vars['offset'] = inits.zeros([1, output_dim], name='offset')\n",
    "                self.vars['scale'] = inits.ones([1, output_dim], name='scale')\n",
    "\n",
    "        if self.logging:\n",
    "            self._log_vars()\n",
    "    \n",
    "\n",
    "    def _call(self, inputs):\n",
    "        # 1) calculate the current embeddings: current_embedding <- dot(edge_weights, embeddings/feature) \n",
    "        # 2) concatenate(current_embedding, previous embedding)\n",
    "        # 3) dropout embedding\n",
    "        # 4) dot (current_embedding, weights)   return matrix of shape : (:, out_channels)\n",
    "        # 5) add bias\n",
    "        # 6) batch normalization\n",
    "        # 7) Activatoin (relu)\n",
    "        \n",
    "        x = inputs\n",
    "\n",
    "        # convolve\n",
    "        if self.precalc:\n",
    "            support = x\n",
    "        else:\n",
    "            support = dot(self.support, x, sparse=True)   \n",
    "            # self.support is the sparse matrix , x is the dense\n",
    "            # SparseTensor format expected by sparse_tensor_dense_matmul: sp_a (indices, values), already converted to tuple\n",
    "            \n",
    "            # self.support is the normalized edge weights, \n",
    "            # support is the current embeddings, \n",
    "            # x is the previous embeddings, or the input features for the first layer\n",
    "            support = tf.concat((support, x), axis=1)   # concatenate the embedding from last layer\n",
    "            # support: is the returned dense matrix\n",
    "\n",
    "        # dropout\n",
    "        support = tf.nn.dropout(support, rate = self.dropout)\n",
    "        \"\"\"\n",
    "            More precisely: With probability rate elements of x are set to 0. \n",
    "            The remaining elements are scaled up by 1.0 / (1 - rate), so that the expected value is preserved.\n",
    "        \"\"\"\n",
    "        \n",
    "#         tf.Print(support, [support], \"During the call of GCN layer, final input to be multiplied with weights: \")\n",
    "\n",
    "#         print('\\n inside the call of convolutiongraph layer: ')\n",
    "#         print('support vecotr dimension is : {} ;'.format(support.shape), 'weight matrix dimension is : {} ;'.format(self.vars['weights'].shape))\n",
    "        \n",
    "        # here support is the \n",
    "        output = dot(support, self.vars['weights'], sparse = self.sparse_inputs)   # sparse == false\n",
    "\n",
    "        # bias\n",
    "        if self.bias:\n",
    "            output += self.vars['bias']\n",
    "\n",
    "        with tf.variable_scope(self.name + '_vars'):\n",
    "            if self.norm:\n",
    "                output = layernorm(output, self.vars['offset'], self.vars['scale'])\n",
    "\n",
    "        return self.act(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "a = tf.constant([[2, 3], [4, 5]])\n",
    "print(a)\n",
    "mean, variance = tf.nn.moments(a, axes=[1], keep_dims=True)\n",
    "print(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct GCN layer nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Collections of different Models.\"\"\"\n",
    "\n",
    "\n",
    "class Model(object):\n",
    "    \"\"\"Model class to be inherited.\"\"\"\n",
    "\n",
    "    def __init__(self, weight_decay = 0, \n",
    "                 num_layers = 2, name = None, logging = False, \n",
    "                 multilabel = False, norm = False, precalc = False):\n",
    "        \n",
    "        self.weight_decay = weight_decay\n",
    "        \n",
    "        \n",
    "        if not name:\n",
    "            name = self.__class__.__name__.lower()\n",
    "        self.name = name\n",
    "\n",
    "        self.logging = logging\n",
    "\n",
    "        self.vars = {}\n",
    "        self.placeholders = {}\n",
    "\n",
    "        self.layers = []\n",
    "        self.activations = []\n",
    "\n",
    "        self.inputs = None\n",
    "        self.outputs = None\n",
    "\n",
    "        self.loss = 0\n",
    "        self.accuracy = 0\n",
    "        self.pred = 0\n",
    "        self.optimizer = None\n",
    "        self.opt_op = None\n",
    "        \n",
    "        self.multilabel = multilabel\n",
    "        self.norm = norm\n",
    "        self.precalc = precalc\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    def _build(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def build(self):\n",
    "        \"\"\"Wrapper for _build().\"\"\"\n",
    "        with tf.variable_scope(self.name):\n",
    "            self._build()\n",
    "\n",
    "        # Build sequential layer model\n",
    "        self.activations.append(self.inputs)\n",
    "        \n",
    "        # debug to output the embedding:\n",
    "#         self.hidden1 = layer(self.activations[-1])\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            hidden = layer(self.activations[-1])\n",
    "        \n",
    "            if isinstance(hidden, tuple):\n",
    "                tf.logging.info('{} shape = {}'.format(layer.name, hidden[0].get_shape()))\n",
    "            else:\n",
    "                tf.logging.info('{} shape = {}'.format(layer.name, hidden.get_shape()))\n",
    "\n",
    "            self.activations.append(hidden)\n",
    "            \n",
    "        self.outputs = self.activations[-1]\n",
    "\n",
    "        # Store model variables for easy access\n",
    "        variables = tf.get_collection( tf.GraphKeys.GLOBAL_VARIABLES, scope=self.name)\n",
    "        # GLOBAL_VARIABLES: the default collection of Variable objects, shared across distributed environment (model variables are subset of these).\n",
    "        self.vars = variables\n",
    "        for k in self.vars:\n",
    "            tf.logging.info((k.name, k.get_shape()))\n",
    "\n",
    "        # Build metrics\n",
    "        self._loss()\n",
    "        self._accuracy()\n",
    "        self._predict()\n",
    "\n",
    "        self.opt_op = self.optimizer.minimize(self.loss)\n",
    "\n",
    "    def _loss(self):\n",
    "        \"\"\"Construct the loss function.\"\"\"\n",
    "        # Weight decay loss\n",
    "        if self.weight_decay > 0.0:\n",
    "            for var in self.layers[0].vars.values():\n",
    "                self.loss += self.weight_decay * tf.nn.l2_loss(var)\n",
    "\n",
    "        # Cross entropy error\n",
    "        if self.multilabel:\n",
    "            self.loss += metrics.masked_sigmoid_cross_entropy(self.outputs, self.placeholders['labels'], self.placeholders['labels_mask'])\n",
    "        else:\n",
    "            self.loss += metrics.masked_softmax_cross_entropy(self.outputs, self.placeholders['labels'], self.placeholders['labels_mask'])\n",
    "\n",
    "    def _accuracy(self):\n",
    "        if self.multilabel:\n",
    "            # use the outputs directly, without any softmax or sigmoid\n",
    "            self.accuracy = metrics.masked_accuracy_multilabel(self.outputs, self.placeholders['labels'], self.placeholders['labels_mask'])\n",
    "        else:\n",
    "            self.accuracy = metrics.masked_accuracy(self.outputs, self.placeholders['labels'], self.placeholders['labels_mask'])\n",
    "\n",
    "    def _predict(self):\n",
    "        # this prediction to generate the possibility distribution is not used\n",
    "        if self.multilabel:\n",
    "            self.pred = tf.nn.sigmoid(self.outputs)\n",
    "        else:\n",
    "            self.pred = tf.nn.softmax(self.outputs)\n",
    "\n",
    "    def save(self, sess=None):\n",
    "        if not sess:\n",
    "            raise AttributeError('TensorFlow session not provided.')\n",
    "        saver = tf.train.Saver(self.vars)\n",
    "        save_path = saver.save(sess, './tmp/%s.ckpt' % self.name)\n",
    "        tf.logging.info('Model saved in file:', save_path)\n",
    "\n",
    "    def load(self, sess=None):\n",
    "        if not sess:\n",
    "            raise AttributeError('TensorFlow session not provided.')\n",
    "        saver = tf.train.Saver(self.vars)\n",
    "        save_path = './tmp/%s.ckpt' % self.name\n",
    "        saver.restore(sess, save_path)\n",
    "        tf.logging.info('Model restored from file:', save_path)\n",
    "\n",
    "\n",
    "class GCN(Model):\n",
    "    \"\"\"Implementation of GCN model.\"\"\"\n",
    "\n",
    "    def __init__(self, placeholders, input_dim, output_dim, hidden_neuron_num, learning_rate = 0.01, \n",
    "                 num_layers = 3, name = None, logging = False, multilabel = False, norm = False, precalc = False):\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.hidden1 = hidden_neuron_num\n",
    "        \n",
    "        super(GCN, self).__init__(weight_decay = 0, num_layers = num_layers, name = name, logging = logging, \\\n",
    "                                  multilabel = multilabel, norm = norm, precalc = precalc)\n",
    "\n",
    "        self.inputs = placeholders['features']\n",
    "        self.input_dim = input_dim\n",
    "#         self.output_dim = placeholders['labels'].get_shape().as_list()[1]\n",
    "        self.output_dim = output_dim\n",
    "    \n",
    "        self.placeholders = placeholders\n",
    "\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate = self.learning_rate)\n",
    "\n",
    "        self.build()\n",
    "\n",
    "    def _build(self):\n",
    "         # so here at least there are two layers...\n",
    "        self.layers.append(\n",
    "            GraphConvolution(\n",
    "                input_dim = self.input_dim if self.precalc else self.input_dim,  # double of the num of columns of original graph feature matrix\n",
    "                output_dim = self.hidden1,\n",
    "                placeholders = self.placeholders,\n",
    "                act=tf.nn.relu,\n",
    "                dropout = True,\n",
    "                sparse_inputs = False,\n",
    "                logging = self.logging,\n",
    "                norm = self.norm,\n",
    "                precalc = self.precalc))\n",
    "        \n",
    "        for _ in range(self.num_layers - 2):\n",
    "            self.layers.append(\n",
    "              GraphConvolution(\n",
    "                  input_dim = self.hidden1 * 2,\n",
    "                  output_dim = self.hidden1,\n",
    "                  placeholders = self.placeholders,\n",
    "                  act=tf.nn.relu,\n",
    "                  dropout = True,\n",
    "                  sparse_inputs = False,\n",
    "                  logging = self.logging,\n",
    "                  norm = self.norm,\n",
    "                  precalc = False))\n",
    "        \n",
    "        # for the last layer: no normalizaton, no activation, can still have dropout !!!\n",
    "        self.layers.append(\n",
    "            GraphConvolution(\n",
    "                input_dim = self.hidden1 * 2,\n",
    "                output_dim = self.output_dim,\n",
    "                placeholders = self.placeholders,\n",
    "                act = lambda x: x,    # for the last layer, no relu\n",
    "                dropout = True,\n",
    "                logging = self.logging,\n",
    "                norm = False,\n",
    "                precalc = False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_scatter import scatter_add\n",
    "from torch_geometric.utils import add_remaining_self_loops\n",
    "\n",
    "def get_edge_weight(edge_index, num_nodes, edge_weight = None, improved = False, dtype=None, store_path='./tmp/'):\n",
    "    \"\"\"\n",
    "        Purpose: get the orignal weights inside the graph\n",
    "        \n",
    "        edge_index(ndarray): undirected edge index (two-directions both included)\n",
    "        num_nodes(int):  number of nodes inside the graph\n",
    "        edge_weight(ndarray): if any weights already assigned, otherwise will be generated \n",
    "        improved(boolean):   may assign 2 to the self loop weight if true\n",
    "        store_path(string): the path of the folder to contain all the clustering information files\n",
    "    \"\"\"\n",
    "    # calculate the global graph properties, global edge weights\n",
    "    if edge_weight is None:\n",
    "        edge_weight = torch.ones((edge_index.size(1), ), dtype=dtype, device=edge_index.device)\n",
    "\n",
    "    fill_value = 1 if not improved else 2\n",
    "    # there are num_nodes self-loop edges added after the edge_index\n",
    "    edge_index, edge_weight = add_remaining_self_loops(edge_index, edge_weight, fill_value, num_nodes)\n",
    "\n",
    "    row, col = edge_index   \n",
    "    # row includes the starting points of the edges  (first row of edge_index)\n",
    "    # col includes the ending points of the edges   (second row of edge_index)\n",
    "\n",
    "    deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n",
    "    # row records the source nodes, which is the index we are trying to add\n",
    "    # deg will record the out-degree of each node of x_i in all edges (x_i, x_j) including self_loops\n",
    "\n",
    "    deg_inv_sqrt = deg.pow(-0.5)\n",
    "    deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "    # normalize the edge weight\n",
    "    normalized_edge_weight = deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n",
    "\n",
    "    # transfer from tensor to the numpy to construct the dict for the edge_weights\n",
    "    edge_index = edge_index.t().numpy()\n",
    "    normalized_edge_weight = normalized_edge_weight.numpy()\n",
    "\n",
    "    num_edge = edge_index.shape[0]\n",
    "\n",
    "    output = ([edge_index[i][0], edge_index[i][1], normalized_edge_weight[i]] for i in range(num_edge))\n",
    "\n",
    "    # output the edge weights as the csv file\n",
    "    input_edge_weight_txt_file = store_path + 'input_edge_weight_list.csv'\n",
    "    os.makedirs(os.path.dirname(input_edge_weight_txt_file), exist_ok=True)\n",
    "    with open(input_edge_weight_txt_file, 'w', newline='\\n') as fp:\n",
    "        wr = csv.writer(fp, delimiter = ' ')\n",
    "        for line in output:\n",
    "            wr.writerow(line)\n",
    "    return input_edge_weight_txt_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import metis\n",
    "import random\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import chain\n",
    "import scipy.sparse as sp\n",
    "\n",
    "def construct_adj(edges, nodes_count):\n",
    "    \"\"\"\n",
    "        This is to genrate the edge weights inside the graph\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compressed Sparse Row matrix\n",
    "    # csr_matrix((data, ij), [shape=(M, N)])\n",
    "    # where data and ij satisfy the relationship a[ij[0, k], ij[1, k]] = data[k]\n",
    "    adj = sp.csr_matrix( ( np.ones((edges.shape[0]), dtype=np.float32), (edges[:, 0], edges[:, 1]) ), shape=(nodes_count, nodes_count) )\n",
    "    adj += adj.transpose()   # double the weight of each edge if it is two direction\n",
    "    return adj\n",
    "\n",
    "class ClusteringMachine(object):\n",
    "    \"\"\"\n",
    "    Clustering the graph, feature set and label. Performed on the CPU side\n",
    "    \"\"\"\n",
    "    def __init__(self, edge_index, features, label, tmp_folder = './tmp/', info_folder = './info/', multilabel = True):\n",
    "        \"\"\"\n",
    "        :param edge_index (torch.tensor): COO format of the edge indices.\n",
    "        :param features (torch.tensor): Feature matrix .\n",
    "        :param label (torch.tensor): label vector .\n",
    "        :tmp_folder(string): the path of the folder to contain all the clustering information files\n",
    "        :multilabel (bool) : whether this is a multilabel task or not\n",
    "        \"\"\"\n",
    "        self.features = features\n",
    "        self.label = label\n",
    "        self.multilabel = multilabel\n",
    "        self._set_sizes()\n",
    "        self.edge_index = edge_index\n",
    "        # store the information folder for memory tracing\n",
    "        self.tmp_folder = tmp_folder\n",
    "        self.info_folder = info_folder\n",
    "        \n",
    "        # first, use the get edge weights func to construct the two-sided self-loop added graph\n",
    "        edge_weight_file = self.tmp_folder + 'input_edge_weight_list.csv'\n",
    "        self.graph = nx.read_weighted_edgelist(edge_weight_file, create_using = nx.Graph, nodetype = int)\n",
    "        \n",
    "#         # second construct teh graph directly from the edge_list, here we are lacking the self-loop age\n",
    "#         tmp = edge_index.t().numpy().tolist()\n",
    "#         self.graph = nx.from_edgelist(tmp)\n",
    "        \n",
    "    def _set_sizes(self):\n",
    "        \"\"\"\n",
    "        Setting the feature and class count.\n",
    "        \"\"\"\n",
    "        self.node_count = self.features.shape[0]\n",
    "        self.feature_count = self.features.shape[1]    # features all always in the columns\n",
    "        if self.multilabel:\n",
    "            self.label_count = self.label.shape[1]\n",
    "        else:\n",
    "            self.label_count = len(np.unique(self.label.numpy()) )\n",
    "    \n",
    "    # 2) first assign train, test, validation nodes, split edges; this is based on the assumption that the clustering is no longer that important\n",
    "    def split_whole_nodes_edges_then_cluster(self, validation_ratio, test_ratio, normalize = False, precalc = True):\n",
    "        \"\"\"\n",
    "            Only split nodes\n",
    "            First create train-test splits, then split train and validation into different batch seeds\n",
    "            Input:  \n",
    "                1) ratio of test, validation\n",
    "                2) partition number of train nodes, test nodes, validation nodes\n",
    "            Output:\n",
    "                1) sg_validation_nodes_global, sg_train_nodes_global, sg_test_nodes_global\n",
    "        \"\"\"\n",
    "        relative_validation_ratio = (validation_ratio) / (1 - test_ratio)\n",
    "        \n",
    "        # first divide the nodes for the whole graph, result will always be a list of lists \n",
    "        model_nodes_global, self.test_nodes_global = train_test_split(list(self.graph.nodes()), test_size = test_ratio)\n",
    "        self.train_nodes_global, self.valid_nodes_global = train_test_split(model_nodes_global, test_size = relative_validation_ratio)\n",
    "        \n",
    "        edges = np.array(self.graph.edges(), dtype=np.int32)\n",
    "        self.feats = np.array(self.features.numpy(), dtype=np.float32)\n",
    "        \n",
    "        \n",
    "        # normalize all the features\n",
    "        if normalize:\n",
    "            scaler = sklearn.preprocessing.StandardScaler()\n",
    "            scaler.fit(self.feats)\n",
    "            self.feats = scaler.transform(self.feats)\n",
    "        \n",
    "        # this is for the multi-class case\n",
    "        if self.multilabel:\n",
    "            self.labels = np.array(self.label.numpy(), dtype=np.float32)\n",
    "        else:\n",
    "            classes = np.array(self.label.numpy(), dtype=np.int32)\n",
    "            # construct the 1-hot labels:\n",
    "            self.labels = np.zeros((self.node_count, self.label_count), dtype=np.float32)\n",
    "            for i, label in enumerate(classes):\n",
    "                self.labels[i, label] = 1\n",
    "            \n",
    "        self.full_adj = construct_adj(edges, self.node_count)\n",
    "    \n",
    "    def train_batch_generation(self, mini_cluster_num = 2, diag_lambda = -1, precalc = True, mini_block_size = 1):\n",
    "        # all the properties needed for the training\n",
    "        train_subgraph = self.graph.subgraph(self.train_nodes_global)\n",
    "        train_edges = np.array(train_subgraph.edges(), dtype=np.int32)\n",
    "        # check the double direction inside this adjacent matrix\n",
    "        train_adj = construct_adj(train_edges, self.node_count)\n",
    "        train_feats = self.feats[self.train_nodes_global]\n",
    "        \n",
    "        y_train = np.zeros(self.labels.shape)\n",
    "        y_train[self.train_nodes_global, :] = self.labels[self.train_nodes_global, :]\n",
    "        train_mask = utils.sample_mask(self.train_nodes_global, self.labels.shape[0])\n",
    "        \n",
    "        if precalc:\n",
    "            train_feats = train_adj.dot(self.feats)   # calculate the feature matrix weighted by the edge weights\n",
    "            # numpy.hstack: Stack arrays in sequence horizontally (column wise).\n",
    "#             train_feats = np.hstack((train_feats, self.feats))\n",
    "        visible_data = self.train_nodes_global\n",
    "        \n",
    "        # Partition graph and do preprocessing\n",
    "        if mini_block_size > 1:\n",
    "            _, parts = partition_utils.partition_graph(train_adj, visible_data, mini_cluster_num)\n",
    "            parts = [np.array(pt) for pt in parts]\n",
    "            # train_adj: adjacent matrix of train nodes\n",
    "            # parts: groups of data divided into different train batches\n",
    "            # only features for train nodes\n",
    "            return train_adj, parts, train_feats, y_train, train_mask\n",
    "        else:\n",
    "            (parts, features_batches, support_batches, y_train_batches, train_mask_batches) = \\\n",
    "            utils.preprocess(train_adj, train_feats, y_train, train_mask, visible_data, mini_cluster_num, diag_lambda)\n",
    "            return parts, features_batches, support_batches, y_train_batches, train_mask_batches\n",
    "    \n",
    "    \n",
    "    \n",
    "    def test_batch_generation(self, test_batch_num = 2, diag_lambda = -1, precalc = True):\n",
    "        # all the properties needed for the test\n",
    "        self.test_feats = self.feats   # why test gives the full feature of the whole graph\n",
    "        y_test = np.zeros(self.labels.shape)\n",
    "        y_test[self.test_nodes_global, :] = self.labels[self.test_nodes_global, :]\n",
    "        test_mask = utils.sample_mask(self.test_nodes_global, self.labels.shape[0])\n",
    "        if precalc:\n",
    "            self.test_feats = self.full_adj.dot(self.feats)\n",
    "#             self.test_feats = np.hstack((self.test_feats, self.feats))\n",
    "            \n",
    "        (_, test_features_batches, test_support_batches, y_test_batches, test_mask_batches) = \\\n",
    "            utils.preprocess(self.full_adj, self.test_feats, y_test, test_mask, np.arange(self.node_count), test_batch_num, diag_lambda)\n",
    "        return test_features_batches, test_support_batches, y_test_batches, test_mask_batches, self.test_nodes_global\n",
    "        \n",
    "    def validation_batch_generation(self, valid_batch_num = 2, diag_lambda = -1, precalc = True):\n",
    "        # all the properties needed for the test\n",
    "        self.val_feats = self.feats   # why test gives the full feature of the whole graph\n",
    "        y_val = np.zeros(self.labels.shape)\n",
    "        y_val[self.valid_nodes_global, :] = self.labels[self.valid_nodes_global, :]\n",
    "        val_mask = utils.sample_mask(self.valid_nodes_global, self.labels.shape[0])\n",
    "        # start to precalculate :\n",
    "        if precalc:\n",
    "            self.val_feats = self.full_adj.dot(self.feats)\n",
    "#             self.val_feats = np.hstack((self.val_feats, self.feats))\n",
    "            \n",
    "        (_, val_features_batches, val_support_batches, y_val_batches, val_mask_batches) = \\\n",
    "            utils.preprocess(self.full_adj, self.val_feats, y_val, val_mask, np.arange(self.node_count), valid_batch_num, diag_lambda)\n",
    "        return val_features_batches, val_support_batches, y_val_batches, val_mask_batches, self.valid_nodes_global\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the Graph_sage format data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy_from_metric(y_pred, y_true, labels_mask, multilabel):\n",
    "    \"\"\" \n",
    "        call the implemented metric masked accuracy inside the framework\n",
    "        For the multi-label: still use the 1-hot \n",
    "        y_pred: whole outputs, did not use any softmax or sigmoid activation, just use the direct final layer output\n",
    "        y_label: whole labels\n",
    "    \"\"\"\n",
    "    if multilabel:\n",
    "        # use the outputs directly, without any softmax or sigmoid\n",
    "        accuracy = metrics.masked_accuracy_multilabel(y_pred, y_true, labels_mask)\n",
    "    else:\n",
    "        accuracy = metrics.masked_accuracy(y_pred, y_true, labels_mask)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def test_customized_accuracy(y_pred, y_true, multilabel):\n",
    "    \"\"\" \n",
    "        Implement a customized version using the same logic\n",
    "        For the multi-label: still use the 1-hot \n",
    "        y_pred : the masked outputs, final layer of the GCN neural nets\n",
    "        y_label: the masked labels\n",
    "    \"\"\"\n",
    "    if multilabel:\n",
    "        y_pred[y_pred > 0] = 1\n",
    "        y_pred[y_pred <= 0] = 0\n",
    "        correct_prediction = tf.equal(y_pred, y_true)\n",
    "    else:\n",
    "        y_true = np.argmax(y_true, axis=1)\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "        correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.argmax(y_true, 1))\n",
    "    \n",
    "    accuracy_all = tf.cast(correct_prediction, tf.float32)\n",
    "#     sklearn.metrics.accuracy_score(y_true, y_pred, normalize=True)\n",
    "\n",
    "    return tf.reduce_mean(accuracy_all)\n",
    "\n",
    "\n",
    "def test_sklearn_accuracy(y_pred, y_true, multilabel):\n",
    "    \"\"\" \n",
    "        Implement a customized version using the same logic\n",
    "        For the multi-label: still use the 1-hot \n",
    "        y_pred : the masked outputs, final layer of the GCN neural nets\n",
    "        y_label: the masked labels\n",
    "    \"\"\"\n",
    "    if multilabel:\n",
    "        y_pred[y_pred > 0] = 1\n",
    "        y_pred[y_pred <= 0] = 0\n",
    "    else:\n",
    "        y_true = np.argmax(y_true, axis=1)\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    return sklearn.metrics.accuracy_score(y_true, y_pred, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model evaluation function\n",
    "def evaluate(sess, model, val_features_batches, val_support_batches,\n",
    "             y_val_batches, val_mask_batches, val_data, placeholders, multilabel = True):\n",
    "    \"\"\"evaluate GCN model.\n",
    "        sess:  graph session\n",
    "        model: \n",
    "    \n",
    "    \"\"\"\n",
    "#     print('inside the evaluation func:')\n",
    "    total_pred = []   # predicted classes from the trained model\n",
    "    total_lab = []    # true labels\n",
    "    total_loss = 0    # miss prediction \n",
    "    total_acc = 0     # accurately predicting the labels\n",
    "\n",
    "    num_batches = len(val_features_batches)\n",
    "    for i in range(num_batches):\n",
    "        features_b = val_features_batches[i]\n",
    "        support_b = val_support_batches[i]\n",
    "        y_val_b = y_val_batches[i]\n",
    "        val_mask_b = val_mask_batches[i]\n",
    "        num_data_b = np.sum(val_mask_b)\n",
    "        if num_data_b == 0:\n",
    "            # there is no validation data:\n",
    "            continue\n",
    "        else:\n",
    "            feed_dict = utils.construct_feed_dict(features_b, support_b, y_val_b, val_mask_b, placeholders)\n",
    "            outs = sess.run([model.loss, model.accuracy, model.outputs], feed_dict = feed_dict)\n",
    "\n",
    "        total_pred.append(outs[2][val_mask_b])\n",
    "        total_lab.append(y_val_b[val_mask_b])\n",
    "        \n",
    "#         # ============ debug purpose to test the accuracy metrics ====================\n",
    "#         print('For the test batch number # ', i)\n",
    "#         # debug here: to calculate each batch accuracy:\n",
    "#         # 1) the reference as from the tensorflow trained model : outs[1]\n",
    "#         print('current accuracy from model.accuracy: ', outs[1])\n",
    "        \n",
    "#         # 2) use the metric accuracy for masked data implemented in this framework\n",
    "#         calc_metric_accuracy = test_accuracy_from_metric(outs[2], y_val_b, val_mask_b, multilabel)\n",
    "#         # for each tensor operation, use the session.run to fetch the value\n",
    "#         calc_metric_accuracy = sess.run(calc_metric_accuracy, feed_dict = feed_dict)\n",
    "#         print('calculated accuracy from model.outputs: ', calc_metric_accuracy)\n",
    "        \n",
    "#         # 3) based on the logic we learn, build our own accuracy calcuation function: extrac all the mask value first\n",
    "#         local_pred = outs[2][val_mask_b]\n",
    "#         local_lab = y_val_b[val_mask_b]\n",
    "        \n",
    "#         custom_accuracy = test_customized_accuracy(local_pred, local_lab, multilabel)\n",
    "#         custom_accuracy = sess.run(custom_accuracy, feed_dict = feed_dict)\n",
    "#         print('customized accuracy from model.outputs: ', custom_accuracy)\n",
    "#         # call the customized metrics to calculate the accray with the masked data\n",
    "        \n",
    "#         # 4) Using the suspicious sklearn logic\n",
    "#         sklearn_accuracy = test_sklearn_accuracy(local_pred, local_lab, multilabel)\n",
    "#         print('sklearn_accuracy from model.outputs: ', sklearn_accuracy)\n",
    "#         # ============ End of debugging ======================\n",
    "        \n",
    "        total_loss += outs[0] * num_data_b\n",
    "        total_acc += outs[1] * num_data_b\n",
    "\n",
    "    total_pred = np.vstack(total_pred)  # \n",
    "    total_lab = np.vstack(total_lab)\n",
    "    loss = total_loss / len(val_data)\n",
    "    # this acc1 is calculated by hand\n",
    "    acc = total_acc / len(val_data)\n",
    "    # this acc is calculated from the sklearn\n",
    "#     print('total accumuated accuracy :')\n",
    "    micro, macro, acc_sklearn = utils.calc_f1(total_pred, total_lab, multilabel)\n",
    "#     print('\\nhand calculated loss rate is: ', loss)\n",
    "#     print('\\nhand calculated accuracy is: ', acc)\n",
    "#     print('sklearn calculated accuracy is: ', acc_sklearn)\n",
    "    return loss, acc, micro, macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aritificial convertion of single label accuracy to multi-label accuracy in 1-hot case\n",
    "def obtain_multi_label_accuracy(label_count, node_count, single_label_accuracy):\n",
    "    accurate_node = int(node_count * single_label_accuracy)\n",
    "    total_label = label_count * node_count\n",
    "    actual_accurate_label = accurate_node * label_count + (node_count - accurate_node) * (label_count - 2)\n",
    "    return actual_accurate_label / total_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusterGCNTrainer(object):\n",
    "    \"\"\"\n",
    "    Training a ClusterGCN.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim, num_layers = 3, hidden_neuron_num = 16, learning_rate = 0.001, \\\n",
    "                 logging = False, multilabel = True, norm = True, precalc = True):\n",
    "        \n",
    "        # Define self.placeholders\n",
    "        self.placeholders = {\n",
    "          'support':\n",
    "              tf.sparse_placeholder(tf.float32),\n",
    "          'features':\n",
    "              tf.placeholder(tf.float32),\n",
    "          'labels':\n",
    "              tf.placeholder(tf.float32),\n",
    "          'labels_mask':\n",
    "              tf.placeholder(tf.int32),\n",
    "          'dropout':\n",
    "              tf.placeholder_with_default(0., shape=()),\n",
    "    #       'num_features_nonzero':\n",
    "    #           tf.placeholder(tf.int32)  # helper variable for sparse dropout\n",
    "        }\n",
    "        \n",
    "        now_input_dim = input_dim if precalc else input_dim\n",
    "        self.model = GCN(self.placeholders,\n",
    "              input_dim = now_input_dim,\n",
    "              output_dim = output_dim,\n",
    "              hidden_neuron_num = hidden_neuron_num,\n",
    "              learning_rate = learning_rate,\n",
    "              logging = logging,\n",
    "              multilabel = multilabel,\n",
    "              norm = norm,\n",
    "              precalc = precalc,\n",
    "              num_layers = num_layers)\n",
    "        \n",
    "        self.multilabel = multilabel\n",
    "        self.precalc = precalc\n",
    "        \n",
    "    def train(self, clustering_machine, seed = 6, diag_lambda = 1, mini_block_size = 1, \\\n",
    "                               mini_cluster_num = 2, valid_batch_num = 2, \\\n",
    "                               epoch_num = 400, mini_epoch_num = 20, dropout = 0.3, train_save_name = './tmp/saver.txt', validation_period = 10):\n",
    "        # load the needed data\n",
    "        # data for training:\n",
    "        if mini_block_size > 1:\n",
    "            train_adj, parts, train_feats, y_train, train_mask = \\\n",
    "                    clustering_machine.train_batch_generation(mini_cluster_num = mini_cluster_num,\n",
    "                                                diag_lambda = diag_lambda, precalc = self.precalc, mini_block_size = mini_block_size)\n",
    "        else:\n",
    "            parts, features_batches, support_batches, y_train_batches, train_mask_batches = \\\n",
    "                    clustering_machine.train_batch_generation(mini_cluster_num = mini_cluster_num, \\\n",
    "                                                diag_lambda = diag_lambda, precalc = self.precalc, mini_block_size = mini_block_size)\n",
    "        \n",
    "        \n",
    "        \n",
    "        val_features_batches, val_support_batches, y_val_batches, val_mask_batches, val_data = \\\n",
    "                    clustering_machine.validation_batch_generation(valid_batch_num = valid_batch_num, diag_lambda = diag_lambda)\n",
    "        \n",
    "        idx_parts = list(range(len(parts)))\n",
    "        \n",
    "        # Initialize session\n",
    "        sess = tf.Session()\n",
    "        tf.set_random_seed(seed)\n",
    "\n",
    "        # Init variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "        cost_val = []\n",
    "        self.time_train_total = 0.0\n",
    "        # Train model:  epoch_num\n",
    "        epoch_partition = epoch_num // mini_epoch_num\n",
    "        for epoch in range(epoch_partition):\n",
    "            t0 = time.time()\n",
    "            np.random.shuffle(idx_parts)\n",
    "\n",
    "            # recombine mini-clusters to form larger batches\n",
    "            if mini_block_size > 1:\n",
    "                (features_batches, support_batches, y_train_batches,\n",
    "                train_mask_batches) = utils.preprocess_multicluster(\n",
    "                   train_adj, parts, train_feats, y_train, train_mask,\n",
    "                   mini_cluster_num, mini_block_size, diag_lambda)\n",
    "\n",
    "                for pid in range(len(features_batches)):\n",
    "                    # Use preprocessed batch data\n",
    "                    features_b = features_batches[pid]\n",
    "                    support_b = support_batches[pid]\n",
    "                    y_train_b = y_train_batches[pid]\n",
    "                    train_mask_b = train_mask_batches[pid]\n",
    "                    # Construct feed dictionary\n",
    "                    feed_dict = utils.construct_feed_dict(features_b, support_b, y_train_b, train_mask_b, self.placeholders)\n",
    "\n",
    "                    feed_dict.update({self.placeholders['dropout']: dropout})\n",
    "                    for real_mini_epoch in range(mini_epoch_num):\n",
    "                        # Training step\n",
    "                        outs = sess.run([self.model.opt_op, self.model.loss, self.model.accuracy], feed_dict = feed_dict)\n",
    "                        real_epoch_num = epoch * mini_epoch_num + real_mini_epoch + 1\n",
    "                        if real_epoch_num % validation_period == 0:\n",
    "                            # Validation\n",
    "                            validation_cost, validation_acc, validation_micro, validation_macro = \\\n",
    "                                                    evaluate( sess, self.model, val_features_batches, val_support_batches, y_val_batches,\n",
    "                                                             val_mask_batches, val_data, self.placeholders, multilabel = self.multilabel)\n",
    "                            print('During Validation epoch {}: val_acc= {:.5f} '.format(real_epoch_num, validation_acc) + 'micro F1= {:.5f} macro F1= {:.5f} '.format(validation_micro, validation_macro) )\n",
    "            else:\n",
    "                np.random.shuffle(idx_parts)\n",
    "\n",
    "                for pid in idx_parts:\n",
    "                    # Use preprocessed batch data\n",
    "                    features_b = features_batches[pid]\n",
    "                    support_b = support_batches[pid]\n",
    "                    y_train_b = y_train_batches[pid]\n",
    "                    train_mask_b = train_mask_batches[pid]\n",
    "\n",
    "                    # Construct feed dictionary\n",
    "                    feed_dict = utils.construct_feed_dict(features_b, support_b, y_train_b, train_mask_b, self.placeholders)\n",
    "\n",
    "                    # investigate the constructed matrix for inputs:\n",
    "                    feed_dict.update({self.placeholders['dropout'] : dropout})\n",
    "                    for real_mini_epoch in range(mini_epoch_num):\n",
    "                        # Training step:\n",
    "                        outs = sess.run([self.model.opt_op, self.model.loss, self.model.accuracy], feed_dict=feed_dict)\n",
    "                        real_epoch_num = epoch * mini_epoch_num + real_mini_epoch + 1\n",
    "                        if real_epoch_num % validation_period == 0:\n",
    "                            # Validation\n",
    "                            validation_cost, validation_acc, validation_micro, validation_macro = \\\n",
    "                                                    evaluate( sess, self.model, val_features_batches, val_support_batches, y_val_batches,\n",
    "                                                             val_mask_batches, val_data, self.placeholders, multilabel = self.multilabel)\n",
    "                            print('During Validation epoch {}: val_acc= {:.5f} '.format(real_epoch_num, validation_acc) + 'micro F1= {:.5f} macro F1= {:.5f} '.format(validation_micro, validation_macro) )\n",
    "\n",
    "                            \n",
    "\n",
    "            self.time_train_total += time.time() - t0\n",
    "            \n",
    "            print_str = 'Epoch: %04d ' % (epoch + 1) + 'training time: {:.5f} '.format(\n",
    "                self.time_train_total) + 'train_acc= {:.5f} '.format(outs[2])\n",
    "            \n",
    "            \n",
    "            \n",
    "#             tf.logging.info(print_str)\n",
    "\n",
    "#             if epoch > early_stopping_epoch_thresh and cost_val[-1] > np.mean(\n",
    "#                 cost_val[-(early_stopping_epoch_thresh + 1):-1]):\n",
    "#                 tf.logging.info('Early stopping...')\n",
    "#                 break\n",
    "\n",
    "        tf.logging.info('Optimization Finished!')\n",
    "\n",
    "        # Save model\n",
    "        saver.save(sess, train_save_name)\n",
    "        \n",
    "    def test(self, clustering_machine, seed = 6, diag_lambda = 1, \\\n",
    "                               test_batch_num = 2, train_save_name = './tmp/saver.txt'):\n",
    "        \n",
    "        test_features_batches, test_support_batches, y_test_batches, test_mask_batches, test_data = \\\n",
    "                    clustering_machine.test_batch_generation(test_batch_num = test_batch_num, diag_lambda = diag_lambda, precalc = self.precalc)\n",
    "        \n",
    "        with tf.device('/cpu:0'):\n",
    "            sess_cpu = tf.Session(config=tf.ConfigProto(device_count={'GPU': 0}))  # set the usable GPU count as 0, therefore only use CPU\n",
    "            sess_cpu.run(tf.global_variables_initializer())\n",
    "            saver = tf.train.Saver()\n",
    "            # load the trained model into the current session through the saved file: train_save_name which contains the trained model saver\n",
    "            saver.restore(sess_cpu, train_save_name)   \n",
    "            # The Saver class adds ops to save and restore variables to and from checkpoints. It also provides convenience methods to run these ops.\n",
    "\n",
    "            # Testing\n",
    "            test_cost, test_acc, micro, macro = evaluate(\n",
    "                sess_cpu, self.model, test_features_batches, test_support_batches, y_test_batches, test_mask_batches, test_data, self.placeholders, multilabel = self.multilabel)\n",
    "\n",
    "            print_str = 'Test set results: ' + 'cost= {:.5f} '.format(test_cost) + 'accuracy= {:.5f} '.format(test_acc) + 'mi F1= {:.5f} ma F1= {:.5f}'.format(micro, macro)\n",
    "            tf.logging.info(print_str)\n",
    "#             print(print_str)\n",
    "        \n",
    "        converted_acc = obtain_multi_label_accuracy(clustering_machine.label_count, clustering_machine.node_count, micro)\n",
    "        \n",
    "        print(\"micro-F1 is : {} and the accuracy is : {}\".format(micro, test_acc))\n",
    "        print(\"artificially converted multi-label accuracy is : {}\".format(converted_acc))\n",
    "        \n",
    "        return micro, test_acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use trivial dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index = torch.tensor([[0, 1, 1, 3, 1, 2, 4, 2, 4, 6, 6, 7, 7, 9, 2, 5, 9, 8], \n",
    "                           [1, 0, 3, 1, 2, 1, 2, 4, 6, 4, 7, 6, 9, 7, 5, 2, 8, 9]])\n",
    "# features = torch.rand(10, 3)\n",
    "features = torch.tensor([[0, 0], [0, 1], [0, 2], [0, 3], [0, 4],  \n",
    "                           [0, 5], [0, 6], [0, 7], [0, 8], [0, 9]], dtype = torch.float)\n",
    "# label = torch.tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "label = torch.tensor([0, 1, 1, 0, 1, 1, 1, 0, 0, 0])\n",
    "print(features, features.shape)\n",
    "\n",
    "clustering_folder = './res_save_batch/clustering/'\n",
    "check_folder_exist(clustering_folder)\n",
    "clustering_file_name = clustering_folder + 'check_clustering_machine.txt'\n",
    "os.makedirs(os.path.dirname(clustering_folder), exist_ok=True)\n",
    "info_folder = './res_save_batch/info/'\n",
    "check_folder_exist(info_folder)\n",
    "os.makedirs(os.path.dirname(info_folder), exist_ok=True)\n",
    "\n",
    "node_count = features.shape[0]\n",
    "get_edge_weight(edge_index, node_count, store_path = clustering_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_machine = ClusteringMachine(edge_index, features, label, clustering_folder, info_folder = clustering_folder, multilabel = False)\n",
    "clustering_machine.split_whole_nodes_edges_then_cluster(0.25, 0.25, normalize = False, precalc = True)\n",
    "\n",
    "gcn_trainer = ClusterGCNTrainer(clustering_machine.feature_count, clustering_machine.label_count, num_layers = 3, hidden_neuron_num = 16, learning_rate = 0.001, \\\n",
    "                 logging = False, multilabel = False, norm = True, precalc = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start the training of the model:\n",
    "gcn_trainer.train(clustering_machine, seed = 6, diag_lambda = -1, mini_block_size = 1, \\\n",
    "                               mini_cluster_num = 2, epoch_num = 400, dropout = 0.3, train_save_name = './tmp/saver.txt', validation_period = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start to test the trained model:\n",
    "gcn_trainer.test(clustering_machine, seed = 6, diag_lambda = -1, test_batch_num = 1, train_save_name = './tmp/saver.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cluster_train_valid_batch_run(clustering_machine, num_layers = 3, hidden_neuron_num = 32, learning_rate = 0.001, \\\n",
    "                 logging = False, multilabel = True, norm = True, precalc = True, diag_lambda = -1, \\\n",
    "                 mini_block_size = 1, mini_cluster_num = 2, test_batch_num = 2, \\\n",
    "                 epoch_num = 400, mini_epoch_num = 20, dropout = 0.3, train_save_name = './tmp/saver.txt'):\n",
    "    \"\"\"\n",
    "        # Run the mini-batch model (train and validate both in batches)\n",
    "        Tuning parameters:  dropout, lr (learning rate), weight_decay: l2 regularization\n",
    "        return: validation accuracy value, validation F-1 value, time_training (ms), time_data_load (ms)\n",
    "    \"\"\"\n",
    "    \n",
    "    gcn_trainer = ClusterGCNTrainer(clustering_machine.feature_count, clustering_machine.label_count, \\\n",
    "                num_layers = num_layers, hidden_neuron_num = hidden_neuron_num, learning_rate = learning_rate, \\\n",
    "                 logging = logging, multilabel = multilabel, norm = norm, precalc = precalc)\n",
    "    \n",
    "    # start to train the model\n",
    "    gcn_trainer.train(clustering_machine, seed = 6, diag_lambda = diag_lambda, mini_block_size = mini_block_size, \\\n",
    "                mini_cluster_num = mini_cluster_num, \\\n",
    "                epoch_num = epoch_num, mini_epoch_num = mini_epoch_num, dropout = dropout, train_save_name = train_save_name)\n",
    "    \n",
    "    # start to test the model\n",
    "    validation_F1, validation_accuracy = gcn_trainer.test(clustering_machine, seed = 6, \\\n",
    "                diag_lambda = diag_lambda, test_batch_num = test_batch_num, train_save_name = train_save_name)\n",
    "    \n",
    "    time_train_total = gcn_trainer.time_train_total\n",
    "#     time_data_load = gcn_trainer.time_train_load_data\n",
    "    \n",
    "    return validation_accuracy, validation_F1, time_train_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"To test one single model for different parameter values\"\"\"\n",
    "def execute_tuning(tune_params, clustering_machine, repeate_time = 7, num_layers = 3, hidden_neuron_num = 32, learning_rate = 0.001, \\\n",
    "                 logging = False, multilabel = True, norm = True, precalc = True, diag_lambda = -1, \\\n",
    "                 mini_block_size = 1, mini_cluster_num = 2, test_batch_num = 2, \\\n",
    "                 epoch_num = 400, mini_epoch_num = 20, dropout = 0.3, train_save_name = './tmp/saver.txt'):\n",
    "    \"\"\"\n",
    "        Tune all the hyperparameters\n",
    "        1) learning rate\n",
    "        2) dropout\n",
    "        3) layer unit number\n",
    "        4) weight decay\n",
    "    \"\"\"\n",
    "    validation_accuracy = {}\n",
    "    validation_f1 = {}\n",
    "    time_total_train = {}\n",
    "#     time_data_load = {}\n",
    "    \n",
    "    res = [{tune_val : Cluster_train_valid_batch_run(clustering_machine, num_layers = num_layers, hidden_neuron_num = hidden_neuron_num, learning_rate = learning_rate, \\\n",
    "                 logging = logging, multilabel = multilabel, norm = norm, precalc = precalc, diag_lambda = diag_lambda, \\\n",
    "                 mini_block_size = mini_block_size, mini_cluster_num = mini_cluster_num, test_batch_num = test_batch_num, \\\n",
    "                 epoch_num = epoch_num, mini_epoch_num = tune_val, dropout = dropout, train_save_name = train_save_name) for tune_val in tune_params} for i in range(repeate_time)]\n",
    "    \n",
    "    for i, ref in enumerate(res):\n",
    "        validation_accuracy[i] = {tune_val : res_lst[0] for tune_val, res_lst in ref.items()}\n",
    "        validation_f1[i] = {tune_val : res_lst[1] for tune_val, res_lst in ref.items()}\n",
    "        time_total_train[i] = {tune_val : res_lst[2] for tune_val, res_lst in ref.items()}\n",
    "#         time_data_load[i] = {tune_val : res_lst[3] for tune_val, res_lst in ref.items()}\n",
    "        \n",
    "    return validation_accuracy, validation_f1, time_total_train\n",
    "\n",
    "def store_data_multi_tuning(tune_params, target, data_name, img_path, comments):\n",
    "    \"\"\"\n",
    "        tune_params: is the tuning parameter list\n",
    "        target: is the result, here should be F1-score, accuraycy, load time, train time\n",
    "    \"\"\"\n",
    "    run_ids = sorted(target.keys())   # key is the run_id\n",
    "    run_data = {'run_id': run_ids}\n",
    "    # the key can be converted to string or not: i.e. str(tune_val)\n",
    "    # here we keep it as integer such that we want it to follow order\n",
    "    tmp = {tune_val : [target[run_id][tune_val] for run_id in run_ids] for tune_val in tune_params}  # the value is list\n",
    "    run_data.update(tmp)\n",
    "    \n",
    "    pickle_filename = img_path + data_name + '_' + comments + '.pkl'\n",
    "    os.makedirs(os.path.dirname(pickle_filename), exist_ok=True)\n",
    "    df = pd.DataFrame(data=run_data, dtype=np.int32)\n",
    "    df.to_pickle(pickle_filename)\n",
    "    return pickle_filename\n",
    "\n",
    "def draw_data_multi_tests(pickle_filename, data_name, comments, xlabel, ylabel):\n",
    "    df = pd.read_pickle(pickle_filename)\n",
    "    df_reshape = df.melt('run_id', var_name = 'model', value_name = ylabel)\n",
    "\n",
    "    plt.clf()\n",
    "    plt.figure()\n",
    "    sns.set(style='whitegrid')\n",
    "    g = sns.catplot(x=\"model\", y=ylabel, kind='box', data=df_reshape)\n",
    "    g.despine(left=True)\n",
    "    g.fig.suptitle(data_name + ' ' + ylabel + ' ' + comments)\n",
    "    g.set_xlabels(xlabel)\n",
    "    g.set_ylabels(ylabel)\n",
    "\n",
    "    img_name = pickle_filename[:-4] + '_img'\n",
    "    os.makedirs(os.path.dirname(img_name), exist_ok=True)\n",
    "    plt.savefig(img_name, bbox_inches='tight')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_tune_param(data, data_name, image_data_path, tmp_folder, partition_nums, layers, hidden_neuron_num = 32, test_batch_num = 2):\n",
    "    node_count = data.x.shape[0]\n",
    "    get_edge_weight(data.edge_index, node_count, store_path = tmp_folder)\n",
    "    \n",
    "    for partn in partition_nums:\n",
    "        for layer_num in layers:\n",
    "            net_layer = layer_num - 1\n",
    "            # Set the tune parameters and name\n",
    "            tune_name = 'batch_epoch_num'\n",
    "            tune_params = [400, 200, 100, 50, 20, 10, 5, 1]\n",
    "#             tune_name = 'weight_decay'\n",
    "#             tune_params = [0.0001, 0.001, 0.01, 0.1]\n",
    "\n",
    "            img_path = image_data_path + 'cluster_num_' + str(partn) + '/' + 'net_layer_' + str(net_layer) + '/'\n",
    "            img_path += 'tune_' + tune_name + '/'  # further subfolder for different task\n",
    "            print('Start tuning for tuning param: ' + tune_name + ' partition num: ' + str(partn) + 'net_layer_' + str(net_layer) )  \n",
    "            \n",
    "            clustering_machine = ClusteringMachine(data.edge_index, data.x, data.y, tmp_folder, info_folder = image_data_path)\n",
    "            clustering_machine.split_whole_nodes_edges_then_cluster(0.05, 0.85, normalize = False, precalc = True)\n",
    "            \n",
    "            validation_accuracy, validation_f1, time_total_train = \\\n",
    "                execute_tuning(tune_params, clustering_machine, repeate_time = 7, num_layers = layer_num, hidden_neuron_num = hidden_neuron_num, learning_rate = 0.001, \\\n",
    "                 logging = False, multilabel = True, norm = True, precalc = True, diag_lambda = 1, \\\n",
    "                 mini_block_size = 2, mini_cluster_num = partn, test_batch_num = test_batch_num, \\\n",
    "                 epoch_num = 400, mini_epoch_num = 20, dropout = 0.3, train_save_name = tmp_folder + 'saver.txt')\n",
    "            \n",
    "\n",
    "            validation_accuracy = store_data_multi_tuning(tune_params,validation_accuracy, data_name, img_path, 'accuracy_cluster_num_' + str(partn))\n",
    "            draw_data_multi_tests(validation_accuracy, data_name, 'Train_cluster_num_' + str(partn) + '_net_layer_' + str(net_layer), tune_name, 'Accuracy')\n",
    "\n",
    "            validation_f1 = store_data_multi_tuning(tune_params, validation_f1, data_name, img_path, 'validation_cluster_num_' + str(partn) )\n",
    "            draw_data_multi_tests(validation_f1, data_name, 'Train_cluster_num_' + str(partn) + '_net_layer_' + str(net_layer), tune_name, 'F1 score')\n",
    "\n",
    "            time_train = store_data_multi_tuning(tune_params, time_total_train, data_name, img_path, 'train_time_cluster_num_' + str(partn) )\n",
    "            draw_data_multi_tests(time_train, data_name, 'train_time_cluster_num_' + str(partn) + '_net_layer_' + str(net_layer), tune_name, 'Train Time (ms)')\n",
    "\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Pytorch Geometric Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_data_root = '/home/xiangli/projects/tmpdata/GCN/Geometric/'\n",
    "test_folder_name = 'Pytorch_Geometric_Graph/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch_geometric.datasets import Planetoid\n",
    "data_name = 'Cora'\n",
    "dataset = Planetoid(root = local_data_root + 'Planetoid/Cora', name=data_name)\n",
    "data = dataset[0]\n",
    "image_data_path = './results/' + data_name + '/' + test_folder_name\n",
    "# set the current folder as the intermediate data folder so that we can easily copy either clustering \n",
    "intermediate_data_folder = './'\n",
    "\n",
    "partition_nums = [4]\n",
    "layers = [3]\n",
    "tmp_folder = './tmp/'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NON multi-label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_count = data.x.shape[0]\n",
    "get_edge_weight(data.edge_index, node_count, store_path = tmp_folder)\n",
    "\n",
    "clustering_machine = ClusteringMachine(data.edge_index, data.x, data.y, tmp_folder, info_folder = image_data_path, multilabel = False)\n",
    "\n",
    "clustering_machine.split_whole_nodes_edges_then_cluster(0.12, 0.22, normalize = True, precalc = True)\n",
    "\n",
    "print('label count: ', clustering_machine.label_count,)\n",
    "gcn_trainer = ClusterGCNTrainer(clustering_machine.feature_count, clustering_machine.label_count, num_layers = 3, hidden_neuron_num = 128, learning_rate = 0.001, \\\n",
    "                 logging = False, multilabel = False, norm = True, precalc = True)\n",
    "\n",
    "gcn_trainer.train(clustering_machine, seed = 6, diag_lambda = 1, mini_block_size = 2, \\\n",
    "        mini_cluster_num = 4, epoch_num = 400, mini_epoch_num = 20, dropout = 0.3, train_save_name = './tmp/saver.txt', validation_period = 50)\n",
    "\n",
    "# print(node_count)\n",
    "# start to test the trained model:\n",
    "gcn_trainer.test(clustering_machine, seed = 6, diag_lambda = 1, test_batch_num = 1, train_save_name = './tmp/saver.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use GraphSaint Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_data_root = '/home/xiangli/projects/tmpdata/GCN/GraphSaint/'\n",
    "test_folder_name = 'GraphSaint_Graphs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GraphSaint_dataset import print_data_info, Flickr, Yelp, PPI_large, Amazon, Reddit, PPI_small\n",
    "# suppose this is on the OSC cluster\n",
    "\n",
    "data_name = 'PPI_small'\n",
    "class_data = eval(data_name)\n",
    "dataset = class_data(root = remote_data_root + data_name)\n",
    "print('number of data', len(dataset))\n",
    "data = dataset[0]\n",
    "\n",
    "intermediate_data_folder = './clusterGCN_multilabel_normalize/'\n",
    "image_data_path = intermediate_data_folder + 'results/' + data_name + '/' + test_folder_name\n",
    "\n",
    "partition_nums = [4]\n",
    "layers = [3]\n",
    "tmp_folder = './tmp/'\n",
    "\n",
    "# image_data_path = './results/' + data_name + '/' + test_folder_name\n",
    "# intermediate_data_folder = './'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_count = data.x.shape[0]\n",
    "get_edge_weight(data.edge_index, node_count, store_path = tmp_folder)\n",
    "\n",
    "clustering_machine = ClusteringMachine(data.edge_index, data.x, data.y, tmp_folder, info_folder = image_data_path, multilabel = True)\n",
    "\n",
    "clustering_machine.split_whole_nodes_edges_then_cluster(0.12, 0.22, normalize = True, precalc = True)\n",
    "print('label count: ', clustering_machine.label_count,)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn_trainer = ClusterGCNTrainer(clustering_machine.feature_count, clustering_machine.label_count, num_layers = 3, hidden_neuron_num = 512, learning_rate = 0.01, \\\n",
    "                 logging = False, multilabel = True, norm = True, precalc = True)\n",
    "\n",
    "\n",
    "gcn_trainer.train(clustering_machine, seed = 6, diag_lambda = 1, mini_block_size = 8, \\\n",
    "    mini_cluster_num = 32, epoch_num = 1000, mini_epoch_num = 10, dropout = 0.1, train_save_name = './tmp/saver.txt', validation_period = 100)\n",
    "\n",
    "# print(node_count)\n",
    "# start to test the trained model:\n",
    "gcn_trainer.test(clustering_machine, seed = 6, diag_lambda = 1, test_batch_num = 1, train_save_name = './tmp/saver.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results for \n",
    "\n",
    "    num_layers = 3, hidden_neuron_num = 512, learning_rate = 0.01\n",
    "    clustering_machine, seed = 6, diag_lambda = 1, mini_block_size = 8, mini_cluster_num = 64, \n",
    "                              epoch_num = 400, mini_epoch_num = 20, dropout = 0.1, \n",
    "\n",
    "    micro-F1 is : 0.8163103152431628 and the accuracy is : 0.8893744945526123\n",
    "    artificially converted multi-label accuracy is : 0.9969630689694766\n",
    "\n",
    "    (0.8163103152431628, 0.8893744945526123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_tune_param(data, data_name, image_data_path, tmp_folder, partition_nums, layers, hidden_neuron_num = 32, test_batch_num = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free GPU memory\n",
    "# !(nvidia-smi | grep 'python' | awk '{ print $3 }' | xargs -n1 kill -9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the torch.tensor\n",
    "import torch\n",
    "import numpy as np\n",
    "# a = torch.tensor([[1, 2], [2, 3]])\n",
    "# b = torch.tensor([[4, 2], [2, 3]])\n",
    "\n",
    "a = np.array([[1, 2], [2, 3]])\n",
    "b = np.array([[4, 2], [2, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_acc(y_test, y_pred):\n",
    "    \"\"\"\n",
    "        y_test (np.array) : the true label for the nodes\n",
    "        y_pred (np.array) : predicted tags for the nodes\n",
    "    \"\"\"\n",
    "    ave_loss = (y_test == y_pred).mean(dtype=np.float).item() \n",
    "    return ave_loss\n",
    "\n",
    "print(binary_acc(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(binary_acc(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = a.flatten()\n",
    "e = b.flatten()\n",
    "print(d.shape, e.shape)\n",
    "print(accuracy_score(d, e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_geom_tensorflow_2]",
   "language": "python",
   "name": "conda-env-pytorch_geom_tensorflow_2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
