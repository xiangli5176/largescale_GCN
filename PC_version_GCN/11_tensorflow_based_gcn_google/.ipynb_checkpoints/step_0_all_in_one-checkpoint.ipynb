{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# package from the pytorch side\n",
    "import copy\n",
    "import csv\n",
    "import sys\n",
    "import torch\n",
    "from pytorch_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "\n",
    "import networkx as nx\n",
    "import metis\n",
    "import time\n",
    "# import models\n",
    "import numpy as np\n",
    "import partition_utils\n",
    "import tensorflow as tf\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "Num CPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "# tf.compat.v1.logging.ERROR\n",
    "# verbose information: tf.logging.INFO  \n",
    "tf.logging.set_verbosity(tf.compat.v1.logging.ERROR)  # Sets the threshold for what messages will be logged.\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Num CPUs Available: \", len(tf.config.experimental.list_physical_devices('CPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCN model used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inits\n",
    "import tensorflow as tf\n",
    "import metrics\n",
    "\n",
    "def sparse_dropout(x, keep_prob, noise_shape):\n",
    "    \"\"\"Dropout for sparse tensors.\"\"\"\n",
    "    random_tensor = keep_prob\n",
    "    random_tensor += tf.compat.v1.random_uniform(noise_shape)\n",
    "    dropout_mask = tf.compat.v1.cast(tf.compat.v1.floor(random_tensor), dtype=tf.compat.v1.bool)\n",
    "    pre_out = tf.compat.v1.sparse_retain(x, dropout_mask)\n",
    "    return pre_out * (1. / keep_prob)\n",
    "\n",
    "\n",
    "def dot(x, y, sparse=False):\n",
    "    \"\"\"Wrapper for tf.compat.v1.matmul (sparse vs dense).\"\"\"\n",
    "    if sparse:\n",
    "        res = tf.compat.v1.sparse_tensor_dense_matmul(x, y)\n",
    "    else:\n",
    "        res = tf.compat.v1.matmul(x, y)\n",
    "    return res\n",
    "\n",
    "\n",
    "def layernorm(x, offset, scale):\n",
    "    mean, variance = tf.compat.v1.nn.moments(x, axes=[1], keep_dims=True)\n",
    "    return tf.compat.v1.nn.batch_normalization(x, mean, variance, offset, scale, 1e-9)\n",
    "\n",
    "\n",
    "class Layer(object):\n",
    "    \"\"\"Base layer class.\n",
    "\n",
    "    Defines basic API for all layer objects.\n",
    "    Implementation inspired by keras (http://keras.io).\n",
    "\n",
    "    # Properties\n",
    "        name: String, defines the variable scope of the layer.\n",
    "        logging: Boolean, switches Tensorflow histogram logging on/off\n",
    "\n",
    "    # Methods\n",
    "        _call(inputs): Defines computation graph of layer\n",
    "            (i.e. takes input, returns output)\n",
    "        __call__(inputs): Wrapper for _call()\n",
    "        _log_vars(): Log all variables\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name = None, logging = False):\n",
    "        \n",
    "        if not name:\n",
    "            layer = self.__class__.__name__.lower()\n",
    "            name = layer + '_' \n",
    "\n",
    "        self.name = name\n",
    "        \n",
    "        self.vars = {}\n",
    "        self.logging = logging\n",
    "        \n",
    "        self.sparse_inputs = False\n",
    "        print('End of the constructor of the Layer class')\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        return inputs\n",
    "\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        print('Layer instance is called or executed')\n",
    "        with tf.compat.v1.name_scope(self.name):\n",
    "          if self.logging and not self.sparse_inputs:\n",
    "            tf.compat.v1.summary.histogram(self.name + '/inputs', inputs)\n",
    "\n",
    "        outputs = self._call(inputs)\n",
    "\n",
    "        if self.logging:\n",
    "            tf.compat.v1.summary.histogram(self.name + '/outputs', outputs)\n",
    "        return outputs\n",
    "\n",
    "    def _log_vars(self):\n",
    "        for var in self.vars:\n",
    "            tf.compat.v1.summary.histogram(self.name + '/vars/' + var, self.vars[var])\n",
    "\n",
    "            \n",
    "class GraphConvolution(Layer):\n",
    "    \"\"\"Graph convolution layer.\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, placeholders,\n",
    "               dropout=0., sparse_inputs=False, act=tf.nn.relu, bias=False, featureless=False, norm=False, precalc=False,\n",
    "               name = None, logging = False):\n",
    "        super(GraphConvolution, self).__init__(name = name, logging = logging)\n",
    "        print('During the constructor of GCN layer, input dim: {} ; output dim: {}'.format(input_dim, output_dim))\n",
    "\n",
    "        if dropout:\n",
    "            self.dropout = placeholders['dropout']\n",
    "        else:\n",
    "            self.dropout = 0.\n",
    "\n",
    "        self.act = act\n",
    "        self.support = placeholders['support']\n",
    "\n",
    "        self.sparse_inputs = sparse_inputs\n",
    "        self.featureless = featureless\n",
    "        self.bias = bias\n",
    "        self.norm = norm\n",
    "        self.precalc = precalc\n",
    "\n",
    "        # helper variable for sparse dropout\n",
    "        # self.num_features_nonzero = placeholders['num_features_nonzero']\n",
    "\n",
    "        with tf.variable_scope(self.name + '_vars'):\n",
    "            self.vars['weights'] = inits.glorot([input_dim, output_dim], name='weights')\n",
    "            if self.bias:\n",
    "                self.vars['bias'] = inits.zeros([output_dim], name='bias')\n",
    "\n",
    "            if self.norm:\n",
    "                self.vars['offset'] = inits.zeros([1, output_dim], name='offset')\n",
    "                self.vars['scale'] = inits.ones([1, output_dim], name='scale')\n",
    "\n",
    "        if self.logging:\n",
    "            self._log_vars()\n",
    "    \n",
    "\n",
    "    def _call(self, inputs):\n",
    "        x = inputs\n",
    "\n",
    "        # convolve\n",
    "        if self.precalc:\n",
    "            support = x\n",
    "        else:\n",
    "            support = dot(self.support, x, sparse=True)\n",
    "            support = tf.concat((support, x), axis=1)\n",
    "\n",
    "        # dropout\n",
    "        support = tf.nn.dropout(support, rate = self.dropout)\n",
    "\n",
    "        tf.Print(support, [support], \"During the call of GCN layer, final input to be multiplied with weights: \")\n",
    "\n",
    "#         print('\\n inside the call of convolutiongraph layer: ')\n",
    "#         print('support vecotr dimension is : {} ;'.format(support.shape), 'weight matrix dimension is : {} ;'.format(self.vars['weights'].shape))\n",
    "\n",
    "        output = dot(support, self.vars['weights'], sparse=self.sparse_inputs)\n",
    "\n",
    "        # bias\n",
    "        if self.bias:\n",
    "            output += self.vars['bias']\n",
    "\n",
    "        with tf.variable_scope(self.name + '_vars'):\n",
    "            if self.norm:\n",
    "                output = layernorm(output, self.vars['offset'], self.vars['scale'])\n",
    "\n",
    "        return self.act(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct GCN layer nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Collections of different Models.\"\"\"\n",
    "\n",
    "\n",
    "class Model(object):\n",
    "    \"\"\"Model class to be inherited.\"\"\"\n",
    "\n",
    "    def __init__(self, weight_decay = 0, num_layers = 2, name = None, logging = False, multilabel = False, norm = False, precalc = False):\n",
    "        self.weight_decay = weight_decay\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        if not name:\n",
    "            name = self.__class__.__name__.lower()\n",
    "        self.name = name\n",
    "\n",
    "        self.logging = logging\n",
    "        self.multilabel = multilabel\n",
    "        self.norm = norm\n",
    "        self.precalc = precalc\n",
    "\n",
    "        self.vars = {}\n",
    "        self.placeholders = {}\n",
    "\n",
    "        self.layers = []\n",
    "        self.activations = []\n",
    "\n",
    "        self.inputs = None\n",
    "        self.outputs = None\n",
    "\n",
    "        self.loss = 0\n",
    "        self.accuracy = 0\n",
    "        self.pred = 0\n",
    "        self.optimizer = None\n",
    "        self.opt_op = None\n",
    "\n",
    "    def _build(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def build(self):\n",
    "        \"\"\"Wrapper for _build().\"\"\"\n",
    "        with tf.variable_scope(self.name):\n",
    "            self._build()\n",
    "\n",
    "        # Build sequential layer model\n",
    "        self.activations.append(self.inputs)\n",
    "        \n",
    "        # debug to output the embedding:\n",
    "#         self.hidden1 = layer(self.activations[-1])\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            hidden = layer(self.activations[-1])\n",
    "        \n",
    "            if isinstance(hidden, tuple):\n",
    "                tf.logging.info('{} shape = {}'.format(layer.name, hidden[0].get_shape()))\n",
    "            else:\n",
    "                tf.logging.info('{} shape = {}'.format(layer.name, hidden.get_shape()))\n",
    "\n",
    "            self.activations.append(hidden)\n",
    "            \n",
    "        self.outputs = self.activations[-1]\n",
    "\n",
    "        # Store model variables for easy access\n",
    "        variables = tf.get_collection( tf.GraphKeys.GLOBAL_VARIABLES, scope=self.name)\n",
    "        self.vars = variables\n",
    "        for k in self.vars:\n",
    "            tf.logging.info((k.name, k.get_shape()))\n",
    "\n",
    "        # Build metrics\n",
    "        self._loss()\n",
    "        self._accuracy()\n",
    "        self._predict()\n",
    "\n",
    "        self.opt_op = self.optimizer.minimize(self.loss)\n",
    "\n",
    "    def _loss(self):\n",
    "        \"\"\"Construct the loss function.\"\"\"\n",
    "        # Weight decay loss\n",
    "        if self.weight_decay > 0.0:\n",
    "            for var in self.layers[0].vars.values():\n",
    "                self.loss += self.weight_decay * tf.nn.l2_loss(var)\n",
    "\n",
    "        # Cross entropy error\n",
    "        if self.multilabel:\n",
    "            self.loss += metrics.masked_sigmoid_cross_entropy(self.outputs, self.placeholders['labels'], self.placeholders['labels_mask'])\n",
    "        else:\n",
    "            self.loss += metrics.masked_softmax_cross_entropy(self.outputs, self.placeholders['labels'], self.placeholders['labels_mask'])\n",
    "\n",
    "    def _accuracy(self):\n",
    "        if self.multilabel:\n",
    "            self.accuracy = metrics.masked_accuracy_multilabel(self.outputs, self.placeholders['labels'], self.placeholders['labels_mask'])\n",
    "        else:\n",
    "            self.accuracy = metrics.masked_accuracy(self.outputs, self.placeholders['labels'], self.placeholders['labels_mask'])\n",
    "\n",
    "    def _predict(self):\n",
    "        if self.multilabel:\n",
    "            self.pred = tf.nn.sigmoid(self.outputs)\n",
    "        else:\n",
    "            self.pred = tf.nn.softmax(self.outputs)\n",
    "\n",
    "    def save(self, sess=None):\n",
    "        if not sess:\n",
    "            raise AttributeError('TensorFlow session not provided.')\n",
    "        saver = tf.train.Saver(self.vars)\n",
    "        save_path = saver.save(sess, './tmp/%s.ckpt' % self.name)\n",
    "        tf.logging.info('Model saved in file:', save_path)\n",
    "\n",
    "    def load(self, sess=None):\n",
    "        if not sess:\n",
    "            raise AttributeError('TensorFlow session not provided.')\n",
    "        saver = tf.train.Saver(self.vars)\n",
    "        save_path = './tmp/%s.ckpt' % self.name\n",
    "        saver.restore(sess, save_path)\n",
    "        tf.logging.info('Model restored from file:', save_path)\n",
    "\n",
    "\n",
    "class GCN(Model):\n",
    "    \"\"\"Implementation of GCN model.\"\"\"\n",
    "\n",
    "    def __init__(self, placeholders, input_dim, output_dim, hidden_neuron_num, learning_rate = 0.01, \n",
    "                 num_layers = 3, name = None, logging = False, multilabel = False, norm = False, precalc = False):\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.hidden1 = hidden_neuron_num\n",
    "        \n",
    "        super(GCN, self).__init__(weight_decay = 0, num_layers = num_layers, name = name, logging = logging, \\\n",
    "                                  multilabel = multilabel, norm = norm, precalc = precalc)\n",
    "\n",
    "        self.inputs = placeholders['features']\n",
    "        self.input_dim = input_dim\n",
    "#         self.output_dim = placeholders['labels'].get_shape().as_list()[1]\n",
    "        self.output_dim = output_dim\n",
    "    \n",
    "        self.placeholders = placeholders\n",
    "\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate = self.learning_rate)\n",
    "\n",
    "        self.build()\n",
    "\n",
    "    def _build(self):\n",
    "        print('\\n build the first layer: ')\n",
    "        \n",
    "        self.layers.append(\n",
    "            GraphConvolution(\n",
    "                input_dim = self.input_dim if self.precalc else self.input_dim * 2,\n",
    "                output_dim = self.hidden1,\n",
    "                placeholders = self.placeholders,\n",
    "                act=tf.nn.relu,\n",
    "                dropout = True,\n",
    "                sparse_inputs = False,\n",
    "                logging = self.logging,\n",
    "                norm = self.norm,\n",
    "                precalc = self.precalc))\n",
    "        print('\\n build the series of hiddle layer: ')\n",
    "        for _ in range(self.num_layers - 2):\n",
    "            self.layers.append(\n",
    "              GraphConvolution(\n",
    "                  input_dim = self.hidden1 * 2,\n",
    "                  output_dim = self.hidden1,\n",
    "                  placeholders = self.placeholders,\n",
    "                  act=tf.nn.relu,\n",
    "                  dropout = True,\n",
    "                  sparse_inputs = False,\n",
    "                  logging = self.logging,\n",
    "                  norm = self.norm,\n",
    "                  precalc = False))\n",
    "            \n",
    "        print('\\n build the last layer: ')\n",
    "        self.layers.append(\n",
    "            GraphConvolution(\n",
    "                input_dim = self.hidden1 * 2,\n",
    "                output_dim = self.output_dim,\n",
    "                placeholders = self.placeholders,\n",
    "                act = lambda x: x,\n",
    "                dropout = True,\n",
    "                logging = self.logging,\n",
    "                norm = False,\n",
    "                precalc = False))\n",
    "        print('======================End of GCN Build===================')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_scatter import scatter_add\n",
    "from torch_geometric.utils import add_remaining_self_loops\n",
    "\n",
    "def get_edge_weight(edge_index, num_nodes, edge_weight = None, improved = False, dtype=None, store_path='./tmp/'):\n",
    "    \"\"\"\n",
    "        edge_index(ndarray): undirected edge index (two-directions both included)\n",
    "        num_nodes(int):  number of nodes inside the graph\n",
    "        edge_weight(ndarray): if any weights already assigned, otherwise will be generated \n",
    "        improved(boolean):   may assign 2 to the self loop weight if true\n",
    "        store_path(string): the path of the folder to contain all the clustering information files\n",
    "    \"\"\"\n",
    "    # calculate the global graph properties, global edge weights\n",
    "    if edge_weight is None:\n",
    "        edge_weight = torch.ones((edge_index.size(1), ), dtype=dtype, device=edge_index.device)\n",
    "\n",
    "    fill_value = 1 if not improved else 2\n",
    "    # there are num_nodes self-loop edges added after the edge_index\n",
    "    edge_index, edge_weight = add_remaining_self_loops(edge_index, edge_weight, fill_value, num_nodes)\n",
    "\n",
    "    row, col = edge_index   \n",
    "    # row includes the starting points of the edges  (first row of edge_index)\n",
    "    # col includes the ending points of the edges   (second row of edge_index)\n",
    "\n",
    "    deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n",
    "    # row records the source nodes, which is the index we are trying to add\n",
    "    # deg will record the out-degree of each node of x_i in all edges (x_i, x_j) including self_loops\n",
    "\n",
    "    deg_inv_sqrt = deg.pow(-0.5)\n",
    "    deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "    # normalize the edge weight\n",
    "    normalized_edge_weight = deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n",
    "\n",
    "    # transfer from tensor to the numpy to construct the dict for the edge_weights\n",
    "    edge_index = edge_index.t().numpy()\n",
    "    normalized_edge_weight = normalized_edge_weight.numpy()\n",
    "\n",
    "    num_edge = edge_index.shape[0]\n",
    "\n",
    "    output = ([edge_index[i][0], edge_index[i][1], normalized_edge_weight[i]] for i in range(num_edge))\n",
    "\n",
    "    # output the edge weights as the csv file\n",
    "    input_edge_weight_txt_file = store_path + 'input_edge_weight_list.csv'\n",
    "    os.makedirs(os.path.dirname(input_edge_weight_txt_file), exist_ok=True)\n",
    "    with open(input_edge_weight_txt_file, 'w', newline='\\n') as fp:\n",
    "        wr = csv.writer(fp, delimiter = ' ')\n",
    "        for line in output:\n",
    "            wr.writerow(line)\n",
    "    return input_edge_weight_txt_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import metis\n",
    "import random\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import chain\n",
    "import scipy.sparse as sp\n",
    "def construct_adj(edges, nodes_count):\n",
    "    # Compressed Sparse Row matrix\n",
    "    # csr_matrix((data, ij), [shape=(M, N)])\n",
    "    # where data and ij satisfy the relationship a[ij[0, k], ij[1, k]] = data[k]\n",
    "    adj = sp.csr_matrix( ( np.ones((edges.shape[0]), dtype=np.float32), (edges[:, 0], edges[:, 1]) ), shape=(nodes_count, nodes_count) )\n",
    "    adj += adj.transpose()   # double the weight of each edge if it is two direction\n",
    "    return adj\n",
    "\n",
    "\n",
    "class ClusteringMachine(object):\n",
    "    \"\"\"\n",
    "    Clustering the graph, feature set and label. Performed on the CPU side\n",
    "    \"\"\"\n",
    "    def __init__(self, edge_index, features, label, tmp_folder = './tmp/', info_folder = './info/'):\n",
    "        \"\"\"\n",
    "        :param edge_index: COO format of the edge indices.\n",
    "        :param features: Feature matrix (ndarray).\n",
    "        :param label: label vector (ndarray).\n",
    "        :tmp_folder(string): the path of the folder to contain all the clustering information files\n",
    "        \"\"\"\n",
    "        self.features = features\n",
    "        self.label = label\n",
    "        self._set_sizes()\n",
    "        self.edge_index = edge_index\n",
    "        # store the information folder for memory tracing\n",
    "        self.tmp_folder = tmp_folder\n",
    "        self.info_folder = info_folder\n",
    "        \n",
    "        # first, use the get edge weights func to construct the two-sided self-loop added graph\n",
    "        edge_weight_file = self.tmp_folder + 'input_edge_weight_list.csv'\n",
    "        self.graph = nx.read_weighted_edgelist(edge_weight_file, create_using = nx.Graph, nodetype = int)\n",
    "        \n",
    "#         # second construct teh graph directly from the edge_list, here we are lacking the self-loop age\n",
    "#         tmp = edge_index.t().numpy().tolist()\n",
    "#         self.graph = nx.from_edgelist(tmp)\n",
    "        \n",
    "    def _set_sizes(self):\n",
    "        \"\"\"\n",
    "        Setting the feature and class count.\n",
    "        \"\"\"\n",
    "        self.node_count = self.features.shape[0]\n",
    "        self.feature_count = self.features.shape[1]    # features all always in the columns\n",
    "        self.label_count = len(np.unique(self.label.numpy()) )\n",
    "    \n",
    "    # 2) first assign train, test, validation nodes, split edges; this is based on the assumption that the clustering is no longer that important\n",
    "    def split_whole_nodes_edges_then_cluster(self, validation_ratio, test_ratio, normalize = False, precalc = True):\n",
    "        \"\"\"\n",
    "            Only split nodes\n",
    "            First create train-test splits, then split train and validation into different batch seeds\n",
    "            Input:  \n",
    "                1) ratio of test, validation\n",
    "                2) partition number of train nodes, test nodes, validation nodes\n",
    "            Output:\n",
    "                1) sg_validation_nodes_global, sg_train_nodes_global, sg_test_nodes_global\n",
    "        \"\"\"\n",
    "        relative_validation_ratio = (validation_ratio) / (1 - test_ratio)\n",
    "        \n",
    "        # first divide the nodes for the whole graph, result will always be a list of lists \n",
    "        model_nodes_global, test_nodes_global = train_test_split(list(self.graph.nodes()), test_size = test_ratio)\n",
    "        train_nodes_global, valid_nodes_global = train_test_split(model_nodes_global, test_size = relative_validation_ratio)\n",
    "        \n",
    "        edges = np.array(self.graph.edges(), dtype=np.int32)\n",
    "        feats = np.array(self.features.numpy(), dtype=np.float32)\n",
    "        \n",
    "        \n",
    "#         # normalize all the features\n",
    "#         if normalize:\n",
    "#             scaler = sklearn.preprocessing.StandardScaler()\n",
    "#             scaler.fit(feats)\n",
    "#             feats = scaler.transform(feats)\n",
    "        \n",
    "        classes = np.array(self.label.numpy(), dtype=np.int32)\n",
    "\n",
    "        # construct the 1-hot labels:\n",
    "        labels = np.zeros((self.node_count, self.label_count), dtype=np.float32)\n",
    "        for i, label in enumerate(classes):\n",
    "            labels[i, label] = 1\n",
    "            \n",
    "        full_adj = construct_adj(edges, self.node_count)\n",
    "        \n",
    "        # all the properties needed for the training\n",
    "        train_subgraph = self.graph.subgraph(train_nodes_global)\n",
    "        train_edges = np.array(train_subgraph.edges(), dtype=np.int32)\n",
    "        # check the double direction inside this adjacent matrix\n",
    "        train_adj = construct_adj(train_edges, self.node_count)\n",
    "        \n",
    "        train_feats = feats[train_nodes_global]\n",
    "        y_train = np.zeros(labels.shape)\n",
    "        y_train[train_nodes_global, :] = labels[train_nodes_global, :]\n",
    "        train_mask = utils.sample_mask(train_nodes_global, labels.shape[0])\n",
    "        \n",
    "        test_feats = feats   # why test gives the full feature of the whole graph\n",
    "        y_test = np.zeros(labels.shape)\n",
    "        y_test[test_nodes_global, :] = labels[test_nodes_global, :]\n",
    "        test_mask = utils.sample_mask(test_nodes_global, labels.shape[0])\n",
    "        \n",
    "        visible_data = train_nodes_global\n",
    "        \n",
    "        y_val = np.zeros(labels.shape)\n",
    "        y_val[valid_nodes_global, :] = labels[valid_nodes_global, :]\n",
    "        val_mask = utils.sample_mask(valid_nodes_global, labels.shape[0])\n",
    "\n",
    "        if precalc:\n",
    "            train_feats = train_adj.dot(feats)   # calculate the feature matrix weighted by the edge weights\n",
    "            # numpy.hstack: Stack arrays in sequence horizontally (column wise).\n",
    "            train_feats = np.hstack((train_feats, feats))\n",
    "            test_feats = full_adj.dot(feats)\n",
    "            test_feats = np.hstack((test_feats, feats))\n",
    "        \n",
    "#         print('after precalc:')\n",
    "#         print('train_feats', train_feats.shape, train_feats)\n",
    "        \n",
    "\n",
    "        return (train_adj, full_adj, train_feats, test_feats, y_train, y_val, y_test,\n",
    "              train_mask, val_mask, test_mask, train_nodes_global, valid_nodes_global, test_nodes_global, self.node_count, self.label_count,\n",
    "                visible_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the Graph_sage format data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model evaluation function\n",
    "def evaluate(sess, model, val_features_batches, val_support_batches,\n",
    "             y_val_batches, val_mask_batches, val_data, placeholders, multilabel = True):\n",
    "    \"\"\"evaluate GCN model.\"\"\"\n",
    "    total_pred = []\n",
    "    total_lab = []\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "\n",
    "    num_batches = len(val_features_batches)\n",
    "    for i in range(num_batches):\n",
    "        features_b = val_features_batches[i]\n",
    "        support_b = val_support_batches[i]\n",
    "        y_val_b = y_val_batches[i]\n",
    "        val_mask_b = val_mask_batches[i]\n",
    "        num_data_b = np.sum(val_mask_b)\n",
    "        if num_data_b == 0:\n",
    "            continue\n",
    "        else:\n",
    "            feed_dict = utils.construct_feed_dict(features_b, support_b, y_val_b, val_mask_b, placeholders)\n",
    "            outs = sess.run([model.loss, model.accuracy, model.outputs], feed_dict = feed_dict)\n",
    "\n",
    "        total_pred.append(outs[2][val_mask_b])\n",
    "        total_lab.append(y_val_b[val_mask_b])\n",
    "        total_loss += outs[0] * num_data_b\n",
    "        total_acc += outs[1] * num_data_b\n",
    "\n",
    "    total_pred = np.vstack(total_pred)\n",
    "    total_lab = np.vstack(total_lab)\n",
    "    loss = total_loss / len(val_data)\n",
    "    acc = total_acc / len(val_data)\n",
    "\n",
    "    micro, macro = utils.calc_f1(total_pred, total_lab, multilabel)\n",
    "    return loss, acc, micro, macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_one(layer_num, mini_cluster_num, epoch_num, dropout = 0.3, early_stopping_epoch_thresh = 300, diag_lambda = 1, \n",
    "               multilabel = True, layer_norm = True, precalc = True, train_save_name = './train_save.txt',\n",
    "               train_batch_num = 2, valid_batch_num = 2, test_batch_num = 2):\n",
    "    \"\"\"Main function for running experiments.\"\"\"\n",
    "    # Load data\n",
    "    (train_adj, full_adj, train_feats, test_feats, y_train, y_val, y_test,\n",
    "    train_mask, val_mask, test_mask, _, val_data, test_data, num_data, num_class,\n",
    "    visible_data) = clustering_machine.split_whole_nodes_edges_then_cluster(0.05, 0.85, normalize = False, precalc = precalc)\n",
    "\n",
    "    # Partition graph and do preprocessing\n",
    "    if mini_cluster_num > 1:\n",
    "        _, parts = partition_utils.partition_graph(train_adj, visible_data, train_batch_num)\n",
    "        parts = [np.array(pt) for pt in parts]\n",
    "    else:\n",
    "        (parts, features_batches, support_batches, y_train_batches, train_mask_batches) = \\\n",
    "        utils.preprocess(train_adj, train_feats, y_train, train_mask, visible_data, train_batch_num, diag_lambda)\n",
    "\n",
    "    (_, val_features_batches, val_support_batches, y_val_batches, val_mask_batches) = \\\n",
    "    utils.preprocess(full_adj, test_feats, y_val, val_mask, np.arange(num_data), valid_batch_num, diag_lambda)\n",
    "\n",
    "    (_, test_features_batches, test_support_batches, y_test_batches, \n",
    "     test_mask_batches) = utils.preprocess(full_adj, test_feats, y_test,\n",
    "                                         test_mask, np.arange(num_data),\n",
    "                                         test_batch_num,\n",
    "                                         diag_lambda)\n",
    "    idx_parts = list(range(len(parts)))\n",
    "\n",
    "    # Define placeholders\n",
    "    placeholders = {\n",
    "      'support':\n",
    "          tf.sparse_placeholder(tf.float32),\n",
    "      'features':\n",
    "          tf.placeholder(tf.float32),\n",
    "      'labels':\n",
    "          tf.placeholder(tf.float32),\n",
    "      'labels_mask':\n",
    "          tf.placeholder(tf.int32),\n",
    "      'dropout':\n",
    "          tf.placeholder_with_default(0., shape=()),\n",
    "#       'num_features_nonzero':\n",
    "#           tf.placeholder(tf.int32)  # helper variable for sparse dropout\n",
    "    }\n",
    "\n",
    "    # Create model\n",
    "    model = GCN(\n",
    "      placeholders,\n",
    "      input_dim = test_feats.shape[1],\n",
    "      output_dim = num_class,\n",
    "      hidden_neuron_num = 16,\n",
    "      learning_rate = 0.001,\n",
    "      logging = False,\n",
    "      multilabel = multilabel,\n",
    "      norm = layer_norm,\n",
    "      precalc = precalc,\n",
    "      num_layers = layer_num)\n",
    "    \n",
    "    # Initialize session\n",
    "    sess = tf.Session()\n",
    "    seed = 6\n",
    "    tf.set_random_seed(seed)\n",
    "\n",
    "    # Init variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    cost_val = []\n",
    "    total_training_time = 0.0\n",
    "    \n",
    "    # Train model:  epoch_num\n",
    "    for epoch in range(epoch_num):\n",
    "        t = time.time()\n",
    "        np.random.shuffle(idx_parts)\n",
    "        \n",
    "        # recombine mini-clusters to form larger batches\n",
    "        if mini_cluster_num > 1:\n",
    "            (features_batches, support_batches, y_train_batches,\n",
    "            train_mask_batches) = utils.preprocess_multicluster(\n",
    "               train_adj, parts, train_feats, y_train, train_mask,\n",
    "               train_batch_num, mini_cluster_num, diag_lambda)\n",
    "            \n",
    "            for pid in range(len(features_batches)):\n",
    "                # Use preprocessed batch data\n",
    "                features_b = features_batches[pid]\n",
    "                support_b = support_batches[pid]\n",
    "                y_train_b = y_train_batches[pid]\n",
    "                train_mask_b = train_mask_batches[pid]\n",
    "                # Construct feed dictionary\n",
    "                feed_dict = utils.construct_feed_dict(features_b, support_b, y_train_b, train_mask_b, placeholders)\n",
    "                \n",
    "                feed_dict.update({placeholders['dropout']: dropout})\n",
    "                # Training step\n",
    "                outs = sess.run([model.opt_op, model.loss, model.accuracy], feed_dict=feed_dict)\n",
    "        else:\n",
    "            np.random.shuffle(idx_parts)\n",
    "            \n",
    "            for pid in idx_parts:\n",
    "                # Use preprocessed batch data\n",
    "                features_b = features_batches[pid]\n",
    "                support_b = support_batches[pid]\n",
    "                y_train_b = y_train_batches[pid]\n",
    "                train_mask_b = train_mask_batches[pid]\n",
    "                \n",
    "                # Construct feed dictionary\n",
    "                feed_dict = utils.construct_feed_dict(features_b, support_b, y_train_b, train_mask_b, placeholders)\n",
    "                \n",
    "                # investigate the constructed matrix for inputs:\n",
    "                \n",
    "                \n",
    "                feed_dict.update({placeholders['dropout'] : dropout})\n",
    "                # Training step\n",
    "                # debug purpose, investigate dimensions of all the support vecotrs as input\n",
    "#                 outs = sess.run(model.hidden1, feed_dict=feed_dict)\n",
    "                # investigate the model vlaues we care about\n",
    "                outs = sess.run([model.opt_op, model.loss, model.accuracy], feed_dict=feed_dict)\n",
    "\n",
    "        total_training_time += time.time() - t\n",
    "        print_str = 'Epoch: %04d ' % (epoch + 1) + 'training time: {:.5f} '.format(\n",
    "            total_training_time) + 'train_acc= {:.5f} '.format(outs[2])\n",
    "\n",
    "        # Validation\n",
    "        cost, acc, micro, macro = evaluate(sess, model, val_features_batches, val_support_batches, y_val_batches,\n",
    "                                         val_mask_batches, val_data, placeholders)\n",
    "        cost_val.append(cost)\n",
    "        print_str += 'val_acc= {:.5f} '.format(acc) + 'mi F1= {:.5f} ma F1= {:.5f} '.format(micro, macro)\n",
    "\n",
    "        tf.logging.info(print_str)\n",
    "\n",
    "        if epoch > early_stopping_epoch_thresh and cost_val[-1] > np.mean(\n",
    "            cost_val[-(early_stopping_epoch_thresh + 1):-1]):\n",
    "            tf.logging.info('Early stopping...')\n",
    "            break\n",
    "\n",
    "    tf.logging.info('Optimization Finished!')\n",
    "\n",
    "    # Save model\n",
    "    saver.save(sess, train_save_name)\n",
    "\n",
    "    # Load model (using CPU for inference)\n",
    "#     /cpu:0\n",
    "    with tf.device('/cpu:0'):\n",
    "        sess_cpu = tf.Session(config=tf.ConfigProto(device_count={'GPU': 0}))\n",
    "        sess_cpu.run(tf.global_variables_initializer())\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess_cpu, train_save_name)\n",
    "        \n",
    "        # Testing\n",
    "        test_cost, test_acc, micro, macro = evaluate(\n",
    "            sess_cpu, model, test_features_batches, test_support_batches, y_test_batches, test_mask_batches, test_data, placeholders)\n",
    "        print_str = 'Test set results: ' + 'cost= {:.5f} '.format(test_cost) + 'accuracy= {:.5f} '.format(test_acc) + 'mi F1= {:.5f} ma F1= {:.5f}'.format(micro, macro)\n",
    "        tf.logging.info(print_str)\n",
    "        print(print_str)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use trivial dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index = torch.tensor([[0, 1, 1, 3, 1, 2, 4, 2, 4, 6, 6, 7, 7, 9, 2, 5, 9, 8], \n",
    "                           [1, 0, 3, 1, 2, 1, 2, 4, 6, 4, 7, 6, 9, 7, 5, 2, 8, 9]])\n",
    "# features = torch.rand(10, 3)\n",
    "features = torch.tensor([[0, 0], [0, 1], [0, 2], [0, 3], [0, 4],  \n",
    "                           [0, 5], [0, 6], [0, 7], [0, 8], [0, 9]], dtype = torch.float)\n",
    "# label = torch.tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "label = torch.tensor([0, 1, 1, 0, 1, 1, 1, 0, 0, 0])\n",
    "print(features, features.shape)\n",
    "\n",
    "clustering_folder = './res_save_batch/clustering/'\n",
    "check_folder_exist(clustering_folder)\n",
    "clustering_file_name = clustering_folder + 'check_clustering_machine.txt'\n",
    "os.makedirs(os.path.dirname(clustering_folder), exist_ok=True)\n",
    "info_folder = './res_save_batch/info/'\n",
    "check_folder_exist(info_folder)\n",
    "os.makedirs(os.path.dirname(info_folder), exist_ok=True)\n",
    "\n",
    "node_count = features.shape[0]\n",
    "get_edge_weight(edge_index, node_count, store_path = clustering_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_machine = ClusteringMachine(edge_index, features, label, clustering_folder, info_folder = clustering_folder)\n",
    "\n",
    "\"\"\"Main function for running experiments.\"\"\"\n",
    "(train_adj, full_adj, train_feats, test_feats, y_train, y_val, y_test,\n",
    "train_mask, val_mask, test_mask, _, val_data, test_data, num_data, num_class,\n",
    "visible_data) = clustering_machine.split_whole_nodes_edges_then_cluster(0.25, 0.25, normalize = False, precalc = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "#     tf.app.run(main)\n",
    "\n",
    "execute_one(2, 1, 400, dropout = 0.3, early_stopping_epoch_thresh = 300, diag_lambda = -1,\n",
    "               multilabel = True, layer_norm = True, precalc = True, train_save_name = './train_save.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Pytorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_data_root = '/media/xiangli/storage1/projects/tmpdata/'\n",
    "test_folder_name = 'test_mem_empty_cache/metis_train_10%_half_train_half_valid_layer_hop_one_less_layer/'\n",
    "from torch_geometric.datasets import Planetoid\n",
    "data_name = 'Cora'\n",
    "dataset = Planetoid(root = local_data_root + 'Planetoid/Cora', name=data_name)\n",
    "data = dataset[0]\n",
    "image_data_path = './results/' + data_name + '/' + test_folder_name\n",
    "# set the current folder as the intermediate data folder so that we can easily copy either clustering \n",
    "intermediate_data_folder = './'\n",
    "partition_nums = [2, 4, 8]\n",
    "layers = [[], [32], [32, 32]]\n",
    "\n",
    "tmp_folder = './tmp/'\n",
    "node_count = data.x.shape[0]\n",
    "get_edge_weight(data.edge_index, node_count, store_path = tmp_folder)\n",
    "\n",
    "clustering_machine = ClusteringMachine(data.edge_index, data.x, data.y, tmp_folder, info_folder = image_data_path)\n",
    "\n",
    "\"\"\"Main function for running experiments.\"\"\"\n",
    "(train_adj, full_adj, train_feats, test_feats, y_train, y_val, y_test,\n",
    "train_mask, val_mask, test_mask, _, val_data, test_data, num_data, num_class,\n",
    "visible_data) = clustering_machine.split_whole_nodes_edges_then_cluster(0.05, 0.85, normalize = False, precalc = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_one(2, 1, 400, dropout = 0.3, early_stopping_epoch_thresh = 300, diag_lambda = 1,\n",
    "               multilabel = True, layer_norm = True, precalc = True, train_save_name = './train_save.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free GPU memory\n",
    "# !(nvidia-smi | grep 'python' | awk '{ print $3 }' | xargs -n1 kill -9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_1_pytorch_geometric]",
   "language": "python",
   "name": "conda-env-tensorflow_1_pytorch_geometric-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
