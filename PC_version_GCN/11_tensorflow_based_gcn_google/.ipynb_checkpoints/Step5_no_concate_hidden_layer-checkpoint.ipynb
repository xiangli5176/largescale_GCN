{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# package from the pytorch side\n",
    "import copy\n",
    "import csv\n",
    "import sys\n",
    "import torch\n",
    "from pytorch_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/xiangli/anaconda3/envs/pytorch_geom_tensorflow_2/lib/python3.7/site-packages/tensorflow_core/python/compat/v2_compat.py:88: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import os\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "\n",
    "import networkx as nx\n",
    "import metis\n",
    "import sklearn\n",
    "import time\n",
    "# import models\n",
    "import numpy as np\n",
    "import partition_utils\n",
    "import utils\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n",
      "Num CPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "# tf.compat.v1.logging.ERROR\n",
    "# verbose information: tf.logging.INFO  \n",
    "\n",
    "tf.logging.set_verbosity(tf.compat.v1.logging.ERROR)  # Sets the threshold for what messages will be logged.\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Num CPUs Available: \", len(tf.config.experimental.list_physical_devices('CPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCN model used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inits\n",
    "import metrics\n",
    "\n",
    "def sparse_dropout(x, keep_prob, noise_shape):\n",
    "    \"\"\"Dropout for sparse tensors.\"\"\"\n",
    "    random_tensor = keep_prob\n",
    "    random_tensor += tf.compat.v1.random_uniform(noise_shape)\n",
    "    dropout_mask = tf.compat.v1.cast(tf.compat.v1.floor(random_tensor), dtype=tf.compat.v1.bool)\n",
    "    pre_out = tf.compat.v1.sparse_retain(x, dropout_mask)\n",
    "    return pre_out * (1. / keep_prob)\n",
    "\n",
    "\n",
    "def dot(x, y, sparse=False):\n",
    "    \"\"\"Wrapper for tf.compat.v1.matmul (sparse vs dense).\"\"\"\n",
    "    if sparse:\n",
    "        res = tf.compat.v1.sparse_tensor_dense_matmul(x, y)\n",
    "    else:\n",
    "        res = tf.compat.v1.matmul(x, y)\n",
    "    return res\n",
    "\n",
    "\n",
    "def layernorm(x, offset, scale):\n",
    "    \"\"\"\n",
    "        tf.nn.moments(x, axes, shift=None, keepdims=False, name=None)\n",
    "        The mean and variance are calculated by aggregating the contents of x across axes\n",
    "        keepdims:  \tproduce moments with the same dimensionality as the input.\n",
    "    \"\"\"\n",
    "    mean, variance = tf.compat.v1.nn.moments(x, axes=[1], keep_dims=True)\n",
    "    return tf.compat.v1.nn.batch_normalization(x, mean, variance, offset, scale, 1e-9)\n",
    "\n",
    "\n",
    "class Layer(object):\n",
    "    \"\"\"Base layer class.\n",
    "\n",
    "    Defines basic API for all layer objects.\n",
    "    Implementation inspired by keras (http://keras.io).\n",
    "\n",
    "    # Properties\n",
    "        name: String, defines the variable scope of the layer.\n",
    "        logging: Boolean, switches Tensorflow histogram logging on/off\n",
    "\n",
    "    # Methods\n",
    "        _call(inputs): Defines computation graph of layer\n",
    "            (i.e. takes input, returns output)\n",
    "        __call__(inputs): Wrapper for _call()\n",
    "        _log_vars(): Log all variables\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name = None, logging = False):\n",
    "        \n",
    "        if not name:\n",
    "            layer = self.__class__.__name__.lower()\n",
    "            name = layer + '_' \n",
    "\n",
    "        self.name = name\n",
    "        \n",
    "        self.vars = {}\n",
    "        self.logging = logging\n",
    "        \n",
    "        self.sparse_inputs = False\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        return inputs\n",
    "\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        with tf.compat.v1.name_scope(self.name):\n",
    "            if self.logging and not self.sparse_inputs:\n",
    "                tf.compat.v1.summary.histogram(self.name + '/inputs', inputs)\n",
    "\n",
    "        outputs = self._call(inputs)\n",
    "\n",
    "        if self.logging:\n",
    "            tf.compat.v1.summary.histogram(self.name + '/outputs', outputs)\n",
    "        return outputs\n",
    "\n",
    "    def _log_vars(self):\n",
    "        for var in self.vars:\n",
    "            tf.compat.v1.summary.histogram(self.name + '/vars/' + var, self.vars[var])\n",
    "\n",
    "            \n",
    "class GraphConvolution(Layer):\n",
    "    \"\"\"\n",
    "        Graph convolution layer.\n",
    "        Here each layer concatenate the embedding alongside the inputs\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, placeholders,\n",
    "               dropout=0., \n",
    "                sparse_inputs=False, \n",
    "                act=tf.nn.relu, \n",
    "                bias=False, \n",
    "                featureless=False, \n",
    "                norm=False,    # whether to do the batch_normalization after each layer\n",
    "                precalc=False,\n",
    "                name = None, \n",
    "                logging = False):\n",
    "        super(GraphConvolution, self).__init__(name = name, logging = logging)\n",
    "#         print('During the constructor of GCN layer, input dim: {} ; output dim: {}'.format(input_dim, output_dim))\n",
    "\n",
    "        if dropout:\n",
    "            self.dropout = placeholders['dropout']\n",
    "        else:\n",
    "            self.dropout = 0.\n",
    "\n",
    "        self.act = act\n",
    "        self.support = placeholders['support']\n",
    "\n",
    "        self.sparse_inputs = sparse_inputs\n",
    "        self.featureless = featureless\n",
    "        self.bias = bias\n",
    "        self.norm = norm\n",
    "        self.precalc = precalc\n",
    "\n",
    "        # helper variable for sparse dropout\n",
    "        # self.num_features_nonzero = placeholders['num_features_nonzero']\n",
    "\n",
    "        with tf.variable_scope(self.name + '_vars'):\n",
    "            self.vars['weights'] = inits.glorot([input_dim, output_dim], name='weights')\n",
    "            if self.bias:\n",
    "                self.vars['bias'] = inits.zeros([output_dim], name='bias')\n",
    "\n",
    "            if self.norm:\n",
    "                self.vars['offset'] = inits.zeros([1, output_dim], name='offset')\n",
    "                self.vars['scale'] = inits.ones([1, output_dim], name='scale')\n",
    "\n",
    "        if self.logging:\n",
    "            self._log_vars()\n",
    "    \n",
    "\n",
    "    def _call(self, inputs):\n",
    "        x = inputs\n",
    "\n",
    "        # convolve\n",
    "        if self.precalc:\n",
    "            support = x\n",
    "        else:\n",
    "            support = dot(self.support, x, sparse=True)\n",
    "#             support = tf.concat((support, x), axis=1)   # concatenate the self-loop features\n",
    "\n",
    "        # dropout\n",
    "        support = tf.nn.dropout(support, rate = self.dropout)\n",
    "        \"\"\"\n",
    "            More precisely: With probability rate elements of x are set to 0. \n",
    "            The remaining elements are scaled up by 1.0 / (1 - rate), so that the expected value is preserved.\n",
    "        \"\"\"\n",
    "        \n",
    "#         tf.Print(support, [support], \"During the call of GCN layer, final input to be multiplied with weights: \")\n",
    "\n",
    "#         print('\\n inside the call of convolutiongraph layer: ')\n",
    "#         print('support vecotr dimension is : {} ;'.format(support.shape), 'weight matrix dimension is : {} ;'.format(self.vars['weights'].shape))\n",
    "\n",
    "        output = dot(support, self.vars['weights'], sparse=self.sparse_inputs)\n",
    "\n",
    "        # bias\n",
    "        if self.bias:\n",
    "            output += self.vars['bias']\n",
    "\n",
    "        with tf.variable_scope(self.name + '_vars'):\n",
    "            if self.norm:\n",
    "                output = layernorm(output, self.vars['offset'], self.vars['scale'])\n",
    "\n",
    "        return self.act(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct GCN layer nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Collections of different Models.\"\"\"\n",
    "\n",
    "\n",
    "class Model(object):\n",
    "    \"\"\"Model class to be inherited.\"\"\"\n",
    "\n",
    "    def __init__(self, weight_decay = 0, \n",
    "                 num_layers = 2, name = None, logging = False, \n",
    "                 multilabel = False, norm = False, precalc = False):\n",
    "        \n",
    "        self.weight_decay = weight_decay\n",
    "        \n",
    "        \n",
    "        if not name:\n",
    "            name = self.__class__.__name__.lower()\n",
    "        self.name = name\n",
    "\n",
    "        self.logging = logging\n",
    "\n",
    "        self.vars = {}\n",
    "        self.placeholders = {}\n",
    "\n",
    "        self.layers = []\n",
    "        self.activations = []\n",
    "\n",
    "        self.inputs = None\n",
    "        self.outputs = None\n",
    "\n",
    "        self.loss = 0\n",
    "        self.accuracy = 0\n",
    "        self.pred = 0\n",
    "        self.optimizer = None\n",
    "        self.opt_op = None\n",
    "        \n",
    "        self.multilabel = multilabel\n",
    "        self.norm = norm\n",
    "        self.precalc = precalc\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    def _build(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def build(self):\n",
    "        \"\"\"Wrapper for _build().\"\"\"\n",
    "        with tf.variable_scope(self.name):\n",
    "            self._build()\n",
    "\n",
    "        # Build sequential layer model\n",
    "        self.activations.append(self.inputs)\n",
    "        \n",
    "        # debug to output the embedding:\n",
    "#         self.hidden1 = layer(self.activations[-1])\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            hidden = layer(self.activations[-1])\n",
    "        \n",
    "            if isinstance(hidden, tuple):\n",
    "                tf.logging.info('{} shape = {}'.format(layer.name, hidden[0].get_shape()))\n",
    "            else:\n",
    "                tf.logging.info('{} shape = {}'.format(layer.name, hidden.get_shape()))\n",
    "\n",
    "            self.activations.append(hidden)\n",
    "            \n",
    "        self.outputs = self.activations[-1]\n",
    "\n",
    "        # Store model variables for easy access\n",
    "        variables = tf.get_collection( tf.GraphKeys.GLOBAL_VARIABLES, scope=self.name)\n",
    "        # GLOBAL_VARIABLES: the default collection of Variable objects, shared across distributed environment (model variables are subset of these).\n",
    "        self.vars = variables\n",
    "        for k in self.vars:\n",
    "            tf.logging.info((k.name, k.get_shape()))\n",
    "\n",
    "        # Build metrics\n",
    "        self._loss()\n",
    "        self._accuracy()\n",
    "        self._predict()\n",
    "\n",
    "        self.opt_op = self.optimizer.minimize(self.loss)\n",
    "\n",
    "    def _loss(self):\n",
    "        \"\"\"Construct the loss function.\"\"\"\n",
    "        # Weight decay loss\n",
    "        if self.weight_decay > 0.0:\n",
    "            for var in self.layers[0].vars.values():\n",
    "                self.loss += self.weight_decay * tf.nn.l2_loss(var)\n",
    "\n",
    "        # Cross entropy error\n",
    "        if self.multilabel:\n",
    "            self.loss += metrics.masked_sigmoid_cross_entropy(self.outputs, self.placeholders['labels'], self.placeholders['labels_mask'])\n",
    "        else:\n",
    "            self.loss += metrics.masked_softmax_cross_entropy(self.outputs, self.placeholders['labels'], self.placeholders['labels_mask'])\n",
    "\n",
    "    def _accuracy(self):\n",
    "        if self.multilabel:\n",
    "            # use the outputs directly, without any softmax or sigmoid\n",
    "            self.accuracy = metrics.masked_accuracy_multilabel(self.outputs, self.placeholders['labels'], self.placeholders['labels_mask'])\n",
    "        else:\n",
    "            self.accuracy = metrics.masked_accuracy(self.outputs, self.placeholders['labels'], self.placeholders['labels_mask'])\n",
    "\n",
    "    def _predict(self):\n",
    "        # this prediction to generate the possibility distribution is not used\n",
    "        if self.multilabel:\n",
    "            self.pred = tf.nn.sigmoid(self.outputs)\n",
    "        else:\n",
    "            self.pred = tf.nn.softmax(self.outputs)\n",
    "\n",
    "    def save(self, sess=None):\n",
    "        if not sess:\n",
    "            raise AttributeError('TensorFlow session not provided.')\n",
    "        saver = tf.train.Saver(self.vars)\n",
    "        save_path = saver.save(sess, './tmp/%s.ckpt' % self.name)\n",
    "        tf.logging.info('Model saved in file:', save_path)\n",
    "\n",
    "    def load(self, sess=None):\n",
    "        if not sess:\n",
    "            raise AttributeError('TensorFlow session not provided.')\n",
    "        saver = tf.train.Saver(self.vars)\n",
    "        save_path = './tmp/%s.ckpt' % self.name\n",
    "        saver.restore(sess, save_path)\n",
    "        tf.logging.info('Model restored from file:', save_path)\n",
    "\n",
    "\n",
    "class GCN(Model):\n",
    "    \"\"\"Implementation of GCN model.\"\"\"\n",
    "\n",
    "    def __init__(self, placeholders, input_dim, output_dim, hidden_neuron_num, learning_rate = 0.01, \n",
    "                 num_layers = 3, name = None, logging = False, multilabel = False, norm = False, precalc = False):\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.hidden1 = hidden_neuron_num\n",
    "        \n",
    "        super(GCN, self).__init__(weight_decay = 0, num_layers = num_layers, name = name, logging = logging, \\\n",
    "                                  multilabel = multilabel, norm = norm, precalc = precalc)\n",
    "\n",
    "        self.inputs = placeholders['features']\n",
    "        self.input_dim = input_dim\n",
    "#         self.output_dim = placeholders['labels'].get_shape().as_list()[1]\n",
    "        self.output_dim = output_dim\n",
    "    \n",
    "        self.placeholders = placeholders\n",
    "\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate = self.learning_rate)\n",
    "\n",
    "        self.build()\n",
    "\n",
    "    def _build(self):\n",
    "         # so here at least there are two layers...\n",
    "        self.layers.append(\n",
    "            GraphConvolution(\n",
    "                input_dim = self.input_dim if self.precalc else self.input_dim * 2,  # double of the num of columns of original graph feature matrix\n",
    "                output_dim = self.hidden1,\n",
    "                placeholders = self.placeholders,\n",
    "                act=tf.nn.relu,\n",
    "                dropout = True,\n",
    "                sparse_inputs = False,\n",
    "                logging = self.logging,\n",
    "                norm = self.norm,\n",
    "                precalc = self.precalc))\n",
    "        \n",
    "        for _ in range(self.num_layers - 2):\n",
    "            self.layers.append(\n",
    "              GraphConvolution(\n",
    "                  input_dim = self.hidden1,\n",
    "                  output_dim = self.hidden1,\n",
    "                  placeholders = self.placeholders,\n",
    "                  act=tf.nn.relu,\n",
    "                  dropout = True,\n",
    "                  sparse_inputs = False,\n",
    "                  logging = self.logging,\n",
    "                  norm = self.norm,\n",
    "                  precalc = False))\n",
    "        \n",
    "        # for the last layer: no normalizaton, no activation, can still have dropout !!!\n",
    "        self.layers.append(\n",
    "            GraphConvolution(\n",
    "                input_dim = self.hidden1,\n",
    "                output_dim = self.output_dim,\n",
    "                placeholders = self.placeholders,\n",
    "                act = lambda x: x,    # for the last layer, no relu\n",
    "                dropout = True,\n",
    "                logging = self.logging,\n",
    "                norm = False,\n",
    "                precalc = False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_scatter import scatter_add\n",
    "from torch_geometric.utils import add_remaining_self_loops\n",
    "\n",
    "def get_edge_weight(edge_index, num_nodes, edge_weight = None, improved = False, dtype=None, store_path='./tmp/'):\n",
    "    \"\"\"\n",
    "        Purpose: get the orignal weights inside the graph\n",
    "        \n",
    "        edge_index(ndarray): undirected edge index (two-directions both included)\n",
    "        num_nodes(int):  number of nodes inside the graph\n",
    "        edge_weight(ndarray): if any weights already assigned, otherwise will be generated \n",
    "        improved(boolean):   may assign 2 to the self loop weight if true\n",
    "        store_path(string): the path of the folder to contain all the clustering information files\n",
    "    \"\"\"\n",
    "    # calculate the global graph properties, global edge weights\n",
    "    if edge_weight is None:\n",
    "        edge_weight = torch.ones((edge_index.size(1), ), dtype=dtype, device=edge_index.device)\n",
    "\n",
    "    fill_value = 1 if not improved else 2\n",
    "    # there are num_nodes self-loop edges added after the edge_index\n",
    "    edge_index, edge_weight = add_remaining_self_loops(edge_index, edge_weight, fill_value, num_nodes)\n",
    "\n",
    "    row, col = edge_index   \n",
    "    # row includes the starting points of the edges  (first row of edge_index)\n",
    "    # col includes the ending points of the edges   (second row of edge_index)\n",
    "\n",
    "    deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n",
    "    # row records the source nodes, which is the index we are trying to add\n",
    "    # deg will record the out-degree of each node of x_i in all edges (x_i, x_j) including self_loops\n",
    "\n",
    "    deg_inv_sqrt = deg.pow(-0.5)\n",
    "    deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "    # normalize the edge weight\n",
    "    normalized_edge_weight = deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n",
    "\n",
    "    # transfer from tensor to the numpy to construct the dict for the edge_weights\n",
    "    edge_index = edge_index.t().numpy()\n",
    "    normalized_edge_weight = normalized_edge_weight.numpy()\n",
    "\n",
    "    num_edge = edge_index.shape[0]\n",
    "\n",
    "    output = ([edge_index[i][0], edge_index[i][1], normalized_edge_weight[i]] for i in range(num_edge))\n",
    "\n",
    "    # output the edge weights as the csv file\n",
    "    input_edge_weight_txt_file = store_path + 'input_edge_weight_list.csv'\n",
    "    os.makedirs(os.path.dirname(input_edge_weight_txt_file), exist_ok=True)\n",
    "    with open(input_edge_weight_txt_file, 'w', newline='\\n') as fp:\n",
    "        wr = csv.writer(fp, delimiter = ' ')\n",
    "        for line in output:\n",
    "            wr.writerow(line)\n",
    "    return input_edge_weight_txt_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import metis\n",
    "import random\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import chain\n",
    "import scipy.sparse as sp\n",
    "\n",
    "def construct_adj(edges, nodes_count):\n",
    "    \"\"\"\n",
    "        This is to genrate the edge weights inside the graph\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compressed Sparse Row matrix\n",
    "    # csr_matrix((data, ij), [shape=(M, N)])\n",
    "    # where data and ij satisfy the relationship a[ij[0, k], ij[1, k]] = data[k]\n",
    "    adj = sp.csr_matrix( ( np.ones((edges.shape[0]), dtype=np.float32), (edges[:, 0], edges[:, 1]) ), shape=(nodes_count, nodes_count) )\n",
    "    adj += adj.transpose()   # double the weight of each edge if it is two direction\n",
    "    return adj\n",
    "\n",
    "class ClusteringMachine(object):\n",
    "    \"\"\"\n",
    "    Clustering the graph, feature set and label. Performed on the CPU side\n",
    "    \"\"\"\n",
    "    def __init__(self, edge_index, features, label, tmp_folder = './tmp/', info_folder = './info/', multilabel = True):\n",
    "        \"\"\"\n",
    "        :param edge_index (torch.tensor): COO format of the edge indices.\n",
    "        :param features (torch.tensor): Feature matrix .\n",
    "        :param label (torch.tensor): label vector .\n",
    "        :tmp_folder(string): the path of the folder to contain all the clustering information files\n",
    "        :multilabel (bool) : whether this is a multilabel task or not\n",
    "        \"\"\"\n",
    "        self.features = features\n",
    "        self.label = label\n",
    "        self.multilabel = multilabel\n",
    "        self._set_sizes()\n",
    "        self.edge_index = edge_index\n",
    "        # store the information folder for memory tracing\n",
    "        self.tmp_folder = tmp_folder\n",
    "        self.info_folder = info_folder\n",
    "        \n",
    "        # first, use the get edge weights func to construct the two-sided self-loop added graph\n",
    "        edge_weight_file = self.tmp_folder + 'input_edge_weight_list.csv'\n",
    "        self.graph = nx.read_weighted_edgelist(edge_weight_file, create_using = nx.Graph, nodetype = int)\n",
    "        \n",
    "#         # second construct teh graph directly from the edge_list, here we are lacking the self-loop age\n",
    "#         tmp = edge_index.t().numpy().tolist()\n",
    "#         self.graph = nx.from_edgelist(tmp)\n",
    "        \n",
    "    def _set_sizes(self):\n",
    "        \"\"\"\n",
    "        Setting the feature and class count.\n",
    "        \"\"\"\n",
    "        self.node_count = self.features.shape[0]\n",
    "        self.feature_count = self.features.shape[1]    # features all always in the columns\n",
    "        if self.multilabel:\n",
    "            self.label_count = self.label.shape[1]\n",
    "        else:\n",
    "            self.label_count = len(np.unique(self.label.numpy()) )\n",
    "    \n",
    "    # 2) first assign train, test, validation nodes, split edges; this is based on the assumption that the clustering is no longer that important\n",
    "    def split_whole_nodes_edges_then_cluster(self, validation_ratio, test_ratio, normalize = False, precalc = True):\n",
    "        \"\"\"\n",
    "            Only split nodes\n",
    "            First create train-test splits, then split train and validation into different batch seeds\n",
    "            Input:  \n",
    "                1) ratio of test, validation\n",
    "                2) partition number of train nodes, test nodes, validation nodes\n",
    "            Output:\n",
    "                1) sg_validation_nodes_global, sg_train_nodes_global, sg_test_nodes_global\n",
    "        \"\"\"\n",
    "        relative_validation_ratio = (validation_ratio) / (1 - test_ratio)\n",
    "        \n",
    "        # first divide the nodes for the whole graph, result will always be a list of lists \n",
    "        model_nodes_global, self.test_nodes_global = train_test_split(list(self.graph.nodes()), test_size = test_ratio)\n",
    "        self.train_nodes_global, self.valid_nodes_global = train_test_split(model_nodes_global, test_size = relative_validation_ratio)\n",
    "        \n",
    "        edges = np.array(self.graph.edges(), dtype=np.int32)\n",
    "        self.feats = np.array(self.features.numpy(), dtype=np.float32)\n",
    "        \n",
    "        \n",
    "        # normalize all the features\n",
    "        if normalize:\n",
    "            scaler = sklearn.preprocessing.StandardScaler()\n",
    "            scaler.fit(self.feats)\n",
    "            self.feats = scaler.transform(self.feats)\n",
    "        \n",
    "        # this is for the multi-class case\n",
    "        if self.multilabel:\n",
    "            self.labels = np.array(self.label.numpy(), dtype=np.float32)\n",
    "        else:\n",
    "            classes = np.array(self.label.numpy(), dtype=np.int32)\n",
    "            # construct the 1-hot labels:\n",
    "            self.labels = np.zeros((self.node_count, self.label_count), dtype=np.float32)\n",
    "            for i, label in enumerate(classes):\n",
    "                self.labels[i, label] = 1\n",
    "            \n",
    "        self.full_adj = construct_adj(edges, self.node_count)\n",
    "    \n",
    "    def train_batch_generation(self, mini_cluster_num = 2, diag_lambda = -1, precalc = True, mini_block_size = 1):\n",
    "        # all the properties needed for the training\n",
    "        train_subgraph = self.graph.subgraph(self.train_nodes_global)\n",
    "        train_edges = np.array(train_subgraph.edges(), dtype=np.int32)\n",
    "        # check the double direction inside this adjacent matrix\n",
    "        train_adj = construct_adj(train_edges, self.node_count)\n",
    "        train_feats = self.feats[self.train_nodes_global]\n",
    "        \n",
    "        y_train = np.zeros(self.labels.shape)\n",
    "        y_train[self.train_nodes_global, :] = self.labels[self.train_nodes_global, :]\n",
    "        train_mask = utils.sample_mask(self.train_nodes_global, self.labels.shape[0])\n",
    "        \n",
    "        if precalc:\n",
    "            train_feats = train_adj.dot(self.feats)   # calculate the feature matrix weighted by the edge weights\n",
    "            # numpy.hstack: Stack arrays in sequence horizontally (column wise).\n",
    "            train_feats = np.hstack((train_feats, self.feats))\n",
    "        visible_data = self.train_nodes_global\n",
    "        \n",
    "        # Partition graph and do preprocessing\n",
    "        if mini_block_size > 1:\n",
    "            _, parts = partition_utils.partition_graph(train_adj, visible_data, mini_cluster_num)\n",
    "            parts = [np.array(pt) for pt in parts]\n",
    "            # train_adj: adjacent matrix of train nodes\n",
    "            # parts: groups of data divided into different train batches\n",
    "            # only features for train nodes\n",
    "            return train_adj, parts, train_feats, y_train, train_mask\n",
    "        else:\n",
    "            (parts, features_batches, support_batches, y_train_batches, train_mask_batches) = \\\n",
    "            utils.preprocess(train_adj, train_feats, y_train, train_mask, visible_data, mini_cluster_num, diag_lambda)\n",
    "            return parts, features_batches, support_batches, y_train_batches, train_mask_batches\n",
    "    \n",
    "    \n",
    "    \n",
    "    def test_batch_generation(self, test_batch_num = 2, diag_lambda = -1, precalc = True):\n",
    "        # all the properties needed for the test\n",
    "        self.test_feats = self.feats   # why test gives the full feature of the whole graph\n",
    "        y_test = np.zeros(self.labels.shape)\n",
    "        y_test[self.test_nodes_global, :] = self.labels[self.test_nodes_global, :]\n",
    "        test_mask = utils.sample_mask(self.test_nodes_global, self.labels.shape[0])\n",
    "        if precalc:\n",
    "            self.test_feats = self.full_adj.dot(self.feats)\n",
    "            self.test_feats = np.hstack((self.test_feats, self.feats))\n",
    "            \n",
    "        (_, test_features_batches, test_support_batches, y_test_batches, test_mask_batches) = \\\n",
    "            utils.preprocess(self.full_adj, self.test_feats, y_test, test_mask, np.arange(self.node_count), test_batch_num, diag_lambda)\n",
    "        return test_features_batches, test_support_batches, y_test_batches, test_mask_batches, self.test_nodes_global\n",
    "        \n",
    "    def validation_batch_generation(self, valid_batch_num = 2, diag_lambda = -1, precalc = True):\n",
    "        # all the properties needed for the test\n",
    "        self.val_feats = self.feats   # why test gives the full feature of the whole graph\n",
    "        y_val = np.zeros(self.labels.shape)\n",
    "        y_val[self.valid_nodes_global, :] = self.labels[self.valid_nodes_global, :]\n",
    "        val_mask = utils.sample_mask(self.valid_nodes_global, self.labels.shape[0])\n",
    "        # start to precalculate :\n",
    "        if precalc:\n",
    "            self.val_feats = self.full_adj.dot(self.feats)\n",
    "            self.val_feats = np.hstack((self.val_feats, self.feats))\n",
    "            \n",
    "        (_, val_features_batches, val_support_batches, y_val_batches, val_mask_batches) = \\\n",
    "            utils.preprocess(self.full_adj, self.val_feats, y_val, val_mask, np.arange(self.node_count), valid_batch_num, diag_lambda)\n",
    "        return val_features_batches, val_support_batches, y_val_batches, val_mask_batches, self.valid_nodes_global\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the Graph_sage format data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy_from_metric(y_pred, y_true, labels_mask, multilabel):\n",
    "    \"\"\" \n",
    "        call the implemented metric masked accuracy inside the framework\n",
    "        For the multi-label: still use the 1-hot \n",
    "        y_pred: whole outputs, did not use any softmax or sigmoid activation, just use the direct final layer output\n",
    "        y_label: whole labels\n",
    "    \"\"\"\n",
    "    if multilabel:\n",
    "        # use the outputs directly, without any softmax or sigmoid\n",
    "        accuracy = metrics.masked_accuracy_multilabel(y_pred, y_true, labels_mask)\n",
    "    else:\n",
    "        accuracy = metrics.masked_accuracy(y_pred, y_true, labels_mask)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def test_customized_accuracy(y_pred, y_true, multilabel):\n",
    "    \"\"\" \n",
    "        Implement a customized version using the same logic\n",
    "        For the multi-label: still use the 1-hot \n",
    "        y_pred : the masked outputs, final layer of the GCN neural nets\n",
    "        y_label: the masked labels\n",
    "    \"\"\"\n",
    "    if multilabel:\n",
    "        y_pred[y_pred > 0] = 1\n",
    "        y_pred[y_pred <= 0] = 0\n",
    "        correct_prediction = tf.equal(y_pred, y_true)\n",
    "    else:\n",
    "        y_true = np.argmax(y_true, axis=1)\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "        correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.argmax(y_true, 1))\n",
    "    \n",
    "    accuracy_all = tf.cast(correct_prediction, tf.float32)\n",
    "#     sklearn.metrics.accuracy_score(y_true, y_pred, normalize=True)\n",
    "\n",
    "    return tf.reduce_mean(accuracy_all)\n",
    "\n",
    "\n",
    "def test_sklearn_accuracy(y_pred, y_true, multilabel):\n",
    "    \"\"\" \n",
    "        Implement a customized version using the same logic\n",
    "        For the multi-label: still use the 1-hot \n",
    "        y_pred : the masked outputs, final layer of the GCN neural nets\n",
    "        y_label: the masked labels\n",
    "    \"\"\"\n",
    "    if multilabel:\n",
    "        y_pred[y_pred > 0] = 1\n",
    "        y_pred[y_pred <= 0] = 0\n",
    "    else:\n",
    "        y_true = np.argmax(y_true, axis=1)\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    return sklearn.metrics.accuracy_score(y_true, y_pred, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model evaluation function\n",
    "def evaluate(sess, model, val_features_batches, val_support_batches,\n",
    "             y_val_batches, val_mask_batches, val_data, placeholders, multilabel = True):\n",
    "    \"\"\"evaluate GCN model.\n",
    "        sess:  graph session\n",
    "        model: \n",
    "    \n",
    "    \"\"\"\n",
    "#     print('inside the evaluation func:')\n",
    "    total_pred = []   # predicted classes from the trained model\n",
    "    total_lab = []    # true labels\n",
    "    total_loss = 0    # miss prediction \n",
    "    total_acc = 0     # accurately predicting the labels\n",
    "\n",
    "    num_batches = len(val_features_batches)\n",
    "    for i in range(num_batches):\n",
    "        features_b = val_features_batches[i]\n",
    "        support_b = val_support_batches[i]\n",
    "        y_val_b = y_val_batches[i]\n",
    "        val_mask_b = val_mask_batches[i]\n",
    "        num_data_b = np.sum(val_mask_b)\n",
    "        if num_data_b == 0:\n",
    "            # there is no validation data:\n",
    "            continue\n",
    "        else:\n",
    "            feed_dict = utils.construct_feed_dict(features_b, support_b, y_val_b, val_mask_b, placeholders)\n",
    "            outs = sess.run([model.loss, model.accuracy, model.outputs], feed_dict = feed_dict)\n",
    "\n",
    "        total_pred.append(outs[2][val_mask_b])\n",
    "        total_lab.append(y_val_b[val_mask_b])\n",
    "        \n",
    "#         # ============ debug purpose to test the accuracy metrics ====================\n",
    "#         print('For the test batch number # ', i)\n",
    "#         # debug here: to calculate each batch accuracy:\n",
    "#         # 1) the reference as from the tensorflow trained model : outs[1]\n",
    "#         print('current accuracy from model.accuracy: ', outs[1])\n",
    "        \n",
    "#         # 2) use the metric accuracy for masked data implemented in this framework\n",
    "#         calc_metric_accuracy = test_accuracy_from_metric(outs[2], y_val_b, val_mask_b, multilabel)\n",
    "#         # for each tensor operation, use the session.run to fetch the value\n",
    "#         calc_metric_accuracy = sess.run(calc_metric_accuracy, feed_dict = feed_dict)\n",
    "#         print('calculated accuracy from model.outputs: ', calc_metric_accuracy)\n",
    "        \n",
    "#         # 3) based on the logic we learn, build our own accuracy calcuation function: extrac all the mask value first\n",
    "#         local_pred = outs[2][val_mask_b]\n",
    "#         local_lab = y_val_b[val_mask_b]\n",
    "        \n",
    "#         custom_accuracy = test_customized_accuracy(local_pred, local_lab, multilabel)\n",
    "#         custom_accuracy = sess.run(custom_accuracy, feed_dict = feed_dict)\n",
    "#         print('customized accuracy from model.outputs: ', custom_accuracy)\n",
    "#         # call the customized metrics to calculate the accray with the masked data\n",
    "        \n",
    "#         # 4) Using the suspicious sklearn logic\n",
    "#         sklearn_accuracy = test_sklearn_accuracy(local_pred, local_lab, multilabel)\n",
    "#         print('sklearn_accuracy from model.outputs: ', sklearn_accuracy)\n",
    "#         # ============ End of debugging ======================\n",
    "        \n",
    "        total_loss += outs[0] * num_data_b\n",
    "        total_acc += outs[1] * num_data_b\n",
    "\n",
    "    total_pred = np.vstack(total_pred)  # \n",
    "    total_lab = np.vstack(total_lab)\n",
    "    loss = total_loss / len(val_data)\n",
    "    # this acc1 is calculated by hand\n",
    "    acc = total_acc / len(val_data)\n",
    "    # this acc is calculated from the sklearn\n",
    "#     print('total accumuated accuracy :')\n",
    "    micro, macro, acc_sklearn = utils.calc_f1(total_pred, total_lab, multilabel)\n",
    "#     print('\\nhand calculated loss rate is: ', loss)\n",
    "#     print('\\nhand calculated accuracy is: ', acc)\n",
    "#     print('sklearn calculated accuracy is: ', acc_sklearn)\n",
    "    return loss, acc, micro, macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aritificial convertion of single label accuracy to multi-label accuracy in 1-hot case\n",
    "def obtain_multi_label_accuracy(label_count, node_count, single_label_accuracy):\n",
    "    accurate_node = int(node_count * single_label_accuracy)\n",
    "    total_label = label_count * node_count\n",
    "    actual_accurate_label = accurate_node * label_count + (node_count - accurate_node) * (label_count - 2)\n",
    "    return actual_accurate_label / total_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusterGCNTrainer(object):\n",
    "    \"\"\"\n",
    "    Training a ClusterGCN.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim, num_layers = 3, hidden_neuron_num = 16, learning_rate = 0.001, \\\n",
    "                 logging = False, multilabel = True, norm = True, precalc = True):\n",
    "        \n",
    "        # Define self.placeholders\n",
    "        self.placeholders = {\n",
    "          'support':\n",
    "              tf.sparse_placeholder(tf.float32),\n",
    "          'features':\n",
    "              tf.placeholder(tf.float32),\n",
    "          'labels':\n",
    "              tf.placeholder(tf.float32),\n",
    "          'labels_mask':\n",
    "              tf.placeholder(tf.int32),\n",
    "          'dropout':\n",
    "              tf.placeholder_with_default(0., shape=()),\n",
    "    #       'num_features_nonzero':\n",
    "    #           tf.placeholder(tf.int32)  # helper variable for sparse dropout\n",
    "        }\n",
    "        \n",
    "        now_input_dim = 2 * input_dim if precalc else input_dim\n",
    "        self.model = GCN(self.placeholders,\n",
    "              input_dim = now_input_dim,\n",
    "              output_dim = output_dim,\n",
    "              hidden_neuron_num = hidden_neuron_num,\n",
    "              learning_rate = learning_rate,\n",
    "              logging = logging,\n",
    "              multilabel = multilabel,\n",
    "              norm = norm,\n",
    "              precalc = precalc,\n",
    "              num_layers = num_layers)\n",
    "        \n",
    "        self.multilabel = multilabel\n",
    "        self.precalc = precalc\n",
    "        \n",
    "    def train(self, clustering_machine, seed = 6, diag_lambda = 1, mini_block_size = 1, \\\n",
    "                               mini_cluster_num = 2, valid_batch_num = 2, \\\n",
    "                               epoch_num = 400, mini_epoch_num = 20, dropout = 0.3, train_save_name = './tmp/saver.txt', validation_period = 10):\n",
    "        # load the needed data\n",
    "        # data for training:\n",
    "        if mini_block_size > 1:\n",
    "            train_adj, parts, train_feats, y_train, train_mask = \\\n",
    "                    clustering_machine.train_batch_generation(mini_cluster_num = mini_cluster_num,\n",
    "                                                diag_lambda = diag_lambda, precalc = self.precalc, mini_block_size = mini_block_size)\n",
    "        else:\n",
    "            parts, features_batches, support_batches, y_train_batches, train_mask_batches = \\\n",
    "                    clustering_machine.train_batch_generation(mini_cluster_num = mini_cluster_num, \\\n",
    "                                                diag_lambda = diag_lambda, precalc = self.precalc, mini_block_size = mini_block_size)\n",
    "        \n",
    "        \n",
    "        \n",
    "        val_features_batches, val_support_batches, y_val_batches, val_mask_batches, val_data = \\\n",
    "                    clustering_machine.validation_batch_generation(valid_batch_num = valid_batch_num, diag_lambda = diag_lambda)\n",
    "        \n",
    "        idx_parts = list(range(len(parts)))\n",
    "        \n",
    "        # Initialize session\n",
    "        sess = tf.Session()\n",
    "        tf.set_random_seed(seed)\n",
    "\n",
    "        # Init variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "        cost_val = []\n",
    "        self.time_train_total = 0.0\n",
    "        # Train model:  epoch_num\n",
    "        epoch_partition = epoch_num // mini_epoch_num\n",
    "        for epoch in range(epoch_partition):\n",
    "            t0 = time.time()\n",
    "            np.random.shuffle(idx_parts)\n",
    "\n",
    "            # recombine mini-clusters to form larger batches\n",
    "            if mini_block_size > 1:\n",
    "                (features_batches, support_batches, y_train_batches,\n",
    "                train_mask_batches) = utils.preprocess_multicluster(\n",
    "                   train_adj, parts, train_feats, y_train, train_mask,\n",
    "                   mini_cluster_num, mini_block_size, diag_lambda)\n",
    "\n",
    "                for pid in range(len(features_batches)):\n",
    "                    # Use preprocessed batch data\n",
    "                    features_b = features_batches[pid]\n",
    "                    support_b = support_batches[pid]\n",
    "                    y_train_b = y_train_batches[pid]\n",
    "                    train_mask_b = train_mask_batches[pid]\n",
    "                    # Construct feed dictionary\n",
    "                    feed_dict = utils.construct_feed_dict(features_b, support_b, y_train_b, train_mask_b, self.placeholders)\n",
    "\n",
    "                    feed_dict.update({self.placeholders['dropout']: dropout})\n",
    "                    for real_mini_epoch in range(mini_epoch_num):\n",
    "                        # Training step\n",
    "                        outs = sess.run([self.model.opt_op, self.model.loss, self.model.accuracy], feed_dict = feed_dict)\n",
    "                        real_epoch_num = epoch * mini_epoch_num + real_mini_epoch + 1\n",
    "                        if real_epoch_num % validation_period == 0:\n",
    "                            # Validation\n",
    "                            validation_cost, validation_acc, validation_micro, validation_macro = \\\n",
    "                                                    evaluate( sess, self.model, val_features_batches, val_support_batches, y_val_batches,\n",
    "                                                             val_mask_batches, val_data, self.placeholders, multilabel = self.multilabel)\n",
    "                            print('During Validation epoch {}: val_acc= {:.5f} '.format(real_epoch_num, validation_acc) + 'micro F1= {:.5f} macro F1= {:.5f} '.format(validation_micro, validation_macro) )\n",
    "            else:\n",
    "                np.random.shuffle(idx_parts)\n",
    "\n",
    "                for pid in idx_parts:\n",
    "                    # Use preprocessed batch data\n",
    "                    features_b = features_batches[pid]\n",
    "                    support_b = support_batches[pid]\n",
    "                    y_train_b = y_train_batches[pid]\n",
    "                    train_mask_b = train_mask_batches[pid]\n",
    "\n",
    "                    # Construct feed dictionary\n",
    "                    feed_dict = utils.construct_feed_dict(features_b, support_b, y_train_b, train_mask_b, self.placeholders)\n",
    "\n",
    "                    # investigate the constructed matrix for inputs:\n",
    "                    feed_dict.update({self.placeholders['dropout'] : dropout})\n",
    "                    for real_mini_epoch in range(mini_epoch_num):\n",
    "                        # Training step:\n",
    "                        outs = sess.run([self.model.opt_op, self.model.loss, self.model.accuracy], feed_dict=feed_dict)\n",
    "                        real_epoch_num = epoch * mini_epoch_num + real_mini_epoch + 1\n",
    "                        if real_epoch_num % validation_period == 0:\n",
    "                            # Validation\n",
    "                            validation_cost, validation_acc, validation_micro, validation_macro = \\\n",
    "                                                    evaluate( sess, self.model, val_features_batches, val_support_batches, y_val_batches,\n",
    "                                                             val_mask_batches, val_data, self.placeholders, multilabel = self.multilabel)\n",
    "                            print('During Validation epoch {}: val_acc= {:.5f} '.format(real_epoch_num, validation_acc) + 'micro F1= {:.5f} macro F1= {:.5f} '.format(validation_micro, validation_macro) )\n",
    "\n",
    "                            \n",
    "\n",
    "            self.time_train_total += time.time() - t0\n",
    "            \n",
    "            print_str = 'Epoch: %04d ' % (epoch + 1) + 'training time: {:.5f} '.format(\n",
    "                self.time_train_total) + 'train_acc= {:.5f} '.format(outs[2])\n",
    "            \n",
    "            \n",
    "            \n",
    "#             tf.logging.info(print_str)\n",
    "\n",
    "#             if epoch > early_stopping_epoch_thresh and cost_val[-1] > np.mean(\n",
    "#                 cost_val[-(early_stopping_epoch_thresh + 1):-1]):\n",
    "#                 tf.logging.info('Early stopping...')\n",
    "#                 break\n",
    "\n",
    "        tf.logging.info('Optimization Finished!')\n",
    "\n",
    "        # Save model\n",
    "        saver.save(sess, train_save_name)\n",
    "        \n",
    "    def test(self, clustering_machine, seed = 6, diag_lambda = 1, \\\n",
    "                               test_batch_num = 2, train_save_name = './tmp/saver.txt'):\n",
    "        \n",
    "        test_features_batches, test_support_batches, y_test_batches, test_mask_batches, test_data = \\\n",
    "                    clustering_machine.test_batch_generation(test_batch_num = test_batch_num, diag_lambda = diag_lambda, precalc = self.precalc)\n",
    "        \n",
    "        with tf.device('/cpu:0'):\n",
    "            sess_cpu = tf.Session(config=tf.ConfigProto(device_count={'GPU': 0}))  # set the usable GPU count as 0, therefore only use CPU\n",
    "            sess_cpu.run(tf.global_variables_initializer())\n",
    "            saver = tf.train.Saver()\n",
    "            # load the trained model into the current session through the saved file: train_save_name which contains the trained model saver\n",
    "            saver.restore(sess_cpu, train_save_name)   \n",
    "            # The Saver class adds ops to save and restore variables to and from checkpoints. It also provides convenience methods to run these ops.\n",
    "\n",
    "            # Testing\n",
    "            test_cost, test_acc, micro, macro = evaluate(\n",
    "                sess_cpu, self.model, test_features_batches, test_support_batches, y_test_batches, test_mask_batches, test_data, self.placeholders, multilabel = self.multilabel)\n",
    "\n",
    "            print_str = 'Test set results: ' + 'cost= {:.5f} '.format(test_cost) + 'accuracy= {:.5f} '.format(test_acc) + 'mi F1= {:.5f} ma F1= {:.5f}'.format(micro, macro)\n",
    "            tf.logging.info(print_str)\n",
    "#             print(print_str)\n",
    "        \n",
    "        converted_acc = obtain_multi_label_accuracy(clustering_machine.label_count, clustering_machine.node_count, micro)\n",
    "        \n",
    "        print(\"micro-F1 is : {} and the accuracy is : {}\".format(micro, test_acc))\n",
    "        print(\"artificially converted multi-label accuracy is : {}\".format(converted_acc))\n",
    "        \n",
    "        return micro, test_acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use trivial dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 2.],\n",
      "        [0., 3.],\n",
      "        [0., 4.],\n",
      "        [0., 5.],\n",
      "        [0., 6.],\n",
      "        [0., 7.],\n",
      "        [0., 8.],\n",
      "        [0., 9.]]) torch.Size([10, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./res_save_batch/clustering/input_edge_weight_list.csv'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_index = torch.tensor([[0, 1, 1, 3, 1, 2, 4, 2, 4, 6, 6, 7, 7, 9, 2, 5, 9, 8], \n",
    "                           [1, 0, 3, 1, 2, 1, 2, 4, 6, 4, 7, 6, 9, 7, 5, 2, 8, 9]])\n",
    "# features = torch.rand(10, 3)\n",
    "features = torch.tensor([[0, 0], [0, 1], [0, 2], [0, 3], [0, 4],  \n",
    "                           [0, 5], [0, 6], [0, 7], [0, 8], [0, 9]], dtype = torch.float)\n",
    "# label = torch.tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "label = torch.tensor([0, 1, 1, 0, 1, 1, 1, 0, 0, 0])\n",
    "print(features, features.shape)\n",
    "\n",
    "clustering_folder = './res_save_batch/clustering/'\n",
    "check_folder_exist(clustering_folder)\n",
    "clustering_file_name = clustering_folder + 'check_clustering_machine.txt'\n",
    "os.makedirs(os.path.dirname(clustering_folder), exist_ok=True)\n",
    "info_folder = './res_save_batch/info/'\n",
    "check_folder_exist(info_folder)\n",
    "os.makedirs(os.path.dirname(info_folder), exist_ok=True)\n",
    "\n",
    "node_count = features.shape[0]\n",
    "get_edge_weight(edge_index, node_count, store_path = clustering_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_machine = ClusteringMachine(edge_index, features, label, clustering_folder, info_folder = clustering_folder, multilabel = False)\n",
    "clustering_machine.split_whole_nodes_edges_then_cluster(0.25, 0.25, normalize = False, precalc = True)\n",
    "\n",
    "gcn_trainer = ClusterGCNTrainer(clustering_machine.feature_count, clustering_machine.label_count, num_layers = 3, hidden_neuron_num = 16, learning_rate = 0.001, \\\n",
    "                 logging = False, multilabel = False, norm = True, precalc = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "During Validation epoch 50: val_acc= 0.33333 micro F1= 0.33333 macro F1= 0.25000 \n",
      "During Validation epoch 50: val_acc= 0.33333 micro F1= 0.33333 macro F1= 0.25000 \n",
      "During Validation epoch 100: val_acc= 0.33333 micro F1= 0.33333 macro F1= 0.25000 \n",
      "During Validation epoch 100: val_acc= 0.33333 micro F1= 0.33333 macro F1= 0.25000 \n",
      "During Validation epoch 150: val_acc= 0.33333 micro F1= 0.33333 macro F1= 0.25000 \n",
      "During Validation epoch 150: val_acc= 0.33333 micro F1= 0.33333 macro F1= 0.25000 \n",
      "During Validation epoch 200: val_acc= 0.33333 micro F1= 0.33333 macro F1= 0.25000 \n",
      "During Validation epoch 200: val_acc= 0.33333 micro F1= 0.33333 macro F1= 0.25000 \n",
      "During Validation epoch 250: val_acc= 0.33333 micro F1= 0.33333 macro F1= 0.25000 \n",
      "During Validation epoch 250: val_acc= 0.33333 micro F1= 0.33333 macro F1= 0.25000 \n",
      "During Validation epoch 300: val_acc= 0.33333 micro F1= 0.33333 macro F1= 0.25000 \n",
      "During Validation epoch 300: val_acc= 0.33333 micro F1= 0.33333 macro F1= 0.25000 \n",
      "During Validation epoch 350: val_acc= 0.33333 micro F1= 0.33333 macro F1= 0.25000 \n",
      "During Validation epoch 350: val_acc= 0.33333 micro F1= 0.33333 macro F1= 0.25000 \n",
      "During Validation epoch 400: val_acc= 0.33333 micro F1= 0.33333 macro F1= 0.25000 \n",
      "During Validation epoch 400: val_acc= 0.33333 micro F1= 0.33333 macro F1= 0.25000 \n"
     ]
    }
   ],
   "source": [
    "# start the training of the model:\n",
    "gcn_trainer.train(clustering_machine, seed = 6, diag_lambda = -1, mini_block_size = 1, \\\n",
    "                               mini_cluster_num = 2, epoch_num = 400, dropout = 0.3, train_save_name = './tmp/saver.txt', validation_period = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro-F1 is : 0.3333333333333333 and the accuracy is : 0.3333333134651184\n",
      "artificially converted multi-label accuracy is : 0.3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.3333333333333333, 0.3333333134651184)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start to test the trained model:\n",
    "gcn_trainer.test(clustering_machine, seed = 6, diag_lambda = -1, test_batch_num = 1, train_save_name = './tmp/saver.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cluster_train_valid_batch_run(clustering_machine, num_layers = 3, hidden_neuron_num = 32, learning_rate = 0.001, \\\n",
    "                 logging = False, multilabel = True, norm = True, precalc = True, diag_lambda = -1, \\\n",
    "                 mini_block_size = 1, mini_cluster_num = 2, test_batch_num = 2, \\\n",
    "                 epoch_num = 400, mini_epoch_num = 20, dropout = 0.3, train_save_name = './tmp/saver.txt'):\n",
    "    \"\"\"\n",
    "        # Run the mini-batch model (train and validate both in batches)\n",
    "        Tuning parameters:  dropout, lr (learning rate), weight_decay: l2 regularization\n",
    "        return: validation accuracy value, validation F-1 value, time_training (ms), time_data_load (ms)\n",
    "    \"\"\"\n",
    "    \n",
    "    gcn_trainer = ClusterGCNTrainer(clustering_machine.feature_count, clustering_machine.label_count, \\\n",
    "                num_layers = num_layers, hidden_neuron_num = hidden_neuron_num, learning_rate = learning_rate, \\\n",
    "                 logging = logging, multilabel = multilabel, norm = norm, precalc = precalc)\n",
    "    \n",
    "    # start to train the model\n",
    "    gcn_trainer.train(clustering_machine, seed = 6, diag_lambda = diag_lambda, mini_block_size = mini_block_size, \\\n",
    "                mini_cluster_num = mini_cluster_num, \\\n",
    "                epoch_num = epoch_num, mini_epoch_num = mini_epoch_num, dropout = dropout, train_save_name = train_save_name)\n",
    "    \n",
    "    # start to test the model\n",
    "    validation_F1, validation_accuracy = gcn_trainer.test(clustering_machine, seed = 6, \\\n",
    "                diag_lambda = diag_lambda, test_batch_num = test_batch_num, train_save_name = train_save_name)\n",
    "    \n",
    "    time_train_total = gcn_trainer.time_train_total\n",
    "#     time_data_load = gcn_trainer.time_train_load_data\n",
    "    \n",
    "    return validation_accuracy, validation_F1, time_train_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"To test one single model for different parameter values\"\"\"\n",
    "def execute_tuning(tune_params, clustering_machine, repeate_time = 7, num_layers = 3, hidden_neuron_num = 32, learning_rate = 0.001, \\\n",
    "                 logging = False, multilabel = True, norm = True, precalc = True, diag_lambda = -1, \\\n",
    "                 mini_block_size = 1, mini_cluster_num = 2, test_batch_num = 2, \\\n",
    "                 epoch_num = 400, mini_epoch_num = 20, dropout = 0.3, train_save_name = './tmp/saver.txt'):\n",
    "    \"\"\"\n",
    "        Tune all the hyperparameters\n",
    "        1) learning rate\n",
    "        2) dropout\n",
    "        3) layer unit number\n",
    "        4) weight decay\n",
    "    \"\"\"\n",
    "    validation_accuracy = {}\n",
    "    validation_f1 = {}\n",
    "    time_total_train = {}\n",
    "#     time_data_load = {}\n",
    "    \n",
    "    res = [{tune_val : Cluster_train_valid_batch_run(clustering_machine, num_layers = num_layers, hidden_neuron_num = hidden_neuron_num, learning_rate = learning_rate, \\\n",
    "                 logging = logging, multilabel = multilabel, norm = norm, precalc = precalc, diag_lambda = diag_lambda, \\\n",
    "                 mini_block_size = mini_block_size, mini_cluster_num = mini_cluster_num, test_batch_num = test_batch_num, \\\n",
    "                 epoch_num = epoch_num, mini_epoch_num = tune_val, dropout = dropout, train_save_name = train_save_name) for tune_val in tune_params} for i in range(repeate_time)]\n",
    "    \n",
    "    for i, ref in enumerate(res):\n",
    "        validation_accuracy[i] = {tune_val : res_lst[0] for tune_val, res_lst in ref.items()}\n",
    "        validation_f1[i] = {tune_val : res_lst[1] for tune_val, res_lst in ref.items()}\n",
    "        time_total_train[i] = {tune_val : res_lst[2] for tune_val, res_lst in ref.items()}\n",
    "#         time_data_load[i] = {tune_val : res_lst[3] for tune_val, res_lst in ref.items()}\n",
    "        \n",
    "    return validation_accuracy, validation_f1, time_total_train\n",
    "\n",
    "def store_data_multi_tuning(tune_params, target, data_name, img_path, comments):\n",
    "    \"\"\"\n",
    "        tune_params: is the tuning parameter list\n",
    "        target: is the result, here should be F1-score, accuraycy, load time, train time\n",
    "    \"\"\"\n",
    "    run_ids = sorted(target.keys())   # key is the run_id\n",
    "    run_data = {'run_id': run_ids}\n",
    "    # the key can be converted to string or not: i.e. str(tune_val)\n",
    "    # here we keep it as integer such that we want it to follow order\n",
    "    tmp = {tune_val : [target[run_id][tune_val] for run_id in run_ids] for tune_val in tune_params}  # the value is list\n",
    "    run_data.update(tmp)\n",
    "    \n",
    "    pickle_filename = img_path + data_name + '_' + comments + '.pkl'\n",
    "    os.makedirs(os.path.dirname(pickle_filename), exist_ok=True)\n",
    "    df = pd.DataFrame(data=run_data, dtype=np.int32)\n",
    "    df.to_pickle(pickle_filename)\n",
    "    return pickle_filename\n",
    "\n",
    "def draw_data_multi_tests(pickle_filename, data_name, comments, xlabel, ylabel):\n",
    "    df = pd.read_pickle(pickle_filename)\n",
    "    df_reshape = df.melt('run_id', var_name = 'model', value_name = ylabel)\n",
    "\n",
    "    plt.clf()\n",
    "    plt.figure()\n",
    "    sns.set(style='whitegrid')\n",
    "    g = sns.catplot(x=\"model\", y=ylabel, kind='box', data=df_reshape)\n",
    "    g.despine(left=True)\n",
    "    g.fig.suptitle(data_name + ' ' + ylabel + ' ' + comments)\n",
    "    g.set_xlabels(xlabel)\n",
    "    g.set_ylabels(ylabel)\n",
    "\n",
    "    img_name = pickle_filename[:-4] + '_img'\n",
    "    os.makedirs(os.path.dirname(img_name), exist_ok=True)\n",
    "    plt.savefig(img_name, bbox_inches='tight')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_tune_param(data, data_name, image_data_path, tmp_folder, partition_nums, layers, hidden_neuron_num = 32, test_batch_num = 2):\n",
    "    node_count = data.x.shape[0]\n",
    "    get_edge_weight(data.edge_index, node_count, store_path = tmp_folder)\n",
    "    \n",
    "    for partn in partition_nums:\n",
    "        for layer_num in layers:\n",
    "            net_layer = layer_num - 1\n",
    "            # Set the tune parameters and name\n",
    "            tune_name = 'batch_epoch_num'\n",
    "            tune_params = [400, 200, 100, 50, 20, 10, 5, 1]\n",
    "#             tune_name = 'weight_decay'\n",
    "#             tune_params = [0.0001, 0.001, 0.01, 0.1]\n",
    "\n",
    "            img_path = image_data_path + 'cluster_num_' + str(partn) + '/' + 'net_layer_' + str(net_layer) + '/'\n",
    "            img_path += 'tune_' + tune_name + '/'  # further subfolder for different task\n",
    "            print('Start tuning for tuning param: ' + tune_name + ' partition num: ' + str(partn) + 'net_layer_' + str(net_layer) )  \n",
    "            \n",
    "            clustering_machine = ClusteringMachine(data.edge_index, data.x, data.y, tmp_folder, info_folder = image_data_path)\n",
    "            clustering_machine.split_whole_nodes_edges_then_cluster(0.05, 0.85, normalize = False, precalc = True)\n",
    "            \n",
    "            validation_accuracy, validation_f1, time_total_train = \\\n",
    "                execute_tuning(tune_params, clustering_machine, repeate_time = 7, num_layers = layer_num, hidden_neuron_num = hidden_neuron_num, learning_rate = 0.001, \\\n",
    "                 logging = False, multilabel = True, norm = True, precalc = True, diag_lambda = 1, \\\n",
    "                 mini_block_size = 2, mini_cluster_num = partn, test_batch_num = test_batch_num, \\\n",
    "                 epoch_num = 400, mini_epoch_num = 20, dropout = 0.3, train_save_name = tmp_folder + 'saver.txt')\n",
    "            \n",
    "\n",
    "            validation_accuracy = store_data_multi_tuning(tune_params,validation_accuracy, data_name, img_path, 'accuracy_cluster_num_' + str(partn))\n",
    "            draw_data_multi_tests(validation_accuracy, data_name, 'Train_cluster_num_' + str(partn) + '_net_layer_' + str(net_layer), tune_name, 'Accuracy')\n",
    "\n",
    "            validation_f1 = store_data_multi_tuning(tune_params, validation_f1, data_name, img_path, 'validation_cluster_num_' + str(partn) )\n",
    "            draw_data_multi_tests(validation_f1, data_name, 'Train_cluster_num_' + str(partn) + '_net_layer_' + str(net_layer), tune_name, 'F1 score')\n",
    "\n",
    "            time_train = store_data_multi_tuning(tune_params, time_total_train, data_name, img_path, 'train_time_cluster_num_' + str(partn) )\n",
    "            draw_data_multi_tests(time_train, data_name, 'train_time_cluster_num_' + str(partn) + '_net_layer_' + str(net_layer), tune_name, 'Train Time (ms)')\n",
    "\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Pytorch Geometric Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_data_root = '/home/xiangli/projects/tmpdata/GCN/Geometric/'\n",
    "test_folder_name = 'Pytorch_Geometric_Graph/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch_geometric.datasets import Planetoid\n",
    "data_name = 'Cora'\n",
    "dataset = Planetoid(root = local_data_root + 'Planetoid/Cora', name=data_name)\n",
    "data = dataset[0]\n",
    "image_data_path = './results/' + data_name + '/' + test_folder_name\n",
    "# set the current folder as the intermediate data folder so that we can easily copy either clustering \n",
    "intermediate_data_folder = './'\n",
    "\n",
    "partition_nums = [4]\n",
    "layers = [3]\n",
    "tmp_folder = './tmp/'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NON multi-label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label count:  7\n",
      "During Validation epoch 50: val_acc= 0.86462 micro F1= 0.86462 macro F1= 0.84520 \n",
      "During Validation epoch 50: val_acc= 0.85846 micro F1= 0.85846 macro F1= 0.83848 \n",
      "During Validation epoch 100: val_acc= 0.85538 micro F1= 0.85538 macro F1= 0.83726 \n",
      "During Validation epoch 100: val_acc= 0.85538 micro F1= 0.85538 macro F1= 0.83815 \n",
      "During Validation epoch 150: val_acc= 0.85846 micro F1= 0.85846 macro F1= 0.84045 \n",
      "During Validation epoch 150: val_acc= 0.85538 micro F1= 0.85538 macro F1= 0.83752 \n",
      "During Validation epoch 200: val_acc= 0.85231 micro F1= 0.85231 macro F1= 0.83882 \n",
      "During Validation epoch 200: val_acc= 0.85846 micro F1= 0.85846 macro F1= 0.84562 \n",
      "During Validation epoch 250: val_acc= 0.85231 micro F1= 0.85231 macro F1= 0.83764 \n",
      "During Validation epoch 250: val_acc= 0.86154 micro F1= 0.86154 macro F1= 0.84716 \n",
      "During Validation epoch 300: val_acc= 0.85846 micro F1= 0.85846 macro F1= 0.84741 \n",
      "During Validation epoch 300: val_acc= 0.85538 micro F1= 0.85538 macro F1= 0.84215 \n",
      "During Validation epoch 350: val_acc= 0.85846 micro F1= 0.85846 macro F1= 0.84077 \n",
      "During Validation epoch 350: val_acc= 0.85231 micro F1= 0.85231 macro F1= 0.83556 \n",
      "During Validation epoch 400: val_acc= 0.85846 micro F1= 0.85846 macro F1= 0.84308 \n",
      "During Validation epoch 400: val_acc= 0.86462 micro F1= 0.86462 macro F1= 0.85049 \n",
      "micro-F1 is : 0.8456375838926175 and the accuracy is : 0.845637321472168\n",
      "artificially converted multi-label accuracy is : 0.9557923612576493\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8456375838926175, 0.845637321472168)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_count = data.x.shape[0]\n",
    "get_edge_weight(data.edge_index, node_count, store_path = tmp_folder)\n",
    "\n",
    "clustering_machine = ClusteringMachine(data.edge_index, data.x, data.y, tmp_folder, info_folder = image_data_path, multilabel = False)\n",
    "\n",
    "clustering_machine.split_whole_nodes_edges_then_cluster(0.12, 0.22, normalize = True, precalc = True)\n",
    "\n",
    "print('label count: ', clustering_machine.label_count,)\n",
    "gcn_trainer = ClusterGCNTrainer(clustering_machine.feature_count, clustering_machine.label_count, num_layers = 3, hidden_neuron_num = 128, learning_rate = 0.001, \\\n",
    "                 logging = False, multilabel = False, norm = True, precalc = True)\n",
    "\n",
    "gcn_trainer.train(clustering_machine, seed = 6, diag_lambda = 1, mini_block_size = 2, \\\n",
    "        mini_cluster_num = 4, epoch_num = 400, mini_epoch_num = 20, dropout = 0.3, train_save_name = './tmp/saver.txt', validation_period = 50)\n",
    "\n",
    "# print(node_count)\n",
    "# start to test the trained model:\n",
    "gcn_trainer.test(clustering_machine, seed = 6, diag_lambda = 1, test_batch_num = 1, train_save_name = './tmp/saver.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use GraphSaint Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_data_root = '/home/xiangli/projects/tmpdata/GCN/GraphSaint/'\n",
    "test_folder_name = 'GraphSaint_Graphs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of data 1\n"
     ]
    }
   ],
   "source": [
    "from GraphSaint_dataset import print_data_info, Flickr, Yelp, PPI_large, Amazon, Reddit, PPI_small\n",
    "# suppose this is on the OSC cluster\n",
    "\n",
    "data_name = 'PPI_small'\n",
    "class_data = eval(data_name)\n",
    "dataset = class_data(root = remote_data_root + data_name)\n",
    "print('number of data', len(dataset))\n",
    "data = dataset[0]\n",
    "\n",
    "intermediate_data_folder = './clusterGCN_multilabel_normalize/'\n",
    "image_data_path = intermediate_data_folder + 'results/' + data_name + '/' + test_folder_name\n",
    "\n",
    "partition_nums = [4]\n",
    "layers = [3]\n",
    "tmp_folder = './tmp/'\n",
    "\n",
    "# image_data_path = './results/' + data_name + '/' + test_folder_name\n",
    "# intermediate_data_folder = './'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label count:  121\n"
     ]
    }
   ],
   "source": [
    "node_count = data.x.shape[0]\n",
    "get_edge_weight(data.edge_index, node_count, store_path = tmp_folder)\n",
    "\n",
    "clustering_machine = ClusteringMachine(data.edge_index, data.x, data.y, tmp_folder, info_folder = image_data_path, multilabel = True)\n",
    "\n",
    "clustering_machine.split_whole_nodes_edges_then_cluster(0.12, 0.22, normalize = True, precalc = True)\n",
    "print('label count: ', clustering_machine.label_count,)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "During Validation epoch 50: val_acc= 0.77645 micro F1= 0.59179 macro F1= 0.42753 \n",
      "During Validation epoch 50: val_acc= 0.77013 micro F1= 0.60413 macro F1= 0.44846 \n",
      "During Validation epoch 50: val_acc= 0.78058 micro F1= 0.59456 macro F1= 0.42817 \n",
      "During Validation epoch 50: val_acc= 0.77596 micro F1= 0.61802 macro F1= 0.47356 \n",
      "During Validation epoch 100: val_acc= 0.81198 micro F1= 0.67510 macro F1= 0.57588 \n",
      "During Validation epoch 100: val_acc= 0.81569 micro F1= 0.67892 macro F1= 0.58464 \n",
      "During Validation epoch 100: val_acc= 0.81706 micro F1= 0.68390 macro F1= 0.59214 \n",
      "During Validation epoch 100: val_acc= 0.81162 micro F1= 0.68639 macro F1= 0.59759 \n",
      "During Validation epoch 150: val_acc= 0.84060 micro F1= 0.73297 macro F1= 0.67054 \n",
      "During Validation epoch 150: val_acc= 0.84341 micro F1= 0.73426 macro F1= 0.67367 \n",
      "During Validation epoch 150: val_acc= 0.83970 micro F1= 0.73420 macro F1= 0.67243 \n",
      "During Validation epoch 150: val_acc= 0.84086 micro F1= 0.73693 macro F1= 0.67537 \n",
      "During Validation epoch 200: val_acc= 0.86141 micro F1= 0.77169 macro F1= 0.72558 \n",
      "During Validation epoch 200: val_acc= 0.86289 micro F1= 0.77121 macro F1= 0.72402 \n",
      "During Validation epoch 200: val_acc= 0.86577 micro F1= 0.77665 macro F1= 0.73559 \n",
      "During Validation epoch 200: val_acc= 0.86343 micro F1= 0.77473 macro F1= 0.73144 \n",
      "During Validation epoch 250: val_acc= 0.87300 micro F1= 0.78845 macro F1= 0.74715 \n",
      "During Validation epoch 250: val_acc= 0.86704 micro F1= 0.78464 macro F1= 0.74517 \n",
      "During Validation epoch 250: val_acc= 0.87393 micro F1= 0.79280 macro F1= 0.75668 \n",
      "During Validation epoch 250: val_acc= 0.87656 micro F1= 0.79573 macro F1= 0.75962 \n",
      "During Validation epoch 300: val_acc= 0.88204 micro F1= 0.80400 macro F1= 0.76988 \n",
      "During Validation epoch 300: val_acc= 0.87942 micro F1= 0.80138 macro F1= 0.76596 \n",
      "During Validation epoch 300: val_acc= 0.88570 micro F1= 0.80899 macro F1= 0.77567 \n",
      "During Validation epoch 300: val_acc= 0.88323 micro F1= 0.80700 macro F1= 0.77389 \n",
      "During Validation epoch 350: val_acc= 0.88670 micro F1= 0.81391 macro F1= 0.78227 \n",
      "During Validation epoch 350: val_acc= 0.88411 micro F1= 0.81155 macro F1= 0.77953 \n",
      "During Validation epoch 350: val_acc= 0.88715 micro F1= 0.81356 macro F1= 0.78191 \n",
      "During Validation epoch 350: val_acc= 0.88869 micro F1= 0.81577 macro F1= 0.78415 \n",
      "During Validation epoch 400: val_acc= 0.89510 micro F1= 0.82443 macro F1= 0.79513 \n",
      "During Validation epoch 400: val_acc= 0.89324 micro F1= 0.82155 macro F1= 0.79051 \n",
      "During Validation epoch 400: val_acc= 0.89259 micro F1= 0.82113 macro F1= 0.79009 \n",
      "During Validation epoch 400: val_acc= 0.89395 micro F1= 0.82340 macro F1= 0.79498 \n",
      "During Validation epoch 450: val_acc= 0.89409 micro F1= 0.82411 macro F1= 0.79516 \n",
      "During Validation epoch 450: val_acc= 0.89511 micro F1= 0.82483 macro F1= 0.79498 \n",
      "During Validation epoch 450: val_acc= 0.89376 micro F1= 0.82459 macro F1= 0.79589 \n",
      "During Validation epoch 450: val_acc= 0.89366 micro F1= 0.82536 macro F1= 0.79749 \n",
      "During Validation epoch 500: val_acc= 0.89882 micro F1= 0.83286 macro F1= 0.80711 \n",
      "During Validation epoch 500: val_acc= 0.89876 micro F1= 0.83142 macro F1= 0.80247 \n",
      "During Validation epoch 500: val_acc= 0.89924 micro F1= 0.83259 macro F1= 0.80545 \n",
      "During Validation epoch 500: val_acc= 0.89749 micro F1= 0.82909 macro F1= 0.80075 \n",
      "During Validation epoch 550: val_acc= 0.89874 micro F1= 0.83221 macro F1= 0.80526 \n",
      "During Validation epoch 550: val_acc= 0.89880 micro F1= 0.83167 macro F1= 0.80445 \n",
      "During Validation epoch 550: val_acc= 0.89900 micro F1= 0.83248 macro F1= 0.80670 \n",
      "During Validation epoch 550: val_acc= 0.90106 micro F1= 0.83511 macro F1= 0.80792 \n",
      "During Validation epoch 600: val_acc= 0.90094 micro F1= 0.83646 macro F1= 0.81068 \n",
      "During Validation epoch 600: val_acc= 0.90128 micro F1= 0.83583 macro F1= 0.80851 \n",
      "During Validation epoch 600: val_acc= 0.90234 micro F1= 0.83828 macro F1= 0.81254 \n",
      "During Validation epoch 600: val_acc= 0.90345 micro F1= 0.83821 macro F1= 0.81158 \n",
      "During Validation epoch 650: val_acc= 0.90156 micro F1= 0.83739 macro F1= 0.81138 \n",
      "During Validation epoch 650: val_acc= 0.90257 micro F1= 0.83800 macro F1= 0.81182 \n",
      "During Validation epoch 650: val_acc= 0.90123 micro F1= 0.83705 macro F1= 0.81120 \n",
      "During Validation epoch 650: val_acc= 0.90271 micro F1= 0.83877 macro F1= 0.81410 \n",
      "During Validation epoch 700: val_acc= 0.90443 micro F1= 0.84039 macro F1= 0.81495 \n",
      "During Validation epoch 700: val_acc= 0.90732 micro F1= 0.84433 macro F1= 0.81974 \n",
      "During Validation epoch 700: val_acc= 0.90455 micro F1= 0.84077 macro F1= 0.81622 \n",
      "During Validation epoch 700: val_acc= 0.90456 micro F1= 0.84056 macro F1= 0.81403 \n",
      "During Validation epoch 750: val_acc= 0.90509 micro F1= 0.84190 macro F1= 0.81725 \n",
      "During Validation epoch 750: val_acc= 0.90465 micro F1= 0.84191 macro F1= 0.81677 \n",
      "During Validation epoch 750: val_acc= 0.90457 micro F1= 0.84106 macro F1= 0.81573 \n",
      "During Validation epoch 750: val_acc= 0.90356 micro F1= 0.84029 macro F1= 0.81503 \n",
      "During Validation epoch 800: val_acc= 0.90662 micro F1= 0.84364 macro F1= 0.81800 \n",
      "During Validation epoch 800: val_acc= 0.90540 micro F1= 0.84230 macro F1= 0.81801 \n",
      "During Validation epoch 800: val_acc= 0.90680 micro F1= 0.84476 macro F1= 0.82013 \n",
      "During Validation epoch 800: val_acc= 0.90835 micro F1= 0.84626 macro F1= 0.82173 \n",
      "micro-F1 is : 0.8505318649822143 and the accuracy is : 0.9108030796051025\n",
      "artificially converted multi-label accuracy is : 0.9975287827910976\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8505318649822143, 0.9108030796051025)"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gcn_trainer = ClusterGCNTrainer(clustering_machine.feature_count, clustering_machine.label_count, num_layers = 3, hidden_neuron_num = 512, learning_rate = 0.01, \\\n",
    "                 logging = False, multilabel = True, norm = True, precalc = True)\n",
    "\n",
    "\n",
    "gcn_trainer.train(clustering_machine, seed = 6, diag_lambda = 1, mini_block_size = 8, \\\n",
    "    mini_cluster_num = 32, epoch_num = 800, mini_epoch_num = 10, dropout = 0.1, train_save_name = './tmp/saver.txt', validation_period = 50)\n",
    "\n",
    "# print(node_count)\n",
    "# start to test the trained model:\n",
    "gcn_trainer.test(clustering_machine, seed = 6, diag_lambda = 1, test_batch_num = 1, train_save_name = './tmp/saver.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results for \n",
    "\n",
    "    num_layers = 3, hidden_neuron_num = 512, learning_rate = 0.01\n",
    "    clustering_machine, seed = 6, diag_lambda = 1, mini_block_size = 8, mini_cluster_num = 64, \n",
    "                              epoch_num = 400, mini_epoch_num = 20, dropout = 0.1, \n",
    "\n",
    "    micro-F1 is : 0.8163103152431628 and the accuracy is : 0.8893744945526123\n",
    "    artificially converted multi-label accuracy is : 0.9969630689694766\n",
    "\n",
    "    (0.8163103152431628, 0.8893744945526123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start tuning for tuning param: batch_epoch_num partition num: 4net_layer_2\n",
      "micro-F1 is : 0.7789863457532978 and the accuracy is : 0.940735402752481\n",
      "artificially converted multi-label accuracy is : 0.9368010128719139\n",
      "micro-F1 is : 0.7715922107674684 and the accuracy is : 0.9381289079228657\n",
      "artificially converted multi-label accuracy is : 0.9346908630512767\n",
      "micro-F1 is : 0.771744132535665 and the accuracy is : 0.9384392042661314\n",
      "artificially converted multi-label accuracy is : 0.9346908630512767\n",
      "micro-F1 is : 0.7799032035031113 and the accuracy is : 0.9407353248159125\n",
      "artificially converted multi-label accuracy is : 0.9370120278539776\n",
      "micro-F1 is : 0.7629058631160944 and the accuracy is : 0.9370118553590816\n",
      "artificially converted multi-label accuracy is : 0.932158683266512\n",
      "micro-F1 is : 0.7670200235571261 and the accuracy is : 0.9386254213459071\n",
      "artificially converted multi-label accuracy is : 0.9334247731588943\n",
      "micro-F1 is : 0.7840516247983407 and the accuracy is : 0.9418523733663517\n",
      "artificially converted multi-label accuracy is : 0.9382781177463599\n",
      "micro-F1 is : 0.7934579439252336 and the accuracy is : 0.9451415776740355\n",
      "artificially converted multi-label accuracy is : 0.9409158050221565\n",
      "micro-F1 is : 0.7807118254879449 and the accuracy is : 0.940735358217299\n",
      "artificially converted multi-label accuracy is : 0.9373285503270732\n",
      "micro-F1 is : 0.7801484230055659 and the accuracy is : 0.9411698046919577\n",
      "artificially converted multi-label accuracy is : 0.9371175353450095\n",
      "micro-F1 is : 0.7831940575673166 and the accuracy is : 0.9420386126360408\n",
      "artificially converted multi-label accuracy is : 0.9379615952732644\n",
      "micro-F1 is : 0.7731329262564585 and the accuracy is : 0.9400527478300106\n",
      "artificially converted multi-label accuracy is : 0.9351128930154041\n",
      "micro-F1 is : 0.7897366030881017 and the accuracy is : 0.9425350673347841\n",
      "artificially converted multi-label accuracy is : 0.9398607301118379\n",
      "micro-F1 is : 0.7810218978102189 and the accuracy is : 0.9404251066940335\n",
      "artificially converted multi-label accuracy is : 0.9374340578181051\n",
      "micro-F1 is : 0.7783959202596199 and the accuracy is : 0.940673315556126\n",
      "artificially converted multi-label accuracy is : 0.9365899978898502\n",
      "micro-F1 is : 0.7913929784824463 and the accuracy is : 0.9428453881723999\n",
      "artificially converted multi-label accuracy is : 0.9403882675669972\n",
      "micro-F1 is : 0.7736376339077782 and the accuracy is : 0.9396803901311525\n",
      "artificially converted multi-label accuracy is : 0.9353239079974678\n",
      "micro-F1 is : 0.7733395262424525 and the accuracy is : 0.939432157163098\n",
      "artificially converted multi-label accuracy is : 0.935218400506436\n",
      "micro-F1 is : 0.7823255813953488 and the accuracy is : 0.9419145015765024\n",
      "artificially converted multi-label accuracy is : 0.9377505802912007\n",
      "micro-F1 is : 0.7681365576102418 and the accuracy is : 0.9393080141262631\n",
      "artificially converted multi-label accuracy is : 0.9337412956319898\n",
      "micro-F1 is : 0.7723726480036713 and the accuracy is : 0.9384391987251262\n",
      "artificially converted multi-label accuracy is : 0.9349018780333403\n",
      "micro-F1 is : 0.7691231343283583 and the accuracy is : 0.9385633245434168\n",
      "artificially converted multi-label accuracy is : 0.9339523106140536\n",
      "micro-F1 is : 0.7766580735880009 and the accuracy is : 0.9408594988979121\n",
      "artificially converted multi-label accuracy is : 0.9361679679257228\n",
      "micro-F1 is : 0.784576310320942 and the accuracy is : 0.9421006601132278\n",
      "artificially converted multi-label accuracy is : 0.9383836252373918\n",
      "micro-F1 is : 0.775813953488372 and the accuracy is : 0.9401767555782838\n",
      "artificially converted multi-label accuracy is : 0.9358514454526271\n",
      "micro-F1 is : 0.7810218978102189 and the accuracy is : 0.9404250942915031\n",
      "artificially converted multi-label accuracy is : 0.9374340578181051\n",
      "micro-F1 is : 0.7771877164627107 and the accuracy is : 0.9401147501247018\n",
      "artificially converted multi-label accuracy is : 0.9362734754167545\n",
      "micro-F1 is : 0.7771028037383177 and the accuracy is : 0.9407974259942433\n",
      "artificially converted multi-label accuracy is : 0.9362734754167545\n",
      "micro-F1 is : 0.7738786892865442 and the accuracy is : 0.9396182374784369\n",
      "artificially converted multi-label accuracy is : 0.9353239079974678\n",
      "micro-F1 is : 0.7823678744518808 and the accuracy is : 0.941480076437303\n",
      "artificially converted multi-label accuracy is : 0.9377505802912007\n",
      "micro-F1 is : 0.7764489420423184 and the accuracy is : 0.9396803564708405\n",
      "artificially converted multi-label accuracy is : 0.9360624604346909\n",
      "micro-F1 is : 0.7802607076350092 and the accuracy is : 0.9414179895775511\n",
      "artificially converted multi-label accuracy is : 0.9371175353450095\n",
      "micro-F1 is : 0.7689776733254994 and the accuracy is : 0.9389977467308658\n",
      "artificially converted multi-label accuracy is : 0.9339523106140536\n",
      "micro-F1 is : 0.7684162062615102 and the accuracy is : 0.9375703964256183\n",
      "artificially converted multi-label accuracy is : 0.9337412956319898\n",
      "micro-F1 is : 0.7731439046746105 and the accuracy is : 0.9385632499210944\n",
      "artificially converted multi-label accuracy is : 0.9351128930154041\n",
      "micro-F1 is : 0.7699115044247788 and the accuracy is : 0.938687380037548\n",
      "artificially converted multi-label accuracy is : 0.9341633255961174\n",
      "micro-F1 is : 0.7749008630744109 and the accuracy is : 0.9401147630450831\n",
      "artificially converted multi-label accuracy is : 0.9356404304705634\n",
      "micro-F1 is : 0.783660429027841 and the accuracy is : 0.9411697504729628\n",
      "artificially converted multi-label accuracy is : 0.9381726102553282\n",
      "micro-F1 is : 0.7765029830197338 and the accuracy is : 0.9395562653226711\n",
      "artificially converted multi-label accuracy is : 0.9360624604346909\n",
      "micro-F1 is : 0.7833787465940055 and the accuracy is : 0.9407974922014878\n",
      "artificially converted multi-label accuracy is : 0.9380671027642963\n",
      "micro-F1 is : 0.7807657247037374 and the accuracy is : 0.9403009441342176\n",
      "artificially converted multi-label accuracy is : 0.9373285503270732\n",
      "micro-F1 is : 0.7790537436839687 and the accuracy is : 0.9403009330780997\n",
      "artificially converted multi-label accuracy is : 0.9368010128719139\n",
      "micro-F1 is : 0.7693389592123769 and the accuracy is : 0.9389355934826216\n",
      "artificially converted multi-label accuracy is : 0.9340578181050855\n",
      "micro-F1 is : 0.7754266211604095 and the accuracy is : 0.938749482924787\n",
      "artificially converted multi-label accuracy is : 0.9357459379615952\n",
      "micro-F1 is : 0.7604263206672845 and the accuracy is : 0.9358327423459442\n",
      "artificially converted multi-label accuracy is : 0.9315256383203208\n",
      "micro-F1 is : 0.7821047751506723 and the accuracy is : 0.9416662241509435\n",
      "artificially converted multi-label accuracy is : 0.9376450728001688\n",
      "micro-F1 is : 0.7788219115287647 and the accuracy is : 0.9401147904911836\n",
      "artificially converted multi-label accuracy is : 0.9368010128719139\n",
      "micro-F1 is : 0.7753690036900369 and the accuracy is : 0.9395562358310593\n",
      "artificially converted multi-label accuracy is : 0.9357459379615952\n",
      "micro-F1 is : 0.7713884992987377 and the accuracy is : 0.9393080272019997\n",
      "artificially converted multi-label accuracy is : 0.9345853555602448\n",
      "micro-F1 is : 0.7784867821330903 and the accuracy is : 0.9396803215935787\n",
      "artificially converted multi-label accuracy is : 0.9366955053808821\n",
      "micro-F1 is : 0.7842598947609243 and the accuracy is : 0.941479980893802\n",
      "artificially converted multi-label accuracy is : 0.9382781177463599\n",
      "micro-F1 is : 0.7919127669241254 and the accuracy is : 0.9431556088576412\n",
      "artificially converted multi-label accuracy is : 0.9404937750580291\n",
      "micro-F1 is : 0.7718560074801308 and the accuracy is : 0.9394320409832365\n",
      "artificially converted multi-label accuracy is : 0.9347963705423085\n",
      "micro-F1 is : 0.776410494543766 and the accuracy is : 0.9402389150925246\n",
      "artificially converted multi-label accuracy is : 0.9360624604346909\n",
      "micro-F1 is : 0.7798081315669256 and the accuracy is : 0.9401768632653896\n",
      "artificially converted multi-label accuracy is : 0.9370120278539776\n",
      "micro-F1 is : 0.7685098406747892 and the accuracy is : 0.938687454116127\n",
      "artificially converted multi-label accuracy is : 0.9338468031230217\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_tune_param(data, data_name, image_data_path, tmp_folder, partition_nums, layers, hidden_neuron_num = 32, test_batch_num = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free GPU memory\n",
    "# !(nvidia-smi | grep 'python' | awk '{ print $3 }' | xargs -n1 kill -9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the torch.tensor\n",
    "import torch\n",
    "import numpy as np\n",
    "# a = torch.tensor([[1, 2], [2, 3]])\n",
    "# b = torch.tensor([[4, 2], [2, 3]])\n",
    "\n",
    "a = np.array([[1, 2], [2, 3]])\n",
    "b = np.array([[4, 2], [2, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n"
     ]
    }
   ],
   "source": [
    "def binary_acc(y_test, y_pred):\n",
    "    \"\"\"\n",
    "        y_test (np.array) : the true label for the nodes\n",
    "        y_pred (np.array) : predicted tags for the nodes\n",
    "    \"\"\"\n",
    "    ave_loss = (y_test == y_pred).mean(dtype=np.float).item() \n",
    "    return ave_loss\n",
    "\n",
    "print(binary_acc(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(binary_acc(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,) (4,)\n",
      "0.75\n"
     ]
    }
   ],
   "source": [
    "d = a.flatten()\n",
    "e = b.flatten()\n",
    "print(d.shape, e.shape)\n",
    "print(accuracy_score(d, e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_geom_tensorflow_2]",
   "language": "python",
   "name": "conda-env-pytorch_geom_tensorflow_2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
