{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PC version of GraphSaint Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import scipy.sparse as sp\n",
    "from layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import pdb\n",
    "import scipy.sparse\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import yaml\n",
    "import scipy.sparse as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from globals import *\n",
    "\n",
    "def load_data(prefix, normalize=True):\n",
    "    \"\"\"\n",
    "        if needed: change the data path as needed\n",
    "        prefix: should be the dataname\n",
    "    \"\"\"\n",
    "    # Load a sparse matrix from a file using .npz format. Return csc_matrix, csr_matrix, bsr_matrix, dia_matrix or coo_matrix\n",
    "    adj_full = scipy.sparse.load_npz('./{}/adj_full.npz'.format(prefix)).astype(np.bool)\n",
    "    adj_train = scipy.sparse.load_npz('./{}/adj_train.npz'.format(prefix)).astype(np.bool)\n",
    "    \n",
    "    role = json.load(open('./{}/role.json'.format(prefix)))\n",
    "    \n",
    "    \"\"\"\n",
    "        .npy:  the standard binary file format in NumPy for persisting a single arbitrary NumPy array on disk.\n",
    "        .npz:  simple way to combine multiple arrays into a single file, one can use ZipFile to contain multiple “.npy” files\n",
    "\n",
    "        .npz is just a ZipFile containing multiple “.npy” files. \n",
    "        And this ZipFile can be either compressed (by using np.savez_compressed) or uncompressed (by using np.savez)\n",
    "    \"\"\"\n",
    "    # Load arrays or pickled objects from .npy, .npz or pickled files.\n",
    "    feats = np.load('./{}/feats.npy'.format(prefix))\n",
    "    \"\"\"\n",
    "        json.load() method (without “s” in “load”) used to read JSON encoded data from a file and convert it into Python dictionary.\n",
    "        json.loads() method, which is used for parse valid JSON String into Python dictionary\n",
    "    \"\"\"\n",
    "    class_map = json.load(open('./{}/class_map.json'.format(prefix)))\n",
    "    class_map = {int(k):v for k, v in class_map.items()}\n",
    "    assert len(class_map) == feats.shape[0]\n",
    "    # ---- normalize feats ----\n",
    "    # scipy.sparse.csr_matrix.nonzero:  Returns a tuple of arrays (row,col) containing the indices of the non-zero elements of the matrix.\n",
    "    train_nodes = np.array(list(set(adj_train.nonzero()[0])))\n",
    "    train_feats = feats[train_nodes]\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train_feats)\n",
    "    # transform the whole feature by fitting the train features \n",
    "    feats = scaler.transform(feats)\n",
    "    # -------------------------\n",
    "    return adj_full, adj_train, feats, class_map, role\n",
    "\n",
    "def process_graph_data(adj_full, adj_train, feats, class_map, role):\n",
    "    \"\"\"\n",
    "    setup vertex property map for output classes, train/val/test masks, and feats\n",
    "    \"\"\"\n",
    "    num_vertices = adj_full. shape[0]\n",
    "    if isinstance(list(class_map.values())[0],list):\n",
    "        num_classes = len(list(class_map.values())[0])\n",
    "        class_arr = np.zeros((num_vertices, num_classes))\n",
    "        for k,v in class_map.items():\n",
    "            class_arr[k] = v\n",
    "    else:\n",
    "        num_classes = max(class_map.values()) - min(class_map.values()) + 1\n",
    "        class_arr = np.zeros((num_vertices, num_classes))\n",
    "        offset = min(class_map.values())\n",
    "        for k,v in class_map.items():\n",
    "            class_arr[k][v-offset] = 1\n",
    "    return adj_full, adj_train, feats, class_arr, role\n",
    "\n",
    "\n",
    "def parse_layer_yml(arch_gcn,dim_input):\n",
    "    num_layers = len(arch_gcn['arch'].split('-'))\n",
    "    # set default values, then update by arch_gcn\n",
    "    bias_layer = [arch_gcn['bias']]*num_layers\n",
    "    act_layer = [arch_gcn['act']]*num_layers\n",
    "    aggr_layer = [arch_gcn['aggr']]*num_layers\n",
    "    dims_layer = [arch_gcn['dim']]*num_layers\n",
    "    order_layer = [int(o) for o in arch_gcn['arch'].split('-')]\n",
    "    return [dim_input]+dims_layer,order_layer,act_layer,bias_layer,aggr_layer\n",
    "\n",
    "\n",
    "\n",
    "def parse_n_prepare(flags):\n",
    "    with open(flags.train_config) as f_train_config:\n",
    "        train_config = yaml.load(f_train_config)\n",
    "    arch_gcn = {'dim':-1,'aggr':'concat','loss':'softmax','arch':'1','act':'I','bias':'norm'}\n",
    "    # check the loss:  default to be softmax, multi-class problem, each node can only belong to just one class at last\n",
    "    arch_gcn.update(train_config['network'][0])\n",
    "    train_params = {'lr':0.01,'weight_decay':0.,'norm_loss':True,'norm_aggr':True,'q_threshold':50,'q_offset':0}\n",
    "    train_params.update(train_config['params'][0])\n",
    "    train_phases = train_config['phase']\n",
    "    for ph in train_phases:\n",
    "        assert 'end' in ph\n",
    "        assert 'sampler' in ph\n",
    "    print(\"Loading training data..\")\n",
    "    temp_data = load_data(flags.data_prefix)\n",
    "    train_data = process_graph_data(*temp_data)\n",
    "    print(\"Done loading training data..\")\n",
    "    return train_params,train_phases,train_data,arch_gcn\n",
    "\n",
    "# mark for global: args_global.dir_log\n",
    "def log_dir(f_train_config, prefix, git_branch, git_rev,timestamp, dir_log):\n",
    "    import getpass\n",
    "    log_dir = dir_log + \"/log_train/\" + prefix.split(\"/\")[-1]\n",
    "    log_dir += \"/{ts}-{model}-{gitrev:s}/\".format(\n",
    "            model='graphsaint',\n",
    "            gitrev=git_rev.strip(),\n",
    "            ts=timestamp)\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "    if f_train_config != '':\n",
    "        from shutil import copyfile\n",
    "        copyfile(f_train_config,'{}/{}'.format(log_dir,f_train_config.split('/')[-1]))\n",
    "    return log_dir\n",
    "\n",
    "def sess_dir(dims, train_config,prefix, git_branch, git_rev,timestamp):\n",
    "    import getpass\n",
    "    log_dir = \"saved_models/\" + prefix.split(\"/\")[-1]\n",
    "    log_dir += \"/{ts}-{model}-{gitrev:s}-{layer}/\".format(\n",
    "            model='graphsaint',\n",
    "            gitrev=git_rev.strip(),\n",
    "            layer='-'.join(dims),\n",
    "            ts=timestamp)\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "    return sess_dir\n",
    "\n",
    "\n",
    "def adj_norm(adj, deg=None, sort_indices=True):\n",
    "    \"\"\"\n",
    "    Normalize adj according to two methods: symmetric normalization and rw normalization.\n",
    "    sym norm is used in the original GCN paper (kipf)\n",
    "    rw norm is used in graphsage and some other variants.\n",
    "\n",
    "    # Procedure: \n",
    "    #       1. adj add self-connection --> adj'\n",
    "    #       2. D' deg matrix from adj'\n",
    "    #       3. norm by D^{-1} x adj'\n",
    "    if sort_indices is True, we re-sort the indices of the returned adj\n",
    "    Note that after 'dot' the indices of a node would be in descending order rather than ascending order\n",
    "    \"\"\"\n",
    "    diag_shape = (adj.shape[0],adj.shape[1])\n",
    "    D = adj.sum(1).flatten() if deg is None else deg\n",
    "    norm_diag = sp.dia_matrix((1/D,0),shape=diag_shape)\n",
    "    adj_norm = norm_diag.dot(adj)\n",
    "    if sort_indices:\n",
    "        adj_norm.sort_indices()\n",
    "    return adj_norm\n",
    "\n",
    "##################\n",
    "# PRINTING UTILS #\n",
    "#----------------#\n",
    "\n",
    "_bcolors = {'header': '\\033[95m',\n",
    "            'blue': '\\033[94m',\n",
    "            'green': '\\033[92m',\n",
    "            'yellow': '\\033[93m',\n",
    "            'red': '\\033[91m',\n",
    "            'bold': '\\033[1m',\n",
    "            'underline': '\\033[4m'}\n",
    "\n",
    "\n",
    "def printf(msg, style=''):\n",
    "    if not style or style == 'black':\n",
    "        print(msg)\n",
    "    else:\n",
    "        print(\"{color1}{msg}{color2}\".format(color1=_bcolors[style],msg=msg,color2='\\033[0m'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mini-batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--num_cpu_core NUM_CPU_CORE]\n",
      "                             [--log_device_placement] --data_prefix\n",
      "                             DATA_PREFIX [--dir_log DIR_LOG] [--gpu GPU]\n",
      "                             [--eval_train_every EVAL_TRAIN_EVERY]\n",
      "                             --train_config TRAIN_CONFIG [--dtype DTYPE]\n",
      "                             [--timeline] [--tensorboard] [--dualGPU]\n",
      "                             [--cpu_eval]\n",
      "                             [--saved_model_path SAVED_MODEL_PATH]\n",
      "ipykernel_launcher.py: error: the following arguments are required: --data_prefix, --train_config\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangli/anaconda3/envs/pytorch_1_4_geometric/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3339: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def _coo_scipy2torch(adj):\n",
    "    \"\"\"\n",
    "    convert a scipy sparse COO matrix to torch\n",
    "    \"\"\"\n",
    "    values = adj.data\n",
    "    indices = np.vstack((adj.row, adj.col))\n",
    "    i = torch.LongTensor(indices)\n",
    "    v = torch.FloatTensor(values)\n",
    "    return torch.sparse.FloatTensor(i,v, torch.Size(adj.shape))\n",
    "\n",
    "\n",
    "\n",
    "class Minibatch:\n",
    "    \"\"\"\n",
    "    This minibatch iterator iterates over nodes for supervised learning.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, adj_full_norm, adj_train, role, train_params, cpu_eval=False):\n",
    "        \"\"\"\n",
    "        role:       array of string (length |V|)\n",
    "                    storing role of the node ('tr'/'va'/'te')\n",
    "        \"\"\"\n",
    "        self.use_cuda = (args_global.gpu >= 0)\n",
    "        if cpu_eval:\n",
    "            self.use_cuda=False\n",
    "\n",
    "        self.node_train = np.array(role['tr'])\n",
    "        self.node_val = np.array(role['va'])\n",
    "        self.node_test = np.array(role['te'])\n",
    "\n",
    "        self.adj_full_norm = _coo_scipy2torch(adj_full_norm.tocoo())\n",
    "        self.adj_train = adj_train\n",
    "        if self.use_cuda:       # now i put everything on GPU. Ideally, full graph adj/feat should be optionally placed on CPU\n",
    "            self.adj_full_norm = self.adj_full_norm.cuda()\n",
    "\n",
    "        # below: book-keeping for mini-batch\n",
    "        self.node_subgraph = None\n",
    "        self.batch_num = -1\n",
    "\n",
    "        self.method_sample = None\n",
    "        self.subgraphs_remaining_indptr = []\n",
    "        self.subgraphs_remaining_indices = []\n",
    "        self.subgraphs_remaining_data = []\n",
    "        self.subgraphs_remaining_nodes = []\n",
    "        self.subgraphs_remaining_edge_index = []\n",
    "        \n",
    "        self.norm_loss_train = np.zeros(self.adj_train.shape[0])\n",
    "        # norm_loss_test is used in full batch evaluation (without sampling). so neighbor features are simply averaged.\n",
    "        self.norm_loss_test = np.zeros(self.adj_full_norm.shape[0])\n",
    "        _denom = len(self.node_train) + len(self.node_val) +  len(self.node_test)\n",
    "        self.norm_loss_test[self.node_train] = 1./_denom     \n",
    "        self.norm_loss_test[self.node_val] = 1./_denom\n",
    "        self.norm_loss_test[self.node_test] = 1./_denom\n",
    "        self.norm_loss_test = torch.from_numpy(self.norm_loss_test.astype(np.float32))\n",
    "        if self.use_cuda:\n",
    "            self.norm_loss_test = self.norm_loss_test.cuda()\n",
    "        self.norm_aggr_train = np.zeros(self.adj_train.size)\n",
    "       \n",
    "        self.sample_coverage = train_params['sample_coverage']\n",
    "        self.deg_train = np.array(self.adj_train.sum(1)).flatten()\n",
    "\n",
    "\n",
    "    def set_sampler(self,train_phases):\n",
    "        self.subgraphs_remaining_indptr = list()\n",
    "        self.subgraphs_remaining_indices = list()\n",
    "        self.subgraphs_remaining_data = list()\n",
    "        self.subgraphs_remaining_nodes = list()\n",
    "        self.subgraphs_remaining_edge_index = list()\n",
    "        self.method_sample = train_phases['sampler']\n",
    "        if self.method_sample == 'mrw':\n",
    "            if 'deg_clip' in train_phases:\n",
    "                _deg_clip = int(train_phases['deg_clip'])\n",
    "            else:\n",
    "                _deg_clip = 100000      # setting this to a large number so essentially there is no clipping in probability\n",
    "            self.size_subg_budget = train_phases['size_subgraph']\n",
    "            self.graph_sampler = mrw_sampling(self.adj_train,self.node_train,\\\n",
    "                self.size_subg_budget,train_phases['size_frontier'],_deg_clip)\n",
    "        elif self.method_sample == 'rw':\n",
    "            self.size_subg_budget = train_phases['num_root']*train_phases['depth']\n",
    "            self.graph_sampler = rw_sampling(self.adj_train,self.node_train,\\\n",
    "                self.size_subg_budget,int(train_phases['num_root']),int(train_phases['depth']))\n",
    "        elif self.method_sample == 'edge':\n",
    "            self.size_subg_budget = train_phases['size_subg_edge']*2\n",
    "            self.graph_sampler = edge_sampling(self.adj_train,self.node_train,train_phases['size_subg_edge'])\n",
    "        elif self.method_sample == 'node':\n",
    "            self.size_subg_budget = train_phases['size_subgraph']\n",
    "            self.graph_sampler = node_sampling(self.adj_train,self.node_train,self.size_subg_budget)\n",
    "        elif self.method_sample == 'full_batch':\n",
    "            self.size_subg_budget = self.node_train.size\n",
    "            self.graph_sampler = full_batch_sampling(self.adj_train,self.node_train,self.size_subg_budget)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        self.norm_loss_train = np.zeros(self.adj_train.shape[0])\n",
    "        self.norm_aggr_train = np.zeros(self.adj_train.size).astype(np.float32)\n",
    "\n",
    "        # For edge sampler, no need to estimate norm factors, we can calculate directly.\n",
    "        # However, for integrity of the framework, we decide to follow the same procedure for all samplers: \n",
    "        # 1. sample enough number of subgraphs\n",
    "        # 2. estimate norm factor alpha and lambda\n",
    "        tot_sampled_nodes = 0\n",
    "        while True:\n",
    "            self.par_graph_sample('train')\n",
    "            tot_sampled_nodes = sum([len(n) for n in self.subgraphs_remaining_nodes])\n",
    "            if tot_sampled_nodes > self.sample_coverage*self.node_train.size:\n",
    "                break\n",
    "        print()\n",
    "        num_subg = len(self.subgraphs_remaining_nodes)\n",
    "        for i in range(num_subg):\n",
    "            self.norm_aggr_train[self.subgraphs_remaining_edge_index[i]] += 1\n",
    "            self.norm_loss_train[self.subgraphs_remaining_nodes[i]] += 1\n",
    "        assert self.norm_loss_train[self.node_val].sum() + self.norm_loss_train[self.node_test].sum() == 0\n",
    "        for v in range(self.adj_train.shape[0]):\n",
    "            i_s = self.adj_train.indptr[v]\n",
    "            i_e = self.adj_train.indptr[v+1]\n",
    "            val = np.clip(self.norm_loss_train[v]/self.norm_aggr_train[i_s:i_e], 0, 1e4)\n",
    "            val[np.isnan(val)] = 0.1\n",
    "            self.norm_aggr_train[i_s:i_e] = val\n",
    "        self.norm_loss_train[np.where(self.norm_loss_train==0)[0]] = 0.1\n",
    "        self.norm_loss_train[self.node_val] = 0\n",
    "        self.norm_loss_train[self.node_test] = 0\n",
    "        self.norm_loss_train[self.node_train] = num_subg/self.norm_loss_train[self.node_train]/self.node_train.size\n",
    "        self.norm_loss_train = torch.from_numpy(self.norm_loss_train.astype(np.float32))\n",
    "        if self.use_cuda:\n",
    "            self.norm_loss_train = self.norm_loss_train.cuda()\n",
    "\n",
    "    def par_graph_sample(self,phase):\n",
    "        t0 = time.time()\n",
    "        _indptr,_indices,_data,_v,_edge_index= self.graph_sampler.par_sample(phase)\n",
    "        t1 = time.time()\n",
    "        print('sampling 200 subgraphs:   time = {:.3f} sec'.format(t1-t0), end=\"\\r\")\n",
    "        self.subgraphs_remaining_indptr.extend(_indptr)\n",
    "        self.subgraphs_remaining_indices.extend(_indices)\n",
    "        self.subgraphs_remaining_data.extend(_data)\n",
    "        self.subgraphs_remaining_nodes.extend(_v)\n",
    "        self.subgraphs_remaining_edge_index.extend(_edge_index)\n",
    "\n",
    "    def one_batch(self,mode='train'):\n",
    "        if mode in ['val','test']:\n",
    "            self.node_subgraph = np.arange(self.adj_full_norm.shape[0])\n",
    "            adj = self.adj_full_norm\n",
    "        else:\n",
    "            assert mode == 'train'\n",
    "            if len(self.subgraphs_remaining_nodes) == 0:\n",
    "                self.par_graph_sample('train')\n",
    "                print()\n",
    "\n",
    "            self.node_subgraph = self.subgraphs_remaining_nodes.pop()\n",
    "            self.size_subgraph = len(self.node_subgraph)\n",
    "            adj = sp.csr_matrix((self.subgraphs_remaining_data.pop(),\\\n",
    "                                 self.subgraphs_remaining_indices.pop(),\\\n",
    "                                 self.subgraphs_remaining_indptr.pop()),\\\n",
    "                                 shape=(self.size_subgraph,self.size_subgraph))\n",
    "            adj_edge_index=self.subgraphs_remaining_edge_index.pop()\n",
    "            #print(\"{} nodes, {} edges, {} degree\".format(self.node_subgraph.size,adj.size,adj.size/self.node_subgraph.size))\n",
    "            norm_aggr(adj.data,adj_edge_index,self.norm_aggr_train,num_proc=args_global.num_cpu_core)\n",
    "            adj = adj_norm(adj, deg=self.deg_train[self.node_subgraph])\n",
    "            adj = _coo_scipy2torch(adj.tocoo())\n",
    "            if self.use_cuda:\n",
    "                adj = adj.cuda()\n",
    "            self.batch_num += 1\n",
    "        norm_loss = self.norm_loss_test if mode in ['val','test'] else self.norm_loss_train\n",
    "        norm_loss = norm_loss[self.node_subgraph]\n",
    "        return self.node_subgraph, adj, norm_loss\n",
    "\n",
    "\n",
    "    def num_training_batches(self):\n",
    "        return math.ceil(self.node_train.shape[0]/float(self.size_subg_budget))\n",
    "\n",
    "    def shuffle(self):\n",
    "        self.node_train = np.random.permutation(self.node_train)\n",
    "        self.batch_num = -1\n",
    "\n",
    "    def end(self):\n",
    "        return (self.batch_num+1)*self.size_subg_budget >= self.node_train.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_1_4_geometric]",
   "language": "python",
   "name": "conda-env-pytorch_1_4_geometric-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
