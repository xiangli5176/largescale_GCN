{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Post_utils import *\n",
    "from multi_exec import *\n",
    "import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = '/home/xiangli/projects/tmpdata/GCN/GraphSaint/'\n",
    "\n",
    "working_dir = './study_eval/'\n",
    "prepare_data_folder = working_dir + 'prepare_data/'\n",
    "img_path = working_dir + 'result/'\n",
    "\n",
    "core_par_sampler = 1\n",
    "samples_per_processor = -(-200 // core_par_sampler) # round up division\n",
    "eval_train_every = 5  # period to record the train loss\n",
    "\n",
    "### ================ Start to do flexible settings according to different dataset: \n",
    "# read the total epoch number from the yml file to determine the mini_epoch_num and eval_train_every\n",
    "data_name = 'Flickr'\n",
    "train_config_yml = './table2/flickr2_e.yml'\n",
    "multilabel_tag = False\n",
    "\n",
    "# data_name = 'PPI_small'\n",
    "# train_config_yml = './table2/ppi2_e.yml'\n",
    "\n",
    "\n",
    "tune_param_name = 'mini_epoch_num'\n",
    "tune_val_label_list = [1, 5] \n",
    "tune_val_list = [val for val in tune_val_label_list]\n",
    "\n",
    "snapshot_period = 5   # period when to take a snapshot of the model for validation later\n",
    "\n",
    "# refer to the yml file to decide the training period:\n",
    "model_epoch_list = list(range(snapshot_period, 16, snapshot_period))    # snapshot epoch list for validation\n",
    "\n",
    "trainer_list = list(range(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 4]\n"
     ]
    }
   ],
   "source": [
    "a = set([2, 3, 4])\n",
    "b = set([3, 4, 6])\n",
    "print(sorted(a & b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangli/projects/GCN_program/largescale_GCN/PC_version_GCN/12_GraphSaint_pytorch/gold_all_in_one/base_exec.py:39: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  train_config = yaml.load(f_train_config)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data..\n",
      "role keys are:  dict_keys(['tr', 'va', 'te'])\n",
      "Done loading training data..\n",
      "type of the train nodes data is: <class 'list'>\n",
      "type of the train nodes data is: <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# =============== Step1 *** prepare for the batches, models, model_evaluation\n",
    "train_params, train_phases, train_data, arch_gcn = train_setting(data_name, datapath, train_config_yml)\n",
    "prepare(working_dir, train_data, train_params, arch_gcn)\n",
    "train_phase_file_name = prepare_data_folder + 'model_train_phase'\n",
    "with open(train_phase_file_name, \"wb\") as fp:\n",
    "    dill.dump(train_phases, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[4mSTART PHASE    0\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangli/projects/GCN_program/largescale_GCN/PC_version_GCN/12_GraphSaint_pytorch/gold_all_in_one/samplers.py:58: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  self.adj_train_norm = scipy.sparse.dia_matrix((1 / self.deg_train,0),shape=adj_train.shape).dot(adj_train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling 200 subgraphs:   time = 0.433 sec\n",
      "\u001b[1mEpoch    1, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    1, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    1, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    1, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    2, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    2, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    2, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    2, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    3, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    3, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    3, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    3, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    4, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    4, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    4, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    4, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    5, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    5, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    5, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    5, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    6, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    6, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    6, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    6, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    7, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    7, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    7, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    7, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    8, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    8, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    8, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    8, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    9, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    9, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    9, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    9, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   10, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   10, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   10, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   10, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   11, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   11, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   11, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   11, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   12, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   12, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   12, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   12, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   13, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   13, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   13, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   13, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   14, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   14, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   14, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   14, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   15, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   15, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   15, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   15, Batch ID 3\u001b[0m\n",
      "\u001b[93mOptimization Finished!\u001b[0m\n",
      "\u001b[91mTotal training time: 2123.66 ms\u001b[0m\n",
      "\u001b[91mTotal train data uploading time: 141.76 ms\u001b[0m\n",
      "\u001b[4mSTART PHASE    0\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangli/projects/GCN_program/largescale_GCN/PC_version_GCN/12_GraphSaint_pytorch/gold_all_in_one/samplers.py:58: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  self.adj_train_norm = scipy.sparse.dia_matrix((1 / self.deg_train,0),shape=adj_train.shape).dot(adj_train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling 200 subgraphs:   time = 0.419 sec\n",
      "\u001b[1mEpoch    1, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    1, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    1, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    1, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    2, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    2, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    2, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    2, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    3, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    3, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    3, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    3, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    4, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    4, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    4, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    4, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    5, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    5, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    5, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    5, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    6, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    6, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    6, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    6, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    7, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    7, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    7, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    7, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    8, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    8, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    8, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    8, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    9, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    9, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    9, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    9, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   10, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   10, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   10, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   10, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   11, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   11, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   11, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   11, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   12, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   12, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   12, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   12, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   13, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   13, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   13, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   13, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   14, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   14, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   14, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   14, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   15, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   15, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   15, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   15, Batch ID 3\u001b[0m\n",
      "\u001b[93mOptimization Finished!\u001b[0m\n",
      "\u001b[91mTotal training time: 1782.85 ms\u001b[0m\n",
      "\u001b[91mTotal train data uploading time: 143.95 ms\u001b[0m\n",
      "\u001b[4mSTART PHASE    0\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangli/projects/GCN_program/largescale_GCN/PC_version_GCN/12_GraphSaint_pytorch/gold_all_in_one/samplers.py:58: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  self.adj_train_norm = scipy.sparse.dia_matrix((1 / self.deg_train,0),shape=adj_train.shape).dot(adj_train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling 200 subgraphs:   time = 0.417 sec\n",
      "\u001b[1mEpoch    1, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    1, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    1, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    1, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    2, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    2, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    2, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    2, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    3, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    3, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    3, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    3, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    4, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    4, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    4, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    4, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    5, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    5, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    5, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    5, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    6, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    6, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    6, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    6, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    7, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    7, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    7, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    7, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    8, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    8, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    8, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    8, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    9, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    9, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    9, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    9, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   10, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   10, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   10, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   10, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   11, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   11, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   11, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   11, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   12, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   12, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   12, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   12, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   13, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   13, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   13, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   13, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   14, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   14, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   14, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   14, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   15, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   15, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   15, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   15, Batch ID 3\u001b[0m\n",
      "\u001b[93mOptimization Finished!\u001b[0m\n",
      "\u001b[91mTotal training time: 1749.67 ms\u001b[0m\n",
      "\u001b[91mTotal train data uploading time: 135.23 ms\u001b[0m\n",
      "\u001b[4mSTART PHASE    0\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangli/projects/GCN_program/largescale_GCN/PC_version_GCN/12_GraphSaint_pytorch/gold_all_in_one/samplers.py:58: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  self.adj_train_norm = scipy.sparse.dia_matrix((1 / self.deg_train,0),shape=adj_train.shape).dot(adj_train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling 200 subgraphs:   time = 0.427 sec\n",
      "\u001b[1mEpoch    1, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    2, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    3, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    4, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    5, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    1, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    2, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    3, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    4, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    5, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    1, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    2, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    3, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    4, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    5, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    1, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    2, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    3, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    4, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    5, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    6, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    7, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    8, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    9, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   10, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    6, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    7, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    8, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    9, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   10, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    6, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    7, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    8, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    9, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   10, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    6, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    7, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    8, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    9, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   10, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   11, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   12, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   13, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   14, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   15, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   11, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   12, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   13, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   14, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   15, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   11, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   12, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   13, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   14, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   15, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   11, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   12, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   13, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   14, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   15, Batch ID 3\u001b[0m\n",
      "\u001b[93mOptimization Finished!\u001b[0m\n",
      "\u001b[91mTotal training time: 1681.33 ms\u001b[0m\n",
      "\u001b[91mTotal train data uploading time:  28.01 ms\u001b[0m\n",
      "\u001b[4mSTART PHASE    0\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangli/projects/GCN_program/largescale_GCN/PC_version_GCN/12_GraphSaint_pytorch/gold_all_in_one/samplers.py:58: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  self.adj_train_norm = scipy.sparse.dia_matrix((1 / self.deg_train,0),shape=adj_train.shape).dot(adj_train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling 200 subgraphs:   time = 0.414 sec\n",
      "\u001b[1mEpoch    1, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    2, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    3, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    4, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    5, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    1, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    2, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    3, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    4, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    5, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    1, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    2, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    3, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    4, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    5, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    1, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    2, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    3, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    4, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    5, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    6, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    7, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    8, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    9, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   10, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    6, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    7, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    8, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    9, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   10, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    6, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    7, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    8, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    9, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   10, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    6, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    7, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    8, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    9, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   10, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   11, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   12, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   13, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   14, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   15, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   11, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   12, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   13, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   14, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   15, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   11, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   12, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   13, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   14, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   15, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   11, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   12, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   13, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   14, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   15, Batch ID 3\u001b[0m\n",
      "\u001b[93mOptimization Finished!\u001b[0m\n",
      "\u001b[91mTotal training time: 1700.17 ms\u001b[0m\n",
      "\u001b[91mTotal train data uploading time:  27.56 ms\u001b[0m\n",
      "\u001b[4mSTART PHASE    0\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangli/projects/GCN_program/largescale_GCN/PC_version_GCN/12_GraphSaint_pytorch/gold_all_in_one/samplers.py:58: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  self.adj_train_norm = scipy.sparse.dia_matrix((1 / self.deg_train,0),shape=adj_train.shape).dot(adj_train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling 200 subgraphs:   time = 0.415 sec\n",
      "\u001b[1mEpoch    1, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    2, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    3, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    4, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    5, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    1, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    2, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    3, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    4, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    5, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    1, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    2, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    3, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    4, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    5, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    1, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    2, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    3, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    4, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    5, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    6, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    7, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    8, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    9, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   10, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    6, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    7, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    8, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    9, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   10, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    6, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    7, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    8, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    9, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   10, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    6, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    7, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    8, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    9, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   10, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   11, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   12, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   13, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   14, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   15, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   11, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   12, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   13, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   14, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   15, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   11, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   12, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   13, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   14, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   15, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   11, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   12, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   13, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   14, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   15, Batch ID 3\u001b[0m\n",
      "\u001b[93mOptimization Finished!\u001b[0m\n",
      "\u001b[91mTotal training time: 1678.76 ms\u001b[0m\n",
      "\u001b[91mTotal train data uploading time:  27.70 ms\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# ============== Step2 *** conduct the training process\n",
    "train_input_file_name = prepare_data_folder + 'model_train_input'\n",
    "with open(train_input_file_name, \"rb\") as fp:\n",
    "    minibatch, model = dill.load(fp)\n",
    "\n",
    "\n",
    "train_phase_file_name = prepare_data_folder + 'model_train_phase'\n",
    "with open(train_phase_file_name, \"rb\") as fp:\n",
    "    train_phases = dill.load(fp)\n",
    "\n",
    "for tune_val_label, tune_val in zip(tune_val_label_list, tune_val_list):\n",
    "    for trainer_id in trainer_list:\n",
    "        execute_train_investigate(img_path, working_dir, train_phases, model, minibatch, eval_train_every, \n",
    "                                  tune_param_name, tune_val_label, tune_val, trainer_id = trainer_id,\n",
    "                                  snapshot_every = snapshot_period, mini_epoch_num = 5, multilabel = multilabel_tag, \n",
    "                                  core_par_sampler = core_par_sampler, samples_per_processor = samples_per_processor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start validation investigate: \n",
      "during 5 epoch of validation snapshot :\n",
      "\n",
      " After loading the exisiting model parameters, the model state_dict is listed as follows:: \n",
      "conv_layers.0.f_lin.0.weight \t torch.Size([256, 500])\n",
      "conv_layers.0.f_lin.1.weight \t torch.Size([256, 500])\n",
      "conv_layers.0.params.0 \t torch.Size([256])\n",
      "conv_layers.0.params.1 \t torch.Size([256])\n",
      "conv_layers.0.params.2 \t torch.Size([256])\n",
      "conv_layers.0.params.3 \t torch.Size([256])\n",
      "conv_layers.0.params.4 \t torch.Size([256])\n",
      "conv_layers.0.params.5 \t torch.Size([256])\n",
      "conv_layers.0.f_bias.0 \t torch.Size([256])\n",
      "conv_layers.0.f_bias.1 \t torch.Size([256])\n",
      "conv_layers.0.offset.0 \t torch.Size([256])\n",
      "conv_layers.0.offset.1 \t torch.Size([256])\n",
      "conv_layers.0.scale.0 \t torch.Size([256])\n",
      "conv_layers.0.scale.1 \t torch.Size([256])\n",
      "conv_layers.1.f_lin.0.weight \t torch.Size([256, 512])\n",
      "conv_layers.1.f_lin.1.weight \t torch.Size([256, 512])\n",
      "conv_layers.1.params.0 \t torch.Size([256])\n",
      "conv_layers.1.params.1 \t torch.Size([256])\n",
      "conv_layers.1.params.2 \t torch.Size([256])\n",
      "conv_layers.1.params.3 \t torch.Size([256])\n",
      "conv_layers.1.params.4 \t torch.Size([256])\n",
      "conv_layers.1.params.5 \t torch.Size([256])\n",
      "conv_layers.1.f_bias.0 \t torch.Size([256])\n",
      "conv_layers.1.f_bias.1 \t torch.Size([256])\n",
      "conv_layers.1.offset.0 \t torch.Size([256])\n",
      "conv_layers.1.offset.1 \t torch.Size([256])\n",
      "conv_layers.1.scale.0 \t torch.Size([256])\n",
      "conv_layers.1.scale.1 \t torch.Size([256])\n",
      "conv_layers.2.f_lin.0.weight \t torch.Size([256, 512])\n",
      "conv_layers.2.params.0 \t torch.Size([256])\n",
      "conv_layers.2.params.1 \t torch.Size([256])\n",
      "conv_layers.2.params.2 \t torch.Size([256])\n",
      "conv_layers.2.f_bias.0 \t torch.Size([256])\n",
      "conv_layers.2.offset.0 \t torch.Size([256])\n",
      "conv_layers.2.scale.0 \t torch.Size([256])\n",
      "classifier.f_lin.0.weight \t torch.Size([7, 256])\n",
      "classifier.params.0 \t torch.Size([7])\n",
      "classifier.params.1 \t torch.Size([7])\n",
      "classifier.params.2 \t torch.Size([7])\n",
      "classifier.f_bias.0 \t torch.Size([7])\n",
      "classifier.offset.0 \t torch.Size([7])\n",
      "classifier.scale.0 \t torch.Size([7])\n",
      "during the evaluation step, report the input matrices size: \n",
      "adj_subgraph size :  torch.Size([89250, 89250]); \t feat_subg size : torch.Size([89250, 500])\n",
      "during 10 epoch of validation snapshot :\n",
      "\n",
      " After loading the exisiting model parameters, the model state_dict is listed as follows:: \n",
      "conv_layers.0.f_lin.0.weight \t torch.Size([256, 500])\n",
      "conv_layers.0.f_lin.1.weight \t torch.Size([256, 500])\n",
      "conv_layers.0.params.0 \t torch.Size([256])\n",
      "conv_layers.0.params.1 \t torch.Size([256])\n",
      "conv_layers.0.params.2 \t torch.Size([256])\n",
      "conv_layers.0.params.3 \t torch.Size([256])\n",
      "conv_layers.0.params.4 \t torch.Size([256])\n",
      "conv_layers.0.params.5 \t torch.Size([256])\n",
      "conv_layers.0.f_bias.0 \t torch.Size([256])\n",
      "conv_layers.0.f_bias.1 \t torch.Size([256])\n",
      "conv_layers.0.offset.0 \t torch.Size([256])\n",
      "conv_layers.0.offset.1 \t torch.Size([256])\n",
      "conv_layers.0.scale.0 \t torch.Size([256])\n",
      "conv_layers.0.scale.1 \t torch.Size([256])\n",
      "conv_layers.1.f_lin.0.weight \t torch.Size([256, 512])\n",
      "conv_layers.1.f_lin.1.weight \t torch.Size([256, 512])\n",
      "conv_layers.1.params.0 \t torch.Size([256])\n",
      "conv_layers.1.params.1 \t torch.Size([256])\n",
      "conv_layers.1.params.2 \t torch.Size([256])\n",
      "conv_layers.1.params.3 \t torch.Size([256])\n",
      "conv_layers.1.params.4 \t torch.Size([256])\n",
      "conv_layers.1.params.5 \t torch.Size([256])\n",
      "conv_layers.1.f_bias.0 \t torch.Size([256])\n",
      "conv_layers.1.f_bias.1 \t torch.Size([256])\n",
      "conv_layers.1.offset.0 \t torch.Size([256])\n",
      "conv_layers.1.offset.1 \t torch.Size([256])\n",
      "conv_layers.1.scale.0 \t torch.Size([256])\n",
      "conv_layers.1.scale.1 \t torch.Size([256])\n",
      "conv_layers.2.f_lin.0.weight \t torch.Size([256, 512])\n",
      "conv_layers.2.params.0 \t torch.Size([256])\n",
      "conv_layers.2.params.1 \t torch.Size([256])\n",
      "conv_layers.2.params.2 \t torch.Size([256])\n",
      "conv_layers.2.f_bias.0 \t torch.Size([256])\n",
      "conv_layers.2.offset.0 \t torch.Size([256])\n",
      "conv_layers.2.scale.0 \t torch.Size([256])\n",
      "classifier.f_lin.0.weight \t torch.Size([7, 256])\n",
      "classifier.params.0 \t torch.Size([7])\n",
      "classifier.params.1 \t torch.Size([7])\n",
      "classifier.params.2 \t torch.Size([7])\n",
      "classifier.f_bias.0 \t torch.Size([7])\n",
      "classifier.offset.0 \t torch.Size([7])\n",
      "classifier.scale.0 \t torch.Size([7])\n",
      "during the evaluation step, report the input matrices size: \n",
      "adj_subgraph size :  torch.Size([89250, 89250]); \t feat_subg size : torch.Size([89250, 500])\n",
      "during 15 epoch of validation snapshot :\n",
      "\n",
      " After loading the exisiting model parameters, the model state_dict is listed as follows:: \n",
      "conv_layers.0.f_lin.0.weight \t torch.Size([256, 500])\n",
      "conv_layers.0.f_lin.1.weight \t torch.Size([256, 500])\n",
      "conv_layers.0.params.0 \t torch.Size([256])\n",
      "conv_layers.0.params.1 \t torch.Size([256])\n",
      "conv_layers.0.params.2 \t torch.Size([256])\n",
      "conv_layers.0.params.3 \t torch.Size([256])\n",
      "conv_layers.0.params.4 \t torch.Size([256])\n",
      "conv_layers.0.params.5 \t torch.Size([256])\n",
      "conv_layers.0.f_bias.0 \t torch.Size([256])\n",
      "conv_layers.0.f_bias.1 \t torch.Size([256])\n",
      "conv_layers.0.offset.0 \t torch.Size([256])\n",
      "conv_layers.0.offset.1 \t torch.Size([256])\n",
      "conv_layers.0.scale.0 \t torch.Size([256])\n",
      "conv_layers.0.scale.1 \t torch.Size([256])\n",
      "conv_layers.1.f_lin.0.weight \t torch.Size([256, 512])\n",
      "conv_layers.1.f_lin.1.weight \t torch.Size([256, 512])\n",
      "conv_layers.1.params.0 \t torch.Size([256])\n",
      "conv_layers.1.params.1 \t torch.Size([256])\n",
      "conv_layers.1.params.2 \t torch.Size([256])\n",
      "conv_layers.1.params.3 \t torch.Size([256])\n",
      "conv_layers.1.params.4 \t torch.Size([256])\n",
      "conv_layers.1.params.5 \t torch.Size([256])\n",
      "conv_layers.1.f_bias.0 \t torch.Size([256])\n",
      "conv_layers.1.f_bias.1 \t torch.Size([256])\n",
      "conv_layers.1.offset.0 \t torch.Size([256])\n",
      "conv_layers.1.offset.1 \t torch.Size([256])\n",
      "conv_layers.1.scale.0 \t torch.Size([256])\n",
      "conv_layers.1.scale.1 \t torch.Size([256])\n",
      "conv_layers.2.f_lin.0.weight \t torch.Size([256, 512])\n",
      "conv_layers.2.params.0 \t torch.Size([256])\n",
      "conv_layers.2.params.1 \t torch.Size([256])\n",
      "conv_layers.2.params.2 \t torch.Size([256])\n",
      "conv_layers.2.f_bias.0 \t torch.Size([256])\n",
      "conv_layers.2.offset.0 \t torch.Size([256])\n",
      "conv_layers.2.scale.0 \t torch.Size([256])\n",
      "classifier.f_lin.0.weight \t torch.Size([7, 256])\n",
      "classifier.params.0 \t torch.Size([7])\n",
      "classifier.params.1 \t torch.Size([7])\n",
      "classifier.params.2 \t torch.Size([7])\n",
      "classifier.f_bias.0 \t torch.Size([7])\n",
      "classifier.offset.0 \t torch.Size([7])\n",
      "classifier.scale.0 \t torch.Size([7])\n",
      "during the evaluation step, report the input matrices size: \n",
      "adj_subgraph size :  torch.Size([89250, 89250]); \t feat_subg size : torch.Size([89250, 500])\n",
      "start validation investigate: \n",
      "during 5 epoch of validation snapshot :\n",
      "\n",
      " After loading the exisiting model parameters, the model state_dict is listed as follows:: \n",
      "conv_layers.0.f_lin.0.weight \t torch.Size([256, 500])\n",
      "conv_layers.0.f_lin.1.weight \t torch.Size([256, 500])\n",
      "conv_layers.0.params.0 \t torch.Size([256])\n",
      "conv_layers.0.params.1 \t torch.Size([256])\n",
      "conv_layers.0.params.2 \t torch.Size([256])\n",
      "conv_layers.0.params.3 \t torch.Size([256])\n",
      "conv_layers.0.params.4 \t torch.Size([256])\n",
      "conv_layers.0.params.5 \t torch.Size([256])\n",
      "conv_layers.0.f_bias.0 \t torch.Size([256])\n",
      "conv_layers.0.f_bias.1 \t torch.Size([256])\n",
      "conv_layers.0.offset.0 \t torch.Size([256])\n",
      "conv_layers.0.offset.1 \t torch.Size([256])\n",
      "conv_layers.0.scale.0 \t torch.Size([256])\n",
      "conv_layers.0.scale.1 \t torch.Size([256])\n",
      "conv_layers.1.f_lin.0.weight \t torch.Size([256, 512])\n",
      "conv_layers.1.f_lin.1.weight \t torch.Size([256, 512])\n",
      "conv_layers.1.params.0 \t torch.Size([256])\n",
      "conv_layers.1.params.1 \t torch.Size([256])\n",
      "conv_layers.1.params.2 \t torch.Size([256])\n",
      "conv_layers.1.params.3 \t torch.Size([256])\n",
      "conv_layers.1.params.4 \t torch.Size([256])\n",
      "conv_layers.1.params.5 \t torch.Size([256])\n",
      "conv_layers.1.f_bias.0 \t torch.Size([256])\n",
      "conv_layers.1.f_bias.1 \t torch.Size([256])\n",
      "conv_layers.1.offset.0 \t torch.Size([256])\n",
      "conv_layers.1.offset.1 \t torch.Size([256])\n",
      "conv_layers.1.scale.0 \t torch.Size([256])\n",
      "conv_layers.1.scale.1 \t torch.Size([256])\n",
      "conv_layers.2.f_lin.0.weight \t torch.Size([256, 512])\n",
      "conv_layers.2.params.0 \t torch.Size([256])\n",
      "conv_layers.2.params.1 \t torch.Size([256])\n",
      "conv_layers.2.params.2 \t torch.Size([256])\n",
      "conv_layers.2.f_bias.0 \t torch.Size([256])\n",
      "conv_layers.2.offset.0 \t torch.Size([256])\n",
      "conv_layers.2.scale.0 \t torch.Size([256])\n",
      "classifier.f_lin.0.weight \t torch.Size([7, 256])\n",
      "classifier.params.0 \t torch.Size([7])\n",
      "classifier.params.1 \t torch.Size([7])\n",
      "classifier.params.2 \t torch.Size([7])\n",
      "classifier.f_bias.0 \t torch.Size([7])\n",
      "classifier.offset.0 \t torch.Size([7])\n",
      "classifier.scale.0 \t torch.Size([7])\n",
      "during the evaluation step, report the input matrices size: \n",
      "adj_subgraph size :  torch.Size([89250, 89250]); \t feat_subg size : torch.Size([89250, 500])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "during 10 epoch of validation snapshot :\n",
      "\n",
      " After loading the exisiting model parameters, the model state_dict is listed as follows:: \n",
      "conv_layers.0.f_lin.0.weight \t torch.Size([256, 500])\n",
      "conv_layers.0.f_lin.1.weight \t torch.Size([256, 500])\n",
      "conv_layers.0.params.0 \t torch.Size([256])\n",
      "conv_layers.0.params.1 \t torch.Size([256])\n",
      "conv_layers.0.params.2 \t torch.Size([256])\n",
      "conv_layers.0.params.3 \t torch.Size([256])\n",
      "conv_layers.0.params.4 \t torch.Size([256])\n",
      "conv_layers.0.params.5 \t torch.Size([256])\n",
      "conv_layers.0.f_bias.0 \t torch.Size([256])\n",
      "conv_layers.0.f_bias.1 \t torch.Size([256])\n",
      "conv_layers.0.offset.0 \t torch.Size([256])\n",
      "conv_layers.0.offset.1 \t torch.Size([256])\n",
      "conv_layers.0.scale.0 \t torch.Size([256])\n",
      "conv_layers.0.scale.1 \t torch.Size([256])\n",
      "conv_layers.1.f_lin.0.weight \t torch.Size([256, 512])\n",
      "conv_layers.1.f_lin.1.weight \t torch.Size([256, 512])\n",
      "conv_layers.1.params.0 \t torch.Size([256])\n",
      "conv_layers.1.params.1 \t torch.Size([256])\n",
      "conv_layers.1.params.2 \t torch.Size([256])\n",
      "conv_layers.1.params.3 \t torch.Size([256])\n",
      "conv_layers.1.params.4 \t torch.Size([256])\n",
      "conv_layers.1.params.5 \t torch.Size([256])\n",
      "conv_layers.1.f_bias.0 \t torch.Size([256])\n",
      "conv_layers.1.f_bias.1 \t torch.Size([256])\n",
      "conv_layers.1.offset.0 \t torch.Size([256])\n",
      "conv_layers.1.offset.1 \t torch.Size([256])\n",
      "conv_layers.1.scale.0 \t torch.Size([256])\n",
      "conv_layers.1.scale.1 \t torch.Size([256])\n",
      "conv_layers.2.f_lin.0.weight \t torch.Size([256, 512])\n",
      "conv_layers.2.params.0 \t torch.Size([256])\n",
      "conv_layers.2.params.1 \t torch.Size([256])\n",
      "conv_layers.2.params.2 \t torch.Size([256])\n",
      "conv_layers.2.f_bias.0 \t torch.Size([256])\n",
      "conv_layers.2.offset.0 \t torch.Size([256])\n",
      "conv_layers.2.scale.0 \t torch.Size([256])\n",
      "classifier.f_lin.0.weight \t torch.Size([7, 256])\n",
      "classifier.params.0 \t torch.Size([7])\n",
      "classifier.params.1 \t torch.Size([7])\n",
      "classifier.params.2 \t torch.Size([7])\n",
      "classifier.f_bias.0 \t torch.Size([7])\n",
      "classifier.offset.0 \t torch.Size([7])\n",
      "classifier.scale.0 \t torch.Size([7])\n",
      "during the evaluation step, report the input matrices size: \n",
      "adj_subgraph size :  torch.Size([89250, 89250]); \t feat_subg size : torch.Size([89250, 500])\n",
      "during 15 epoch of validation snapshot :\n",
      "\n",
      " After loading the exisiting model parameters, the model state_dict is listed as follows:: \n",
      "conv_layers.0.f_lin.0.weight \t torch.Size([256, 500])\n",
      "conv_layers.0.f_lin.1.weight \t torch.Size([256, 500])\n",
      "conv_layers.0.params.0 \t torch.Size([256])\n",
      "conv_layers.0.params.1 \t torch.Size([256])\n",
      "conv_layers.0.params.2 \t torch.Size([256])\n",
      "conv_layers.0.params.3 \t torch.Size([256])\n",
      "conv_layers.0.params.4 \t torch.Size([256])\n",
      "conv_layers.0.params.5 \t torch.Size([256])\n",
      "conv_layers.0.f_bias.0 \t torch.Size([256])\n",
      "conv_layers.0.f_bias.1 \t torch.Size([256])\n",
      "conv_layers.0.offset.0 \t torch.Size([256])\n",
      "conv_layers.0.offset.1 \t torch.Size([256])\n",
      "conv_layers.0.scale.0 \t torch.Size([256])\n",
      "conv_layers.0.scale.1 \t torch.Size([256])\n",
      "conv_layers.1.f_lin.0.weight \t torch.Size([256, 512])\n",
      "conv_layers.1.f_lin.1.weight \t torch.Size([256, 512])\n",
      "conv_layers.1.params.0 \t torch.Size([256])\n",
      "conv_layers.1.params.1 \t torch.Size([256])\n",
      "conv_layers.1.params.2 \t torch.Size([256])\n",
      "conv_layers.1.params.3 \t torch.Size([256])\n",
      "conv_layers.1.params.4 \t torch.Size([256])\n",
      "conv_layers.1.params.5 \t torch.Size([256])\n",
      "conv_layers.1.f_bias.0 \t torch.Size([256])\n",
      "conv_layers.1.f_bias.1 \t torch.Size([256])\n",
      "conv_layers.1.offset.0 \t torch.Size([256])\n",
      "conv_layers.1.offset.1 \t torch.Size([256])\n",
      "conv_layers.1.scale.0 \t torch.Size([256])\n",
      "conv_layers.1.scale.1 \t torch.Size([256])\n",
      "conv_layers.2.f_lin.0.weight \t torch.Size([256, 512])\n",
      "conv_layers.2.params.0 \t torch.Size([256])\n",
      "conv_layers.2.params.1 \t torch.Size([256])\n",
      "conv_layers.2.params.2 \t torch.Size([256])\n",
      "conv_layers.2.f_bias.0 \t torch.Size([256])\n",
      "conv_layers.2.offset.0 \t torch.Size([256])\n",
      "conv_layers.2.scale.0 \t torch.Size([256])\n",
      "classifier.f_lin.0.weight \t torch.Size([7, 256])\n",
      "classifier.params.0 \t torch.Size([7])\n",
      "classifier.params.1 \t torch.Size([7])\n",
      "classifier.params.2 \t torch.Size([7])\n",
      "classifier.f_bias.0 \t torch.Size([7])\n",
      "classifier.offset.0 \t torch.Size([7])\n",
      "classifier.scale.0 \t torch.Size([7])\n",
      "during the evaluation step, report the input matrices size: \n",
      "adj_subgraph size :  torch.Size([89250, 89250]); \t feat_subg size : torch.Size([89250, 500])\n",
      "start validation investigate: \n",
      "during 5 epoch of validation snapshot :\n",
      "\n",
      " After loading the exisiting model parameters, the model state_dict is listed as follows:: \n",
      "conv_layers.0.f_lin.0.weight \t torch.Size([256, 500])\n",
      "conv_layers.0.f_lin.1.weight \t torch.Size([256, 500])\n",
      "conv_layers.0.params.0 \t torch.Size([256])\n",
      "conv_layers.0.params.1 \t torch.Size([256])\n",
      "conv_layers.0.params.2 \t torch.Size([256])\n",
      "conv_layers.0.params.3 \t torch.Size([256])\n",
      "conv_layers.0.params.4 \t torch.Size([256])\n",
      "conv_layers.0.params.5 \t torch.Size([256])\n",
      "conv_layers.0.f_bias.0 \t torch.Size([256])\n",
      "conv_layers.0.f_bias.1 \t torch.Size([256])\n",
      "conv_layers.0.offset.0 \t torch.Size([256])\n",
      "conv_layers.0.offset.1 \t torch.Size([256])\n",
      "conv_layers.0.scale.0 \t torch.Size([256])\n",
      "conv_layers.0.scale.1 \t torch.Size([256])\n",
      "conv_layers.1.f_lin.0.weight \t torch.Size([256, 512])\n",
      "conv_layers.1.f_lin.1.weight \t torch.Size([256, 512])\n",
      "conv_layers.1.params.0 \t torch.Size([256])\n",
      "conv_layers.1.params.1 \t torch.Size([256])\n",
      "conv_layers.1.params.2 \t torch.Size([256])\n",
      "conv_layers.1.params.3 \t torch.Size([256])\n",
      "conv_layers.1.params.4 \t torch.Size([256])\n",
      "conv_layers.1.params.5 \t torch.Size([256])\n",
      "conv_layers.1.f_bias.0 \t torch.Size([256])\n",
      "conv_layers.1.f_bias.1 \t torch.Size([256])\n",
      "conv_layers.1.offset.0 \t torch.Size([256])\n",
      "conv_layers.1.offset.1 \t torch.Size([256])\n",
      "conv_layers.1.scale.0 \t torch.Size([256])\n",
      "conv_layers.1.scale.1 \t torch.Size([256])\n",
      "conv_layers.2.f_lin.0.weight \t torch.Size([256, 512])\n",
      "conv_layers.2.params.0 \t torch.Size([256])\n",
      "conv_layers.2.params.1 \t torch.Size([256])\n",
      "conv_layers.2.params.2 \t torch.Size([256])\n",
      "conv_layers.2.f_bias.0 \t torch.Size([256])\n",
      "conv_layers.2.offset.0 \t torch.Size([256])\n",
      "conv_layers.2.scale.0 \t torch.Size([256])\n",
      "classifier.f_lin.0.weight \t torch.Size([7, 256])\n",
      "classifier.params.0 \t torch.Size([7])\n",
      "classifier.params.1 \t torch.Size([7])\n",
      "classifier.params.2 \t torch.Size([7])\n",
      "classifier.f_bias.0 \t torch.Size([7])\n",
      "classifier.offset.0 \t torch.Size([7])\n",
      "classifier.scale.0 \t torch.Size([7])\n",
      "during the evaluation step, report the input matrices size: \n",
      "adj_subgraph size :  torch.Size([89250, 89250]); \t feat_subg size : torch.Size([89250, 500])\n",
      "during 10 epoch of validation snapshot :\n",
      "\n",
      " After loading the exisiting model parameters, the model state_dict is listed as follows:: \n",
      "conv_layers.0.f_lin.0.weight \t torch.Size([256, 500])\n",
      "conv_layers.0.f_lin.1.weight \t torch.Size([256, 500])\n",
      "conv_layers.0.params.0 \t torch.Size([256])\n",
      "conv_layers.0.params.1 \t torch.Size([256])\n",
      "conv_layers.0.params.2 \t torch.Size([256])\n",
      "conv_layers.0.params.3 \t torch.Size([256])\n",
      "conv_layers.0.params.4 \t torch.Size([256])\n",
      "conv_layers.0.params.5 \t torch.Size([256])\n",
      "conv_layers.0.f_bias.0 \t torch.Size([256])\n",
      "conv_layers.0.f_bias.1 \t torch.Size([256])\n",
      "conv_layers.0.offset.0 \t torch.Size([256])\n",
      "conv_layers.0.offset.1 \t torch.Size([256])\n",
      "conv_layers.0.scale.0 \t torch.Size([256])\n",
      "conv_layers.0.scale.1 \t torch.Size([256])\n",
      "conv_layers.1.f_lin.0.weight \t torch.Size([256, 512])\n",
      "conv_layers.1.f_lin.1.weight \t torch.Size([256, 512])\n",
      "conv_layers.1.params.0 \t torch.Size([256])\n",
      "conv_layers.1.params.1 \t torch.Size([256])\n",
      "conv_layers.1.params.2 \t torch.Size([256])\n",
      "conv_layers.1.params.3 \t torch.Size([256])\n",
      "conv_layers.1.params.4 \t torch.Size([256])\n",
      "conv_layers.1.params.5 \t torch.Size([256])\n",
      "conv_layers.1.f_bias.0 \t torch.Size([256])\n",
      "conv_layers.1.f_bias.1 \t torch.Size([256])\n",
      "conv_layers.1.offset.0 \t torch.Size([256])\n",
      "conv_layers.1.offset.1 \t torch.Size([256])\n",
      "conv_layers.1.scale.0 \t torch.Size([256])\n",
      "conv_layers.1.scale.1 \t torch.Size([256])\n",
      "conv_layers.2.f_lin.0.weight \t torch.Size([256, 512])\n",
      "conv_layers.2.params.0 \t torch.Size([256])\n",
      "conv_layers.2.params.1 \t torch.Size([256])\n",
      "conv_layers.2.params.2 \t torch.Size([256])\n",
      "conv_layers.2.f_bias.0 \t torch.Size([256])\n",
      "conv_layers.2.offset.0 \t torch.Size([256])\n",
      "conv_layers.2.scale.0 \t torch.Size([256])\n",
      "classifier.f_lin.0.weight \t torch.Size([7, 256])\n",
      "classifier.params.0 \t torch.Size([7])\n",
      "classifier.params.1 \t torch.Size([7])\n",
      "classifier.params.2 \t torch.Size([7])\n",
      "classifier.f_bias.0 \t torch.Size([7])\n",
      "classifier.offset.0 \t torch.Size([7])\n",
      "classifier.scale.0 \t torch.Size([7])\n",
      "during the evaluation step, report the input matrices size: \n",
      "adj_subgraph size :  torch.Size([89250, 89250]); \t feat_subg size : torch.Size([89250, 500])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "during 15 epoch of validation snapshot :\n",
      "\n",
      " After loading the exisiting model parameters, the model state_dict is listed as follows:: \n",
      "conv_layers.0.f_lin.0.weight \t torch.Size([256, 500])\n",
      "conv_layers.0.f_lin.1.weight \t torch.Size([256, 500])\n",
      "conv_layers.0.params.0 \t torch.Size([256])\n",
      "conv_layers.0.params.1 \t torch.Size([256])\n",
      "conv_layers.0.params.2 \t torch.Size([256])\n",
      "conv_layers.0.params.3 \t torch.Size([256])\n",
      "conv_layers.0.params.4 \t torch.Size([256])\n",
      "conv_layers.0.params.5 \t torch.Size([256])\n",
      "conv_layers.0.f_bias.0 \t torch.Size([256])\n",
      "conv_layers.0.f_bias.1 \t torch.Size([256])\n",
      "conv_layers.0.offset.0 \t torch.Size([256])\n",
      "conv_layers.0.offset.1 \t torch.Size([256])\n",
      "conv_layers.0.scale.0 \t torch.Size([256])\n",
      "conv_layers.0.scale.1 \t torch.Size([256])\n",
      "conv_layers.1.f_lin.0.weight \t torch.Size([256, 512])\n",
      "conv_layers.1.f_lin.1.weight \t torch.Size([256, 512])\n",
      "conv_layers.1.params.0 \t torch.Size([256])\n",
      "conv_layers.1.params.1 \t torch.Size([256])\n",
      "conv_layers.1.params.2 \t torch.Size([256])\n",
      "conv_layers.1.params.3 \t torch.Size([256])\n",
      "conv_layers.1.params.4 \t torch.Size([256])\n",
      "conv_layers.1.params.5 \t torch.Size([256])\n",
      "conv_layers.1.f_bias.0 \t torch.Size([256])\n",
      "conv_layers.1.f_bias.1 \t torch.Size([256])\n",
      "conv_layers.1.offset.0 \t torch.Size([256])\n",
      "conv_layers.1.offset.1 \t torch.Size([256])\n",
      "conv_layers.1.scale.0 \t torch.Size([256])\n",
      "conv_layers.1.scale.1 \t torch.Size([256])\n",
      "conv_layers.2.f_lin.0.weight \t torch.Size([256, 512])\n",
      "conv_layers.2.params.0 \t torch.Size([256])\n",
      "conv_layers.2.params.1 \t torch.Size([256])\n",
      "conv_layers.2.params.2 \t torch.Size([256])\n",
      "conv_layers.2.f_bias.0 \t torch.Size([256])\n",
      "conv_layers.2.offset.0 \t torch.Size([256])\n",
      "conv_layers.2.scale.0 \t torch.Size([256])\n",
      "classifier.f_lin.0.weight \t torch.Size([7, 256])\n",
      "classifier.params.0 \t torch.Size([7])\n",
      "classifier.params.1 \t torch.Size([7])\n",
      "classifier.params.2 \t torch.Size([7])\n",
      "classifier.f_bias.0 \t torch.Size([7])\n",
      "classifier.offset.0 \t torch.Size([7])\n",
      "classifier.scale.0 \t torch.Size([7])\n",
      "during the evaluation step, report the input matrices size: \n",
      "adj_subgraph size :  torch.Size([89250, 89250]); \t feat_subg size : torch.Size([89250, 500])\n",
      "start validation investigate: \n",
      "during 5 epoch of validation snapshot :\n",
      "\n",
      " After loading the exisiting model parameters, the model state_dict is listed as follows:: \n",
      "conv_layers.0.f_lin.0.weight \t torch.Size([256, 500])\n",
      "conv_layers.0.f_lin.1.weight \t torch.Size([256, 500])\n",
      "conv_layers.0.params.0 \t torch.Size([256])\n",
      "conv_layers.0.params.1 \t torch.Size([256])\n",
      "conv_layers.0.params.2 \t torch.Size([256])\n",
      "conv_layers.0.params.3 \t torch.Size([256])\n",
      "conv_layers.0.params.4 \t torch.Size([256])\n",
      "conv_layers.0.params.5 \t torch.Size([256])\n",
      "conv_layers.0.f_bias.0 \t torch.Size([256])\n",
      "conv_layers.0.f_bias.1 \t torch.Size([256])\n",
      "conv_layers.0.offset.0 \t torch.Size([256])\n",
      "conv_layers.0.offset.1 \t torch.Size([256])\n",
      "conv_layers.0.scale.0 \t torch.Size([256])\n",
      "conv_layers.0.scale.1 \t torch.Size([256])\n",
      "conv_layers.1.f_lin.0.weight \t torch.Size([256, 512])\n",
      "conv_layers.1.f_lin.1.weight \t torch.Size([256, 512])\n",
      "conv_layers.1.params.0 \t torch.Size([256])\n",
      "conv_layers.1.params.1 \t torch.Size([256])\n",
      "conv_layers.1.params.2 \t torch.Size([256])\n",
      "conv_layers.1.params.3 \t torch.Size([256])\n",
      "conv_layers.1.params.4 \t torch.Size([256])\n",
      "conv_layers.1.params.5 \t torch.Size([256])\n",
      "conv_layers.1.f_bias.0 \t torch.Size([256])\n",
      "conv_layers.1.f_bias.1 \t torch.Size([256])\n",
      "conv_layers.1.offset.0 \t torch.Size([256])\n",
      "conv_layers.1.offset.1 \t torch.Size([256])\n",
      "conv_layers.1.scale.0 \t torch.Size([256])\n",
      "conv_layers.1.scale.1 \t torch.Size([256])\n",
      "conv_layers.2.f_lin.0.weight \t torch.Size([256, 512])\n",
      "conv_layers.2.params.0 \t torch.Size([256])\n",
      "conv_layers.2.params.1 \t torch.Size([256])\n",
      "conv_layers.2.params.2 \t torch.Size([256])\n",
      "conv_layers.2.f_bias.0 \t torch.Size([256])\n",
      "conv_layers.2.offset.0 \t torch.Size([256])\n",
      "conv_layers.2.scale.0 \t torch.Size([256])\n",
      "classifier.f_lin.0.weight \t torch.Size([7, 256])\n",
      "classifier.params.0 \t torch.Size([7])\n",
      "classifier.params.1 \t torch.Size([7])\n",
      "classifier.params.2 \t torch.Size([7])\n",
      "classifier.f_bias.0 \t torch.Size([7])\n",
      "classifier.offset.0 \t torch.Size([7])\n",
      "classifier.scale.0 \t torch.Size([7])\n",
      "during the evaluation step, report the input matrices size: \n",
      "adj_subgraph size :  torch.Size([89250, 89250]); \t feat_subg size : torch.Size([89250, 500])\n",
      "during 10 epoch of validation snapshot :\n",
      "\n",
      " After loading the exisiting model parameters, the model state_dict is listed as follows:: \n",
      "conv_layers.0.f_lin.0.weight \t torch.Size([256, 500])\n",
      "conv_layers.0.f_lin.1.weight \t torch.Size([256, 500])\n",
      "conv_layers.0.params.0 \t torch.Size([256])\n",
      "conv_layers.0.params.1 \t torch.Size([256])\n",
      "conv_layers.0.params.2 \t torch.Size([256])\n",
      "conv_layers.0.params.3 \t torch.Size([256])\n",
      "conv_layers.0.params.4 \t torch.Size([256])\n",
      "conv_layers.0.params.5 \t torch.Size([256])\n",
      "conv_layers.0.f_bias.0 \t torch.Size([256])\n",
      "conv_layers.0.f_bias.1 \t torch.Size([256])\n",
      "conv_layers.0.offset.0 \t torch.Size([256])\n",
      "conv_layers.0.offset.1 \t torch.Size([256])\n",
      "conv_layers.0.scale.0 \t torch.Size([256])\n",
      "conv_layers.0.scale.1 \t torch.Size([256])\n",
      "conv_layers.1.f_lin.0.weight \t torch.Size([256, 512])\n",
      "conv_layers.1.f_lin.1.weight \t torch.Size([256, 512])\n",
      "conv_layers.1.params.0 \t torch.Size([256])\n",
      "conv_layers.1.params.1 \t torch.Size([256])\n",
      "conv_layers.1.params.2 \t torch.Size([256])\n",
      "conv_layers.1.params.3 \t torch.Size([256])\n",
      "conv_layers.1.params.4 \t torch.Size([256])\n",
      "conv_layers.1.params.5 \t torch.Size([256])\n",
      "conv_layers.1.f_bias.0 \t torch.Size([256])\n",
      "conv_layers.1.f_bias.1 \t torch.Size([256])\n",
      "conv_layers.1.offset.0 \t torch.Size([256])\n",
      "conv_layers.1.offset.1 \t torch.Size([256])\n",
      "conv_layers.1.scale.0 \t torch.Size([256])\n",
      "conv_layers.1.scale.1 \t torch.Size([256])\n",
      "conv_layers.2.f_lin.0.weight \t torch.Size([256, 512])\n",
      "conv_layers.2.params.0 \t torch.Size([256])\n",
      "conv_layers.2.params.1 \t torch.Size([256])\n",
      "conv_layers.2.params.2 \t torch.Size([256])\n",
      "conv_layers.2.f_bias.0 \t torch.Size([256])\n",
      "conv_layers.2.offset.0 \t torch.Size([256])\n",
      "conv_layers.2.scale.0 \t torch.Size([256])\n",
      "classifier.f_lin.0.weight \t torch.Size([7, 256])\n",
      "classifier.params.0 \t torch.Size([7])\n",
      "classifier.params.1 \t torch.Size([7])\n",
      "classifier.params.2 \t torch.Size([7])\n",
      "classifier.f_bias.0 \t torch.Size([7])\n",
      "classifier.offset.0 \t torch.Size([7])\n",
      "classifier.scale.0 \t torch.Size([7])\n",
      "during the evaluation step, report the input matrices size: \n",
      "adj_subgraph size :  torch.Size([89250, 89250]); \t feat_subg size : torch.Size([89250, 500])\n",
      "during 15 epoch of validation snapshot :\n",
      "\n",
      " After loading the exisiting model parameters, the model state_dict is listed as follows:: \n",
      "conv_layers.0.f_lin.0.weight \t torch.Size([256, 500])\n",
      "conv_layers.0.f_lin.1.weight \t torch.Size([256, 500])\n",
      "conv_layers.0.params.0 \t torch.Size([256])\n",
      "conv_layers.0.params.1 \t torch.Size([256])\n",
      "conv_layers.0.params.2 \t torch.Size([256])\n",
      "conv_layers.0.params.3 \t torch.Size([256])\n",
      "conv_layers.0.params.4 \t torch.Size([256])\n",
      "conv_layers.0.params.5 \t torch.Size([256])\n",
      "conv_layers.0.f_bias.0 \t torch.Size([256])\n",
      "conv_layers.0.f_bias.1 \t torch.Size([256])\n",
      "conv_layers.0.offset.0 \t torch.Size([256])\n",
      "conv_layers.0.offset.1 \t torch.Size([256])\n",
      "conv_layers.0.scale.0 \t torch.Size([256])\n",
      "conv_layers.0.scale.1 \t torch.Size([256])\n",
      "conv_layers.1.f_lin.0.weight \t torch.Size([256, 512])\n",
      "conv_layers.1.f_lin.1.weight \t torch.Size([256, 512])\n",
      "conv_layers.1.params.0 \t torch.Size([256])\n",
      "conv_layers.1.params.1 \t torch.Size([256])\n",
      "conv_layers.1.params.2 \t torch.Size([256])\n",
      "conv_layers.1.params.3 \t torch.Size([256])\n",
      "conv_layers.1.params.4 \t torch.Size([256])\n",
      "conv_layers.1.params.5 \t torch.Size([256])\n",
      "conv_layers.1.f_bias.0 \t torch.Size([256])\n",
      "conv_layers.1.f_bias.1 \t torch.Size([256])\n",
      "conv_layers.1.offset.0 \t torch.Size([256])\n",
      "conv_layers.1.offset.1 \t torch.Size([256])\n",
      "conv_layers.1.scale.0 \t torch.Size([256])\n",
      "conv_layers.1.scale.1 \t torch.Size([256])\n",
      "conv_layers.2.f_lin.0.weight \t torch.Size([256, 512])\n",
      "conv_layers.2.params.0 \t torch.Size([256])\n",
      "conv_layers.2.params.1 \t torch.Size([256])\n",
      "conv_layers.2.params.2 \t torch.Size([256])\n",
      "conv_layers.2.f_bias.0 \t torch.Size([256])\n",
      "conv_layers.2.offset.0 \t torch.Size([256])\n",
      "conv_layers.2.scale.0 \t torch.Size([256])\n",
      "classifier.f_lin.0.weight \t torch.Size([7, 256])\n",
      "classifier.params.0 \t torch.Size([7])\n",
      "classifier.params.1 \t torch.Size([7])\n",
      "classifier.params.2 \t torch.Size([7])\n",
      "classifier.f_bias.0 \t torch.Size([7])\n",
      "classifier.offset.0 \t torch.Size([7])\n",
      "classifier.scale.0 \t torch.Size([7])\n",
      "during the evaluation step, report the input matrices size: \n",
      "adj_subgraph size :  torch.Size([89250, 89250]); \t feat_subg size : torch.Size([89250, 500])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start validation investigate: \n",
      "during 5 epoch of validation snapshot :\n",
      "\n",
      " After loading the exisiting model parameters, the model state_dict is listed as follows:: \n",
      "conv_layers.0.f_lin.0.weight \t torch.Size([256, 500])\n",
      "conv_layers.0.f_lin.1.weight \t torch.Size([256, 500])\n",
      "conv_layers.0.params.0 \t torch.Size([256])\n",
      "conv_layers.0.params.1 \t torch.Size([256])\n",
      "conv_layers.0.params.2 \t torch.Size([256])\n",
      "conv_layers.0.params.3 \t torch.Size([256])\n",
      "conv_layers.0.params.4 \t torch.Size([256])\n",
      "conv_layers.0.params.5 \t torch.Size([256])\n",
      "conv_layers.0.f_bias.0 \t torch.Size([256])\n",
      "conv_layers.0.f_bias.1 \t torch.Size([256])\n",
      "conv_layers.0.offset.0 \t torch.Size([256])\n",
      "conv_layers.0.offset.1 \t torch.Size([256])\n",
      "conv_layers.0.scale.0 \t torch.Size([256])\n",
      "conv_layers.0.scale.1 \t torch.Size([256])\n",
      "conv_layers.1.f_lin.0.weight \t torch.Size([256, 512])\n",
      "conv_layers.1.f_lin.1.weight \t torch.Size([256, 512])\n",
      "conv_layers.1.params.0 \t torch.Size([256])\n",
      "conv_layers.1.params.1 \t torch.Size([256])\n",
      "conv_layers.1.params.2 \t torch.Size([256])\n",
      "conv_layers.1.params.3 \t torch.Size([256])\n",
      "conv_layers.1.params.4 \t torch.Size([256])\n",
      "conv_layers.1.params.5 \t torch.Size([256])\n",
      "conv_layers.1.f_bias.0 \t torch.Size([256])\n",
      "conv_layers.1.f_bias.1 \t torch.Size([256])\n",
      "conv_layers.1.offset.0 \t torch.Size([256])\n",
      "conv_layers.1.offset.1 \t torch.Size([256])\n",
      "conv_layers.1.scale.0 \t torch.Size([256])\n",
      "conv_layers.1.scale.1 \t torch.Size([256])\n",
      "conv_layers.2.f_lin.0.weight \t torch.Size([256, 512])\n",
      "conv_layers.2.params.0 \t torch.Size([256])\n",
      "conv_layers.2.params.1 \t torch.Size([256])\n",
      "conv_layers.2.params.2 \t torch.Size([256])\n",
      "conv_layers.2.f_bias.0 \t torch.Size([256])\n",
      "conv_layers.2.offset.0 \t torch.Size([256])\n",
      "conv_layers.2.scale.0 \t torch.Size([256])\n",
      "classifier.f_lin.0.weight \t torch.Size([7, 256])\n",
      "classifier.params.0 \t torch.Size([7])\n",
      "classifier.params.1 \t torch.Size([7])\n",
      "classifier.params.2 \t torch.Size([7])\n",
      "classifier.f_bias.0 \t torch.Size([7])\n",
      "classifier.offset.0 \t torch.Size([7])\n",
      "classifier.scale.0 \t torch.Size([7])\n",
      "during the evaluation step, report the input matrices size: \n",
      "adj_subgraph size :  torch.Size([89250, 89250]); \t feat_subg size : torch.Size([89250, 500])\n",
      "during 10 epoch of validation snapshot :\n",
      "\n",
      " After loading the exisiting model parameters, the model state_dict is listed as follows:: \n",
      "conv_layers.0.f_lin.0.weight \t torch.Size([256, 500])\n",
      "conv_layers.0.f_lin.1.weight \t torch.Size([256, 500])\n",
      "conv_layers.0.params.0 \t torch.Size([256])\n",
      "conv_layers.0.params.1 \t torch.Size([256])\n",
      "conv_layers.0.params.2 \t torch.Size([256])\n",
      "conv_layers.0.params.3 \t torch.Size([256])\n",
      "conv_layers.0.params.4 \t torch.Size([256])\n",
      "conv_layers.0.params.5 \t torch.Size([256])\n",
      "conv_layers.0.f_bias.0 \t torch.Size([256])\n",
      "conv_layers.0.f_bias.1 \t torch.Size([256])\n",
      "conv_layers.0.offset.0 \t torch.Size([256])\n",
      "conv_layers.0.offset.1 \t torch.Size([256])\n",
      "conv_layers.0.scale.0 \t torch.Size([256])\n",
      "conv_layers.0.scale.1 \t torch.Size([256])\n",
      "conv_layers.1.f_lin.0.weight \t torch.Size([256, 512])\n",
      "conv_layers.1.f_lin.1.weight \t torch.Size([256, 512])\n",
      "conv_layers.1.params.0 \t torch.Size([256])\n",
      "conv_layers.1.params.1 \t torch.Size([256])\n",
      "conv_layers.1.params.2 \t torch.Size([256])\n",
      "conv_layers.1.params.3 \t torch.Size([256])\n",
      "conv_layers.1.params.4 \t torch.Size([256])\n",
      "conv_layers.1.params.5 \t torch.Size([256])\n",
      "conv_layers.1.f_bias.0 \t torch.Size([256])\n",
      "conv_layers.1.f_bias.1 \t torch.Size([256])\n",
      "conv_layers.1.offset.0 \t torch.Size([256])\n",
      "conv_layers.1.offset.1 \t torch.Size([256])\n",
      "conv_layers.1.scale.0 \t torch.Size([256])\n",
      "conv_layers.1.scale.1 \t torch.Size([256])\n",
      "conv_layers.2.f_lin.0.weight \t torch.Size([256, 512])\n",
      "conv_layers.2.params.0 \t torch.Size([256])\n",
      "conv_layers.2.params.1 \t torch.Size([256])\n",
      "conv_layers.2.params.2 \t torch.Size([256])\n",
      "conv_layers.2.f_bias.0 \t torch.Size([256])\n",
      "conv_layers.2.offset.0 \t torch.Size([256])\n",
      "conv_layers.2.scale.0 \t torch.Size([256])\n",
      "classifier.f_lin.0.weight \t torch.Size([7, 256])\n",
      "classifier.params.0 \t torch.Size([7])\n",
      "classifier.params.1 \t torch.Size([7])\n",
      "classifier.params.2 \t torch.Size([7])\n",
      "classifier.f_bias.0 \t torch.Size([7])\n",
      "classifier.offset.0 \t torch.Size([7])\n",
      "classifier.scale.0 \t torch.Size([7])\n",
      "during the evaluation step, report the input matrices size: \n",
      "adj_subgraph size :  torch.Size([89250, 89250]); \t feat_subg size : torch.Size([89250, 500])\n",
      "during 15 epoch of validation snapshot :\n",
      "\n",
      " After loading the exisiting model parameters, the model state_dict is listed as follows:: \n",
      "conv_layers.0.f_lin.0.weight \t torch.Size([256, 500])\n",
      "conv_layers.0.f_lin.1.weight \t torch.Size([256, 500])\n",
      "conv_layers.0.params.0 \t torch.Size([256])\n",
      "conv_layers.0.params.1 \t torch.Size([256])\n",
      "conv_layers.0.params.2 \t torch.Size([256])\n",
      "conv_layers.0.params.3 \t torch.Size([256])\n",
      "conv_layers.0.params.4 \t torch.Size([256])\n",
      "conv_layers.0.params.5 \t torch.Size([256])\n",
      "conv_layers.0.f_bias.0 \t torch.Size([256])\n",
      "conv_layers.0.f_bias.1 \t torch.Size([256])\n",
      "conv_layers.0.offset.0 \t torch.Size([256])\n",
      "conv_layers.0.offset.1 \t torch.Size([256])\n",
      "conv_layers.0.scale.0 \t torch.Size([256])\n",
      "conv_layers.0.scale.1 \t torch.Size([256])\n",
      "conv_layers.1.f_lin.0.weight \t torch.Size([256, 512])\n",
      "conv_layers.1.f_lin.1.weight \t torch.Size([256, 512])\n",
      "conv_layers.1.params.0 \t torch.Size([256])\n",
      "conv_layers.1.params.1 \t torch.Size([256])\n",
      "conv_layers.1.params.2 \t torch.Size([256])\n",
      "conv_layers.1.params.3 \t torch.Size([256])\n",
      "conv_layers.1.params.4 \t torch.Size([256])\n",
      "conv_layers.1.params.5 \t torch.Size([256])\n",
      "conv_layers.1.f_bias.0 \t torch.Size([256])\n",
      "conv_layers.1.f_bias.1 \t torch.Size([256])\n",
      "conv_layers.1.offset.0 \t torch.Size([256])\n",
      "conv_layers.1.offset.1 \t torch.Size([256])\n",
      "conv_layers.1.scale.0 \t torch.Size([256])\n",
      "conv_layers.1.scale.1 \t torch.Size([256])\n",
      "conv_layers.2.f_lin.0.weight \t torch.Size([256, 512])\n",
      "conv_layers.2.params.0 \t torch.Size([256])\n",
      "conv_layers.2.params.1 \t torch.Size([256])\n",
      "conv_layers.2.params.2 \t torch.Size([256])\n",
      "conv_layers.2.f_bias.0 \t torch.Size([256])\n",
      "conv_layers.2.offset.0 \t torch.Size([256])\n",
      "conv_layers.2.scale.0 \t torch.Size([256])\n",
      "classifier.f_lin.0.weight \t torch.Size([7, 256])\n",
      "classifier.params.0 \t torch.Size([7])\n",
      "classifier.params.1 \t torch.Size([7])\n",
      "classifier.params.2 \t torch.Size([7])\n",
      "classifier.f_bias.0 \t torch.Size([7])\n",
      "classifier.offset.0 \t torch.Size([7])\n",
      "classifier.scale.0 \t torch.Size([7])\n",
      "during the evaluation step, report the input matrices size: \n",
      "adj_subgraph size :  torch.Size([89250, 89250]); \t feat_subg size : torch.Size([89250, 500])\n",
      "start validation investigate: \n",
      "during 5 epoch of validation snapshot :\n",
      "\n",
      " After loading the exisiting model parameters, the model state_dict is listed as follows:: \n",
      "conv_layers.0.f_lin.0.weight \t torch.Size([256, 500])\n",
      "conv_layers.0.f_lin.1.weight \t torch.Size([256, 500])\n",
      "conv_layers.0.params.0 \t torch.Size([256])\n",
      "conv_layers.0.params.1 \t torch.Size([256])\n",
      "conv_layers.0.params.2 \t torch.Size([256])\n",
      "conv_layers.0.params.3 \t torch.Size([256])\n",
      "conv_layers.0.params.4 \t torch.Size([256])\n",
      "conv_layers.0.params.5 \t torch.Size([256])\n",
      "conv_layers.0.f_bias.0 \t torch.Size([256])\n",
      "conv_layers.0.f_bias.1 \t torch.Size([256])\n",
      "conv_layers.0.offset.0 \t torch.Size([256])\n",
      "conv_layers.0.offset.1 \t torch.Size([256])\n",
      "conv_layers.0.scale.0 \t torch.Size([256])\n",
      "conv_layers.0.scale.1 \t torch.Size([256])\n",
      "conv_layers.1.f_lin.0.weight \t torch.Size([256, 512])\n",
      "conv_layers.1.f_lin.1.weight \t torch.Size([256, 512])\n",
      "conv_layers.1.params.0 \t torch.Size([256])\n",
      "conv_layers.1.params.1 \t torch.Size([256])\n",
      "conv_layers.1.params.2 \t torch.Size([256])\n",
      "conv_layers.1.params.3 \t torch.Size([256])\n",
      "conv_layers.1.params.4 \t torch.Size([256])\n",
      "conv_layers.1.params.5 \t torch.Size([256])\n",
      "conv_layers.1.f_bias.0 \t torch.Size([256])\n",
      "conv_layers.1.f_bias.1 \t torch.Size([256])\n",
      "conv_layers.1.offset.0 \t torch.Size([256])\n",
      "conv_layers.1.offset.1 \t torch.Size([256])\n",
      "conv_layers.1.scale.0 \t torch.Size([256])\n",
      "conv_layers.1.scale.1 \t torch.Size([256])\n",
      "conv_layers.2.f_lin.0.weight \t torch.Size([256, 512])\n",
      "conv_layers.2.params.0 \t torch.Size([256])\n",
      "conv_layers.2.params.1 \t torch.Size([256])\n",
      "conv_layers.2.params.2 \t torch.Size([256])\n",
      "conv_layers.2.f_bias.0 \t torch.Size([256])\n",
      "conv_layers.2.offset.0 \t torch.Size([256])\n",
      "conv_layers.2.scale.0 \t torch.Size([256])\n",
      "classifier.f_lin.0.weight \t torch.Size([7, 256])\n",
      "classifier.params.0 \t torch.Size([7])\n",
      "classifier.params.1 \t torch.Size([7])\n",
      "classifier.params.2 \t torch.Size([7])\n",
      "classifier.f_bias.0 \t torch.Size([7])\n",
      "classifier.offset.0 \t torch.Size([7])\n",
      "classifier.scale.0 \t torch.Size([7])\n",
      "during the evaluation step, report the input matrices size: \n",
      "adj_subgraph size :  torch.Size([89250, 89250]); \t feat_subg size : torch.Size([89250, 500])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "during 10 epoch of validation snapshot :\n",
      "\n",
      " After loading the exisiting model parameters, the model state_dict is listed as follows:: \n",
      "conv_layers.0.f_lin.0.weight \t torch.Size([256, 500])\n",
      "conv_layers.0.f_lin.1.weight \t torch.Size([256, 500])\n",
      "conv_layers.0.params.0 \t torch.Size([256])\n",
      "conv_layers.0.params.1 \t torch.Size([256])\n",
      "conv_layers.0.params.2 \t torch.Size([256])\n",
      "conv_layers.0.params.3 \t torch.Size([256])\n",
      "conv_layers.0.params.4 \t torch.Size([256])\n",
      "conv_layers.0.params.5 \t torch.Size([256])\n",
      "conv_layers.0.f_bias.0 \t torch.Size([256])\n",
      "conv_layers.0.f_bias.1 \t torch.Size([256])\n",
      "conv_layers.0.offset.0 \t torch.Size([256])\n",
      "conv_layers.0.offset.1 \t torch.Size([256])\n",
      "conv_layers.0.scale.0 \t torch.Size([256])\n",
      "conv_layers.0.scale.1 \t torch.Size([256])\n",
      "conv_layers.1.f_lin.0.weight \t torch.Size([256, 512])\n",
      "conv_layers.1.f_lin.1.weight \t torch.Size([256, 512])\n",
      "conv_layers.1.params.0 \t torch.Size([256])\n",
      "conv_layers.1.params.1 \t torch.Size([256])\n",
      "conv_layers.1.params.2 \t torch.Size([256])\n",
      "conv_layers.1.params.3 \t torch.Size([256])\n",
      "conv_layers.1.params.4 \t torch.Size([256])\n",
      "conv_layers.1.params.5 \t torch.Size([256])\n",
      "conv_layers.1.f_bias.0 \t torch.Size([256])\n",
      "conv_layers.1.f_bias.1 \t torch.Size([256])\n",
      "conv_layers.1.offset.0 \t torch.Size([256])\n",
      "conv_layers.1.offset.1 \t torch.Size([256])\n",
      "conv_layers.1.scale.0 \t torch.Size([256])\n",
      "conv_layers.1.scale.1 \t torch.Size([256])\n",
      "conv_layers.2.f_lin.0.weight \t torch.Size([256, 512])\n",
      "conv_layers.2.params.0 \t torch.Size([256])\n",
      "conv_layers.2.params.1 \t torch.Size([256])\n",
      "conv_layers.2.params.2 \t torch.Size([256])\n",
      "conv_layers.2.f_bias.0 \t torch.Size([256])\n",
      "conv_layers.2.offset.0 \t torch.Size([256])\n",
      "conv_layers.2.scale.0 \t torch.Size([256])\n",
      "classifier.f_lin.0.weight \t torch.Size([7, 256])\n",
      "classifier.params.0 \t torch.Size([7])\n",
      "classifier.params.1 \t torch.Size([7])\n",
      "classifier.params.2 \t torch.Size([7])\n",
      "classifier.f_bias.0 \t torch.Size([7])\n",
      "classifier.offset.0 \t torch.Size([7])\n",
      "classifier.scale.0 \t torch.Size([7])\n",
      "during the evaluation step, report the input matrices size: \n",
      "adj_subgraph size :  torch.Size([89250, 89250]); \t feat_subg size : torch.Size([89250, 500])\n",
      "during 15 epoch of validation snapshot :\n",
      "\n",
      " After loading the exisiting model parameters, the model state_dict is listed as follows:: \n",
      "conv_layers.0.f_lin.0.weight \t torch.Size([256, 500])\n",
      "conv_layers.0.f_lin.1.weight \t torch.Size([256, 500])\n",
      "conv_layers.0.params.0 \t torch.Size([256])\n",
      "conv_layers.0.params.1 \t torch.Size([256])\n",
      "conv_layers.0.params.2 \t torch.Size([256])\n",
      "conv_layers.0.params.3 \t torch.Size([256])\n",
      "conv_layers.0.params.4 \t torch.Size([256])\n",
      "conv_layers.0.params.5 \t torch.Size([256])\n",
      "conv_layers.0.f_bias.0 \t torch.Size([256])\n",
      "conv_layers.0.f_bias.1 \t torch.Size([256])\n",
      "conv_layers.0.offset.0 \t torch.Size([256])\n",
      "conv_layers.0.offset.1 \t torch.Size([256])\n",
      "conv_layers.0.scale.0 \t torch.Size([256])\n",
      "conv_layers.0.scale.1 \t torch.Size([256])\n",
      "conv_layers.1.f_lin.0.weight \t torch.Size([256, 512])\n",
      "conv_layers.1.f_lin.1.weight \t torch.Size([256, 512])\n",
      "conv_layers.1.params.0 \t torch.Size([256])\n",
      "conv_layers.1.params.1 \t torch.Size([256])\n",
      "conv_layers.1.params.2 \t torch.Size([256])\n",
      "conv_layers.1.params.3 \t torch.Size([256])\n",
      "conv_layers.1.params.4 \t torch.Size([256])\n",
      "conv_layers.1.params.5 \t torch.Size([256])\n",
      "conv_layers.1.f_bias.0 \t torch.Size([256])\n",
      "conv_layers.1.f_bias.1 \t torch.Size([256])\n",
      "conv_layers.1.offset.0 \t torch.Size([256])\n",
      "conv_layers.1.offset.1 \t torch.Size([256])\n",
      "conv_layers.1.scale.0 \t torch.Size([256])\n",
      "conv_layers.1.scale.1 \t torch.Size([256])\n",
      "conv_layers.2.f_lin.0.weight \t torch.Size([256, 512])\n",
      "conv_layers.2.params.0 \t torch.Size([256])\n",
      "conv_layers.2.params.1 \t torch.Size([256])\n",
      "conv_layers.2.params.2 \t torch.Size([256])\n",
      "conv_layers.2.f_bias.0 \t torch.Size([256])\n",
      "conv_layers.2.offset.0 \t torch.Size([256])\n",
      "conv_layers.2.scale.0 \t torch.Size([256])\n",
      "classifier.f_lin.0.weight \t torch.Size([7, 256])\n",
      "classifier.params.0 \t torch.Size([7])\n",
      "classifier.params.1 \t torch.Size([7])\n",
      "classifier.params.2 \t torch.Size([7])\n",
      "classifier.f_bias.0 \t torch.Size([7])\n",
      "classifier.offset.0 \t torch.Size([7])\n",
      "classifier.scale.0 \t torch.Size([7])\n",
      "during the evaluation step, report the input matrices size: \n",
      "adj_subgraph size :  torch.Size([89250, 89250]); \t feat_subg size : torch.Size([89250, 500])\n"
     ]
    }
   ],
   "source": [
    "# ================ Step3*** investigate validation:\n",
    "evaluation_input_file_name = prepare_data_folder + 'model_eval_input'\n",
    "with open(evaluation_input_file_name, \"rb\") as fp:\n",
    "    minibatch_eval, model_eval = dill.load(fp)\n",
    "\n",
    "for tune_val_label, tune_val in zip(tune_val_label_list, tune_val_list):\n",
    "    for trainer_id in trainer_list:\n",
    "        execute_validation_investigate(img_path, working_dir, minibatch_eval, model_eval, model_epoch_list, \n",
    "                                tune_param_name, tune_val_label, tune_val, trainer_id = trainer_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ================= Step4*** investigate test:\n",
    "evaluation_input_file_name = prepare_data_folder + 'model_eval_input'\n",
    "with open(evaluation_input_file_name, \"rb\") as fp:\n",
    "    minibatch_eval, model_eval = dill.load(fp)\n",
    "\n",
    "for tune_val_label, tune_val in zip(tune_val_label_list, tune_val_list):\n",
    "    for trainer_id in trainer_list:\n",
    "        execute_test_tuning(img_path, working_dir, minibatch_eval, model_eval, model_epoch_list, \n",
    "                                tune_param_name, tune_val_label, tune_val, trainer_id = trainer_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tune_val_label, tune_val in zip(tune_val_label_list, tune_val_list):\n",
    "    for trainer_id in trainer_list:\n",
    "        step51_run_investigation_summarize_whole(data_name, img_path,\n",
    "                                         tune_param_name, tune_val_label, tune_val,\n",
    "                                            trainer_list, model_epoch_list)\n",
    "    \n",
    "for tune_val_label, tune_val in zip(tune_val_label_list, tune_val_list):\n",
    "    for trainer_id in trainer_list:\n",
    "        step50_run_tune_summarize_whole(data_name, img_path, \n",
    "                                    tune_param_name, tune_val_label_list, tune_val_list,\n",
    "                                    trainer_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_1_4_geometric]",
   "language": "python",
   "name": "conda-env-pytorch_1_4_geometric-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
