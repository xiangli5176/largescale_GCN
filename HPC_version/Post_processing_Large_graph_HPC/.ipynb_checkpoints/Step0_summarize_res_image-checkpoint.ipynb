{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-batch model for hpc run with input edge weights in csv file\n",
    "\n",
    "Comments:\n",
    "\n",
    "By using the read weighted edge list from a csv file, it saves much space on self.graph\n",
    "\n",
    "This will for specific batch number and hop-layer number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import csv\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import shutil\n",
    "\n",
    "from utils import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test and compare different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_data_multi_tests(f1_data, data_name, graph_model, img_path, comments):\n",
    "    run_id = sorted(f1_data.keys())\n",
    "    run_data = {'run_id': run_id}\n",
    "    \n",
    "    run_data.update({model_name : [f1_data[key][idx] for key in run_id] for idx, model_name in enumerate(graph_model)})\n",
    "    \n",
    "    pickle_filename = img_path + data_name + '_' + comments + '.pkl'\n",
    "    os.makedirs(os.path.dirname(pickle_filename), exist_ok=True)\n",
    "    df = pd.DataFrame(data=run_data, dtype=np.int32)\n",
    "    df.to_pickle(pickle_filename)\n",
    "    return pickle_filename\n",
    "\n",
    "def draw_data_multi_tests(pickle_filename, data_name, comments, xlabel, ylabel):\n",
    "    df = pd.read_pickle(pickle_filename)\n",
    "    df_reshape = df.melt('run_id', var_name = 'model', value_name = ylabel)\n",
    "\n",
    "    plt.clf()\n",
    "    plt.figure()\n",
    "    sns.set(style='whitegrid')\n",
    "    g = sns.catplot(x=\"model\", y=ylabel, kind='box', data=df_reshape)\n",
    "    g.despine(left=True)\n",
    "    g.fig.suptitle(data_name + ' ' + ylabel + ' ' + comments)\n",
    "    g.set_xlabels(xlabel)\n",
    "    g.set_ylabels(ylabel)\n",
    "\n",
    "    img_name = pickle_filename[:-4] + '_img'\n",
    "    os.makedirs(os.path.dirname(img_name), exist_ok=True)\n",
    "    plt.savefig(img_name, bbox_inches='tight')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_tuning_res(image_path, mini_batch_folder, tune_param_name, tune_val_list, trainer_list):\n",
    "    validation_accuracy = {}\n",
    "    validation_f1 = {}\n",
    "    time_total_train = {}\n",
    "    time_data_load = {}\n",
    "    \n",
    "    res = []\n",
    "    for trainer_id in trainer_list:\n",
    "        ref = {}\n",
    "        for tune_val in tune_val_list:\n",
    "            test_res_folder = image_path + 'test_res/tune_' + tune_param_name + '_' + str(tune_val) + '/'\n",
    "            test_res_file = test_res_folder + 'res_trainer_' + str(trainer_id)\n",
    "            with open(test_res_file, \"rb\") as fp:\n",
    "                ref[tune_val] = pickle.load(fp)\n",
    "        res.append(ref)\n",
    "    \n",
    "    for i, ref in enumerate(res):\n",
    "        validation_accuracy[i] = {tune_val : res_lst[0] for tune_val, res_lst in ref.items()}\n",
    "        validation_f1[i] = {tune_val : res_lst[1] for tune_val, res_lst in ref.items()}\n",
    "        time_total_train[i] = {tune_val : res_lst[2] for tune_val, res_lst in ref.items()}\n",
    "        time_data_load[i] = {tune_val : res_lst[3] for tune_val, res_lst in ref.items()}\n",
    "        \n",
    "    return validation_accuracy, validation_f1, time_total_train, time_data_load\n",
    "\n",
    "def store_data_multi_tuning(tune_params, target, data_name, img_path, comments):\n",
    "    \"\"\"\n",
    "        tune_params: is the tuning parameter list\n",
    "        target: is the result, here should be F1-score, accuraycy, load time, train time\n",
    "    \"\"\"\n",
    "    run_ids = sorted(target.keys())   # key is the run_id\n",
    "    run_data = {'run_id': run_ids}\n",
    "    # the key can be converted to string or not: i.e. str(tune_val)\n",
    "    # here we keep it as integer such that we want it to follow order\n",
    "    tmp = {tune_val : [target[run_id][tune_val] for run_id in run_ids] for tune_val in tune_params}  # the value is list\n",
    "    run_data.update(tmp)\n",
    "    \n",
    "    pickle_filename = img_path + data_name + '_' + comments + '.pkl'\n",
    "    os.makedirs(os.path.dirname(pickle_filename), exist_ok=True)\n",
    "    df = pd.DataFrame(data=run_data, dtype=np.int32)\n",
    "    df.to_pickle(pickle_filename)\n",
    "    return pickle_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seperate steps for parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step50_run_tune_summarize_whole(data_name, image_data_path, intermediate_data_path, tune_param_name, tune_val_list, \\\n",
    "                                    train_batch_num, hop_layer_num, net_layer_num, trainer_list): \n",
    "    \n",
    "        print('Start running training for partition num: ' + str(train_batch_num) + ' hop layer ' + str(hop_layer_num))\n",
    "        # set the batch for validation and train\n",
    "        img_path = image_data_path + 'cluster_num_' + str(train_batch_num) + '/' + 'net_layer_num_' + str(net_layer_num) + '_hop_layer_num_' + str(hop_layer_num) + '/'\n",
    "        img_path += 'tuning_parameters/'  # further subfolder for different task\n",
    "        \n",
    "        # start to summarize the results into images for output\n",
    "\n",
    "        validation_accuracy, validation_f1, time_total_train, time_data_load = summarize_tuning_res(img_path, intermediate_data_path, tune_param_name, tune_val_list, trainer_list)\n",
    "\n",
    "        validation_accuracy = store_data_multi_tuning(tune_val_list, validation_accuracy, data_name, img_path, 'accuracy_cluster_num_' + str(train_batch_num) + '_hops_' + str(hop_layer_num))\n",
    "        draw_data_multi_tests(validation_accuracy, data_name, 'vali_cluster_num_' + str(train_batch_num) + '_hop_' + str(hop_layer_num), 'epochs_per_batch', 'Accuracy')\n",
    "\n",
    "        validation_f1 = store_data_multi_tuning(tune_val_list, validation_f1, data_name, img_path, 'validation_cluster_num_' + str(train_batch_num) + '_hops_' + str(hop_layer_num))\n",
    "        draw_data_multi_tests(validation_f1, data_name, 'vali_cluster_num_' + str(train_batch_num) + '_hop_' + str(hop_layer_num), 'epochs_per_batch', 'F1 score')\n",
    "\n",
    "        time_train = store_data_multi_tuning(tune_val_list, time_total_train, data_name, img_path, 'train_time_cluster_num_' + str(train_batch_num) + '_hops_' + str(hop_layer_num))\n",
    "        draw_data_multi_tests(time_train, data_name, 'train_time_cluster_num_' + str(train_batch_num) + '_hop_' + str(hop_layer_num), 'epochs_per_batch', 'Train Time (ms)')\n",
    "\n",
    "        time_load = store_data_multi_tuning(tune_val_list, time_data_load, data_name, img_path, 'load_time_cluster_num_' + str(train_batch_num) + '_hops_' + str(hop_layer_num))\n",
    "        draw_data_multi_tests(time_load, data_name, 'load_time_cluster_num_' + str(train_batch_num) + '_hop_' + str(hop_layer_num), 'epochs_per_batch', 'Load Time (ms)')\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isolate_step50_run_tune_summarize_whole(data_name, image_data_path, intermediate_data_path, tune_param_name, tune_val_list, \\\n",
    "                                    train_batch_num, hop_layer_num, net_layer_num, trainer_list): \n",
    "    \n",
    "        print('Start running training for partition num: ' + str(train_batch_num) + ' hop layer ' + str(hop_layer_num))\n",
    "        # set the batch for validation and train\n",
    "        img_path = image_data_path + 'cluster_num_' + str(train_batch_num) + '/' + 'net_layer_num_' + str(net_layer_num) + '/'\n",
    "        img_path += 'tuning_parameters/'  # further subfolder for different task\n",
    "        \n",
    "        # start to summarize the results into images for output\n",
    "\n",
    "        validation_accuracy, validation_f1, time_total_train, time_data_load = summarize_tuning_res(img_path, intermediate_data_path, tune_param_name, tune_val_list, trainer_list)\n",
    "\n",
    "        validation_accuracy = store_data_multi_tuning(tune_val_list, validation_accuracy, data_name, img_path, 'accuracy_cluster_num_' + str(train_batch_num) + '_hops_' + str(hop_layer_num))\n",
    "        draw_data_multi_tests(validation_accuracy, data_name, 'vali_cluster_num_' + str(train_batch_num) + '_hop_' + str(hop_layer_num), 'epochs_per_batch', 'Accuracy')\n",
    "\n",
    "        validation_f1 = store_data_multi_tuning(tune_val_list, validation_f1, data_name, img_path, 'validation_cluster_num_' + str(train_batch_num) + '_hops_' + str(hop_layer_num))\n",
    "        draw_data_multi_tests(validation_f1, data_name, 'vali_cluster_num_' + str(train_batch_num) + '_hop_' + str(hop_layer_num), 'epochs_per_batch', 'F1 score')\n",
    "\n",
    "        time_train = store_data_multi_tuning(tune_val_list, time_total_train, data_name, img_path, 'train_time_cluster_num_' + str(train_batch_num) + '_hops_' + str(hop_layer_num))\n",
    "        draw_data_multi_tests(time_train, data_name, 'train_time_cluster_num_' + str(train_batch_num) + '_hop_' + str(hop_layer_num), 'epochs_per_batch', 'Train Time (ms)')\n",
    "\n",
    "        time_load = store_data_multi_tuning(tune_val_list, time_data_load, data_name, img_path, 'load_time_cluster_num_' + str(train_batch_num) + '_hops_' + str(hop_layer_num))\n",
    "        draw_data_multi_tests(time_load, data_name, 'load_time_cluster_num_' + str(train_batch_num) + '_hop_' + str(hop_layer_num), 'epochs_per_batch', 'Load Time (ms)')\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use data from pytorch geometric datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = 'Reddit'\n",
    "test_folder_name = 'train_10%_full_neigh/'\n",
    "image_data_path = './results/' + data_name + '/' + test_folder_name\n",
    "intermediate_data_folder = './'\n",
    "train_batch_num = 64 * 2\n",
    "GCN_layer = [32]\n",
    "net_layer_num = len(GCN_layer) + 1\n",
    "# for non-optimization: hop_layer_num == net_layer_num\n",
    "hop_layer_num = net_layer_num - 1\n",
    "# to tune the parameters:\n",
    "tune_param_name = 'batch_epoch_num'\n",
    "# tune_val_list = [5]\n",
    "tune_val_list = [400, 200, 100, 50, 20, 10, 5, 1]\n",
    "trainer_list = list(range(7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start running training for partition num: 64 hop layer 1\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './results/Reddit/train_10%_full_neigh/cluster_num_64/net_layer_num_2_hop_layer_num_1/tuning_parameters/test_res/tune_batch_epoch_num_400/res_trainer_0'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-3fb11babac21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m step50_run_tune_summarize_whole(data_name, image_data_path, intermediate_data_folder, tune_param_name, tune_val_list, \\\n\u001b[0;32m----> 2\u001b[0;31m                                 train_batch_num, hop_layer_num, net_layer_num, trainer_list)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-9e54021e141c>\u001b[0m in \u001b[0;36mstep50_run_tune_summarize_whole\u001b[0;34m(data_name, image_data_path, intermediate_data_path, tune_param_name, tune_val_list, train_batch_num, hop_layer_num, net_layer_num, trainer_list)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m# start to summarize the results into images for output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mvalidation_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_f1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_total_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_data_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummarize_tuning_res\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintermediate_data_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtune_param_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtune_val_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mvalidation_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstore_data_multi_tuning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtune_val_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'accuracy_cluster_num_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_batch_num\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_hops_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhop_layer_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-9a92553347ac>\u001b[0m in \u001b[0;36msummarize_tuning_res\u001b[0;34m(image_path, mini_batch_folder, tune_param_name, tune_val_list, trainer_list)\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mtest_res_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'test_res/tune_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtune_param_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtune_val\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mtest_res_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_res_folder\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'res_trainer_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_res_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m                 \u001b[0mref\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtune_val\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './results/Reddit/train_10%_full_neigh/cluster_num_64/net_layer_num_2_hop_layer_num_1/tuning_parameters/test_res/tune_batch_epoch_num_400/res_trainer_0'"
     ]
    }
   ],
   "source": [
    "isolate_step50_run_tune_summarize_whole(data_name, image_data_path, intermediate_data_folder, tune_param_name, tune_val_list, \\\n",
    "                                train_batch_num, hop_layer_num, net_layer_num, trainer_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free GPU memory\n",
    "# !(nvidia-smi | grep 'python' | awk '{ print $3 }' | xargs -n1 kill -9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_geometric]",
   "language": "python",
   "name": "conda-env-pytorch_geometric-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
