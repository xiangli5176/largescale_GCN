{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrate model inter-cluster with three clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "from utils import *\n",
    "from Custom_GCNConv import Net\n",
    "from Cluster_Machine import ClusteringMachine\n",
    "from Cluster_Trainer import ClusterGCNTrainer_mini_Train, wholeClusterGCNTrainer_sequence\n",
    "\n",
    "# import data\n",
    "from torch_geometric.datasets import CoraFull"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Trivial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Trivial data'''\n",
    "edge_index = torch.tensor([[0, 1, 1, 3, 1, 2, 4, 2, 4, 6, 6, 7, 7, 9, 2, 5, 9, 8], \n",
    "                           [1, 0, 3, 1, 2, 1, 2, 4, 6, 4, 7, 6, 9, 7, 5, 2, 8, 9]])\n",
    "# features = torch.rand(10, 3)\n",
    "features = torch.tensor([[0, 0], [0, 1], [0, 2], [0, 3], [0, 4],  \n",
    "                           [0, 5], [0, 6], [0, 7], [0, 8], [0, 9]], dtype = torch.float)\n",
    "# label = torch.tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "\n",
    "label = torch.tensor([0, 1, 1, 0, 1, 1, 1, 0, 0, 0])\n",
    "print(features, features.shape)\n",
    "\n",
    "check_clustering_machine = ClusteringMachine(edge_index, features, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mini batch train nodes and valid throughout whole graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=orange>\n",
    "Note: the all_overlap and train_overlap are the same effects in the train process\n",
    "    \n",
    "These two differ in the validation part. Train_overlap will lose some overalpping in the validation nodes which affect F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mini-batch of the all_overlap\n",
    "clustering_machine = copy.deepcopy(check_clustering_machine)\n",
    "clustering_machine.split_whole_nodes_edges_then_cluster(0.4, 0.4, valid_part_num = 2, train_part_num = 2, test_part_num = 2)\n",
    "clustering_machine.mini_batch_train_clustering(1) # include number of layers\n",
    "# check_clustering(clustering_machine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn_trainer_batch = ClusterGCNTrainer_mini_Train(clustering_machine, 2, 2, input_layers = [16], dropout=0.3)\n",
    "gcn_trainer_batch.train(1, 0.0001, 0.1)\n",
    "gcn_trainer_batch.validate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### minibatch train nodes and batch validatioin "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_machine = copy.deepcopy(check_clustering_machine)\n",
    "clustering_machine.split_whole_nodes_edges_then_cluster(0.4, 0.4, valid_part_num = 2, train_part_num = 2, test_part_num = 2)\n",
    "clustering_machine.mini_batch_train_clustering(1) # include number of layers\n",
    "gcn_trainer_batch = ClusterGCNTrainer_mini_Train(clustering_machine, 2, 2, input_layers = [16], dropout=0.3)\n",
    "gcn_trainer_batch.train(1, 0.0001, 0.1)\n",
    "gcn_trainer_batch.batch_validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Default whole graph (recombine train nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default brute force case: recombination whole graph\n",
    "clustering_machine = copy.deepcopy(check_clustering_machine)\n",
    "clustering_machine.split_whole_nodes_edges_then_cluster(0.4, 0.4, valid_part_num = 2, train_part_num = 2, test_part_num = 2)\n",
    "clustering_machine.mini_batch_train_clustering(0)      \n",
    "gcn_trainer_whole = wholeClusterGCNTrainer_sequence(clustering_machine, 2, 2, input_layers = [16], dropout=0.3)\n",
    "gcn_trainer_whole.train(1, 0.0001, 0.1)\n",
    "gcn_trainer_whole.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#### mini-batch train nodes only in the isolated cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mini-batch of the isolate\n",
    "clustering_machine = copy.deepcopy(check_clustering_machine)\n",
    "clustering_machine.split_cluster_nodes_edges(0.2, 0.4, partition_num = 2)\n",
    "clustering_machine.general_isolate_clustering(2) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn_trainer_isolate = ClusterGCNTrainer_mini_Train(clustering_machine, 2, 2, input_layers = [16])\n",
    "gcn_trainer_isolate.train(1,  0.0001, 0.1)\n",
    "gcn_trainer_isolate.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use library data to check the results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions to be used and adjusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_clustering_machine(data, test_ratio = 0.05, validation_ratio = 0.85, valid_part_num = 8, train_part_num = 8, test_part_num = 8):\n",
    "    connect_edge_index, connect_features, connect_label = filter_out_isolate(data.edge_index, data.x, data.y)\n",
    "    clustering_machine = ClusteringMachine(connect_edge_index, connect_features, connect_label)\n",
    "#     clustering_machine.split_cluster_nodes_edges(test_ratio, validation_ratio, partition_num = train_part_num)\n",
    "    # mini-batch only: split to train test valid before clustering\n",
    "    clustering_machine.split_whole_nodes_edges_then_cluster(test_ratio, validation_ratio, valid_part_num = valid_part_num, train_part_num = train_part_num, test_part_num = test_part_num)\n",
    "    return clustering_machine\n",
    "\n",
    "def execute_one(clustering_machine, image_path, repeate_time = 5, input_layer = [32, 16], epoch_num = 300, layer_num = 1, \\\n",
    "                dropout = 0.3, lr = 0.0001, weight_decay = 0.01, mini_epoch_num = 5):\n",
    "    \"\"\"\n",
    "        return all test-F1 and validation-F1 for all four models\n",
    "    \"\"\"\n",
    "#     test_f1 = {}\n",
    "    validation_accuracy = {}\n",
    "    validation_f1 = {}\n",
    "    time_total_train = {}\n",
    "    time_data_load = {}\n",
    "    \n",
    "    # For large dataset we have to do the batch validate\n",
    "    graph_model = ['batch_valid']\n",
    "    # Each graph model corresponds to one function below\n",
    "#     graph_model = ['batch_valid', 'train_batch', 'whole_graph', 'isolate']\n",
    "    for i in range(repeate_time):\n",
    "        model_res = []\n",
    "        model_res.append(Cluster_train_valid_batch_run(clustering_machine, data_name, dataset, image_path, input_layer = input_layer, epochs=epoch_num, neigh_layer = layer_num, \\\n",
    "                                                         dropout = dropout, lr = lr, weight_decay = weight_decay, mini_epoch_num = mini_epoch_num)[:4])\n",
    "        \n",
    "        \n",
    "        validation_accuracy[i], validation_f1[i], time_total_train[i], time_data_load[i] = zip(*model_res)\n",
    "    return graph_model, validation_accuracy, validation_f1, time_total_train, time_data_load\n",
    "\n",
    "\n",
    "\"\"\"To test one single model for different parameter values, always use the batch validate\"\"\"\n",
    "def execute_tuning(tune_params, clustering_machine, image_path, repeate_time = 7, input_layer = [32, 32], epoch_num = 400, layer_num = 1, \\\n",
    "                  dropout = 0.1, lr = 0.0001, weight_decay = 0.1):\n",
    "    \"\"\"\n",
    "        Tune all the hyperparameters\n",
    "        1) learning rate\n",
    "        2) dropout\n",
    "        3) layer unit number\n",
    "        4) weight decay\n",
    "    \"\"\"\n",
    "    validation_accuracy = {}\n",
    "    validation_f1 = {}\n",
    "    time_total_train = {}\n",
    "    time_data_load = {}\n",
    "    \n",
    "    res = [{tune_val : Cluster_train_valid_batch_run(clustering_machine, data_name, dataset, image_path, \\\n",
    "            input_layer = input_layer, epochs=epoch_num, neigh_layer = layer_num, \\\n",
    "            dropout = dropout, lr = lr, weight_decay = weight_decay, mini_epoch_num = tune_val)[:4] for tune_val in tune_params} for i in range(repeate_time)]\n",
    "    \n",
    "    for i, ref in enumerate(res):\n",
    "        validation_accuracy[i] = {tune_val : res_lst[0] for tune_val, res_lst in ref.items()}\n",
    "        validation_f1[i] = {tune_val : res_lst[1] for tune_val, res_lst in ref.items()}\n",
    "        time_total_train[i] = {tune_val : res_lst[2] for tune_val, res_lst in ref.items()}\n",
    "        time_data_load[i] = {tune_val : res_lst[3] for tune_val, res_lst in ref.items()}\n",
    "        \n",
    "    return validation_accuracy, validation_f1, time_total_train, time_data_load\n",
    "\n",
    "\n",
    "def check_train_loss_converge(clustering_machine, data_name, dataset, image_path,  comments, input_layer = [32, 16], epoch_num = 300, layer_num = 1, dropout = 0.3, lr = 0.0001, weight_decay = 0.01, mini_epoch_num = 5):\n",
    "    # mini-batch, but valid also in batches\n",
    "    a3, v3, time3, load3, Cluster_train_valid_batch_trainer = Cluster_train_valid_batch_run(clustering_machine, data_name, dataset, image_path, input_layer = input_layer, epochs=epoch_num, neigh_layer = layer_num, \\\n",
    "                                                                               dropout = dropout, lr = lr, weight_decay = weight_decay, mini_epoch_num = mini_epoch_num)\n",
    "    draw_Cluster_train_valid_batch = draw_trainer_info(data_name, Cluster_train_valid_batch_trainer, image_path, 'train_valid_batch_' + comments)\n",
    "    draw_Cluster_train_valid_batch.draw_ave_loss_per_node()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use data from pytorch geometric datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-run of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_train_loss(data, data_name, dataset, image_data_path, partition_nums, layers, dropout = 0.1, lr = 0.0001, weight_decay = 0.1, mini_epoch_num = 1, valid_part_num = 64):\n",
    "    for partn in partition_nums:\n",
    "        for GCN_layer in layers:\n",
    "            net_layer = len(GCN_layer) + 1\n",
    "            hop_layer = net_layer\n",
    "            clustering_machine = set_clustering_machine(data, test_ratio = 0.05, validation_ratio = 0.85, valid_part_num = valid_part_num, train_part_num = partn, test_part_num = 8)\n",
    "            print('Start checking train loss for partition num: ' + str(partn) + ' hop layer: ' + str(hop_layer))\n",
    "            img_path = image_data_path + 'cluster_num_' + str(partn) + '/' + 'net_layer_' + str(net_layer) + '_hop_layer_' + str(hop_layer) + '/'\n",
    "            check_train_loss_converge(clustering_machine, data_name, dataset, img_path, 'part_num_' + str(partn), input_layer = GCN_layer, epoch_num = 400, layer_num = hop_layer, \\\n",
    "                                     dropout = dropout, lr = lr, weight_decay = weight_decay, mini_epoch_num = mini_epoch_num)\n",
    "            \n",
    "#             # for the large dataset and split first case, the cluster info cannot be generated\n",
    "#             clustering_machine.mini_batch_train_clustering(hop_layer)\n",
    "#             draw_cluster_info(clustering_machine, data_name, img_path, comments = '_cluster_node_distr_' + str(hop_layer) + '_hops')\n",
    "            \n",
    "def output_F1_score(data, data_name, dataset, image_data_path, partition_nums, layers, dropout = 0.1, lr = 0.0001, weight_decay = 0.1, mini_epoch_num = 20, valid_part_num = 64):            \n",
    "    for partn in partition_nums:\n",
    "        for GCN_layer in layers:\n",
    "            net_layer = len(GCN_layer) + 1\n",
    "            hop_layer = net_layer\n",
    "            # here for the more custom way of train and validation, validatoin nodes and train nodes may have different number of batches\n",
    "            clustering_machine = set_clustering_machine(data, test_ratio = 0.05, validation_ratio = 0.85, valid_part_num = valid_part_num, train_part_num = partn, test_part_num = 8)\n",
    "            print('Start running for partition num: ' + str(partn) + ' hop layer ' + str(hop_layer))\n",
    "            img_path = image_data_path + 'cluster_num_' + str(partn) + '/' + 'net_layer_' + str(net_layer) + '_hop_layer_' + str(hop_layer) + '/'\n",
    "\n",
    "            graph_model, validation_accuracy, validation_f1, time_total_train, time_data_load = execute_one(clustering_machine, img_path, repeate_time = 7, input_layer = GCN_layer, epoch_num = 400, layer_num = hop_layer, \\\n",
    "                                                                dropout = dropout, lr = lr, weight_decay = weight_decay, mini_epoch_num = mini_epoch_num)\n",
    "\n",
    "            validation_accuracy = store_data_multi_tests(validation_accuracy, data_name, graph_model, img_path, 'test_cluster_num_' + str(partn) + '_hops_' + str(hop_layer))\n",
    "            draw_data_multi_tests(validation_accuracy, data_name, 'vali_cluster_num_' + str(partn) + '_hop_' + str(hop_layer), 'models', 'Accuracy')\n",
    "\n",
    "            validation_f1 = store_data_multi_tests(validation_f1, data_name, graph_model, img_path, 'validation_cluster_num_' + str(partn) + '_hops_' + str(hop_layer))\n",
    "            draw_data_multi_tests(validation_f1, data_name, 'vali_cluster_num_' + str(partn) + '_hop_' + str(hop_layer), 'models', 'F1 score')\n",
    "\n",
    "            time_train = store_data_multi_tests(time_total_train, data_name, graph_model, img_path, 'train_time_cluster_num_' + str(partn) + '_hops_' + str(hop_layer))\n",
    "            draw_data_multi_tests(time_train, data_name, 'train_time_cluster_num_' + str(partn) + '_hop_' + str(hop_layer), 'models', 'Train Time (ms)')\n",
    "\n",
    "            time_load = store_data_multi_tests(time_data_load, data_name, graph_model, img_path, 'load_time_cluster_num_' + str(partn) + '_hops_' + str(hop_layer))\n",
    "            draw_data_multi_tests(time_load, data_name, 'load_time_cluster_num_' + str(partn) + '_hop_' + str(hop_layer), 'models', 'Load Time (ms)')\n",
    "            \n",
    "def output_tune_param(data, data_name, dataset, image_data_path, partition_nums, layers, dropout = 0.1, lr = 0.0001, weight_decay = 0.1, valid_part_num = 64):\n",
    "    for partn in partition_nums:\n",
    "        for GCN_layer in layers:\n",
    "            net_layer = len(GCN_layer) + 1\n",
    "            hop_layer = net_layer\n",
    "            clustering_machine = set_clustering_machine(data, test_ratio = 0.05, validation_ratio = 0.85, valid_part_num = valid_part_num, train_part_num = partn, test_part_num = 8)\n",
    "\n",
    "            # Set the tune parameters and name\n",
    "            tune_name = 'batch_epoch_num'\n",
    "            tune_params = [400, 200, 100, 50, 20, 10, 5, 1]\n",
    "\n",
    "            img_path = image_data_path + 'cluster_num_' + str(partn) + '/' + 'net_layer_' + str(net_layer) + '_hop_layer_' + str(hop_layer) + '/' + 'tune_' + tune_name + '/'\n",
    "            print('Start tuning for tuning param: ' + tune_name + ' partition num: ' + str(partn) + ' hop layer ' + str(hop_layer))\n",
    "\n",
    "\n",
    "            validation_accuracy, validation_f1, time_total_train, time_data_load = execute_tuning(tune_params, clustering_machine, img_path, repeate_time = 7, \\\n",
    "                                                                                    input_layer = GCN_layer, epoch_num = 400, layer_num = hop_layer, \\\n",
    "                                                                                    dropout = dropout, lr = lr, weight_decay = weight_decay)\n",
    "\n",
    "            validation_accuracy = store_data_multi_tuning(tune_params,validation_accuracy, data_name, img_path, 'accuracy_cluster_num_' + str(partn) + '_hops_' + str(hop_layer))\n",
    "            draw_data_multi_tests(validation_accuracy, data_name, 'vali_cluster_num_' + str(partn) + '_hop_' + str(hop_layer), 'epochs_per_batch', 'Accuracy')\n",
    "\n",
    "            validation_f1 = store_data_multi_tuning(tune_params, validation_f1, data_name, img_path, 'validation_cluster_num_' + str(partn) + '_hops_' + str(hop_layer))\n",
    "            draw_data_multi_tests(validation_f1, data_name, 'vali_cluster_num_' + str(partn) + '_hop_' + str(hop_layer), 'epochs_per_batch', 'F1 score')\n",
    "\n",
    "            time_train = store_data_multi_tuning(tune_params, time_total_train, data_name, img_path, 'train_time_cluster_num_' + str(partn) + '_hops_' + str(hop_layer))\n",
    "            draw_data_multi_tests(time_train, data_name, 'train_time_cluster_num_' + str(partn) + '_hop_' + str(hop_layer), 'epochs_per_batch', 'Train Time (ms)')\n",
    "\n",
    "            time_load = store_data_multi_tuning(tune_params, time_data_load, data_name, img_path, 'load_time_cluster_num_' + str(partn) + '_hops_' + str(hop_layer))\n",
    "            draw_data_multi_tests(time_load, data_name, 'load_time_cluster_num_' + str(partn) + '_hop_' + str(hop_layer), 'epochs_per_batch', 'Load Time (ms)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_data_root = '/media/xiangli/storage1/projects/tmpdata/'\n",
    "test_folder_name = 'hpc_large_scale_1/train_10%_full_neigh_random_split_tr_valid_first/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cora dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "data_name = 'Cora'\n",
    "dataset = Planetoid(root = local_data_root + 'Planetoid/Cora', name=data_name)\n",
    "data = dataset[0]\n",
    "image_data_path = './results/' + data_name + '/' + test_folder_name\n",
    "\n",
    "partition_nums = [4]\n",
    "layers = [[], [32]]\n",
    "\n",
    "# partition_nums = [2, 4, 8]\n",
    "# layers = [[], [32], [32, 32], [32, 32, 32]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check convergence\n",
    "output_train_loss(data, data_name, dataset, image_data_path, partition_nums, layers, dropout = 0.1, lr = 0.0001, weight_decay = 0.1, mini_epoch_num = 1, valid_part_num = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output accuracy, F1, time (train, load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check F1-score\n",
    "output_F1_score(data, data_name, dataset, image_data_path, partition_nums, layers, dropout = 0.1, lr = 0.0001, weight_decay = 0.1, mini_epoch_num = 20, valid_part_num = 4)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tuning epoch number for each train-batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuning the mini_epoch\n",
    "output_tune_param(data, data_name, dataset, image_data_path, partition_nums, layers, dropout = 0.1, lr = 0.0001, weight_decay = 0.1, valid_part_num = 4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CoraFull Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import CoraFull\n",
    "data_name = 'CoraFull'\n",
    "dataset = CoraFull(root = local_data_root + 'CoralFull')\n",
    "print('number of data: ', len(dataset))\n",
    "data = dataset[0]\n",
    "image_data_path = './results/' + data_name + '/' + test_folder_name\n",
    "\n",
    "partition_nums = [32]\n",
    "layers = [[32]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check convergence\n",
    "output_train_loss(data, data_name, dataset, image_data_path, partition_nums, layers, dropout = 0.1, lr = 0.0001, weight_decay = 0.1, mini_epoch_num = 1, valid_part_num = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check F1-score\n",
    "output_F1_score(data, data_name, dataset, image_data_path, partition_nums, layers, dropout = 0.1, lr = 0.0001, weight_decay = 0.1, mini_epoch_num = 20, valid_part_num = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuning the mini_epoch\n",
    "output_tune_param(data, data_name, dataset, image_data_path, partition_nums, layers, dropout = 0.1, lr = 0.0001, weight_decay = 0.1, valid_part_num = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free GPU memory\n",
    "!(nvidia-smi | grep 'python' | awk '{ print $3 }' | xargs -n1 kill -9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_geometric]",
   "language": "python",
   "name": "conda-env-pytorch_geometric-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
