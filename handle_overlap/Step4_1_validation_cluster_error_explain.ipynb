{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrate model inter-cluster with three clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explain:  Purpose of this test: Dig into the messagepassenger to find how to do validation for each cluster on validation node\n",
    "\n",
    "After dig into the MessagePassenger class, we see that the scatter_ function in propagate plays a vital role\n",
    "\n",
    "To simply test the validation nodes in each isolated cluster won't work:\n",
    "\n",
    "**Because the edge_weights may need to reference another node outside of the current cluster**\n",
    "\n",
    "To validate, just like train nodes, we need to create mini-batch for each bunch of validiation nodes also. Generate the mini-batch with K-hop for validation data\n",
    "\n",
    "This time we do not need handle validation data (85% of each cluster at the same time)\n",
    "\n",
    "Instead we can further divide it into even smaller portion, e.g., each time still select 10% of the cluster data which are validation data as the seed, then genrate the corresponding mini-batch to validate it for the F1-score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from utils import filter_out_isolate, draw_cluster_info, draw_isolate_cluster_info, draw_trainer_info, print_data_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import torch\n",
    "from torch_geometric.utils import scatter_\n",
    "\n",
    "special_args = [\n",
    "    'edge_index', 'edge_index_i', 'edge_index_j', 'size', 'size_i', 'size_j'\n",
    "]\n",
    "__size_error_msg__ = ('All tensors which should get mapped to the same source '\n",
    "                      'or target nodes must be of same size in dimension 0.')\n",
    "\n",
    "is_python2 = sys.version_info[0] < 3\n",
    "getargspec = inspect.getargspec if is_python2 else inspect.getfullargspec\n",
    "\n",
    "\n",
    "class MessagePassing(torch.nn.Module):\n",
    "    r\"\"\"Base class for creating message passing layers\n",
    "\n",
    "    .. math::\n",
    "        \\mathbf{x}_i^{\\prime} = \\gamma_{\\mathbf{\\Theta}} \\left( \\mathbf{x}_i,\n",
    "        \\square_{j \\in \\mathcal{N}(i)} \\, \\phi_{\\mathbf{\\Theta}}\n",
    "        \\left(\\mathbf{x}_i, \\mathbf{x}_j,\\mathbf{e}_{i,j}\\right) \\right),\n",
    "\n",
    "    where :math:`\\square` denotes a differentiable, permutation invariant\n",
    "    function, *e.g.*, sum, mean or max, and :math:`\\gamma_{\\mathbf{\\Theta}}`\n",
    "    and :math:`\\phi_{\\mathbf{\\Theta}}` denote differentiable functions such as\n",
    "    MLPs.\n",
    "    See `here <https://pytorch-geometric.readthedocs.io/en/latest/notes/\n",
    "    create_gnn.html>`__ for the accompanying tutorial.\n",
    "\n",
    "    Args:\n",
    "        aggr (string, optional): The aggregation scheme to use\n",
    "            (:obj:`\"add\"`, :obj:`\"mean\"` or :obj:`\"max\"`).\n",
    "            (default: :obj:`\"add\"`)\n",
    "        flow (string, optional): The flow direction of message passing\n",
    "            (:obj:`\"source_to_target\"` or :obj:`\"target_to_source\"`).\n",
    "            (default: :obj:`\"source_to_target\"`)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, aggr='add', flow='source_to_target'):\n",
    "        super(MessagePassing, self).__init__()\n",
    "\n",
    "        self.aggr = aggr\n",
    "        assert self.aggr in ['add', 'mean', 'max']\n",
    "\n",
    "        self.flow = flow\n",
    "        # give a warning if the option is not valid\n",
    "        assert self.flow in ['source_to_target', 'target_to_source']\n",
    "\n",
    "        self.__message_args__ = getargspec(self.message)[0][1:]\n",
    "        # we will have [x_j, norm ] put into self.__message_args__\n",
    "        \n",
    "        self.__special_args__ = [(i, arg)\n",
    "                                 for i, arg in enumerate(self.__message_args__)\n",
    "                                 if arg in special_args]\n",
    "        # nothing in the self.__special_args__\n",
    "        \n",
    "        self.__message_args__ = [arg for arg in self.__message_args__ if arg not in special_args]\n",
    "        \n",
    "        self.__update_args__ = getargspec(self.update)[0][2:]\n",
    "        # empty, since there is nothing beyond: agg_out\n",
    "\n",
    "#     function call: res = self.propagate(edge_index, x=x, norm=norm)\n",
    "    def propagate(self, edge_index, size=None, **kwargs):\n",
    "        r\"\"\"The initial call to start propagating messages.\n",
    "\n",
    "        Args:\n",
    "            edge_index (Tensor): The indices of a general (sparse) assignment\n",
    "                matrix with shape :obj:`[N, M]` (can be directed or\n",
    "                undirected).\n",
    "            size (list or tuple, optional): The size :obj:`[N, M]` of the\n",
    "                assignment matrix. If set to :obj:`None`, the size is tried to\n",
    "                get automatically inferred. (default: :obj:`None`)\n",
    "            **kwargs: Any additional data which is needed to construct messages\n",
    "                and to update node embeddings.\n",
    "        \"\"\"\n",
    "        dim = 0\n",
    "        size = [None, None] if size is None else list(size)\n",
    "        assert len(size) == 2\n",
    "\n",
    "        i, j = (0, 1) if self.flow == 'target_to_source' else (1, 0)\n",
    "        # here (i, j) == (1, 0)\n",
    "        ij = {\"_i\": i, \"_j\": j}\n",
    "\n",
    "        message_args = []\n",
    "        \n",
    "        \n",
    "        for arg in self.__message_args__:\n",
    "#             arg[-2] == '_j'\n",
    "            if arg[-2:] in ij.keys():\n",
    "#                 tmp == x, is inside the dwargs\n",
    "                # for now, kwargs contains: x, norm\n",
    "                tmp = kwargs.get(arg[:-2], None)   # get the value of the parameter\n",
    "#                 print('Initial tmp is: ', tmp.shape, '\\n', tmp)\n",
    "                if tmp is None:  # pragma: no cover\n",
    "                    message_args.append(tmp)\n",
    "                else:\n",
    "                    idx = ij[arg[-2:]]    # idx == 0\n",
    "                    if isinstance(tmp, tuple) or isinstance(tmp, list):\n",
    "                        assert len(tmp) == 2\n",
    "                        if tmp[1 - idx] is not None:\n",
    "                            if size[1 - idx] is None:\n",
    "                                size[1 - idx] = tmp[1 - idx].size(dim)\n",
    "                            if size[1 - idx] != tmp[1 - idx].size(dim):\n",
    "                                raise ValueError(__size_error_msg__)\n",
    "                        tmp = tmp[idx]\n",
    "                    \n",
    "                    if tmp is None:\n",
    "                        message_args.append(tmp)\n",
    "                    else:\n",
    "                        if size[idx] is None:\n",
    "                            size[idx] = tmp.size(dim)          # dim = 0, x.size(0), i.e. select the first dimension size\n",
    "                        if size[idx] != tmp.size(dim):\n",
    "                            raise ValueError(__size_error_msg__)\n",
    "                        # dim == 0, we duplicate part of the embeddings x by using the edge_index[idx]\n",
    "                        print('Inside the propagate, edge_index[idx]: \\n', edge_index[idx].shape, '\\n', edge_index[idx])\n",
    "                        tmp = torch.index_select(tmp, dim, edge_index[idx])\n",
    "                        print('Inside propogate, tmp : ', tmp.shape, '\\n', tmp)\n",
    "                        message_args.append(tmp)   # here we append duplicated x from the kwargs\n",
    "            else:\n",
    "                message_args.append(kwargs.get(arg, None))   # here we append norm\n",
    "        \n",
    "#         message_args are: x_j, norm \n",
    "#         size:  [8, None] \n",
    "#         kwargs:  dict_keys(['x', 'norm']) \n",
    "#         special keys:  []\n",
    "        \n",
    "        size[0] = size[1] if size[0] is None else size[0]\n",
    "        size[1] = size[0] if size[1] is None else size[1]\n",
    "\n",
    "        kwargs['edge_index'] = edge_index\n",
    "        kwargs['size'] = size\n",
    "        \n",
    "#         print('__special_args__ : ', self.__special_args__)\n",
    "        # for now self.__special_args__ is empty\n",
    "        for (idx, arg) in self.__special_args__:\n",
    "            if arg[-2:] in ij.keys():\n",
    "                # here we will change the content of x (features)\n",
    "                # features will be corresponds to edge_index\n",
    "                message_args.insert(idx, kwargs[arg[:-2]][ij[arg[-2:]]])\n",
    "            else:\n",
    "                message_args.insert(idx, kwargs[arg])\n",
    "\n",
    "#         print('__update_args__ : ', self.__update_args__)\n",
    "        update_args = [kwargs[arg] for arg in self.__update_args__]\n",
    "#         message_args are: x_j, norm \n",
    "        out = self.message(*message_args)\n",
    "        print('Inside propogate, before scattering, out : ', out.shape, '\\n', out)\n",
    "        # here i = 1, edge_index is the target endpoint of an edge, size[i] is the size of target endpoints\n",
    "        print('Inside propogate, before scattering, edge_index : ', edge_index.shape, '\\n', edge_index)\n",
    "        out = scatter_(self.aggr, out, edge_index[i], dim_size=size[i])\n",
    "        \n",
    "        out = self.update(out, *update_args)\n",
    "        return out\n",
    "\n",
    "    def message(self, x_j):  # pragma: no cover\n",
    "        r\"\"\"Constructs messages in analogy to :math:`\\phi_{\\mathbf{\\Theta}}`\n",
    "        for each edge in :math:`(i,j) \\in \\mathcal{E}`.\n",
    "        Can take any argument which was initially passed to :meth:`propagate`.\n",
    "        In addition, features can be lifted to the source node :math:`i` and\n",
    "        target node :math:`j` by appending :obj:`_i` or :obj:`_j` to the\n",
    "        variable name, *.e.g.* :obj:`x_i` and :obj:`x_j`.\"\"\"\n",
    "\n",
    "        return x_j\n",
    "\n",
    "    def update(self, aggr_out):  # pragma: no cover\n",
    "        r\"\"\"Updates node embeddings in analogy to\n",
    "        :math:`\\gamma_{\\mathbf{\\Theta}}` for each node\n",
    "        :math:`i \\in \\mathcal{V}`.\n",
    "        Takes in the output of aggregation as first argument and any argument\n",
    "        which was initially passed to :meth:`propagate`.\"\"\"\n",
    "\n",
    "        return aggr_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "from torch.nn import Parameter\n",
    "from torch_scatter import scatter_add\n",
    "# from torch_geometric.nn import MessagePassing\n",
    "import torch.nn.functional as F\n",
    "# from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import add_remaining_self_loops\n",
    "\n",
    "### ================== Definition of custom GCN\n",
    "\n",
    "def glorot(tensor):\n",
    "    if tensor is not None:\n",
    "#         stdv = math.sqrt(6.0 / (tensor.size(-2) + tensor.size(-1)))\n",
    "#         tensor.data.uniform_(-stdv, stdv)\n",
    "        tensor.data.fill_(1.0)   # trivial example\n",
    "        \n",
    "def zeros(tensor):\n",
    "    if tensor is not None:\n",
    "        tensor.data.fill_(0)\n",
    "\n",
    "class custom_GCNConv(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels, improved=False, cached=False,\n",
    "                 bias=True, **kwargs):\n",
    "        super().__init__(aggr='add', **kwargs)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.improved = improved\n",
    "        self.cached = cached\n",
    "        self.weight = Parameter(torch.Tensor(in_channels, out_channels))\n",
    "\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        glorot(self.weight)\n",
    "        zeros(self.bias)\n",
    "        self.cached_result = None\n",
    "        self.cached_num_edges = None\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def norm(edge_index, num_nodes, edge_weight=None, improved=False, dtype=None):\n",
    "        \n",
    "        if edge_weight is None:\n",
    "            edge_weight = torch.ones((edge_index.size(1), ), dtype=dtype, device=edge_index.device)\n",
    "        \n",
    "        fill_value = 1 if not improved else 2\n",
    "        \n",
    "        edge_index, edge_weight = add_remaining_self_loops(\n",
    "            edge_index, edge_weight, fill_value, num_nodes)\n",
    "        \n",
    "        row, col = edge_index   \n",
    "        # row includes the starting points of the edges  (first row of edge_index)\n",
    "        # col includes the ending points of the edges   (second row of edge_index)\n",
    "\n",
    "        deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n",
    "        # row records the source nodes, which is the index we are trying to add\n",
    "        # deg will record the out-degree of each node of x_i in all edges (x_i, x_j) including self_loops\n",
    "        \n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "        normalized_edge_weight = deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n",
    "        \n",
    "#         print('whole GCN training normalized_edge_weight: \\n', normalized_edge_weight)\n",
    "        return edge_index, normalized_edge_weight\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight = None):\n",
    "        print('New Layer ========================= Inside custom GCN: ')\n",
    "        print('current embeddings: ', x.shape, '\\n', x)\n",
    "        \n",
    "        x = torch.matmul(x, self.weight)   # update x (embeddings)\n",
    "#         print('embeddings before propagate: ', x.shape, '\\n', x)\n",
    "        \n",
    "#         print('inside custom_GCN, edge_index: ', edge_index.shape, '\\n', edge_index)\n",
    "#         print('inside custom_GCN, edge_weight: ', edge_weight.shape, '\\n', edge_weight)\n",
    "        \n",
    "        # for convenience, we use all the edge_weights with value ones\n",
    "        trial_weight = edge_weight.data.fill_(1.0)   \n",
    "        res = self.propagate(edge_index, x = x, norm = trial_weight)\n",
    "        print('resulting embeddings: ', res.shape, '\\n', res)\n",
    "        return res\n",
    "\n",
    "    # self is the first parameter of the message func\n",
    "    def message(self, x_j, norm):\n",
    "        # in source code of the MessagePassing:\n",
    "#         self.__message_args__ = getargspec(self.message)[0][1:]  : will be initialized as [x_j, norm]\n",
    "        \n",
    "        # view is to reshape the tensor, here make it only a single column\n",
    "        # use the normalized weights multiplied by the feature of the target nodes\n",
    "        '''\n",
    "        For each of extended edge_index:(x_i, x_j), assume there is N such edges\n",
    "        x_j of shape (N, k) , assume there is k features, value along each row are the same\n",
    "        norm of shape (1, m), assume there is m edges (including self loops), 1-D tensor\n",
    "        '''\n",
    "        print('inside the message custom_GCN: norm \\n', norm.shape, '\\n', norm)\n",
    "        print('inside the message custom_GCN: x_j \\n', x_j.shape, '\\n', x_j)\n",
    "        res = norm.view(-1, 1) * x_j  # use the element wise multiplication\n",
    "        return res\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        # update the embeddings of each node\n",
    "        if self.bias is not None:\n",
    "            aggr_out = aggr_out + self.bias\n",
    "        return aggr_out\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}({}, {})'.format(self.__class__.__name__, self.in_channels,\n",
    "                                   self.out_channels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ====================== Establish a GCN based model ========================\n",
    "class ListModule(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Abstract list layer class.\n",
    "    \"\"\"\n",
    "    def __init__(self, *args):\n",
    "        \"\"\"\n",
    "        Module initializing.\n",
    "        \"\"\"\n",
    "        super(ListModule, self).__init__()\n",
    "        idx = 0\n",
    "        for module in args:\n",
    "            self.add_module(str(idx), module)\n",
    "            idx += 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Getting the indexed layer.\n",
    "        \"\"\"\n",
    "        if idx < 0 or idx >= len(self._modules):\n",
    "            raise IndexError('index {} is out of range'.format(idx))\n",
    "        it = iter(self._modules.values())\n",
    "        for i in range(idx):\n",
    "            next(it)\n",
    "        return next(it)\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Iterating on the layers.\n",
    "        \"\"\"\n",
    "        return iter(self._modules.values())\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Number of layers.\n",
    "        \"\"\"\n",
    "        return len(self._modules)\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, input_layers = [16, 16], dropout=0.3):\n",
    "        \"\"\"\n",
    "        input layers: list of integers\n",
    "        dropout: probability of droping out \n",
    "        \"\"\"\n",
    "        super(Net, self).__init__()\n",
    "        # one trivial example\n",
    "#         self.conv1 = custom_GCNConv(in_channels, out_channels)\n",
    "#         self.conv2 = GCNConv(16, dataset.num_classes)\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.input_layers = input_layers\n",
    "        self.dropout = dropout\n",
    "        self.setup_layers()\n",
    "\n",
    "    def setup_layers(self):\n",
    "        \"\"\"\n",
    "        Creating the layes based on the args.\n",
    "        \"\"\"\n",
    "        self.layers = []\n",
    "        self.input_layers = [self.in_channels] + self.input_layers + [self.out_channels]\n",
    "        for i, _ in enumerate(self.input_layers[:-1]):\n",
    "            self.layers.append(custom_GCNConv(self.input_layers[i],self.input_layers[i+1]))\n",
    "        self.layers = ListModule(*self.layers)\n",
    "\n",
    "    # change the dropout positions: \n",
    "    def forward(self, edge_index, features, edge_weights = None):\n",
    "        if len(self.layers) > 1:\n",
    "            for i in range(len(self.layers)-1):\n",
    "                features = F.relu(self.layers[i](features, edge_index, edge_weights))\n",
    "#                 if i>0:\n",
    "                features = F.dropout(features, p = self.dropout, training = self.training)\n",
    "                    \n",
    "            features = self.layers[len(self.layers)-1](features, edge_index, edge_weights)\n",
    "        else:\n",
    "            features = self.layers[0](features, edge_index, edge_weights)    # for a single layer case\n",
    "\n",
    "        predictions = F.log_softmax(features, dim=1)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "class single_Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, input_layers = [], dropout=0.3):\n",
    "        \"\"\"\n",
    "        input layers: list of integers\n",
    "        dropout: probability of droping out \n",
    "        \"\"\"\n",
    "        super(single_Net, self).__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.dropout = dropout\n",
    "        # here we just initialize the model\n",
    "        self.conv1 = custom_GCNConv(self.in_channels, self.out_channels)\n",
    "        \n",
    "\n",
    "    def forward(self, edge_index, features, edge_weights = None):\n",
    "        # call the instance of the custom_GCNConv\n",
    "        z = self.conv1(features, edge_index, edge_weights)    # for a single layer case, z is embeddings\n",
    "#         print('embeddings inside the net work model, result is: \\n', z)\n",
    "        \n",
    "        predictions = F.log_softmax(z, dim=1)\n",
    "#         print('calibration inside the net work model, result is: \\n', predictions)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partition GCN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Partition method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import metis\n",
    "import random\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import chain\n",
    "\n",
    "class ClusteringMachine(object):\n",
    "    \"\"\"\n",
    "    Clustering the graph, feature set and label. Performed on the CPU side\n",
    "    \"\"\"\n",
    "    def __init__(self, edge_index, features, label, partition_num = 2):\n",
    "        \"\"\"\n",
    "        :param edge_index: COO format of the edge indices.\n",
    "        :param features: Feature matrix (ndarray).\n",
    "        :param label: label vector (ndarray).\n",
    "        \"\"\"\n",
    "        tmp = edge_index.t().numpy().tolist()\n",
    "        self.graph = nx.from_edgelist(tmp)\n",
    "        self.features = features\n",
    "        self.label = label\n",
    "        self.partition_num = partition_num\n",
    "        self._set_sizes()\n",
    "        self.edge_index = edge_index\n",
    "        # this will get the edge weights in a complete graph\n",
    "        self.get_edge_weight(self.edge_index, self.node_count)\n",
    "\n",
    "    def _set_sizes(self):\n",
    "        \"\"\"\n",
    "        Setting the feature and class count.\n",
    "        \"\"\"\n",
    "        self.node_count = self.features.shape[0]\n",
    "        self.feature_count = self.features.shape[1]    # features all always in the columns\n",
    "        self.label_count = len(np.unique(self.label.numpy()) )\n",
    "        \n",
    "    def get_edge_weight(self, edge_index, num_nodes, edge_weight=None, improved=False, dtype=None):\n",
    "        \n",
    "        if edge_weight is None:\n",
    "            edge_weight = torch.ones((edge_index.size(1), ), dtype=dtype, device=edge_index.device)\n",
    "        \n",
    "        fill_value = 1 if not improved else 2\n",
    "        # edge_index is already double direction if undirect, then add num_nodes self-loop edges added after the edge_index\n",
    "        edge_index, edge_weight = add_remaining_self_loops(edge_index, edge_weight, fill_value, num_nodes)\n",
    "        # after this edge_index is a 2 by (edge_num + node_num) tensor\n",
    "        row, col = edge_index   \n",
    "        # row includes the starting points of the edges  (first row of edge_index)\n",
    "        # col includes the ending points of the edges   (second row of edge_index)\n",
    "\n",
    "        deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n",
    "        # row records the source nodes, which is the index we are trying to add\n",
    "        # deg will record the out-degree of each node of x_i in all edges (x_i, x_j) including self_loops\n",
    "        \n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "        normalized_edge_weight = deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n",
    "        # self.edge_index_global_self_loops :  a 2 by (2 * M + N) tensor:   M : number of edges, N : number of nodes\n",
    "        self.edge_index_global_self_loops = edge_index\n",
    "        \n",
    "        # transfer from tensor to the numpy to construct the dict for the edge_weights\n",
    "        edge_index = edge_index.t().numpy()\n",
    "        normalized_edge_weight = normalized_edge_weight.numpy()\n",
    "        num_edge, _ = edge_index.shape\n",
    "        # this info can also be stored as matrix considering the memory, depends whether the matrix is sparse or not\n",
    "        \n",
    "        self.edge_index_global_dict = {(edge_index[i][0], edge_index[i][1]) : i for i in range(num_edge)}\n",
    "        self.edge_weight_global_dict = {(edge_index[i][0], edge_index[i][1]) : normalized_edge_weight[i] for i in range(num_edge)}\n",
    "        self.edge_weight_global = [ self.edge_weight_global_dict[(edge[0], edge[1])] for edge in edge_index ]\n",
    "        \n",
    "    def decompose(self, test_ratio, validation_ratio):\n",
    "        \"\"\"\n",
    "        Decomposing the graph, partitioning the features and label, creating Torch arrays.\n",
    "        \"\"\"\n",
    "        # to keep the edge weights of the original whole graph:\n",
    "        \n",
    "        self.metis_clustering()\n",
    "#         self.random_clustering()\n",
    "        self._set_inter_clusters()\n",
    "        self.general_global_isolate_partitioning(test_ratio, validation_ratio)\n",
    "        # for the wholeGCNTraniner Purpose\n",
    "        self.general_accumulate_partition()\n",
    "        \n",
    "    def _set_inter_clusters(self):\n",
    "        # independent of the clustering method:\n",
    "        self.intersect_cluster = []\n",
    "        for i in range(1, self.partition_num):\n",
    "            tmp = [(m, n) for m, n in zip(self.clusters, self.clusters[i:])]\n",
    "            self.intersect_cluster.extend(tmp)\n",
    "\n",
    "    # just allocate each node to arandom cluster, store the membership inside each dict\n",
    "    def random_clustering(self):\n",
    "        \"\"\"\n",
    "        Random clustering the nodes.\n",
    "        \"\"\"\n",
    "        self.clusters = [cluster for cluster in range(self.partition_num)]\n",
    "        # randomly divide into two clusters\n",
    "        self.cluster_membership = {node: random.choice(self.clusters) for node in self.graph.nodes()}\n",
    "\n",
    "    def metis_clustering(self):\n",
    "        \"\"\"\n",
    "        Clustering the graph with Metis. For details see:\n",
    "        \"\"\"\n",
    "        (st, parts) = metis.part_graph(self.graph, self.partition_num)\n",
    "        self.clusters = list(set(parts))\n",
    "        self.cluster_membership = {node: membership for node, membership in enumerate(parts)}\n",
    "\n",
    "\n",
    "    def general_global_isolate_partitioning(self, test_ratio, validation_ratio):\n",
    "        \"\"\"\n",
    "        Creating data partitions and train-test splits.\n",
    "        \"\"\"\n",
    "        self.type = 'general'\n",
    "        relative_test_ratio = (test_ratio) / (1 - validation_ratio)\n",
    "        # all the global characteristics on record\n",
    "        self.sg_nodes_global = {}\n",
    "        self.sg_edges_global = {}  # keep all the global idx of the endpoints of edges\n",
    "        self.sg_edges_global_index = {}    # keep a global ordering indices for the edges\n",
    "        self.sg_subgraph = {}\n",
    "        \n",
    "        self.sg_model_nodes_global = {}\n",
    "        self.sg_validation_nodes_global = {}\n",
    "        self.sg_train_nodes_global = {}\n",
    "        self.sg_test_nodes_global = {}\n",
    "        # all the local characteristics on local cluster, later for purpose of validation calculation cluster-by-cluster\n",
    "        self.sg_node_mapper = {}\n",
    "        self.sg_edges_local = {}\n",
    "        self.sg_edge_mapper = {}\n",
    "        self.sg_edge_weight_local = {}\n",
    "        self.sg_edge_weight_selfloop_local = {}\n",
    "        self.sg_validation_nodes_local = {}\n",
    "        self.sg_features = {}\n",
    "        self.sg_labels = {}\n",
    "        \n",
    "        # keep the info of each cluster:\n",
    "        self.info_isolate_cluster_size = {}\n",
    "        self.info_model_cluster_size = {}\n",
    "        self.info_validation_cluster_size = {}\n",
    "        self.info_train_cluster_size = {}\n",
    "        self.info_test_cluster_size = {}\n",
    "        \n",
    "        print('global edge weight dictionary is: edge_weight_global_dict')\n",
    "        print(self.edge_weight_global_dict)\n",
    "        \n",
    "        for cluster in self.clusters:\n",
    "            self.sg_subgraph[cluster] = self.graph.subgraph([node for node in self.graph.nodes() if self.cluster_membership[node] == cluster])\n",
    "            self.sg_nodes_global[cluster] = sorted(node for node in self.sg_subgraph[cluster].nodes())\n",
    "            # this is to make sure the index as returned from the edge_index_global_dict is always unique, since edge_index_global_dict contains two directions\n",
    "            self.sg_edges_global[cluster] = {tuple(sorted(edge)) for edge in self.sg_subgraph[cluster].edges()}\n",
    "            self.sg_edges_global_index[cluster] = [self.edge_index_global_dict[edge] for edge in self.sg_edges_global[cluster]]\n",
    "            \n",
    "            self.sg_model_nodes_global[cluster], self.sg_validation_nodes_global[cluster] = train_test_split(self.sg_nodes_global[cluster], test_size = validation_ratio)\n",
    "            self.sg_model_nodes_global[cluster] = sorted(self.sg_model_nodes_global[cluster])\n",
    "            self.sg_validation_nodes_global[cluster] = sorted(self.sg_validation_nodes_global[cluster])\n",
    "            \n",
    "            self.sg_train_nodes_global[cluster], self.sg_test_nodes_global[cluster] = train_test_split(self.sg_model_nodes_global[cluster], test_size = relative_test_ratio)\n",
    "            self.sg_train_nodes_global[cluster] = sorted(self.sg_train_nodes_global[cluster])\n",
    "            self.sg_test_nodes_global[cluster] = sorted(self.sg_test_nodes_global[cluster])\n",
    "            \n",
    "            # remap into local index inside each cluster as long as global labels or features cannot fit into GPU memories\n",
    "            # 1) start remapping and interleave local edges, edge_weights, features, labels, train_nodes\n",
    "            self.sg_node_mapper[cluster] = {node: i for i, node in enumerate(self.sg_nodes_global[cluster])}\n",
    "            \n",
    "            # the problem is that convert the dictionary into tensor operations so that it can all be performed on GPU\n",
    "            self.sg_edges_local[cluster] = [ [ self.sg_node_mapper[cluster][edge[0]], self.sg_node_mapper[cluster][edge[1]] ] for edge in self.sg_edges_global[cluster] ]\n",
    "            # mapping the global edge idx to the local idx\n",
    "            self.sg_edge_mapper = {self.edge_index_global_dict[edge] : idx for idx, edge in enumerate(self.sg_edges_global[cluster]) }\n",
    "            \n",
    "            self.sg_edge_weight_local[cluster] = [ self.edge_weight_global_dict[(edge[0], edge[1])] for edge in self.sg_edges_global[cluster] ]\n",
    "            self.sg_edge_weight_selfloop_local[cluster] = [ self.edge_weight_global_dict[(i, i)] for i in self.sg_nodes_global[cluster] ]\n",
    "            \n",
    "#             print('train nodes global for the cluster # ' + str(cluster), self.sg_train_nodes_global[cluster])\n",
    "            self.sg_validation_nodes_local[cluster] = [ self.sg_node_mapper[cluster][global_idx] for global_idx in self.sg_validation_nodes_global[cluster] ]\n",
    "        \n",
    "            self.sg_features[cluster] = self.features[self.sg_nodes_global[cluster],:]\n",
    "            self.sg_labels[cluster] = self.label[self.sg_nodes_global[cluster]]\n",
    "            \n",
    "#             print('Examine all the other properties for cluster: ', cluster)\n",
    "#             print('sg_nodes_global: ', self.sg_nodes_global[cluster])\n",
    "#             print('sg_edges_global: ', self.sg_edges_global[cluster])\n",
    "#             print('sg_edges_local: ', self.sg_edges_local[cluster])\n",
    "#             print('sg_features: ', self.sg_features[cluster])\n",
    "#             print('sg_validation_nodes_global: ', self.sg_validation_nodes_global[cluster] )\n",
    "#             print('sg_validation_nodes_local: ', self.sg_validation_nodes_local[cluster] )\n",
    "            \n",
    "#             print('sg_edge_weight_local: ', self.sg_edge_weight_local[cluster])\n",
    "#             print('sg_edge_weight_selfloop_local: ', self.sg_edge_weight_selfloop_local[cluster])\n",
    "            \n",
    "            # record the information of each cluster:\n",
    "            self.info_isolate_cluster_size[cluster] = len(self.sg_nodes_global[cluster])\n",
    "            self.info_model_cluster_size[cluster] = len(self.sg_model_nodes_global[cluster])\n",
    "            self.info_validation_cluster_size[cluster] = len(self.sg_validation_nodes_global[cluster])\n",
    "            \n",
    "            self.info_train_cluster_size[cluster] = len(self.sg_train_nodes_global[cluster])\n",
    "            self.info_test_cluster_size[cluster] = len(self.sg_test_nodes_global[cluster])\n",
    "    \n",
    "    # accumulate all the train, test, and validation nodes \n",
    "    def general_accumulate_partition(self):\n",
    "        # sum up different parts of the data\n",
    "#         self.total_sg_train_nodes_global = sorted(chain.from_iterable(self.sg_train_nodes_global[cluster] for cluster in self.clusters))\n",
    "#         self.total_sg_test_nodes_global = sorted(chain.from_iterable(self.sg_test_nodes_global[cluster] for cluster in self.clusters)) \n",
    "        self.total_sg_train_nodes_global = sorted(self.sg_train_nodes_global[0])\n",
    "        self.total_sg_test_nodes_global = sorted(self.sg_test_nodes_global[0])\n",
    "        self.total_sg_validation_nodes_global = sorted(chain.from_iterable(self.sg_validation_nodes_global[cluster] for cluster in self.clusters))\n",
    "    \n",
    "    \n",
    "    def general_isolate_clustering(self, k):\n",
    "        \"\"\"\n",
    "            Still find the train batch, but cannot exceed the scope of the isolated clustering\n",
    "        \"\"\"\n",
    "        self.sg_mini_edges_global = {}\n",
    "        self.sg_mini_nodes_global = {}\n",
    "        \n",
    "        self.sg_mini_train_nodes_local = {}\n",
    "        self.sg_mini_edges_local = {}\n",
    "        self.sg_mini_edge_weight_local = {}\n",
    "        self.sg_mini_edge_weight_selfloop_local = {}\n",
    "        \n",
    "        self.sg_mini_features = {}\n",
    "        self.sg_mini_labels = {}\n",
    "        self.sg_mini_node_mapper = {}\n",
    "        \n",
    "        self.neighbor = defaultdict(dict)   # keep layer nodes of each layer\n",
    "        self.accum_neighbor = defaultdict(set)\n",
    "        \n",
    "        self.info_train_batch_size = {}\n",
    "        \n",
    "        for cluster in self.clusters:\n",
    "            self.neighbor[cluster] = {0 : set(self.sg_train_nodes_global[cluster])}\n",
    "            for layer in range(k):\n",
    "                # first accumulate last layer\n",
    "                self.accum_neighbor[cluster] |= self.neighbor[cluster][layer]\n",
    "                tmp_level = set()\n",
    "                for node in self.neighbor[cluster][layer]:\n",
    "                    tmp_level |= set(self.sg_subgraph[cluster].neighbors(node))\n",
    "                # add the new layer of neighbors\n",
    "                self.neighbor[cluster][layer+1] = tmp_level - self.accum_neighbor[cluster]\n",
    "            # the most outside layer: kth layer will be added:\n",
    "            self.accum_neighbor[cluster] |= self.neighbor[cluster][k]\n",
    "            batch_subgraph = self.sg_subgraph[cluster].subgraph(self.accum_neighbor[cluster])\n",
    "            \n",
    "            # first select all the overlapping nodes of the train nodes\n",
    "            self.sg_mini_edges_global[cluster] = {tuple(sorted(edge)) for edge in batch_subgraph.edges()}\n",
    "            self.sg_mini_nodes_global[cluster] = sorted(node for node in batch_subgraph.nodes())\n",
    "            \n",
    "            \n",
    "            self.sg_mini_node_mapper[cluster] = {node: i for i, node in enumerate(self.sg_mini_nodes_global[cluster])}\n",
    "            sg_node_index_local = sorted(self.sg_mini_node_mapper[cluster].values())\n",
    "            \n",
    "            ### store edge related info\n",
    "            self.sg_mini_edges_local[cluster] = [ [ self.sg_mini_node_mapper[cluster][edge[0]], self.sg_mini_node_mapper[cluster][edge[1]] ] for edge in self.sg_mini_edges_global[cluster] ]\n",
    "            \n",
    "            self.sg_mini_edge_weight_local[cluster] = [ self.edge_weight_global_dict[(edge[0], edge[1])] for edge in self.sg_mini_edges_global[cluster] ]\n",
    "            \n",
    "            self.sg_mini_edge_weight_selfloop_local[cluster] = [ self.edge_weight_global_dict[(i, i)] for i in self.sg_mini_nodes_global[cluster] ]\n",
    "            \n",
    "#             print('train nodes global for the cluster # ' + str(cluster), self.sg_train_nodes_global[cluster])\n",
    "            self.sg_mini_train_nodes_local[cluster] = [ self.sg_mini_node_mapper[cluster][global_idx] for global_idx in self.sg_train_nodes_global[cluster] ]\n",
    "            \n",
    "            self.sg_mini_features[cluster] = self.features[self.sg_mini_nodes_global[cluster],:]\n",
    "            self.sg_mini_labels[cluster] = self.label[self.sg_mini_nodes_global[cluster]]\n",
    "            \n",
    "            # record information \n",
    "            self.info_train_batch_size[cluster] = len(self.sg_mini_nodes_global[cluster])\n",
    "        \n",
    "        # at last, out of all the cluster loop do the data transfer\n",
    "        self.transfer_edges_and_nodes()\n",
    "        self.mini_transfer_edges_and_nodes()\n",
    "        \n",
    "    def print_neighbor_list(self):\n",
    "        for cluster in self.clusters:\n",
    "            train_set = set(self.sg_train_nodes_global[cluster])\n",
    "            for node in train_set:\n",
    "                print('node ' + str(node) + ' : ', list(self.graph.neighbors(node)), type(self.graph.neighbors(node)))\n",
    "                \n",
    "    def get_train_neighbor(self, k):\n",
    "        \"\"\"\n",
    "            get a collection of nodes: including k layers of neighbors together with original isolate cluster nodes\n",
    "            k: number of layers of neighbors\n",
    "        \"\"\"\n",
    "        # this self.neighbor keeps a record: in each cluster, the nodes of different layer of neighbors\n",
    "        self.neighbor = defaultdict(dict)   # keep layer nodes of each layer\n",
    "        self.accum_neighbor = defaultdict(set)\n",
    "        for cluster in self.clusters:\n",
    "            self.neighbor[cluster] = {0 : set(self.sg_train_nodes_global[cluster])}\n",
    "            \n",
    "            for layer in range(k):\n",
    "                # first accumulate last layer\n",
    "                self.accum_neighbor[cluster] |= self.neighbor[cluster][layer]\n",
    "                tmp_level = set()\n",
    "                for node in self.neighbor[cluster][layer]:\n",
    "                    tmp_level |= set(self.graph.neighbors(node))\n",
    "                # add the new layer of neighbors\n",
    "                self.neighbor[cluster][layer+1] = tmp_level - self.accum_neighbor[cluster]\n",
    "            # the most outside layer: kth layer will be added:\n",
    "            self.accum_neighbor[cluster] |= self.neighbor[cluster][k]\n",
    "            # after getting the train k layer neighbor nodes, generating the graph\n",
    "            batch_subgraph = self.graph.subgraph(self.accum_neighbor[cluster])\n",
    "            print('nodes for cluster ' + str(cluster) + ' are: ', sorted(node for node in batch_subgraph.nodes()))\n",
    "            \n",
    "            print('edges for cluster ' + str(cluster) + ' are: ', {edge for edge in batch_subgraph.edges()} ) \n",
    "        \n",
    "    # select the training nodes as the mini-batch for each cluster\n",
    "    def mini_batch_train_sample(self, cluster, k, frac = 1):\n",
    "        self.neighbor[cluster] = {0 : set(self.sg_train_nodes_global[cluster])}\n",
    "        for layer in range(k):\n",
    "            # first accumulate last layer\n",
    "            self.accum_neighbor[cluster] |= self.neighbor[cluster][layer]\n",
    "            tmp_level = set()\n",
    "            for node in self.neighbor[cluster][layer]:\n",
    "                tmp_level |= set(self.graph.neighbors(node))\n",
    "            # add the new layer of neighbors\n",
    "            tmp_level -= self.accum_neighbor[cluster]\n",
    "            # each layer will only contains partial nodes from the previous layer\n",
    "            self.neighbor[cluster][layer+1] = set(random.sample(tmp_level, int(len(tmp_level) * frac) ) ) if 0 < frac < 1 else tmp_level\n",
    "        # the most outside layer: kth layer will be added:\n",
    "        self.accum_neighbor[cluster] |= self.neighbor[cluster][k]\n",
    "        \n",
    "    def mini_batch_train_clustering(self, k, fraction = 1.0):\n",
    "        \"\"\"\n",
    "            create the mini-batch focused on the train nodes only\n",
    "            Include a total of k layers of neighbors of the original training nodes\n",
    "            k: number of layers of neighbors for each training node\n",
    "        \"\"\"\n",
    "        self.sg_mini_edges_global = {}\n",
    "        self.sg_mini_edges_global_index = {}\n",
    "        self.sg_mini_nodes_global = {}\n",
    "        \n",
    "        self.sg_mini_train_nodes_local = {}\n",
    "        self.sg_mini_edges_local = {}\n",
    "        self.sg_mini_edge_weight_local = {}\n",
    "        self.sg_mini_edge_weight_selfloop_local = {}\n",
    "        self.sg_mini_edge_mapper = {}\n",
    "        \n",
    "        self.sg_mini_features = {}\n",
    "        self.sg_mini_labels = {}\n",
    "        self.sg_mini_node_mapper = {}\n",
    "        \n",
    "        self.neighbor = defaultdict(dict)   # keep layer nodes of each layer\n",
    "        self.accum_neighbor = defaultdict(set)\n",
    "        \n",
    "        self.info_train_batch_size = {}\n",
    "        \n",
    "        for cluster in self.clusters:\n",
    "            self.mini_batch_train_sample(cluster, k, frac = fraction)\n",
    "            batch_subgraph = self.graph.subgraph(self.accum_neighbor[cluster])\n",
    "            \n",
    "            # first select all the overlapping nodes of the train nodes\n",
    "            self.sg_mini_edges_global[cluster] = {tuple(sorted(edge)) for edge in batch_subgraph.edges()}\n",
    "            self.sg_mini_edges_global_index[cluster] = [self.edge_index_global_dict[edge] for edge in self.sg_mini_edges_global[cluster]]\n",
    "#             print(self.sg_mini_edges_global_index[cluster])\n",
    "            \n",
    "            self.sg_mini_nodes_global[cluster] = sorted(node for node in batch_subgraph.nodes())\n",
    "            \n",
    "            # 1) start remapping and interleave local edges, edge_weights, features, labels, train_nodes\n",
    "            self.sg_mini_node_mapper[cluster] = {node: i for i, node in enumerate(self.sg_mini_nodes_global[cluster])}\n",
    "            \n",
    "            # the problem is that convert the dictionary into tensor operations so that it can all be performed on GPU\n",
    "            self.sg_mini_edges_local[cluster] = [ [ self.sg_mini_node_mapper[cluster][edge[0]], self.sg_mini_node_mapper[cluster][edge[1]] ] for edge in self.sg_mini_edges_global[cluster] ]\n",
    "            # mapping the global edge idx to the local idx\n",
    "            self.sg_mini_edge_mapper = {self.edge_index_global_dict[edge] : idx for idx, edge in enumerate(self.sg_mini_edges_global[cluster]) }\n",
    "            \n",
    "            self.sg_mini_edge_weight_local[cluster] = [ self.edge_weight_global_dict[(edge[0], edge[1])] for edge in self.sg_mini_edges_global[cluster] ]\n",
    "            self.sg_mini_edge_weight_selfloop_local[cluster] = [ self.edge_weight_global_dict[(i, i)] for i in self.sg_mini_nodes_global[cluster] ]\n",
    "            \n",
    "#             print('train nodes global for the cluster # ' + str(cluster), self.sg_train_nodes_global[cluster])\n",
    "            self.sg_mini_train_nodes_local[cluster] = [ self.sg_mini_node_mapper[cluster][global_idx] for global_idx in self.sg_train_nodes_global[cluster] ]\n",
    "            \n",
    "            self.sg_mini_features[cluster] = self.features[self.sg_mini_nodes_global[cluster],:]\n",
    "            self.sg_mini_labels[cluster] = self.label[self.sg_mini_nodes_global[cluster]]\n",
    "            \n",
    "            \n",
    "            # 3) record information \n",
    "            self.info_train_batch_size[cluster] = len(self.sg_mini_nodes_global[cluster])\n",
    "        \n",
    "        self.batch_overlap()\n",
    "        # at last, out of all the cluster loop do the data transfer\n",
    "        self.transfer_edges_and_nodes()\n",
    "        self.mini_transfer_edges_and_nodes()\n",
    "    \n",
    "    def batch_overlap(self):\n",
    "        # global size of overlapping nodes and edges between different train-batches\n",
    "        self.overlap_batch_nodes = {}\n",
    "        self.overlap_batch_edges = {}\n",
    "        \n",
    "        self.info_overlap_batch_nodes = {}\n",
    "        self.info_overlap_batch_edges = {}\n",
    "        \n",
    "        for i in range(1, len(self.clusters)):\n",
    "            for left, right in zip(self.clusters, self.clusters[i:]):\n",
    "                self.overlap_batch_nodes[(left, right)] = list(set(self.sg_mini_nodes_global[left]) & set(self.sg_mini_nodes_global[right]))\n",
    "                self.overlap_batch_nodes[(right, left)] = self.overlap_batch_nodes[(left, right)]\n",
    "                self.info_overlap_batch_nodes[(left, right)] = len(self.overlap_batch_nodes[(left, right)])\n",
    "                \n",
    "                self.overlap_batch_edges[(left, right)] = list(set(self.sg_mini_edges_global_index[left]) & set(self.sg_mini_edges_global_index[right]))\n",
    "                self.overlap_batch_edges[(right, left)] = self.overlap_batch_edges[(left, right)]\n",
    "                self.info_overlap_batch_edges[(left, right)] = len(self.overlap_batch_edges[(left, right)])\n",
    "                \n",
    "    \n",
    "    def mini_transfer_edges_and_nodes(self):\n",
    "        for cluster in self.clusters:\n",
    "            self.sg_mini_edges_local[cluster] = torch.LongTensor(self.sg_mini_edges_local[cluster])\n",
    "            \n",
    "            self.sg_mini_edge_weight_local[cluster] = torch.FloatTensor(self.sg_mini_edge_weight_local[cluster])\n",
    "            self.sg_mini_edge_weight_selfloop_local[cluster] = torch.FloatTensor(self.sg_mini_edge_weight_selfloop_local[cluster])\n",
    "            \n",
    "            self.sg_mini_train_nodes_local[cluster] = torch.LongTensor(self.sg_mini_train_nodes_local[cluster])\n",
    "            self.sg_mini_features[cluster] = torch.FloatTensor(self.sg_mini_features[cluster])\n",
    "            self.sg_mini_labels[cluster] = torch.LongTensor(self.sg_mini_labels[cluster])\n",
    "        \n",
    "        \n",
    "    def transfer_edges_and_nodes(self):\n",
    "        \"\"\"\n",
    "        For each isolated cluster\n",
    "        Transfering the data to PyTorch tensor format.\n",
    "        \"\"\"\n",
    "        self.edge_weight_global = torch.FloatTensor(self.edge_weight_global)\n",
    "#         self.edge_index_global_self_loops = self.edge_index_global_self_loops\n",
    "        for cluster in self.clusters:\n",
    "            self.sg_train_nodes_global[cluster] = torch.LongTensor(self.sg_train_nodes_global[cluster])\n",
    "            self.sg_test_nodes_global[cluster] = torch.LongTensor(self.sg_test_nodes_global[cluster])\n",
    "            self.sg_validation_nodes_global[cluster] = torch.LongTensor(self.sg_validation_nodes_global[cluster])\n",
    "            \n",
    "            self.sg_edges_local[cluster] = torch.LongTensor(self.sg_edges_local[cluster])\n",
    "            self.sg_edge_weight_local[cluster] = torch.FloatTensor(self.sg_edge_weight_local[cluster])\n",
    "            self.sg_edge_weight_selfloop_local[cluster] = torch.FloatTensor(self.sg_edge_weight_selfloop_local[cluster])\n",
    "            \n",
    "            self.sg_validation_nodes_local[cluster] = torch.LongTensor(self.sg_validation_nodes_local[cluster])\n",
    "            self.sg_features[cluster] = torch.FloatTensor(self.sg_features[cluster])\n",
    "            self.sg_labels[cluster] = torch.LongTensor(self.sg_labels[cluster])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Partition Graph with trainiing and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Custom_GCN_layer import Net\n",
    "import time\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class ClusterGCNTrainer_mini_Train(object):\n",
    "    \"\"\"\n",
    "    Training a ClusterGCN.\n",
    "    \"\"\"\n",
    "    def __init__(self, clustering_machine, in_channels, out_channels, input_layers = [32, 16], dropout=0.3):\n",
    "        \"\"\"\n",
    "        :param in_channels, out_channels: input and output feature dimension\n",
    "        :param clustering_machine:\n",
    "        \"\"\"  \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.clustering_machine = clustering_machine\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.input_layers = input_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.create_model()\n",
    "\n",
    "    def create_model(self):\n",
    "        \"\"\"\n",
    "        Creating a StackedGCN and transferring to CPU/GPU.\n",
    "        \"\"\"\n",
    "#         print('used layers are: ', str(self.input_layers))\n",
    "        self.model = Net(self.in_channels, self.out_channels, input_layers = self.input_layers, dropout = self.dropout)\n",
    "        self.model = self.model.to(self.device)\n",
    "    \n",
    "    def update_average_loss(self, batch_average_loss, node_count, isolate = True):\n",
    "        \"\"\"\n",
    "        Updating the average loss in the epoch.\n",
    "        :param batch_average_loss: Loss of the cluster. \n",
    "        :param node_count: Number of nodes in currently processed cluster.\n",
    "        :return average_loss: Average loss in the epoch.\n",
    "        \"\"\"\n",
    "        self.accumulated_training_loss = self.accumulated_training_loss + batch_average_loss.item()*node_count\n",
    "        if isolate:\n",
    "            self.node_count_seen = self.node_count_seen + node_count\n",
    "        average_loss = self.accumulated_training_loss / self.node_count_seen\n",
    "        return average_loss\n",
    "    \n",
    "    # call the forward function batch by batch\n",
    "    def _edge_double_add_self_loop(self, edges, num_nodes):\n",
    "        \"\"\"\n",
    "            edges (on GPU): tensor M by 2 (M is the number of edges (one direction only))\n",
    "            num_nodes:  total of numbers covered of all endpoints of edges\n",
    "        \"\"\"\n",
    "        target_tsr = edges.t()\n",
    "        # generate the other direction edges:\n",
    "        start, end = target_tsr\n",
    "        start, end = start.unsqueeze(0), end.unsqueeze(0)\n",
    "        target_rever = torch.cat([end, start], dim=0)\n",
    "        target_comb = torch.cat([target_tsr, target_rever], dim=1)\n",
    "        # genrate self loops:\n",
    "        loop_index = torch.arange(0, num_nodes, dtype=target_tsr.dtype, device=target_tsr.device)\n",
    "        loop_index = loop_index.unsqueeze(0).repeat(2, 1)\n",
    "        target_final = torch.cat([target_comb, loop_index], dim=1)\n",
    "        return target_final\n",
    "    \n",
    "    def _edge_weight_double_add_self_loop(self, edge_weights, edge_selfloop_weights):\n",
    "        \"\"\"\n",
    "            edges_weights (on GPU): 1D tensor of size M  (M is the number of edges (one direction only))\n",
    "            edge_selfloop_weights (on GPU)::  1D tensor of size N (N is the number of edge endpoints)\n",
    "        \"\"\"\n",
    "        return torch.cat([edge_weights, edge_weights, edge_selfloop_weights], dim=0)\n",
    "    \n",
    "    def do_forward_pass(self, cluster):\n",
    "        \"\"\"\n",
    "        Making a forward pass with data from a given partition.\n",
    "        :param cluster: Cluster index.\n",
    "        :return average_loss: Average loss on the cluster.\n",
    "        :return node_count: Number of nodes.\n",
    "        \"\"\"\n",
    "        \n",
    "        # transfer all the five tensors associated with the train-batch nodes on to GPU\n",
    "        t1 = time.time()\n",
    "        \n",
    "        features = self.clustering_machine.sg_mini_features[cluster].to(self.device)\n",
    "        target = self.clustering_machine.sg_mini_labels[cluster].to(self.device)\n",
    "        \n",
    "        # deal with edge index tensors, load\n",
    "        edges = self.clustering_machine.sg_mini_edges_local[cluster].to(self.device)\n",
    "        edges = self._edge_double_add_self_loop(edges, len(target))\n",
    "        # deal with edge_weights:  two parts, 1) sigle direction , 2) self-loop\n",
    "        edge_weights_single = self.clustering_machine.sg_mini_edge_weight_local[cluster].to(self.device)\n",
    "        edge_weights_selfloop = self.clustering_machine.sg_mini_edge_weight_selfloop_local[cluster].to(self.device)\n",
    "        edge_weights = torch.cat([edge_weights_single, edge_weights_single, edge_weights_selfloop], dim=0)\n",
    "        \n",
    "        train_nodes = self.clustering_machine.sg_mini_train_nodes_local[cluster].to(self.device)\n",
    "        # torch.squeeze()  removes all the dimension with value 1, change the target from 2-D  (N by 1) into 1-D N tensor\n",
    "        self.time_train_load_data += (time.time() - t1) * 1000\n",
    "        \n",
    "        '''Target and features are one-one mapping'''\n",
    "        # calculate the probabilites from log_sofmax\n",
    "        predictions = self.model(edges, features, edge_weights)\n",
    "        ave_loss = torch.nn.functional.nll_loss(predictions[train_nodes], target[train_nodes])\n",
    "        node_count = train_nodes.shape[0]\n",
    "        \n",
    "#         print('train-batch version of train forward info for cluster : ', cluster)\n",
    "#         print('prediction values: ')\n",
    "#         print(predictions)\n",
    "#         print('prediction values for train nodes: ')\n",
    "#         print(predictions[train_nodes])\n",
    "        \n",
    "        # for each cluster keep track of the counts of the nodes\n",
    "        return ave_loss, node_count\n",
    "\n",
    "\n",
    "    # iterate through epoch and also the clusters\n",
    "    def train(self, epoch_num=10, learning_rate=0.01, weight_decay = 0.01):\n",
    "        \"\"\"\n",
    "        Training a model.\n",
    "        \"\"\"\n",
    "        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        self.model.train()\n",
    "        self.record_ave_training_loss = []\n",
    "        self.time_train_load_data = 0\n",
    "        \n",
    "        t0 = time.time()\n",
    "        for epoch in range(epoch_num):\n",
    "#             For test purpose, we let the clusters to follow specific order\n",
    "#             random.shuffle(self.clustering_machine.clusters)\n",
    "            self.node_count_seen = 0\n",
    "            self.accumulated_training_loss = 0\n",
    "            for cluster in self.clustering_machine.clusters:\n",
    "                self.optimizer.zero_grad()\n",
    "                batch_ave_loss, node_count = self.do_forward_pass(cluster)\n",
    "                batch_ave_loss.backward()\n",
    "                self.optimizer.step()\n",
    "                ave_loss = self.update_average_loss(batch_ave_loss, node_count)\n",
    "            \n",
    "            self.record_ave_training_loss.append(ave_loss)\n",
    "        # convert to ms\n",
    "        self.time_train_total = ((time.time() - t0) * 1000)\n",
    "        \n",
    "    def do_prediction(self, cluster):\n",
    "        \"\"\"\n",
    "        Scoring a cluster.\n",
    "        :param cluster: Cluster index.\n",
    "        :return prediction: Prediction matrix with probabilities.\n",
    "        :return target: Target vector.\n",
    "        \"\"\"\n",
    "        # currently , the prediction use all the features, labels, edges and edge_weights\n",
    "        nodes_num = self.clustering_machine.info_isolate_cluster_size[cluster]\n",
    "        features = self.clustering_machine.sg_features[cluster].to(self.device)\n",
    "        labels = self.clustering_machine.sg_labels[cluster].to(self.device)\n",
    "        \n",
    "        # deal with edge index tensors, load\n",
    "        edges = self.clustering_machine.sg_edges_local[cluster].to(self.device)\n",
    "        edges = self._edge_double_add_self_loop(edges, nodes_num)\n",
    "        # deal with edge_weights:  two parts, 1) sigle direction , 2) self-loop\n",
    "        edge_weights_single = self.clustering_machine.sg_edge_weight_local[cluster].to(self.device)\n",
    "        edge_weights_selfloop = self.clustering_machine.sg_edge_weight_selfloop_local[cluster].to(self.device)\n",
    "        edge_weights = torch.cat([edge_weights_single, edge_weights_single, edge_weights_selfloop], dim=0)\n",
    "        \n",
    "        test_nodes = self.clustering_machine.sg_test_local[cluster].to(self.device)\n",
    "        predictions = self.model(edges, features, edge_weights)\n",
    "        \n",
    "        return prediction[test_nodes], labels[test_nodes]\n",
    "    \n",
    "    def test(self):\n",
    "        \"\"\"\n",
    "        Scoring the test and printing the F-1 score.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        self.predictions = []\n",
    "        self.targets = []\n",
    "        \n",
    "        for cluster in self.clustering_machine.clusters:\n",
    "            prediction, target = self.do_prediction(cluster)\n",
    "\n",
    "            self.predictions.append(prediction.cpu().detach().numpy())\n",
    "            self.targets.append(target.cpu().detach().numpy())\n",
    "        \n",
    "        # concatenate all the ndarrays inside this list\n",
    "        self.targets = np.concatenate(self.targets)\n",
    "        self.predictions = np.concatenate(self.predictions).argmax(1)  # return the indices of maximum probability \n",
    "        \n",
    "        f1 = f1_score(self.targets, self.predictions, average=\"micro\")\n",
    "        accuracy = accuracy_score(self.targets, self.predictions)\n",
    "#         print(\"\\nTest F-1 score: {:.4f}\".format(score))\n",
    "        return (f1, accuracy)\n",
    "    \n",
    "    def do_validation_prediction(self, cluster):\n",
    "        \"\"\"\n",
    "        Scoring a cluster.\n",
    "        :param cluster: Cluster index.\n",
    "        :return prediction: Prediction matrix with probabilities.\n",
    "        :return target: Target vector.\n",
    "        \"\"\"\n",
    "        nodes_num = self.clustering_machine.info_isolate_cluster_size[cluster]\n",
    "        features = self.clustering_machine.sg_features[cluster].to(self.device)\n",
    "        labels = self.clustering_machine.sg_labels[cluster].to(self.device)\n",
    "        \n",
    "        # deal with edge index tensors, load\n",
    "        edges = self.clustering_machine.sg_edges_local[cluster].to(self.device)\n",
    "        edges = self._edge_double_add_self_loop(edges, nodes_num)\n",
    "        # deal with edge_weights:  two parts, 1) sigle direction , 2) self-loop\n",
    "        edge_weights_single = self.clustering_machine.sg_edge_weight_local[cluster].to(self.device)\n",
    "        edge_weights_selfloop = self.clustering_machine.sg_edge_weight_selfloop_local[cluster].to(self.device)\n",
    "        edge_weights = torch.cat([edge_weights_single, edge_weights_single, edge_weights_selfloop], dim=0)\n",
    "        \n",
    "        validation_nodes = self.clustering_machine.sg_validation_nodes_local[cluster].to(self.device)\n",
    "        \n",
    "        print('In validation using local cluster for the cluster: ', cluster)\n",
    "        \n",
    "        predictions = self.model(edges, features, edge_weights)\n",
    "        \n",
    "#         print('Used features :')\n",
    "#         print(features)\n",
    "#         print('Used edges :')\n",
    "#         print(edges)\n",
    "#         print('Used weights :')\n",
    "#         print(edge_weights)\n",
    "#         print('predictions for all nodes')\n",
    "#         print(predictions)\n",
    "#         print('prediction values for the validation nodes: ')\n",
    "#         print(predictions[validation_nodes])\n",
    "        \n",
    "        return predictions[validation_nodes], labels[validation_nodes]\n",
    "    \n",
    "    def do_global_validation_prediction(self, cluster):\n",
    "        # global validation\n",
    "        validation_nodes = self.clustering_machine.sg_validation_nodes_global[cluster].to(self.device)\n",
    "        \n",
    "        print('In validation global using whole graph for the cluster: ', cluster)\n",
    "        predictions = self.model(self.edges, self.features, self.edge_weights)\n",
    "        \n",
    "#         print('Used features :')\n",
    "#         print(self.features)\n",
    "#         print('Used edges :')\n",
    "#         print(self.edges)\n",
    "#         print('Used weights :')\n",
    "#         print(self.edge_weights)\n",
    "#         print('predictions for all nodes')\n",
    "#         print(predictions)\n",
    "#         print('values for the validation nodes: ')\n",
    "#         print(predictions[validation_nodes])\n",
    "        \n",
    "#         return predictions[validation_nodes], self.label[validation_nodes]\n",
    "\n",
    "    def validate(self):\n",
    "        \"\"\"\n",
    "        Scoring the test and printing the F-1 score.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        self.predictions = []\n",
    "        self.targets = []\n",
    "        # global validation\n",
    "        self.edges = self.clustering_machine.edge_index_global_self_loops.to(self.device)\n",
    "        self.features = self.clustering_machine.features.to(self.device)\n",
    "        self.edge_weights = self.clustering_machine.edge_weight_global.to(self.device)\n",
    "        self.label = self.clustering_machine.label.to(self.device)\n",
    "        \n",
    "        for cluster in self.clustering_machine.clusters:\n",
    "            prediction, target = self.do_validation_prediction(cluster)\n",
    "            self.do_global_validation_prediction(cluster)\n",
    "            \n",
    "            self.predictions.append(prediction.cpu().detach().numpy())\n",
    "            self.targets.append(target.cpu().detach().numpy())\n",
    "        \n",
    "        # concatenate all the ndarrays inside this list\n",
    "        self.targets = np.concatenate(self.targets)\n",
    "        # along axis:    axis == 1\n",
    "        self.predictions = np.concatenate(self.predictions).argmax(1)  # return the indices of maximum probability \n",
    "#         print('shape of the targets and predictions are: ', self.targets.shape, self.predictions.shape)\n",
    "        \n",
    "        f1 = f1_score(self.targets, self.predictions, average=\"micro\")\n",
    "        accuracy = accuracy_score(self.targets, self.predictions)\n",
    "#         print(\"\\nTest F-1 score: {:.4f}\".format(score))\n",
    "        return (f1, accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whole input graph as base case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "class wholeClusterGCNTrainer_sequence(object):\n",
    "    \"\"\"\n",
    "    Training a ClusterGCN.\n",
    "    \"\"\"\n",
    "    def __init__(self, clustering_machine, in_channels, out_channels, input_layers = [16, 16], dropout=0.3):\n",
    "        \"\"\"\n",
    "        :param in_channels, out_channels: input and output feature dimension\n",
    "        :param clustering_machine:\n",
    "        \"\"\"  \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.clustering_machine = clustering_machine\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.input_layers = input_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.create_model()\n",
    "\n",
    "    def create_model(self):\n",
    "        \"\"\"\n",
    "        Creating a StackedGCN and transferring to CPU/GPU.\n",
    "        \"\"\"\n",
    "#         print('used layers are: ', str(self.input_layers))\n",
    "        self.model = Net(self.in_channels, self.out_channels, input_layers = self.input_layers, dropout = self.dropout)\n",
    "        self.model = self.model.to(self.device)\n",
    "    \n",
    "    # call the forward function batch by batch\n",
    "    def do_forward_pass(self, cluster):\n",
    "        \"\"\"\n",
    "        Making a forward pass with data from a given partition.\n",
    "        :param cluster: Cluster index.\n",
    "        :return average_loss: Average loss on the cluster.\n",
    "        :return node_count: Number of nodes.\n",
    "        \"\"\"\n",
    "        \n",
    "        train_nodes = self.clustering_machine.sg_train_nodes_global[cluster].to(self.device)\n",
    "        \n",
    "        '''Target and features are one-one mapping'''\n",
    "        # calculate the probabilites from log_sofmax\n",
    "        predictions = self.model(self.edges, self.features, self.edge_weights)\n",
    "#         print('whole graph version of train forward info for cluster : ', cluster)\n",
    "#         print('prediction values: ')\n",
    "#         print(predictions)\n",
    "#         print('prediction values for train nodes: ')\n",
    "#         print(predictions[train_nodes])\n",
    "        \n",
    "        ave_loss = torch.nn.functional.nll_loss(predictions[train_nodes], self.label[train_nodes])\n",
    "        node_count = train_nodes.shape[0]\n",
    "\n",
    "        # for each cluster keep track of the counts of the nodes\n",
    "        return ave_loss, node_count\n",
    "\n",
    "\n",
    "    def update_average_loss(self, batch_average_loss, node_count, isolate = True):\n",
    "        \"\"\"\n",
    "        Updating the average loss in the epoch.\n",
    "        :param batch_average_loss: Loss of the cluster. \n",
    "        :param node_count: Number of nodes in currently processed cluster.\n",
    "        :return average_loss: Average loss in the epoch.\n",
    "        \"\"\"\n",
    "        self.accumulated_training_loss = self.accumulated_training_loss + batch_average_loss.item()*node_count\n",
    "        if isolate:\n",
    "            self.node_count_seen = self.node_count_seen + node_count\n",
    "        average_loss = self.accumulated_training_loss / self.node_count_seen\n",
    "        return average_loss\n",
    "\n",
    "    # iterate through epoch and also the clusters\n",
    "    def train(self, epoch_num=10, learning_rate=0.01, weight_decay = 0.01):\n",
    "        \"\"\"\n",
    "        Training a model.\n",
    "        \"\"\"\n",
    "        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        self.model.train()   # set self.training = True : will be used in the dropout\n",
    "        self.record_ave_training_loss = []\n",
    "        self.time_train_load_data = 0\n",
    "        \n",
    "        t0 = time.time()\n",
    "        # first transfer the whole graph data to the GPU device\n",
    "        \n",
    "        t1 = time.time()\n",
    "        self.edges = self.clustering_machine.edge_index_global_self_loops.to(self.device)\n",
    "        self.features = self.clustering_machine.features.to(self.device)\n",
    "        self.edge_weights = self.clustering_machine.edge_weight_global.to(self.device)\n",
    "        self.label = self.clustering_machine.label.to(self.device)\n",
    "        self.time_train_load_data += (time.time() - t1) * 1000\n",
    "        \n",
    "        for epoch in range(epoch_num):\n",
    "            random.shuffle(self.clustering_machine.clusters)\n",
    "            self.node_count_seen = 0\n",
    "            self.accumulated_training_loss = 0\n",
    "            for cluster in self.clustering_machine.clusters:\n",
    "                self.optimizer.zero_grad()\n",
    "                batch_ave_loss, node_count = self.do_forward_pass(cluster)\n",
    "                batch_ave_loss.backward()\n",
    "                self.optimizer.step()\n",
    "                ave_loss = self.update_average_loss(batch_ave_loss, node_count)\n",
    "            \n",
    "            self.record_ave_training_loss.append(ave_loss)\n",
    "        # convert to ms\n",
    "        self.time_train_total = ((time.time() - t0) * 1000)\n",
    "        \n",
    "#         epochs.set_description(\"Ave Train Loss per node: %g \" % round(ave_loss,6))\n",
    "#         print(\"Train ave loss of overlapping clusters per node : %g\" % round(ave_loss,6))\n",
    "\n",
    "    def do_prediction(self, cluster):\n",
    "        \"\"\"\n",
    "        Scoring a cluster.\n",
    "        :param cluster: Cluster index.\n",
    "        :return prediction: Prediction matrix with probabilities.\n",
    "        :return target: Target vector.\n",
    "        \"\"\"\n",
    "        test_nodes = self.clustering_machine.sg_test_nodes_global[cluster].to(self.device)\n",
    "        target = self.clustering_machine.label.to(self.device)\n",
    "        # directly use the whole graph tensors as stored by the train procedure: edges, features, edge_weights\n",
    "        prediction = self.model(self.edges, self.features, self.edge_weights)\n",
    "        \n",
    "        return prediction[test_nodes], target[test_nodes]\n",
    "    \n",
    "    def test(self):\n",
    "        \"\"\"\n",
    "        Scoring the test and printing the F-1 score.\n",
    "        \"\"\"\n",
    "        self.model.eval()   # set self.training = false\n",
    "        self.predictions = []\n",
    "        self.targets = []\n",
    "        for cluster in self.clustering_machine.clusters:\n",
    "            prediction, target = self.do_prediction(cluster)\n",
    "\n",
    "            self.predictions.append(prediction.cpu().detach().numpy())\n",
    "            self.targets.append(target.cpu().detach().numpy())\n",
    "        \n",
    "        # concatenate all the ndarrays inside this list\n",
    "        self.targets = np.concatenate(self.targets)\n",
    "        # along axis:    axis == 1\n",
    "        self.predictions = np.concatenate(self.predictions).argmax(1)  # return the indices of maximum probability \n",
    "#         print('shape of the targets and predictions are: ', self.targets.shape, self.predictions.shape)\n",
    "        \n",
    "        f1_score = f1_score(self.targets, self.predictions, average=\"micro\")\n",
    "        accuracy = accuracy_score(self.targets, self.predictions)\n",
    "#         print(\"\\nTest F-1 score: {:.4f}\".format(score))\n",
    "        return (f1_score, accuracy)\n",
    "    \n",
    "    def do_validation_prediction(self, cluster):\n",
    "        \"\"\"\n",
    "        Scoring a cluster.\n",
    "        :param cluster: Cluster index.\n",
    "        :return prediction: Prediction matrix with probabilities.\n",
    "        :return target: Target vector.\n",
    "        \"\"\"\n",
    "        validation_nodes = self.clustering_machine.sg_validation_nodes_global[cluster].to(self.device)\n",
    "        target = self.clustering_machine.label.to(self.device)\n",
    "        \n",
    "        prediction = self.model(self.edges, self.features, self.edge_weights)\n",
    "        \n",
    "        return prediction[validation_nodes], target[validation_nodes]\n",
    "\n",
    "    def validate(self):\n",
    "        \"\"\"\n",
    "        Scoring the test and printing the F-1 score.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        self.predictions = []\n",
    "        self.targets = []\n",
    "        for cluster in self.clustering_machine.clusters:\n",
    "            prediction, target = self.do_validation_prediction(cluster)\n",
    "\n",
    "            self.predictions.append(prediction.cpu().detach().numpy())\n",
    "            self.targets.append(target.cpu().detach().numpy())\n",
    "        \n",
    "        # concatenate all the ndarrays inside this list\n",
    "        self.targets = np.concatenate(self.targets)\n",
    "        # along axis:    axis == 1\n",
    "        self.predictions = np.concatenate(self.predictions).argmax(1)  # return the indices of maximum probability \n",
    "#         print('shape of the targets and predictions are: ', self.targets.shape, self.predictions.shape)\n",
    "        \n",
    "        f1 = f1_score(self.targets, self.predictions, average=\"micro\")\n",
    "        accuracy = accuracy_score(self.targets, self.predictions)\n",
    "#         print(\"\\nTest F-1 score: {:.4f}\".format(score))\n",
    "        return (f1, accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the mini clustering basic info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_clustering(clustering_machine):\n",
    "    whole_graph = clustering_machine.graph\n",
    "\n",
    "    isolate_clusters_global = [ clustering_machine.sg_nodes_global[cluster]\n",
    "                             for cluster in clustering_machine.clusters]\n",
    "    \n",
    "    modeling_clusters_global = [ clustering_machine.sg_model_nodes_global[cluster]\n",
    "                             for cluster in clustering_machine.clusters]\n",
    "\n",
    "    validation_clusters_global = [ clustering_machine.sg_validation_nodes_global[cluster]\n",
    "                             for cluster in clustering_machine.clusters]\n",
    "\n",
    "    training_clusters_global = clustering_machine.sg_train_nodes_global\n",
    "\n",
    "    testing_clusters_global =  clustering_machine.sg_test_nodes_global\n",
    "\n",
    "    sg_edges_clusters_global = [  clustering_machine.sg_edges_global[cluster]\n",
    "                             for cluster in clustering_machine.clusters]\n",
    "\n",
    "    print('node cluster memeber ship: ', clustering_machine.cluster_membership)\n",
    "    print('isolated clusters are: ', isolate_clusters_global)\n",
    "    \n",
    "\n",
    "    print('training nodes global ids are: \\n', training_clusters_global)\n",
    "    \n",
    "    print('testing global clusters are: ', testing_clusters_global)\n",
    "    print('modeling global clusters are: ', modeling_clusters_global)\n",
    "    print('validation global clusters are: ', validation_clusters_global)\n",
    "\n",
    "    subgraphs = [clustering_machine.graph.subgraph(isolate_clusters_global[cluster]) \\\n",
    "                                               for cluster in clustering_machine.clusters]\n",
    "    plt.subplot(231)\n",
    "    nx.draw(whole_graph, with_labels=True, font_weight='bold')\n",
    "    # 2) the two halves of the graph\n",
    "    plt.subplot(232)\n",
    "    nx.draw(subgraphs[0], with_labels=True, font_weight='bold')\n",
    "    plt.subplot(233)\n",
    "    nx.draw(subgraphs[1], with_labels=True, font_weight='bold')\n",
    "    \n",
    "    print('Info about the mini_batch only with training nodes: ')\n",
    "    sg_mini_edges_clusters_global = [  clustering_machine.sg_mini_edges_global[cluster] for cluster in clustering_machine.clusters]\n",
    "    sg_mini_nodes_clusters_global = clustering_machine.sg_mini_nodes_global\n",
    "    print('mini train edges of each cluster, global ids: ', sg_mini_edges_clusters_global)\n",
    "    print('mini train overlapping nodes global ids: ', sg_mini_nodes_clusters_global)\n",
    "\n",
    "    mini_subgraphs = [clustering_machine.graph.subgraph(clustering_machine.accum_neighbor[cluster]) \\\n",
    "                                           for cluster in clustering_machine.clusters]\n",
    "    plt.subplot(234)\n",
    "    nx.draw(mini_subgraphs[0], with_labels=True, font_weight='bold')\n",
    "    plt.subplot(235)\n",
    "    nx.draw(mini_subgraphs[1], with_labels=True, font_weight='bold')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Trivial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global edge weight dictionary is: edge_weight_global_dict\n",
      "{(0, 1): 0.35355338, (1, 0): 0.35355338, (1, 3): 0.35355338, (3, 1): 0.35355338, (1, 2): 0.25, (2, 1): 0.25, (4, 2): 0.28867513, (2, 4): 0.28867513, (4, 6): 0.3333333, (6, 4): 0.3333333, (6, 7): 0.40824828, (7, 6): 0.40824828, (2, 5): 0.35355338, (5, 2): 0.35355338, (0, 0): 0.49999997, (1, 1): 0.25, (2, 2): 0.25, (3, 3): 0.49999997, (4, 4): 0.3333333, (5, 5): 0.49999997, (6, 6): 0.3333333, (7, 7): 0.49999997}\n"
     ]
    }
   ],
   "source": [
    "'''Trivial data'''\n",
    "edge_index = torch.tensor([[0, 1, 1, 3, 1, 2, 4, 2, 4, 6, 6, 7, 2, 5], \n",
    "                           [1, 0, 3, 1, 2, 1, 2, 4, 6, 4, 7, 6, 5, 2]])\n",
    "# features = torch.rand(10, 3)\n",
    "features = torch.tensor([[0, 0], [0, 1], [0, 2], [0, 3], [0, 4],  \n",
    "                           [0, 5], [0, 6], [0, 7] ], dtype = torch.float)\n",
    "# label = torch.tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "\n",
    "label = torch.tensor([0, 1, 1, 0, 1, 1, 1, 0])\n",
    "# print(features, features.shape)\n",
    "\n",
    "check_clustering_machine = ClusteringMachine(edge_index, features, label, partition_num = 2)\n",
    "check_clustering_machine.decompose(0.25, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mini batch train nodes of whole graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=orange>\n",
    "Note: the all_overlap and train_overlap are the same effects in the train process\n",
    "    \n",
    "These two differ in the validation part. Train_overlap will lose some overalpping in the validation nodes which affect F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node cluster memeber ship:  {0: 0, 1: 0, 2: 0, 3: 0, 4: 1, 5: 1, 6: 1, 7: 1}\n",
      "isolated clusters are:  [[0, 1, 2, 3], [4, 5, 6, 7]]\n",
      "training nodes global ids are: \n",
      " {0: tensor([3]), 1: tensor([4])}\n",
      "testing global clusters are:  {0: tensor([1]), 1: tensor([6])}\n",
      "modeling global clusters are:  [[1, 3], [4, 6]]\n",
      "validation global clusters are:  [tensor([0, 2]), tensor([5, 7])]\n",
      "Info about the mini_batch only with training nodes: \n",
      "mini train edges of each cluster, global ids:  [{(0, 1), (1, 3), (1, 2)}, {(1, 2), (6, 7), (4, 6), (2, 5), (2, 4)}]\n",
      "mini train overlapping nodes global ids:  {0: [0, 1, 2, 3], 1: [1, 2, 4, 5, 6, 7]}\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# mini-batch of the all_overlap\n",
    "clustering_machine = copy.deepcopy(check_clustering_machine)\n",
    "clustering_machine.mini_batch_train_clustering(2) # separate into two clusters\n",
    "check_clustering(clustering_machine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Layer ========================= Inside custom GCN: \n",
      "current embeddings:  torch.Size([4, 2]) \n",
      " tensor([[0., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 2.],\n",
      "        [0., 3.]], device='cuda:0')\n",
      "Inside the propagate, edge_index[idx]: \n",
      " torch.Size([10]) \n",
      " tensor([0, 1, 1, 1, 3, 2, 0, 1, 2, 3], device='cuda:0')\n",
      "Inside propogate, tmp :  torch.Size([10, 4]) \n",
      " tensor([[0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [2., 2., 2., 2.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [2., 2., 2., 2.],\n",
      "        [3., 3., 3., 3.]], device='cuda:0', grad_fn=<IndexSelectBackward>)\n",
      "inside the message custom_GCN: norm \n",
      " torch.Size([10]) \n",
      " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')\n",
      "inside the message custom_GCN: x_j \n",
      " torch.Size([10, 4]) \n",
      " tensor([[0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [2., 2., 2., 2.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [2., 2., 2., 2.],\n",
      "        [3., 3., 3., 3.]], device='cuda:0', grad_fn=<IndexSelectBackward>)\n",
      "Inside propogate, before scattering, out :  torch.Size([10, 4]) \n",
      " tensor([[0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [2., 2., 2., 2.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [2., 2., 2., 2.],\n",
      "        [3., 3., 3., 3.]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Inside propogate, before scattering, edge_index :  torch.Size([2, 10]) \n",
      " tensor([[0, 1, 1, 1, 3, 2, 0, 1, 2, 3],\n",
      "        [1, 3, 2, 0, 1, 1, 0, 1, 2, 3]], device='cuda:0')\n",
      "resulting embeddings:  torch.Size([4, 4]) \n",
      " tensor([[1., 1., 1., 1.],\n",
      "        [6., 6., 6., 6.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [4., 4., 4., 4.]], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "New Layer ========================= Inside custom GCN: \n",
      "current embeddings:  torch.Size([4, 4]) \n",
      " tensor([[1.4286, 1.4286, 1.4286, 0.0000],\n",
      "        [8.5714, 8.5714, 0.0000, 8.5714],\n",
      "        [4.2857, 4.2857, 4.2857, 0.0000],\n",
      "        [5.7143, 0.0000, 5.7143, 5.7143]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward>)\n",
      "Inside the propagate, edge_index[idx]: \n",
      " torch.Size([10]) \n",
      " tensor([0, 1, 1, 1, 3, 2, 0, 1, 2, 3], device='cuda:0')\n",
      "Inside propogate, tmp :  torch.Size([10, 2]) \n",
      " tensor([[ 4.2857,  4.2857],\n",
      "        [25.7143, 25.7143],\n",
      "        [25.7143, 25.7143],\n",
      "        [25.7143, 25.7143],\n",
      "        [17.1429, 17.1429],\n",
      "        [12.8571, 12.8571],\n",
      "        [ 4.2857,  4.2857],\n",
      "        [25.7143, 25.7143],\n",
      "        [12.8571, 12.8571],\n",
      "        [17.1429, 17.1429]], device='cuda:0', grad_fn=<IndexSelectBackward>)\n",
      "inside the message custom_GCN: norm \n",
      " torch.Size([10]) \n",
      " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')\n",
      "inside the message custom_GCN: x_j \n",
      " torch.Size([10, 2]) \n",
      " tensor([[ 4.2857,  4.2857],\n",
      "        [25.7143, 25.7143],\n",
      "        [25.7143, 25.7143],\n",
      "        [25.7143, 25.7143],\n",
      "        [17.1429, 17.1429],\n",
      "        [12.8571, 12.8571],\n",
      "        [ 4.2857,  4.2857],\n",
      "        [25.7143, 25.7143],\n",
      "        [12.8571, 12.8571],\n",
      "        [17.1429, 17.1429]], device='cuda:0', grad_fn=<IndexSelectBackward>)\n",
      "Inside propogate, before scattering, out :  torch.Size([10, 2]) \n",
      " tensor([[ 4.2857,  4.2857],\n",
      "        [25.7143, 25.7143],\n",
      "        [25.7143, 25.7143],\n",
      "        [25.7143, 25.7143],\n",
      "        [17.1429, 17.1429],\n",
      "        [12.8571, 12.8571],\n",
      "        [ 4.2857,  4.2857],\n",
      "        [25.7143, 25.7143],\n",
      "        [12.8571, 12.8571],\n",
      "        [17.1429, 17.1429]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Inside propogate, before scattering, edge_index :  torch.Size([2, 10]) \n",
      " tensor([[0, 1, 1, 1, 3, 2, 0, 1, 2, 3],\n",
      "        [1, 3, 2, 0, 1, 1, 0, 1, 2, 3]], device='cuda:0')\n",
      "resulting embeddings:  torch.Size([4, 2]) \n",
      " tensor([[30.0000, 30.0000],\n",
      "        [60.0000, 60.0000],\n",
      "        [38.5714, 38.5714],\n",
      "        [42.8571, 42.8571]], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "New Layer ========================= Inside custom GCN: \n",
      "current embeddings:  torch.Size([6, 2]) \n",
      " tensor([[0., 1.],\n",
      "        [0., 2.],\n",
      "        [0., 4.],\n",
      "        [0., 5.],\n",
      "        [0., 6.],\n",
      "        [0., 7.]], device='cuda:0')\n",
      "Inside the propagate, edge_index[idx]: \n",
      " torch.Size([16]) \n",
      " tensor([0, 4, 2, 1, 1, 1, 5, 4, 3, 2, 0, 1, 2, 3, 4, 5], device='cuda:0')\n",
      "Inside propogate, tmp :  torch.Size([16, 4]) \n",
      " tensor([[0.9999, 0.9999, 0.9999, 0.9999],\n",
      "        [5.9993, 5.9994, 5.9994, 5.9993],\n",
      "        [3.9996, 3.9996, 3.9996, 3.9996],\n",
      "        [1.9998, 1.9998, 1.9998, 1.9998],\n",
      "        [1.9998, 1.9998, 1.9998, 1.9998],\n",
      "        [1.9998, 1.9998, 1.9998, 1.9998],\n",
      "        [6.9992, 6.9992, 6.9993, 6.9992],\n",
      "        [5.9993, 5.9994, 5.9994, 5.9993],\n",
      "        [4.9995, 4.9995, 4.9995, 4.9995],\n",
      "        [3.9996, 3.9996, 3.9996, 3.9996],\n",
      "        [0.9999, 0.9999, 0.9999, 0.9999],\n",
      "        [1.9998, 1.9998, 1.9998, 1.9998],\n",
      "        [3.9996, 3.9996, 3.9996, 3.9996],\n",
      "        [4.9995, 4.9995, 4.9995, 4.9995],\n",
      "        [5.9993, 5.9994, 5.9994, 5.9993],\n",
      "        [6.9992, 6.9992, 6.9993, 6.9992]], device='cuda:0',\n",
      "       grad_fn=<IndexSelectBackward>)\n",
      "inside the message custom_GCN: norm \n",
      " torch.Size([16]) \n",
      " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "       device='cuda:0')\n",
      "inside the message custom_GCN: x_j \n",
      " torch.Size([16, 4]) \n",
      " tensor([[0.9999, 0.9999, 0.9999, 0.9999],\n",
      "        [5.9993, 5.9994, 5.9994, 5.9993],\n",
      "        [3.9996, 3.9996, 3.9996, 3.9996],\n",
      "        [1.9998, 1.9998, 1.9998, 1.9998],\n",
      "        [1.9998, 1.9998, 1.9998, 1.9998],\n",
      "        [1.9998, 1.9998, 1.9998, 1.9998],\n",
      "        [6.9992, 6.9992, 6.9993, 6.9992],\n",
      "        [5.9993, 5.9994, 5.9994, 5.9993],\n",
      "        [4.9995, 4.9995, 4.9995, 4.9995],\n",
      "        [3.9996, 3.9996, 3.9996, 3.9996],\n",
      "        [0.9999, 0.9999, 0.9999, 0.9999],\n",
      "        [1.9998, 1.9998, 1.9998, 1.9998],\n",
      "        [3.9996, 3.9996, 3.9996, 3.9996],\n",
      "        [4.9995, 4.9995, 4.9995, 4.9995],\n",
      "        [5.9993, 5.9994, 5.9994, 5.9993],\n",
      "        [6.9992, 6.9992, 6.9993, 6.9992]], device='cuda:0',\n",
      "       grad_fn=<IndexSelectBackward>)\n",
      "Inside propogate, before scattering, out :  torch.Size([16, 4]) \n",
      " tensor([[0.9999, 0.9999, 0.9999, 0.9999],\n",
      "        [5.9993, 5.9994, 5.9994, 5.9993],\n",
      "        [3.9996, 3.9996, 3.9996, 3.9996],\n",
      "        [1.9998, 1.9998, 1.9998, 1.9998],\n",
      "        [1.9998, 1.9998, 1.9998, 1.9998],\n",
      "        [1.9998, 1.9998, 1.9998, 1.9998],\n",
      "        [6.9992, 6.9992, 6.9993, 6.9992],\n",
      "        [5.9993, 5.9994, 5.9994, 5.9993],\n",
      "        [4.9995, 4.9995, 4.9995, 4.9995],\n",
      "        [3.9996, 3.9996, 3.9996, 3.9996],\n",
      "        [0.9999, 0.9999, 0.9999, 0.9999],\n",
      "        [1.9998, 1.9998, 1.9998, 1.9998],\n",
      "        [3.9996, 3.9996, 3.9996, 3.9996],\n",
      "        [4.9995, 4.9995, 4.9995, 4.9995],\n",
      "        [5.9993, 5.9994, 5.9994, 5.9993],\n",
      "        [6.9992, 6.9992, 6.9993, 6.9992]], device='cuda:0',\n",
      "       grad_fn=<MulBackward0>)\n",
      "Inside propogate, before scattering, edge_index :  torch.Size([2, 16]) \n",
      " tensor([[0, 4, 2, 1, 1, 1, 5, 4, 3, 2, 0, 1, 2, 3, 4, 5],\n",
      "        [1, 5, 4, 3, 2, 0, 4, 2, 1, 1, 0, 1, 2, 3, 4, 5]], device='cuda:0')\n",
      "resulting embeddings:  torch.Size([6, 4]) \n",
      " tensor([[ 2.9996,  2.9996,  2.9996,  2.9996],\n",
      "        [11.9986, 11.9986, 11.9986, 11.9986],\n",
      "        [11.9986, 11.9986, 11.9986, 11.9986],\n",
      "        [ 6.9991,  6.9992,  6.9992,  6.9991],\n",
      "        [16.9981, 16.9981, 16.9981, 16.9981],\n",
      "        [12.9985, 12.9985, 12.9985, 12.9985]], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "New Layer ========================= Inside custom GCN: \n",
      "current embeddings:  torch.Size([6, 4]) \n",
      " tensor([[ 0.0000,  0.0000,  4.2851,  0.0000],\n",
      "        [17.1409, 17.1409, 17.1409, 17.1409],\n",
      "        [17.1409,  0.0000, 17.1409, 17.1409],\n",
      "        [ 9.9988,  0.0000,  0.0000,  0.0000],\n",
      "        [24.2829, 24.2830, 24.2830,  0.0000],\n",
      "        [18.5693,  0.0000, 18.5693, 18.5693]], device='cuda:0',\n",
      "       grad_fn=<FusedDropoutBackward>)\n",
      "Inside the propagate, edge_index[idx]: \n",
      " torch.Size([16]) \n",
      " tensor([0, 4, 2, 1, 1, 1, 5, 4, 3, 2, 0, 1, 2, 3, 4, 5], device='cuda:0')\n",
      "Inside propogate, tmp :  torch.Size([16, 2]) \n",
      " tensor([[ 4.2855,  4.2847],\n",
      "        [72.8555, 72.8409],\n",
      "        [51.4273, 51.4170],\n",
      "        [68.5697, 68.5560],\n",
      "        [68.5697, 68.5560],\n",
      "        [68.5697, 68.5560],\n",
      "        [55.7129, 55.7018],\n",
      "        [72.8555, 72.8409],\n",
      "        [ 9.9997,  9.9977],\n",
      "        [51.4273, 51.4170],\n",
      "        [ 4.2855,  4.2847],\n",
      "        [68.5697, 68.5560],\n",
      "        [51.4273, 51.4170],\n",
      "        [ 9.9997,  9.9977],\n",
      "        [72.8555, 72.8409],\n",
      "        [55.7129, 55.7018]], device='cuda:0', grad_fn=<IndexSelectBackward>)\n",
      "inside the message custom_GCN: norm \n",
      " torch.Size([16]) \n",
      " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "       device='cuda:0')\n",
      "inside the message custom_GCN: x_j \n",
      " torch.Size([16, 2]) \n",
      " tensor([[ 4.2855,  4.2847],\n",
      "        [72.8555, 72.8409],\n",
      "        [51.4273, 51.4170],\n",
      "        [68.5697, 68.5560],\n",
      "        [68.5697, 68.5560],\n",
      "        [68.5697, 68.5560],\n",
      "        [55.7129, 55.7018],\n",
      "        [72.8555, 72.8409],\n",
      "        [ 9.9997,  9.9977],\n",
      "        [51.4273, 51.4170],\n",
      "        [ 4.2855,  4.2847],\n",
      "        [68.5697, 68.5560],\n",
      "        [51.4273, 51.4170],\n",
      "        [ 9.9997,  9.9977],\n",
      "        [72.8555, 72.8409],\n",
      "        [55.7129, 55.7018]], device='cuda:0', grad_fn=<IndexSelectBackward>)\n",
      "Inside propogate, before scattering, out :  torch.Size([16, 2]) \n",
      " tensor([[ 4.2855,  4.2847],\n",
      "        [72.8555, 72.8409],\n",
      "        [51.4273, 51.4170],\n",
      "        [68.5697, 68.5560],\n",
      "        [68.5697, 68.5560],\n",
      "        [68.5697, 68.5560],\n",
      "        [55.7129, 55.7018],\n",
      "        [72.8555, 72.8409],\n",
      "        [ 9.9997,  9.9977],\n",
      "        [51.4273, 51.4170],\n",
      "        [ 4.2855,  4.2847],\n",
      "        [68.5697, 68.5560],\n",
      "        [51.4273, 51.4170],\n",
      "        [ 9.9997,  9.9977],\n",
      "        [72.8555, 72.8409],\n",
      "        [55.7129, 55.7018]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Inside propogate, before scattering, edge_index :  torch.Size([2, 16]) \n",
      " tensor([[0, 4, 2, 1, 1, 1, 5, 4, 3, 2, 0, 1, 2, 3, 4, 5],\n",
      "        [1, 5, 4, 3, 2, 0, 4, 2, 1, 1, 0, 1, 2, 3, 4, 5]], device='cuda:0')\n",
      "resulting embeddings:  torch.Size([6, 2]) \n",
      " tensor([[ 72.8553,  72.8405],\n",
      "        [134.2823, 134.2552],\n",
      "        [192.8526, 192.8138],\n",
      "        [ 78.5695,  78.5536],\n",
      "        [179.9958, 179.9596],\n",
      "        [128.5685, 128.5426]], device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "gcn_trainer_batch = ClusterGCNTrainer_mini_Train(clustering_machine, 2, 2, input_layers = [4], dropout=0.3)\n",
    "gcn_trainer_batch.train(1, 0.0001, 0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In validation using local cluster for the cluster:  0\n",
      "New Layer ========================= Inside custom GCN: \n",
      "current embeddings:  torch.Size([4, 2]) \n",
      " tensor([[0., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 2.],\n",
      "        [0., 3.]], device='cuda:0')\n",
      "Inside the propagate, edge_index[idx]: \n",
      " torch.Size([10]) \n",
      " tensor([0, 1, 1, 1, 3, 2, 0, 1, 2, 3], device='cuda:0')\n",
      "Inside propogate, tmp :  torch.Size([10, 4]) \n",
      " tensor([[0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.9998, 0.9998, 0.9998, 0.9998],\n",
      "        [0.9998, 0.9998, 0.9998, 0.9998],\n",
      "        [0.9998, 0.9998, 0.9998, 0.9998],\n",
      "        [2.9994, 2.9994, 2.9994, 2.9994],\n",
      "        [1.9996, 1.9996, 1.9996, 1.9996],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.9998, 0.9998, 0.9998, 0.9998],\n",
      "        [1.9996, 1.9996, 1.9996, 1.9996],\n",
      "        [2.9994, 2.9994, 2.9994, 2.9994]], device='cuda:0',\n",
      "       grad_fn=<IndexSelectBackward>)\n",
      "inside the message custom_GCN: norm \n",
      " torch.Size([10]) \n",
      " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')\n",
      "inside the message custom_GCN: x_j \n",
      " torch.Size([10, 4]) \n",
      " tensor([[0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.9998, 0.9998, 0.9998, 0.9998],\n",
      "        [0.9998, 0.9998, 0.9998, 0.9998],\n",
      "        [0.9998, 0.9998, 0.9998, 0.9998],\n",
      "        [2.9994, 2.9994, 2.9994, 2.9994],\n",
      "        [1.9996, 1.9996, 1.9996, 1.9996],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.9998, 0.9998, 0.9998, 0.9998],\n",
      "        [1.9996, 1.9996, 1.9996, 1.9996],\n",
      "        [2.9994, 2.9994, 2.9994, 2.9994]], device='cuda:0',\n",
      "       grad_fn=<IndexSelectBackward>)\n",
      "Inside propogate, before scattering, out :  torch.Size([10, 4]) \n",
      " tensor([[0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.9998, 0.9998, 0.9998, 0.9998],\n",
      "        [0.9998, 0.9998, 0.9998, 0.9998],\n",
      "        [0.9998, 0.9998, 0.9998, 0.9998],\n",
      "        [2.9994, 2.9994, 2.9994, 2.9994],\n",
      "        [1.9996, 1.9996, 1.9996, 1.9996],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.9998, 0.9998, 0.9998, 0.9998],\n",
      "        [1.9996, 1.9996, 1.9996, 1.9996],\n",
      "        [2.9994, 2.9994, 2.9994, 2.9994]], device='cuda:0',\n",
      "       grad_fn=<MulBackward0>)\n",
      "Inside propogate, before scattering, edge_index :  torch.Size([2, 10]) \n",
      " tensor([[0, 1, 1, 1, 3, 2, 0, 1, 2, 3],\n",
      "        [1, 3, 2, 0, 1, 1, 0, 1, 2, 3]], device='cuda:0')\n",
      "resulting embeddings:  torch.Size([4, 4]) \n",
      " tensor([[0.9996, 0.9996, 0.9996, 0.9996],\n",
      "        [5.9987, 5.9987, 5.9987, 5.9987],\n",
      "        [2.9993, 2.9993, 2.9993, 2.9993],\n",
      "        [3.9991, 3.9991, 3.9991, 3.9991]], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "New Layer ========================= Inside custom GCN: \n",
      "current embeddings:  torch.Size([4, 4]) \n",
      " tensor([[0.9996, 0.9996, 0.9996, 0.9996],\n",
      "        [5.9987, 5.9987, 5.9987, 5.9987],\n",
      "        [2.9993, 2.9993, 2.9993, 2.9993],\n",
      "        [3.9991, 3.9991, 3.9991, 3.9991]], device='cuda:0',\n",
      "       grad_fn=<ReluBackward0>)\n",
      "Inside the propagate, edge_index[idx]: \n",
      " torch.Size([10]) \n",
      " tensor([0, 1, 1, 1, 3, 2, 0, 1, 2, 3], device='cuda:0')\n",
      "Inside propogate, tmp :  torch.Size([10, 2]) \n",
      " tensor([[ 3.9987,  3.9983],\n",
      "        [23.9953, 23.9932],\n",
      "        [23.9953, 23.9932],\n",
      "        [23.9953, 23.9932],\n",
      "        [15.9966, 15.9953],\n",
      "        [11.9973, 11.9963],\n",
      "        [ 3.9987,  3.9983],\n",
      "        [23.9953, 23.9932],\n",
      "        [11.9973, 11.9963],\n",
      "        [15.9966, 15.9953]], device='cuda:0', grad_fn=<IndexSelectBackward>)\n",
      "inside the message custom_GCN: norm \n",
      " torch.Size([10]) \n",
      " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')\n",
      "inside the message custom_GCN: x_j \n",
      " torch.Size([10, 2]) \n",
      " tensor([[ 3.9987,  3.9983],\n",
      "        [23.9953, 23.9932],\n",
      "        [23.9953, 23.9932],\n",
      "        [23.9953, 23.9932],\n",
      "        [15.9966, 15.9953],\n",
      "        [11.9973, 11.9963],\n",
      "        [ 3.9987,  3.9983],\n",
      "        [23.9953, 23.9932],\n",
      "        [11.9973, 11.9963],\n",
      "        [15.9966, 15.9953]], device='cuda:0', grad_fn=<IndexSelectBackward>)\n",
      "Inside propogate, before scattering, out :  torch.Size([10, 2]) \n",
      " tensor([[ 3.9987,  3.9983],\n",
      "        [23.9953, 23.9932],\n",
      "        [23.9953, 23.9932],\n",
      "        [23.9953, 23.9932],\n",
      "        [15.9966, 15.9953],\n",
      "        [11.9973, 11.9963],\n",
      "        [ 3.9987,  3.9983],\n",
      "        [23.9953, 23.9932],\n",
      "        [11.9973, 11.9963],\n",
      "        [15.9966, 15.9953]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Inside propogate, before scattering, edge_index :  torch.Size([2, 10]) \n",
      " tensor([[0, 1, 1, 1, 3, 2, 0, 1, 2, 3],\n",
      "        [1, 3, 2, 0, 1, 1, 0, 1, 2, 3]], device='cuda:0')\n",
      "resulting embeddings:  torch.Size([4, 2]) \n",
      " tensor([[27.9940, 27.9914],\n",
      "        [55.9880, 55.9830],\n",
      "        [35.9927, 35.9894],\n",
      "        [39.9920, 39.9884]], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In validation global using whole graph for the cluster:  0\n",
      "New Layer ========================= Inside custom GCN: \n",
      "current embeddings:  torch.Size([8, 2]) \n",
      " tensor([[0., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 2.],\n",
      "        [0., 3.],\n",
      "        [0., 4.],\n",
      "        [0., 5.],\n",
      "        [0., 6.],\n",
      "        [0., 7.]], device='cuda:0')\n",
      "Inside the propagate, edge_index[idx]: \n",
      " torch.Size([22]) \n",
      " tensor([0, 1, 1, 3, 1, 2, 4, 2, 4, 6, 6, 7, 2, 5, 0, 1, 2, 3, 4, 5, 6, 7],\n",
      "       device='cuda:0')\n",
      "Inside propogate, tmp :  torch.Size([22, 4]) \n",
      " tensor([[0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.9998, 0.9998, 0.9998, 0.9998],\n",
      "        [0.9998, 0.9998, 0.9998, 0.9998],\n",
      "        [2.9994, 2.9994, 2.9994, 2.9994],\n",
      "        [0.9998, 0.9998, 0.9998, 0.9998],\n",
      "        [1.9996, 1.9996, 1.9996, 1.9996],\n",
      "        [3.9992, 3.9992, 3.9992, 3.9992],\n",
      "        [1.9996, 1.9996, 1.9996, 1.9996],\n",
      "        [3.9992, 3.9992, 3.9992, 3.9992],\n",
      "        [5.9988, 5.9988, 5.9989, 5.9988],\n",
      "        [5.9988, 5.9988, 5.9989, 5.9988],\n",
      "        [6.9986, 6.9987, 6.9987, 6.9986],\n",
      "        [1.9996, 1.9996, 1.9996, 1.9996],\n",
      "        [4.9990, 4.9990, 4.9990, 4.9990],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.9998, 0.9998, 0.9998, 0.9998],\n",
      "        [1.9996, 1.9996, 1.9996, 1.9996],\n",
      "        [2.9994, 2.9994, 2.9994, 2.9994],\n",
      "        [3.9992, 3.9992, 3.9992, 3.9992],\n",
      "        [4.9990, 4.9990, 4.9990, 4.9990],\n",
      "        [5.9988, 5.9988, 5.9989, 5.9988],\n",
      "        [6.9986, 6.9987, 6.9987, 6.9986]], device='cuda:0',\n",
      "       grad_fn=<IndexSelectBackward>)\n",
      "inside the message custom_GCN: norm \n",
      " torch.Size([22]) \n",
      " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1.], device='cuda:0')\n",
      "inside the message custom_GCN: x_j \n",
      " torch.Size([22, 4]) \n",
      " tensor([[0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.9998, 0.9998, 0.9998, 0.9998],\n",
      "        [0.9998, 0.9998, 0.9998, 0.9998],\n",
      "        [2.9994, 2.9994, 2.9994, 2.9994],\n",
      "        [0.9998, 0.9998, 0.9998, 0.9998],\n",
      "        [1.9996, 1.9996, 1.9996, 1.9996],\n",
      "        [3.9992, 3.9992, 3.9992, 3.9992],\n",
      "        [1.9996, 1.9996, 1.9996, 1.9996],\n",
      "        [3.9992, 3.9992, 3.9992, 3.9992],\n",
      "        [5.9988, 5.9988, 5.9989, 5.9988],\n",
      "        [5.9988, 5.9988, 5.9989, 5.9988],\n",
      "        [6.9986, 6.9987, 6.9987, 6.9986],\n",
      "        [1.9996, 1.9996, 1.9996, 1.9996],\n",
      "        [4.9990, 4.9990, 4.9990, 4.9990],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.9998, 0.9998, 0.9998, 0.9998],\n",
      "        [1.9996, 1.9996, 1.9996, 1.9996],\n",
      "        [2.9994, 2.9994, 2.9994, 2.9994],\n",
      "        [3.9992, 3.9992, 3.9992, 3.9992],\n",
      "        [4.9990, 4.9990, 4.9990, 4.9990],\n",
      "        [5.9988, 5.9988, 5.9989, 5.9988],\n",
      "        [6.9986, 6.9987, 6.9987, 6.9986]], device='cuda:0',\n",
      "       grad_fn=<IndexSelectBackward>)\n",
      "Inside propogate, before scattering, out :  torch.Size([22, 4]) \n",
      " tensor([[0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.9998, 0.9998, 0.9998, 0.9998],\n",
      "        [0.9998, 0.9998, 0.9998, 0.9998],\n",
      "        [2.9994, 2.9994, 2.9994, 2.9994],\n",
      "        [0.9998, 0.9998, 0.9998, 0.9998],\n",
      "        [1.9996, 1.9996, 1.9996, 1.9996],\n",
      "        [3.9992, 3.9992, 3.9992, 3.9992],\n",
      "        [1.9996, 1.9996, 1.9996, 1.9996],\n",
      "        [3.9992, 3.9992, 3.9992, 3.9992],\n",
      "        [5.9988, 5.9988, 5.9989, 5.9988],\n",
      "        [5.9988, 5.9988, 5.9989, 5.9988],\n",
      "        [6.9986, 6.9987, 6.9987, 6.9986],\n",
      "        [1.9996, 1.9996, 1.9996, 1.9996],\n",
      "        [4.9990, 4.9990, 4.9990, 4.9990],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.9998, 0.9998, 0.9998, 0.9998],\n",
      "        [1.9996, 1.9996, 1.9996, 1.9996],\n",
      "        [2.9994, 2.9994, 2.9994, 2.9994],\n",
      "        [3.9992, 3.9992, 3.9992, 3.9992],\n",
      "        [4.9990, 4.9990, 4.9990, 4.9990],\n",
      "        [5.9988, 5.9988, 5.9989, 5.9988],\n",
      "        [6.9986, 6.9987, 6.9987, 6.9986]], device='cuda:0',\n",
      "       grad_fn=<MulBackward0>)\n",
      "Inside propogate, before scattering, edge_index :  torch.Size([2, 22]) \n",
      " tensor([[0, 1, 1, 3, 1, 2, 4, 2, 4, 6, 6, 7, 2, 5, 0, 1, 2, 3, 4, 5, 6, 7],\n",
      "        [1, 0, 3, 1, 2, 1, 2, 4, 6, 4, 7, 6, 5, 2, 0, 1, 2, 3, 4, 5, 6, 7]],\n",
      "       device='cuda:0')\n",
      "resulting embeddings:  torch.Size([8, 4]) \n",
      " tensor([[ 0.9996,  0.9996,  0.9996,  0.9996],\n",
      "        [ 5.9987,  5.9987,  5.9987,  5.9987],\n",
      "        [11.9975, 11.9975, 11.9975, 11.9975],\n",
      "        [ 3.9991,  3.9991,  3.9991,  3.9991],\n",
      "        [11.9975, 11.9975, 11.9975, 11.9975],\n",
      "        [ 6.9985,  6.9985,  6.9985,  6.9985],\n",
      "        [16.9965, 16.9966, 16.9966, 16.9965],\n",
      "        [12.9973, 12.9973, 12.9974, 12.9973]], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "New Layer ========================= Inside custom GCN: \n",
      "current embeddings:  torch.Size([8, 4]) \n",
      " tensor([[ 0.9996,  0.9996,  0.9996,  0.9996],\n",
      "        [ 5.9987,  5.9987,  5.9987,  5.9987],\n",
      "        [11.9975, 11.9975, 11.9975, 11.9975],\n",
      "        [ 3.9991,  3.9991,  3.9991,  3.9991],\n",
      "        [11.9975, 11.9975, 11.9975, 11.9975],\n",
      "        [ 6.9985,  6.9985,  6.9985,  6.9985],\n",
      "        [16.9965, 16.9966, 16.9966, 16.9965],\n",
      "        [12.9973, 12.9973, 12.9974, 12.9973]], device='cuda:0',\n",
      "       grad_fn=<ReluBackward0>)\n",
      "Inside the propagate, edge_index[idx]: \n",
      " torch.Size([22]) \n",
      " tensor([0, 1, 1, 3, 1, 2, 4, 2, 4, 6, 6, 7, 2, 5, 0, 1, 2, 3, 4, 5, 6, 7],\n",
      "       device='cuda:0')\n",
      "Inside propogate, tmp :  torch.Size([22, 2]) \n",
      " tensor([[ 3.9987,  3.9983],\n",
      "        [23.9953, 23.9932],\n",
      "        [23.9953, 23.9932],\n",
      "        [15.9966, 15.9953],\n",
      "        [23.9953, 23.9932],\n",
      "        [47.9912, 47.9871],\n",
      "        [47.9912, 47.9871],\n",
      "        [47.9912, 47.9871],\n",
      "        [47.9912, 47.9871],\n",
      "        [67.9878, 67.9820],\n",
      "        [67.9878, 67.9820],\n",
      "        [51.9905, 51.9861],\n",
      "        [47.9912, 47.9871],\n",
      "        [27.9946, 27.9922],\n",
      "        [ 3.9987,  3.9983],\n",
      "        [23.9953, 23.9932],\n",
      "        [47.9912, 47.9871],\n",
      "        [15.9966, 15.9953],\n",
      "        [47.9912, 47.9871],\n",
      "        [27.9946, 27.9922],\n",
      "        [67.9878, 67.9820],\n",
      "        [51.9905, 51.9861]], device='cuda:0', grad_fn=<IndexSelectBackward>)\n",
      "inside the message custom_GCN: norm \n",
      " torch.Size([22]) \n",
      " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1.], device='cuda:0')\n",
      "inside the message custom_GCN: x_j \n",
      " torch.Size([22, 2]) \n",
      " tensor([[ 3.9987,  3.9983],\n",
      "        [23.9953, 23.9932],\n",
      "        [23.9953, 23.9932],\n",
      "        [15.9966, 15.9953],\n",
      "        [23.9953, 23.9932],\n",
      "        [47.9912, 47.9871],\n",
      "        [47.9912, 47.9871],\n",
      "        [47.9912, 47.9871],\n",
      "        [47.9912, 47.9871],\n",
      "        [67.9878, 67.9820],\n",
      "        [67.9878, 67.9820],\n",
      "        [51.9905, 51.9861],\n",
      "        [47.9912, 47.9871],\n",
      "        [27.9946, 27.9922],\n",
      "        [ 3.9987,  3.9983],\n",
      "        [23.9953, 23.9932],\n",
      "        [47.9912, 47.9871],\n",
      "        [15.9966, 15.9953],\n",
      "        [47.9912, 47.9871],\n",
      "        [27.9946, 27.9922],\n",
      "        [67.9878, 67.9820],\n",
      "        [51.9905, 51.9861]], device='cuda:0', grad_fn=<IndexSelectBackward>)\n",
      "Inside propogate, before scattering, out :  torch.Size([22, 2]) \n",
      " tensor([[ 3.9987,  3.9983],\n",
      "        [23.9953, 23.9932],\n",
      "        [23.9953, 23.9932],\n",
      "        [15.9966, 15.9953],\n",
      "        [23.9953, 23.9932],\n",
      "        [47.9912, 47.9871],\n",
      "        [47.9912, 47.9871],\n",
      "        [47.9912, 47.9871],\n",
      "        [47.9912, 47.9871],\n",
      "        [67.9878, 67.9820],\n",
      "        [67.9878, 67.9820],\n",
      "        [51.9905, 51.9861],\n",
      "        [47.9912, 47.9871],\n",
      "        [27.9946, 27.9922],\n",
      "        [ 3.9987,  3.9983],\n",
      "        [23.9953, 23.9932],\n",
      "        [47.9912, 47.9871],\n",
      "        [15.9966, 15.9953],\n",
      "        [47.9912, 47.9871],\n",
      "        [27.9946, 27.9922],\n",
      "        [67.9878, 67.9820],\n",
      "        [51.9905, 51.9861]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Inside propogate, before scattering, edge_index :  torch.Size([2, 22]) \n",
      " tensor([[0, 1, 1, 3, 1, 2, 4, 2, 4, 6, 6, 7, 2, 5, 0, 1, 2, 3, 4, 5, 6, 7],\n",
      "        [1, 0, 3, 1, 2, 1, 2, 4, 6, 4, 7, 6, 5, 2, 0, 1, 2, 3, 4, 5, 6, 7]],\n",
      "       device='cuda:0')\n",
      "resulting embeddings:  torch.Size([8, 2]) \n",
      " tensor([[ 27.9940,  27.9914],\n",
      "        [ 91.9819,  91.9738],\n",
      "        [147.9724, 147.9595],\n",
      "        [ 39.9920,  39.9884],\n",
      "        [163.9704, 163.9560],\n",
      "        [ 75.9859,  75.9792],\n",
      "        [167.9697, 167.9550],\n",
      "        [119.9785, 119.9679]], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In validation using local cluster for the cluster:  1\n",
      "New Layer ========================= Inside custom GCN: \n",
      "current embeddings:  torch.Size([4, 2]) \n",
      " tensor([[0., 4.],\n",
      "        [0., 5.],\n",
      "        [0., 6.],\n",
      "        [0., 7.]], device='cuda:0')\n",
      "Inside the propagate, edge_index[idx]: \n",
      " torch.Size([8]) \n",
      " tensor([2, 0, 3, 2, 0, 1, 2, 3], device='cuda:0')\n",
      "Inside propogate, tmp :  torch.Size([8, 4]) \n",
      " tensor([[5.9988, 5.9988, 5.9989, 5.9988],\n",
      "        [3.9992, 3.9992, 3.9992, 3.9992],\n",
      "        [6.9986, 6.9987, 6.9987, 6.9986],\n",
      "        [5.9988, 5.9988, 5.9989, 5.9988],\n",
      "        [3.9992, 3.9992, 3.9992, 3.9992],\n",
      "        [4.9990, 4.9990, 4.9990, 4.9990],\n",
      "        [5.9988, 5.9988, 5.9989, 5.9988],\n",
      "        [6.9986, 6.9987, 6.9987, 6.9986]], device='cuda:0',\n",
      "       grad_fn=<IndexSelectBackward>)\n",
      "inside the message custom_GCN: norm \n",
      " torch.Size([8]) \n",
      " tensor([1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')\n",
      "inside the message custom_GCN: x_j \n",
      " torch.Size([8, 4]) \n",
      " tensor([[5.9988, 5.9988, 5.9989, 5.9988],\n",
      "        [3.9992, 3.9992, 3.9992, 3.9992],\n",
      "        [6.9986, 6.9987, 6.9987, 6.9986],\n",
      "        [5.9988, 5.9988, 5.9989, 5.9988],\n",
      "        [3.9992, 3.9992, 3.9992, 3.9992],\n",
      "        [4.9990, 4.9990, 4.9990, 4.9990],\n",
      "        [5.9988, 5.9988, 5.9989, 5.9988],\n",
      "        [6.9986, 6.9987, 6.9987, 6.9986]], device='cuda:0',\n",
      "       grad_fn=<IndexSelectBackward>)\n",
      "Inside propogate, before scattering, out :  torch.Size([8, 4]) \n",
      " tensor([[5.9988, 5.9988, 5.9989, 5.9988],\n",
      "        [3.9992, 3.9992, 3.9992, 3.9992],\n",
      "        [6.9986, 6.9987, 6.9987, 6.9986],\n",
      "        [5.9988, 5.9988, 5.9989, 5.9988],\n",
      "        [3.9992, 3.9992, 3.9992, 3.9992],\n",
      "        [4.9990, 4.9990, 4.9990, 4.9990],\n",
      "        [5.9988, 5.9988, 5.9989, 5.9988],\n",
      "        [6.9986, 6.9987, 6.9987, 6.9986]], device='cuda:0',\n",
      "       grad_fn=<MulBackward0>)\n",
      "Inside propogate, before scattering, edge_index :  torch.Size([2, 8]) \n",
      " tensor([[2, 0, 3, 2, 0, 1, 2, 3],\n",
      "        [3, 2, 2, 0, 0, 1, 2, 3]], device='cuda:0')\n",
      "resulting embeddings:  torch.Size([4, 4]) \n",
      " tensor([[ 9.9979,  9.9979,  9.9979,  9.9979],\n",
      "        [ 4.9989,  4.9989,  4.9989,  4.9989],\n",
      "        [16.9965, 16.9966, 16.9966, 16.9965],\n",
      "        [12.9973, 12.9973, 12.9974, 12.9973]], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "New Layer ========================= Inside custom GCN: \n",
      "current embeddings:  torch.Size([4, 4]) \n",
      " tensor([[ 9.9979,  9.9979,  9.9979,  9.9979],\n",
      "        [ 4.9989,  4.9989,  4.9989,  4.9989],\n",
      "        [16.9965, 16.9966, 16.9966, 16.9965],\n",
      "        [12.9973, 12.9973, 12.9974, 12.9973]], device='cuda:0',\n",
      "       grad_fn=<ReluBackward0>)\n",
      "Inside the propagate, edge_index[idx]: \n",
      " torch.Size([8]) \n",
      " tensor([2, 0, 3, 2, 0, 1, 2, 3], device='cuda:0')\n",
      "Inside propogate, tmp :  torch.Size([8, 2]) \n",
      " tensor([[67.9878, 67.9820],\n",
      "        [39.9926, 39.9891],\n",
      "        [51.9905, 51.9861],\n",
      "        [67.9878, 67.9820],\n",
      "        [39.9926, 39.9891],\n",
      "        [19.9960, 19.9942],\n",
      "        [67.9878, 67.9820],\n",
      "        [51.9905, 51.9861]], device='cuda:0', grad_fn=<IndexSelectBackward>)\n",
      "inside the message custom_GCN: norm \n",
      " torch.Size([8]) \n",
      " tensor([1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')\n",
      "inside the message custom_GCN: x_j \n",
      " torch.Size([8, 2]) \n",
      " tensor([[67.9878, 67.9820],\n",
      "        [39.9926, 39.9891],\n",
      "        [51.9905, 51.9861],\n",
      "        [67.9878, 67.9820],\n",
      "        [39.9926, 39.9891],\n",
      "        [19.9960, 19.9942],\n",
      "        [67.9878, 67.9820],\n",
      "        [51.9905, 51.9861]], device='cuda:0', grad_fn=<IndexSelectBackward>)\n",
      "Inside propogate, before scattering, out :  torch.Size([8, 2]) \n",
      " tensor([[67.9878, 67.9820],\n",
      "        [39.9926, 39.9891],\n",
      "        [51.9905, 51.9861],\n",
      "        [67.9878, 67.9820],\n",
      "        [39.9926, 39.9891],\n",
      "        [19.9960, 19.9942],\n",
      "        [67.9878, 67.9820],\n",
      "        [51.9905, 51.9861]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Inside propogate, before scattering, edge_index :  torch.Size([2, 8]) \n",
      " tensor([[2, 0, 3, 2, 0, 1, 2, 3],\n",
      "        [3, 2, 2, 0, 0, 1, 2, 3]], device='cuda:0')\n",
      "resulting embeddings:  torch.Size([4, 2]) \n",
      " tensor([[107.9805, 107.9710],\n",
      "        [ 19.9960,  19.9941],\n",
      "        [159.9710, 159.9571],\n",
      "        [119.9785, 119.9679]], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In validation global using whole graph for the cluster:  1\n",
      "New Layer ========================= Inside custom GCN: \n",
      "current embeddings:  torch.Size([8, 2]) \n",
      " tensor([[0., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 2.],\n",
      "        [0., 3.],\n",
      "        [0., 4.],\n",
      "        [0., 5.],\n",
      "        [0., 6.],\n",
      "        [0., 7.]], device='cuda:0')\n",
      "Inside the propagate, edge_index[idx]: \n",
      " torch.Size([22]) \n",
      " tensor([0, 1, 1, 3, 1, 2, 4, 2, 4, 6, 6, 7, 2, 5, 0, 1, 2, 3, 4, 5, 6, 7],\n",
      "       device='cuda:0')\n",
      "Inside propogate, tmp :  torch.Size([22, 4]) \n",
      " tensor([[0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.9998, 0.9998, 0.9998, 0.9998],\n",
      "        [0.9998, 0.9998, 0.9998, 0.9998],\n",
      "        [2.9994, 2.9994, 2.9994, 2.9994],\n",
      "        [0.9998, 0.9998, 0.9998, 0.9998],\n",
      "        [1.9996, 1.9996, 1.9996, 1.9996],\n",
      "        [3.9992, 3.9992, 3.9992, 3.9992],\n",
      "        [1.9996, 1.9996, 1.9996, 1.9996],\n",
      "        [3.9992, 3.9992, 3.9992, 3.9992],\n",
      "        [5.9988, 5.9988, 5.9989, 5.9988],\n",
      "        [5.9988, 5.9988, 5.9989, 5.9988],\n",
      "        [6.9986, 6.9987, 6.9987, 6.9986],\n",
      "        [1.9996, 1.9996, 1.9996, 1.9996],\n",
      "        [4.9990, 4.9990, 4.9990, 4.9990],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.9998, 0.9998, 0.9998, 0.9998],\n",
      "        [1.9996, 1.9996, 1.9996, 1.9996],\n",
      "        [2.9994, 2.9994, 2.9994, 2.9994],\n",
      "        [3.9992, 3.9992, 3.9992, 3.9992],\n",
      "        [4.9990, 4.9990, 4.9990, 4.9990],\n",
      "        [5.9988, 5.9988, 5.9989, 5.9988],\n",
      "        [6.9986, 6.9987, 6.9987, 6.9986]], device='cuda:0',\n",
      "       grad_fn=<IndexSelectBackward>)\n",
      "inside the message custom_GCN: norm \n",
      " torch.Size([22]) \n",
      " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1.], device='cuda:0')\n",
      "inside the message custom_GCN: x_j \n",
      " torch.Size([22, 4]) \n",
      " tensor([[0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.9998, 0.9998, 0.9998, 0.9998],\n",
      "        [0.9998, 0.9998, 0.9998, 0.9998],\n",
      "        [2.9994, 2.9994, 2.9994, 2.9994],\n",
      "        [0.9998, 0.9998, 0.9998, 0.9998],\n",
      "        [1.9996, 1.9996, 1.9996, 1.9996],\n",
      "        [3.9992, 3.9992, 3.9992, 3.9992],\n",
      "        [1.9996, 1.9996, 1.9996, 1.9996],\n",
      "        [3.9992, 3.9992, 3.9992, 3.9992],\n",
      "        [5.9988, 5.9988, 5.9989, 5.9988],\n",
      "        [5.9988, 5.9988, 5.9989, 5.9988],\n",
      "        [6.9986, 6.9987, 6.9987, 6.9986],\n",
      "        [1.9996, 1.9996, 1.9996, 1.9996],\n",
      "        [4.9990, 4.9990, 4.9990, 4.9990],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.9998, 0.9998, 0.9998, 0.9998],\n",
      "        [1.9996, 1.9996, 1.9996, 1.9996],\n",
      "        [2.9994, 2.9994, 2.9994, 2.9994],\n",
      "        [3.9992, 3.9992, 3.9992, 3.9992],\n",
      "        [4.9990, 4.9990, 4.9990, 4.9990],\n",
      "        [5.9988, 5.9988, 5.9989, 5.9988],\n",
      "        [6.9986, 6.9987, 6.9987, 6.9986]], device='cuda:0',\n",
      "       grad_fn=<IndexSelectBackward>)\n",
      "Inside propogate, before scattering, out :  torch.Size([22, 4]) \n",
      " tensor([[0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.9998, 0.9998, 0.9998, 0.9998],\n",
      "        [0.9998, 0.9998, 0.9998, 0.9998],\n",
      "        [2.9994, 2.9994, 2.9994, 2.9994],\n",
      "        [0.9998, 0.9998, 0.9998, 0.9998],\n",
      "        [1.9996, 1.9996, 1.9996, 1.9996],\n",
      "        [3.9992, 3.9992, 3.9992, 3.9992],\n",
      "        [1.9996, 1.9996, 1.9996, 1.9996],\n",
      "        [3.9992, 3.9992, 3.9992, 3.9992],\n",
      "        [5.9988, 5.9988, 5.9989, 5.9988],\n",
      "        [5.9988, 5.9988, 5.9989, 5.9988],\n",
      "        [6.9986, 6.9987, 6.9987, 6.9986],\n",
      "        [1.9996, 1.9996, 1.9996, 1.9996],\n",
      "        [4.9990, 4.9990, 4.9990, 4.9990],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.9998, 0.9998, 0.9998, 0.9998],\n",
      "        [1.9996, 1.9996, 1.9996, 1.9996],\n",
      "        [2.9994, 2.9994, 2.9994, 2.9994],\n",
      "        [3.9992, 3.9992, 3.9992, 3.9992],\n",
      "        [4.9990, 4.9990, 4.9990, 4.9990],\n",
      "        [5.9988, 5.9988, 5.9989, 5.9988],\n",
      "        [6.9986, 6.9987, 6.9987, 6.9986]], device='cuda:0',\n",
      "       grad_fn=<MulBackward0>)\n",
      "Inside propogate, before scattering, edge_index :  torch.Size([2, 22]) \n",
      " tensor([[0, 1, 1, 3, 1, 2, 4, 2, 4, 6, 6, 7, 2, 5, 0, 1, 2, 3, 4, 5, 6, 7],\n",
      "        [1, 0, 3, 1, 2, 1, 2, 4, 6, 4, 7, 6, 5, 2, 0, 1, 2, 3, 4, 5, 6, 7]],\n",
      "       device='cuda:0')\n",
      "resulting embeddings:  torch.Size([8, 4]) \n",
      " tensor([[ 0.9996,  0.9996,  0.9996,  0.9996],\n",
      "        [ 5.9987,  5.9987,  5.9987,  5.9987],\n",
      "        [11.9975, 11.9975, 11.9975, 11.9975],\n",
      "        [ 3.9991,  3.9991,  3.9991,  3.9991],\n",
      "        [11.9975, 11.9975, 11.9975, 11.9975],\n",
      "        [ 6.9985,  6.9985,  6.9985,  6.9985],\n",
      "        [16.9965, 16.9966, 16.9966, 16.9965],\n",
      "        [12.9973, 12.9973, 12.9974, 12.9973]], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "New Layer ========================= Inside custom GCN: \n",
      "current embeddings:  torch.Size([8, 4]) \n",
      " tensor([[ 0.9996,  0.9996,  0.9996,  0.9996],\n",
      "        [ 5.9987,  5.9987,  5.9987,  5.9987],\n",
      "        [11.9975, 11.9975, 11.9975, 11.9975],\n",
      "        [ 3.9991,  3.9991,  3.9991,  3.9991],\n",
      "        [11.9975, 11.9975, 11.9975, 11.9975],\n",
      "        [ 6.9985,  6.9985,  6.9985,  6.9985],\n",
      "        [16.9965, 16.9966, 16.9966, 16.9965],\n",
      "        [12.9973, 12.9973, 12.9974, 12.9973]], device='cuda:0',\n",
      "       grad_fn=<ReluBackward0>)\n",
      "Inside the propagate, edge_index[idx]: \n",
      " torch.Size([22]) \n",
      " tensor([0, 1, 1, 3, 1, 2, 4, 2, 4, 6, 6, 7, 2, 5, 0, 1, 2, 3, 4, 5, 6, 7],\n",
      "       device='cuda:0')\n",
      "Inside propogate, tmp :  torch.Size([22, 2]) \n",
      " tensor([[ 3.9987,  3.9983],\n",
      "        [23.9953, 23.9932],\n",
      "        [23.9953, 23.9932],\n",
      "        [15.9966, 15.9953],\n",
      "        [23.9953, 23.9932],\n",
      "        [47.9912, 47.9871],\n",
      "        [47.9912, 47.9871],\n",
      "        [47.9912, 47.9871],\n",
      "        [47.9912, 47.9871],\n",
      "        [67.9878, 67.9820],\n",
      "        [67.9878, 67.9820],\n",
      "        [51.9905, 51.9861],\n",
      "        [47.9912, 47.9871],\n",
      "        [27.9946, 27.9922],\n",
      "        [ 3.9987,  3.9983],\n",
      "        [23.9953, 23.9932],\n",
      "        [47.9912, 47.9871],\n",
      "        [15.9966, 15.9953],\n",
      "        [47.9912, 47.9871],\n",
      "        [27.9946, 27.9922],\n",
      "        [67.9878, 67.9820],\n",
      "        [51.9905, 51.9861]], device='cuda:0', grad_fn=<IndexSelectBackward>)\n",
      "inside the message custom_GCN: norm \n",
      " torch.Size([22]) \n",
      " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1.], device='cuda:0')\n",
      "inside the message custom_GCN: x_j \n",
      " torch.Size([22, 2]) \n",
      " tensor([[ 3.9987,  3.9983],\n",
      "        [23.9953, 23.9932],\n",
      "        [23.9953, 23.9932],\n",
      "        [15.9966, 15.9953],\n",
      "        [23.9953, 23.9932],\n",
      "        [47.9912, 47.9871],\n",
      "        [47.9912, 47.9871],\n",
      "        [47.9912, 47.9871],\n",
      "        [47.9912, 47.9871],\n",
      "        [67.9878, 67.9820],\n",
      "        [67.9878, 67.9820],\n",
      "        [51.9905, 51.9861],\n",
      "        [47.9912, 47.9871],\n",
      "        [27.9946, 27.9922],\n",
      "        [ 3.9987,  3.9983],\n",
      "        [23.9953, 23.9932],\n",
      "        [47.9912, 47.9871],\n",
      "        [15.9966, 15.9953],\n",
      "        [47.9912, 47.9871],\n",
      "        [27.9946, 27.9922],\n",
      "        [67.9878, 67.9820],\n",
      "        [51.9905, 51.9861]], device='cuda:0', grad_fn=<IndexSelectBackward>)\n",
      "Inside propogate, before scattering, out :  torch.Size([22, 2]) \n",
      " tensor([[ 3.9987,  3.9983],\n",
      "        [23.9953, 23.9932],\n",
      "        [23.9953, 23.9932],\n",
      "        [15.9966, 15.9953],\n",
      "        [23.9953, 23.9932],\n",
      "        [47.9912, 47.9871],\n",
      "        [47.9912, 47.9871],\n",
      "        [47.9912, 47.9871],\n",
      "        [47.9912, 47.9871],\n",
      "        [67.9878, 67.9820],\n",
      "        [67.9878, 67.9820],\n",
      "        [51.9905, 51.9861],\n",
      "        [47.9912, 47.9871],\n",
      "        [27.9946, 27.9922],\n",
      "        [ 3.9987,  3.9983],\n",
      "        [23.9953, 23.9932],\n",
      "        [47.9912, 47.9871],\n",
      "        [15.9966, 15.9953],\n",
      "        [47.9912, 47.9871],\n",
      "        [27.9946, 27.9922],\n",
      "        [67.9878, 67.9820],\n",
      "        [51.9905, 51.9861]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Inside propogate, before scattering, edge_index :  torch.Size([2, 22]) \n",
      " tensor([[0, 1, 1, 3, 1, 2, 4, 2, 4, 6, 6, 7, 2, 5, 0, 1, 2, 3, 4, 5, 6, 7],\n",
      "        [1, 0, 3, 1, 2, 1, 2, 4, 6, 4, 7, 6, 5, 2, 0, 1, 2, 3, 4, 5, 6, 7]],\n",
      "       device='cuda:0')\n",
      "resulting embeddings:  torch.Size([8, 2]) \n",
      " tensor([[ 27.9940,  27.9914],\n",
      "        [ 91.9819,  91.9738],\n",
      "        [147.9724, 147.9595],\n",
      "        [ 39.9920,  39.9884],\n",
      "        [163.9704, 163.9560],\n",
      "        [ 75.9859,  75.9792],\n",
      "        [167.9697, 167.9550],\n",
      "        [119.9785, 119.9679]], device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5, 0.5)"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gcn_trainer_batch.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Default whole graph (recombine train nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whole graph version of train forward info for cluster :  0\n",
      "prediction values: \n",
      "tensor([[-0.6931, -0.6931],\n",
      "        [-0.6931, -0.6931],\n",
      "        [-0.6931, -0.6931],\n",
      "        [-0.6931, -0.6931],\n",
      "        [-0.6931, -0.6931],\n",
      "        [-0.6931, -0.6931],\n",
      "        [-0.6931, -0.6931],\n",
      "        [-0.6931, -0.6931],\n",
      "        [-0.6931, -0.6931],\n",
      "        [-0.6931, -0.6931]], device='cuda:0', grad_fn=<LogSoftmaxBackward>)\n",
      "prediction values for train nodes: \n",
      "tensor([[-0.6931, -0.6931],\n",
      "        [-0.6931, -0.6931]], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "whole graph version of train forward info for cluster :  1\n",
      "prediction values: \n",
      "tensor([[-0.6945, -0.6918],\n",
      "        [-0.6970, -0.6893],\n",
      "        [-0.6999, -0.6865],\n",
      "        [-0.6960, -0.6903],\n",
      "        [-0.7011, -0.6853],\n",
      "        [-0.6986, -0.6878],\n",
      "        [-0.7033, -0.6831],\n",
      "        [-0.7058, -0.6806],\n",
      "        [-0.7058, -0.6806],\n",
      "        [-0.7077, -0.6788]], device='cuda:0', grad_fn=<LogSoftmaxBackward>)\n",
      "prediction values for train nodes: \n",
      "tensor([[-0.7011, -0.6853],\n",
      "        [-0.7033, -0.6831]], device='cuda:0', grad_fn=<IndexBackward>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5, 0.5)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# default brute force case: recombination whole graph\n",
    "clustering_machine = copy.deepcopy(check_clustering_machine)\n",
    "clustering_machine.mini_batch_train_clustering(0)      \n",
    "gcn_trainer_whole = wholeClusterGCNTrainer_sequence(clustering_machine, 2, 2, input_layers = [16], dropout=0.3)\n",
    "gcn_trainer_whole.train(1, 0.0001, 0.1)\n",
    "gcn_trainer_whole.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#### mini-batch train nodes only in the isolated cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node cluster memeber ship:  {0: 0, 1: 0, 2: 0, 3: 0, 4: 1, 5: 1, 6: 1, 7: 1, 8: 0, 9: 1}\n",
      "isolated clusters are:  [[0, 1, 2, 3, 8], [4, 5, 6, 7, 9]]\n",
      "training nodes global ids are: \n",
      " {0: tensor([0, 3]), 1: tensor([5, 7])}\n",
      "testing global clusters are:  {0: tensor([8]), 1: tensor([6])}\n",
      "modeling global clusters are:  [[0, 3, 8], [5, 6, 7]]\n",
      "validation global clusters are:  [tensor([1, 2]), tensor([4, 9])]\n",
      "Info about the mini_batch only with training nodes: \n",
      "mini train edges of each cluster, global ids:  [{(0, 1), (1, 3), (1, 2)}, {(6, 7), (4, 6), (7, 9)}]\n",
      "mini train overlapping nodes global ids:  {0: [0, 1, 2, 3], 1: [4, 5, 6, 7, 9]}\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAADnCAYAAAC9roUQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nO3dfXhU9Z3w//f3nBmSkBDyQAAhCUEjJGoDAgq3ogatxWUrroqFtaxV23Wrdu+1ra3bctWfe1/LXus+VHf31nW1vb23db3BtfVhW7fFKPiAoCIKogkhYIAQEpKQkAcyYWbO9/fHZJJM5sxkksxTMp/XdfWqmXPmcJKZ+Zwz38/n+/kqrbVGCCFEXBiJPgEhhEglEnSFECKOJOgKIUQcSdAVQog4kqArhBBx5Ej0CUx2rd19vPhRAzVNnXS6PGSnOyibnc1tSwvJz0pL9OkJIeJMSclYbOw73sETO+p4q7YFgD6PNbAt3WGggcqFBdx3TSmLinISdJbB5CIhRGxJ0I2B53bXs/m1GlweL+H+ukpBusNk05oyNq4oidv52ZmoFwkhJhoJulHmC7jV9LqtkXful+E02LSmPGGBdyJeJISYqCToRtG+4x1seGY3vW7vwGP69HG8H/0K3XYUvOcgawbGwlWYZZUBz81wmmy9ZwUVhfG9i5yIFwkhJjJJpEXREzvqcHm8AY95tj8JPW2QMxeVPQt97GOsD55H5czGmF02sJ/L4+XJHXU8tXFZ3M533/EONr9WExBwPe/9At1yGHpOg+FAFczHXLoOlTNnYJ9et8Xm12qoKMyJ+0VCiIlOgm6UtHb38VZtS8DXc2154OxpABxXfQuVOxfPbzf77nq72wKerzVsP9hCW3df3BJWdhcJXfcuasZ8KLkM3VSNPnEAT3sDjps3o0znwH6JuEiI2JIkanxI0I2SFz9qCHpMGQ6MsuuwqqvwvPtz1LSZ6LZjkFuIKr40eH/gxb0N/NnVF8T8fO0uEgDmDQ9hzPT9+7q7Fc+vfwxnO6CjEfLnDeyXiIuEiI3wSdQmHquqlSRqFMnkiCipaeoMeLP6qeLFkJUP7Q3oY3vBMDCKFoMjPWhfl8ei5mRXPE7X9iIBDARcAKz+u2ClIGN60L7+i4SYuJ7bXc+GZ3bzenUzfR4r6D3s6n9s2+fNbHhmN8/trk/MiU4icqcbJZ0uT9Bj2tWN941/Bs85zNU/QOXMwVv1T1j7fwPp2UHJNN9x3HE429AXCT/tduHd+SwAxkXXo6YG3+HE8yIhom80SVStodftZfNr1QCSRB0HCbpRkp0e/KfU3a3gOQeGiZpR4hsTnX4etNXDmZMhjuO0fTza7C4SftrVhfeNf0G31aMuvApjya1hjjP6i4SMHSaeXRI1EpJEHT8JulFSNjubNEdTwN2jmn4eTMmEcz14t/0Ups1E13/g2zazNOgY6Q6DsvOmxeV87S4SALq7DU/V49DZjHHJDZhLbhnhOJFfJGTsMHnYJVEBdEcj3r2/9lWweD2o88owL9uAysof2EeSqOMjdbpR0trdx5WPvhn0ld1qOYL1ySu+BJq/TnfB1Zjl1wUdI81h8N5D18blbu+ptw7zWFVt0Pm6//OH0NsBmXkYQ5J9av7lGDPmB+zrMBQLZ2Uxa3rGiHerMgEjeYR6r+pzZ/G88gj0dqAKK8Aw0cc+hpw5OG58GKUGU0DxfK9ONnKnGyUzstK4ZkEBr1c3BwQVo+B8jOu/G9ExLijIjNubeN3SQh6rqg3e0Nvh+/+e01jVbww8bOYWwbCg67E0n53s4rP+cd1Qd6sydphcQiVR9ak63+uflY/j2u8A4P6v/9WfBP4YNW/pwL7xrLSZbCToRtH9laW8Vdtim6CK5Gvb4VM97G/oiMtYWaiLhPOOp8d8TFf/773t82berm1l05oyvjQ3R8YOk0zIJKq/DruvB93VAoYDes8AoNsbYEjQlSTq2EnQjaJFRTnMn5FJTVPgm1GfO4vn9ceDvrZ5uloCvrads6y4jpXdX1nKO4daA6YtR8PQu9WSGZm2Y4eed59Fn6yGvm5wpqHySzAvvRmVXzywj4wdxkaoJKqatQBVcAG65TCelzYFbNO9nTbHiU+lzWQjdbpR1NrdxxetPUGPD//a5qi8F3ILoaPRN2bm32/IhIN4WFSUw6Y1ZWQ4Y/M26HVbVJ/ssh/D7WlDzVqAUXolpGWhGz/Ds+PJgF3i/fdIFaGSqMowMVd/H3PlNzG+9AcYK76Omn+5b1t6ls1x4lNpM9nInW4UhRorG83XtniPlfnHTCNJcg3l/bwKq24nnGkErTEqvoq5eG3E/65j9YMD/220HcXz281wth1teVDG4NtSxg6jz67SZoAG4/zlvv90deH5+BUA1HnlAbvFs9JmspGgG0UhZ6WN4mtbIsbKNq4ooaIwhyd31LH9YAuKwfHZUPTpo6i0TPTUPF9DnzHw1rwJHSexmmqA/kkYRuBbUsYOoy9kEhXwVj0OaVkwJQN94gD0daPmfimgOROABtYtKYzD2U4+EnSjKORYWf/XNl3/EfpMI2TmoZsPob/4wPZrWyLGyioKc3hq4zLauvt4cW8DNSe76HS5aep0UXOyE++wO2DHym8C4Nn+BHqMQVcf3Ytu7v/wT81FFQTXLoOMHUZbqCQqgMotxKr/EPp6IGM6xsU3YCy+MXAfBasWFki52BhJ0I2iUGNlQMRf23zHSdxYWX5WWsBX+Qe2fsxnjcFJlGhwrH4Q7XWjT3yG961/xfvWU6ib/xqVNSNgPxk7jL5QSVTz8g2Yl28I+9x0h8l9lfYXSDEyCbpRFG6sLNKvbck2VhZuuvBYac85X69ew/BNjZ57MTjSwO1Cd7cGBN1k+3tMFv4k6tga2JdJGd84SNCNonBjZZF8bYPkGysLe/c+Rrr1C7zv/Aw160LUlEysU4fA7YL0aai84sB9Sa6/x2QymiSqzBSMHgm6URRurCySr23JOFYWNtM9Rmpqjm8VjcZqtMcFadNQ85ZiVnwVNWXq4H5J+PeYbEZKovoXJV21sID7KkvlDjcKpPdClO073sG6f3sP9/DMUwQStU6aHX8nsH3H2/n9581Yw34d69A7WKfq0Cdr4Gy7rzF7XhFG0eKAng3jkUx/j1TgT6L+48/+H5etrGTm9CzKzpvGuiXS/S2a5E43yj490cFYrmNOUyXFWFm4TmBDWafq0Id3DT7Q3oBub0Bn5sPQRjkKymdP44vWHhk7THL5WWncc9X5/O03fs7Pn36Q9PTgRvti/CToRpG/R+lYvolrTcIDTKSdwAAcV94FV9414jHTHSZ/e0sF+xs6ZOxwAjh79iymaUrAjSEJulFk16PUajqId9s/2u5vXnEnRukVAHi1TmifgbEsxT6SoXer/v/J2GFy6+joICdH/vaxJEE3SkIt9Kim5mIM6Z2r3X3ound9P0wrGHw8gQs9jnUVgVBC3a2GmoCRne6UscMk0d7eTm5ubqJPY1KToBslofouqOyZmJetH/jZW/0mGiCvCGPWhYH7kpg+A7ZLsXvdWHtexDq6B9wuVF4xxrLbMArOD3mcSO9Wh0/AEInnT5y+vf8Ep8pv4YGtH8sSSjEiQTdKRlroEUBrjVXzJgBm+ZeDtieiz0CoO3Trw61YtW9DzhzU7DJ0/R68VY+jbt6MSg+crGAouOHi2SwqypG71QnGNnGaWczLnzTKEkr9or2mnwTdKIlk5pZu2A9dpyAjG1VyWYjjxLfPgN0duu7t9HUQUwrH9d9DZWTjMQz0kfexarYHdRObYhosKsqRu9cJZqTEqV1T+lRKbsZqTT/ppxslkczc8i9/YyysRJn2+8e7z4DdHbo+0wiWFzLzUBnZAKj8Et+29uNBx5BOYBPPYOJ05EqVoU3pn9tdH5fzS7Tndtez4ZndvF7dTJ/HCvqMuPof2/Z5Mxue2T2qv4vc6UbJSDO3dHsDuqkGTCfGgmts90lEnwHbO3R/u0nH4Fcn5ZgSuC3oONIJbKIYnjiNtMImVZZQivWafhJ0oyRc3wUAb/9drpq/PGhM1C8RfQZs79D7727xDK7YoN19gduCjiOdwCaK4YnTSCtsYPIvoRSqksfz+38YbEPqlzMH59pHgNFdkCToRslA34XPmxn+bU27utBffABgu/Q6JK7PgN0dupo+BwwTek6jeztRGdnotnrfttyioGNIJ7CJwy5xOpoKm0SWNsaDXSXPUEMvTmRMD9gW6QVJgm6U7DvewZled1DABVDp03B+/Ymwz09Uj1K7O3SVkY264Ar0oXfwvP5TVM4cdP1H4EjDKFsVdAzpBDZxhFxSqt9IFTYweZdQClXJM9TQi9NwkV6QJOhGwdAs8Fgkss9AqM5o5mXrsQwTq34PuvMUqmA+xrLbgoZGpBPYxDJSaWMkFTaTNXE60gUJwL3lAQBf3fqSWzBmlARsj+SCJEF3nMYzfTZZ+gzYrSKgHFMwl9+Oufz2sM+VVQQmlpFKGyOpsPEdZ/IlTsNekJzpqMIK1NQcrJYj6KYaX936TX+FGjLMEMkFSYLuOIx3+uyK+Xn86A/KE54JllUEUke40sZIKmwGjzP5EqfhLkjmqvtRSgFgeD14Xv4J9LShmw4OLFM/eJzwFyQJuuMQatDdOvYx1qevoTsawTBROXMxr/0OKi0zYL/pGc6kCViyikBqCLukVAQVNjB5E6ehLkja0wfnemGqzWdVBU91GOmCJEF3jEJOn/3iA7zv/My3BljxYnCk+zL/3nNAYNBNtiywrCIw+YUqbYykwmZgXyZn4jTkBcnVheflh1GzF6Ky8rFajkBPG6Rno2YvDNg1kguSBN0xsp0+qzXej34FgPnlv8AY9oIMl4xZYOkENrmFSpxGUmEDkztxGrLWPi0Ldf4KdFONr1bXmYEqWox56R8FfSOI5IIkQXeMbAfdu075lq4xnVif/R7vm/8bMrIxyr+MaVNqlcxZYOkENnmFWn49EpM5cRryguRMx3HFHSM+P9ILkvReGCO7QXft6vb9h9eN7mpBzVsKZzuwPvh/WMc+DnGcyZcFFsnNnzjNcI7u458KidP7K0tJd5hjem6kFyQJumNkN+iu0rMG/ttc+U0cV96JUXolANbxfSGOM/mywCL5bVxRwqY15WQ4TfqT8iEp5VskdNOa8kmfOI3HBUmGF8bIdtA9Mx+c6eB2DT7W/z1FOYPXnJqsWWAxMQxPnPa5XOBvbETqJk5jXckjS7CPUWt3H1c++mbQuK73k1ex9v8Gps9GzTgfXf8hWF7MG34YtOpCmsPgvYeunZRJCTGxfPz5Ie74/57ky+u+IYnTfvsbOmJSySN3umMUatDdqPhDsDxYh99DH92DypmDsWhtUMCdzFlgMfF0tjRykdHIY+sXJ/pUkkasKnkk6I7S0KU7TvecwwCG5oCVYWIuuQVzyS1hjzOZs8Bi4jl69Cjz5s1L9GkkpWhX8kjQjVC4pTtGKxWywGJiOXr0KMXFxYk+jZQgQTcCI60lNUjjm/JgT6bPimR17NgxFi+WoYV4kKA7gtF1EbMPuKmaBRYThwwvxI8E3TBG6iI20GcBX0f5oQ2OTQVLinMpzJ2a8llgkdy01hw7dkyGF+JEgm4Y4Zbu0D3teN9/3tdlSAcHZQvIz5oi2WCRlIYmhNu6zuJedjtb9rVx29IMuTmIManTDSFUHS70N7Z5/TF07xlU7lx0/Z6gO12QOlyRfMIlhP3DYJULC7jvmlIWFckwWCzINOAQwi3dYVVXoU/V4bjqW2CGnsbr7yImRDJ4bnc9G57ZzevVzfR5rKAbClf/Y9s+b2bDM7t5bnd9Yk50kpPhhRBCLd2h209g7X0JY/FaVF7wyrhDJXMXMZFaRpMQ1hp63V42v1YNIJU2USZBN4RQS3dYx/aC5UU31+JpPoRu993JWsf3gekMmhQhXcREooVLCIdb5aTXbbH5tRoqCnOk4iaKZHghhJBrSWkNaPSJA+gTn/r65wJ0t6JbjtgcR7qIicQKuazUFx/g3fGv6PYTqKJFqHnLfEvTeM8N7OPyeHlyR108T3fSkzvdEEIt3WEuXou5eO3Az56dz6IP77JNpEkXMZFooZaVinSVE62Tb1mpiU7udENYt3T8a0BN1rWkxMQRMiE8bJUT9/N/jvulTXhrtgftKgnh6JKgG4K/i9hIDZ4dV96F846ng+5ypYuYSAYhE8KjWOVEEsLRJcMLBBaKd7o8ZKc7KJudzcblxbKWlJjQQiWEh69yYswoweuYgnVwB9bxfRjFlw47jiSEoyWlg274QvEmNHB+QSa1J8/gHcWXAukiJpJFyITwKFc5kYRw9KRs0B2pc5i/S3zNyS60ZeFwmHi1jvrSHULEUqiEsDIdGOVfxtr/G7w7/w+Wf5UTZaDmXx6wrySEoyslx3QHC8VHatXoS4ZhODAVlM+eRprDIN0R+GdLdxikOQxWXzSLrfeskIArkka4hLBR8YcYl9wA584OrHJirro/aJUTSQhHV8r1Xth3vIMNz+wOGKf1vPcLdMth6DkNhgNVMB9z6TpUzpyA52Y4TZ7+k6V8frIzakt3CBFr9/xyT9CyUpFSClZfNIunNi6L/omlqJQbXrArFNd176JmzIeSy9BN1egTB/C0N+C4eTNqSG8Fl8fLf7x/VN6AYkK5v7JUEsJJJKWGF0IVips3PIRjzY9wXHEHjq983/fg2Q7oaAzYb2ihuBATxaKiHDatKSPDObqPuySEYyNqd7qhyq5uW5o8X7tDFYobM4csOmf13w0oBRnTg/b1F4oPX6huIvz+InX58wyRLDslCeHYGnfQHans6rGq2nH154xmMAtVKO6n3S68O58FwLjoetTU4PMdXige699fiGjZuKKEisIcntxRx/aDLfT19QW0JpVlpeJjXIm0SBdsHMuVMxbNlu/+9w95s+aU7Tbt6sL7xr+g2+pRF16FuWIjKsR0tOvKZvLzb1wW099fiFg62tTGqrt/xNpv3EuXyyMJ4Tga851uLPtzRlpDu+3zZt6ubY04mIUqFNfdbXiqHofOZoxLbghqzxh8HKf0JxUTWtPROi4yGnl8/aUj7yyiakxBd6QFG0OJpD9nLIJZd3c327ZtY99bNZBZDo4pAds9//0o9HZAZh543Xg/3AqAmn85xoz5Afs6FWSlGba/v+f3/4Burg38x3Pm4Fz7SMS/vxDxcODAAS655JJEn0ZKGlPQtS278rqx9ryIdXQPuF2ovGKMZbcFFVr7+3PalV1FM5j39vbyxhtv8Oqrr/L222+zfPlyvvEHa/mbA2mc8w67fe7t8P1/z2ms6jcGHjZzi2BY0PV4PWz5TRXuggWEWnLdKL9u8Idhybhwv78QsTI8N3KoRlM+Z6m0bEyAUY/phlqw0bv7OazatyFnDirHt1gjzjRfrWt64BTCUAs22hVxez+vwqrbCWcaQWuMiq8G9LMd+EUUXF82k9vO6+DVV1+lqqqKRYsWcdNNN3HDDTeQm5sb8t+IlFJQuaCAdw+14raCD+C/03Xe8XTY48iClSJewuVGppiglCGJ3jgb9Z2uXdmV7u30BUalcFz/PVRGNh7DQB95H6tme1CQtCu7Ctls+fRRVFomemoe9LSFPC+tYduBEzS+upV1X13Nww8/TEFBQdB+4y0UL86bimEosAm6fu4tDwD47vaX3IIxoyRge6iyMyGiaaTcyDkvgDXq3IgYn1FPjrAru9JnGn31rZl5qIxsAFR+iW9b+/GgY9j15wxVQ+tY+U0cqx9E5Y089zttyhRu/J9/zZ133mkbcGH8heJnet2hy86c6ajCCoySZZCZh26qwVv1OLr3TMBu0p9UxNqo+osMyY3ICsCxN+o7Xdv+nL2d/Ucb/Lqs/Mkq/7Zhtu/czd2//2cyMzOZOnUqHzovps+bP9rTCdDn1REFs/EUir95sCXkvuaq+wfKzAyvB8/LP4GeNnTTwaDOTdKfVMRKLBPdYvxGHXRty676727xDE6P1e6+wG3DXHzhfG4rnUdPTw89PT3sqc+A0X/jDxJpMBteKK4YLEWD0IXiIcvOPH1wrhdsJlSggu+qpT+piJVQC1H6WV98gPednwEEre0nid7YG3XQtevPqabPAcOEntPo3k5URja6rd63Lbco6BjpDoOrF5XyB0PGND/e+jGHP2kM2ne0RhPMKgpzeGrjMtq6+3hxb0NEncNC9SfF1YXn5YdRsxeisvKxWo74xqDTs1HDFv2T/qSTUzJMBQ+VG/HTPe1433/edyOgbZbxkYUoY27UQXfd0kIeqwqsRVUZ2agLrkAfegfP6z9F5cxB138EjjSMslVBxxjan9P/Rq1t7kYpxlRV4DfWYJaflRZxUsvu9wcgLQt1/gp0U42vVteZgSpajHnpHwVVb0h/0sklmaaCh1yIkv4VgHc+CxnTUXMu8lUY2ZBEb2yNOuj6F2wcXnZlXrYeyzCx6vegO0+hCuZjLLstKOD4F2xsaO/lRy99avtGHco69A7WqTp02zHfz8c/Qfe0YRQtDlrHKR7BLNTvr5zpOK64Y8Tny4KVk0usZk+OVbj+IlZ1FfpUHY41P8JbXRXyGJLoja0xTY6wK7tSjimYy2/HXH572OemO0zmz8hkwzO7R0xiAb6Ae3jX4APtDej2BnRmPgwJuvEMZtKfVEBsp8KPVaiFKHX7Cay9L2EsXovKCx7yCz6OJHpjZUxB1192Fekbzi/DafDl8pn83/fqI36e48q74Mq7RtwvnsFsPL+/9CedHOwqBPTp43g/+hW67Sh4z0HWDIyFqzDLKgf2iXWFQKhEr3VsL1hedHMtnuZD6HbfMIR1fB+YzqB+I5LojZ0xN7wZS9nVN/7HPP5919HgngXvPos+WQ193eBMQ+WXYF56Myq/OKJzSUQwk/6kqc2uQsCz/Ulf8jRnLip7FvrYx1gfPI/KmY0xu2xgv1hWCIRM9GoNaPSJA4GPd7eiW44EPCSJ3tgaVz/d0ZZd/e/tIUpZetpQsxagpmRgNdWgGz/Dc+Ykzlv/Nuy/n+hgNtayMzGx2VUIaMsDZ08D4LjqW6jcuXh+u9l319sdOJMylhUCoRK95uK1ATNDPTufRR/eFVQyBpLojbVxNzGPtOwqXCmLY/WDA/9ttB3F89vNcLYdbXlQRvApJlMwG0vZmZjY7CoElOHAKLsOq7oKz7s/R02b6Uv+5haiioPbJ8aqQiBUojdSkuiNvagt1zNS2VW4UhYAb82b0HESq6kG6F+5YVjANRSUz85m7eI5SRfMRlN2Jia2UBUCqngxHP94INmLYWIULQZHetC+sawQiCTRGypXIone2IvbwpQjLpVzdC9W7VvQ2QxTc1EFwS+8pWHBrGn82dUXJFXAFanFrkJAu7rxvvHP0N2GufoHONY/hsotwtr/G1/3PdvjxKZCQBaiTG5xC7qhSln8HKsfxPH1JzAr74PeDrxvPYXubrU5jpSyiMSyqxDQ3a3gOQeGiZpRgkrLhOnn+TaeORniOLGrENi4ooRNa8rJcJqEWHVqgFKQ4TTZtKZcEr1xELXhhZGE7llwDgwHyjBQphPmXuxrnON2obtbUVkzhh1HSllEYtlPhT8PpmTCuR68234K02ai6z/wbZsZ/K0tHhUCkuhNTnELuqFKWXTrF3jf+Rlq1oWoKZlYpw6B2wXp01B5gSVjUsoikoHtVHhnGuZ1f471ySu+BNrpYzBtJsaCqzHmXxZ0jHhVCEiiN/mMazXg0Qi14oTubMa765fo9hPgcUHaNNTMCzArvorKnRuwr6y4IJLFeFcgWX3RLOnklaLidqcbsmdB9qyAkrFQpJRFJBOZCi7GKm6JNPC9UdMd5pieK29UkUykQkCMVVyDrrxRxWQiFQJiLOI2pjvUSO3w/BI9zVeISOxv6JAKARGxhARdkDeqmHykQkBEImFBVwghUlFcx3SFECLVSdAVQog4kqArhBBxJEFXCCHiSIKuEELEkQRdIYSIIwm6QggRRxJ0hRAijiToCiFEHEnQFUKIOJKgK4QQcSRBVwgh4kiCrhBCxJEEXSGEiCMJukIIEUcSdIUQIo4k6AohRBxJ0BVCiDhyJPoEkkVrdx8vftRATVMnnS4P2ekOymZnc9tSWd9KCBE9Kb9G2r7jHTyxo463alsA6LNZILNyYQH3XVPKoiJZIFMEkou1GK2UDrqyFLwYK7lYi7FK2aDrC7jV9LqtkXful+E02LSmXAJvipOLtRiPlAy6+453sOGZ3fS6vQOPed77BbrlMPScBsOBKpiPuXQdKmdOwHMznCZb71lBRaHcvaQiuViL8UrJRNoTO+pwebwBj+m6d1Ez5kPJZeimavSJA3jaG3DcvBllOgf2c3m8PLmjjqc2Lov3aYsE23e8g82v1QQFXM/v/wHdXBu4c84cnGsfAaDXbbH5tRoqCnPkYi1SL+i2dvfxVm1L0NdC84aHMGZeAIDubsXz6x/D2Q7oaIT8eQP7aQ3bD7bQ1t0niZIUY3exHsoov27wh4zpAdvkYi38Ui7ovvhRg+3j/oALgNX/wVIq6MMDoIAX9zbwZ1dfELRNTE6hLtZDmZetD7lNLtbCL+WCbk1TZ0CmeTjtduHd+SwAxkXXo6YGfx10eSxqTnbF7BxF8gl1sR7KveUBAFReMcaSWzBmlARsl4u1gBSckdbp8oTcpl1deLf9FN1yBHXhVRhLbg1zHHcsTk8kqbAXa2c6qrACo2QZZOahm2rwVj2O7j0TsJtcrAWk4J1udrr9r6y72/BUPQ6dzRiX3IC55JYRjuMMu11MLuEu1uaq+1FKAWB4PXhe/gn0tKGbDqLmXz7sOHKxTnUpF3TLZmeT5mgKumvx/Pej0NsBmXngdeP9cCsAav7lGDPmB+yb7jAoO29a3M5ZJF7Ii7WnD871gs0wFCr4i6RcrEXKBd11Swt5rKo2eENvh+//e05jVb8x8LCZWwTDgq4G1i0pjOFZimQT6mKNqwvPyw+jZi9EZeVjtRyBnjZIz0bNXhiwq1ysBaRg0J2RlcY1Cwp4vbo5IBPtvOPpiJ6vFKxaWCAZ6BQT8mKdloU6fwW6qcZXq+vMQBUtxrz0j1DpgQFWLtYCUjDoAtxfWco7h1oDZqRFKt1hcl9laQzOSiSzUNVOvfMAABG1SURBVBdr5UzHccUdIz5fLtbCL+WqFwAWFeWwaU0ZGc7R/fq+6ZxlMqsoRd1fWUq6wxzTc+ViLfxSMugCbFxRwqY15WQ4TfoTzyEp5eu5IPPnU5tcrEU0pGTDm6H2N3Tw5I463jzYwjmXCxxTBrb5W/StWljAfZWl8qERgHQZE+OT8kHXb+eeffz5Y//BNWtvp9PlJjvdSdl501i3RJpRi2D+i/X2gy0ofBMf/ORiLcJJyUSanZP1h1gxvZvH1i9O9KmICaCiMIenNi6jrbuPF/c2UHOySy7WIiISdPvV1tayYMGCRJ+GmGDys9Kkl4IYlZRNpA1XU1NDWVlZok9DCDHJSdDtJ3e6Qoh4kKAL9PT00NLSwrx580beWQghxkGCLr673NLSUkxzbIXvQggRKQm6wMGDB1m4cOHIOwohxDhJ0EWCrhAifiToIkFXCBE/EnSRoCuEiJ+UD7pnzpyhq6uLuXPnJvpUhBApIOWDrr8+1zBS/k8hhIiDlI80NTU1MrQghIiblA+6MhNNCBFPKR90peeCECKeUj7oyp2uECKeUjrotrW14Xa7mT17dqJPRQiRIlI66B48eJAFCxagRlokTQghoiSlg25tba1ULggh4iqlg66Uiwkh4i2lg67c6Qoh4i1lg67WWnouCCHiLmWD7qlTpzBNkxkzZiT6VIQQKSRlg66/ckEIIeIppYOuzEQTQsRbSgddudMVQsRbSgddudMVQsRbSgZdrbX0XBBCJERKBt3GxkaysrLIyclJ9KkIIVJMSgbdmpoaucsVQiSE0lrrRJ9EPLR29/HiRw3UNHXyaU0d+txZvvaVldy2tJD8rLREn54QIkVM+qC773gHT+yo463aFgD6PNbAtnSHgQYqFxZw3zWlLCqS4QYhRGxN6qD73O56Nr9Wg8vjJdxvqRSkO0w2rSlj44qSuJ2fECL1OBJ9ArHiC7jV9LqtEffVGnrdXja/Vg0ggVcIETOT8k533/EONjyzm163d+Ax7+dVWHU74UwjaI1R8VXMxWuDnpvhNNl6zwoqCmWoQQgRfZOyeuGJHXW4PN6Ax/Tpo6i0TJiaF/a5Lo+XJ3fUxfL0hBApbNIF3dbuPt6qbQkaw3Ws/CaO1Q+i8grDPl9r2H6whbbuvhiepRAiVU26oPviRw3jPoYCXtw7/uMIIcRwky7o1jR1BpSFjYXLY1FzsitKZySEEIMmXdDtdHmidBx3VI4jhBBDTbqgm50enSq47HRnVI4jhBBDTZo6XcuyeO+99zj4/vtgng+OKYHbD72DdaoO3XbM9/PxT9A9bRhFizGKLw3YN91hUHbetLiduxAidUz4oNvQ0MALL7zACy+8QFZWFjeuu53DDWmc8waWL1in6tCHdw0+0N6Abm9AZ+bDsKCrgXVLwlc5CCHEWEzIyRG9vb387ne/Y8uWLXz22WfcdNNNbNiwgUsuuQSlFPf8cg+vVzeHnfobilKw+qJZPLVxWfRPXAiR8ibMna7Wmn379rFlyxb+67/+i8WLF3P77bezevVq0tPTA/a9v7KUdw61BsxIi1SaaXBfZWm0TlsIIQIkfdBtbW3lV7/6FVu3bqWvr4+vfe1rbNu2jblz54Z8zqKiHDatKYu494KfAy/p1b9juvciQKYBCyGiLymDrtvtZvv27WzdupVdu3bxla98hb/5m79h+fLlKKUiOoa/ac3ouoxdjKfmNDfffDPPPvssixYtisJvI4QQg8Y0pju0IXiny0N2uoOy2dnjbgheW1vL1q1b+fWvf01xcTEbNmzgxhtvJCsra8zH3N/QwZM76th+sAWFb+KDn7+f7qqFBdxXWTrQ5OZ3v/sdP/jBD/inf/onrr322jH/2xNdrF5nIVLZqIJuLBqCd3Z28uqrr7J161ZOnDjBunXr+NrXvkZpaXTHVdu6+3hxbwM1J7vodLnJTndSdt401i2xDyAffvghf/qnf8pDDz3EH//xH0f1XJKdNH4XInYiDrrRbAhuWRa7du1iy5YtVFVVsXLlStavX09lZSUOR/KMeNTV1fEnf/InrFu3ju9973sRD21MZNL4XYjYiijojqYhuF+G02DTmvKAD2RDQwP/+Z//yQsvvEBmZiYbNmzglltuIS8vfLvFRGppaeGOO+6gvLycRx99FKdz8s5Ui9brHIoMVwgRQdC1awjuefdZ9Mlq6OsGZxoqvwTz0ptR+cUBz81wmvziG5dy4lPfXe2BAweCamongp6eHr797W+jtebf/u3fyMzMTPQpRZ3d66w7GvHu/TW65TB4PajzyjAv24DKyg947kiN32W4QohBIwZdu4kGnt//A2TkoKZkYDXVQGczZObhvPVvhz1bYzYeYLl7P+vXr7etqZ0oPB4Pf/mXf8mBAwf4xS9+wcyZMxN9SlE1/HXW587ieeUR6O1AFVaAYaKPfQw5c3Dc+DBKDbbtCDehRIYrhAgUtuFNyIbgqx/EcfW3MFd8HcdV3/I9eLYdbQ3v8KUwiyr4l6ef5aabbpqwARfA4XDw93//96xevZqbbrqJurrJs7qE3eusT9VBbwdk5eO49js4Ku+F3ELoaPQF3yFCNX4fHK4IH3D9x/CvU/fc7voo/WZCJJ+wQTdcQ3BvzZt4d/8Hnnd+5jvQRdejjOAkmKHUpGkIrpTiu9/9Lg888ADr1q3jww8/TPQpRYXt62z2j1339aC7WtA97dB7BgDdHrz/8Mbv+453sPm1mlGNDwP0ui02v1bD/oaOUT1PiIkibKlAuIbg+uhedHOt74epuagC+xKvydgQfP369cycOZO7776bRx99lDVr1iT6lMbF7nVWsxagCi5AtxzG89KmgG26tzPoGMNfZ7t16qymg3i3/aPtOZhX3IlRekX/sXzr1En/CzEZhQ264RqCO1Y/iPa60Sc+w/vWv+J96ynUzX+Nypphc5zJ1xB81apVPP/889x55500NTVx9913J/qUxszudVaGibn6++j6j9BnGiEzD918CP3FB6h0+8kqpzq60FrT1nPOdlhKTc3FKL9u4Gft7kPXvev7YVrB4ONDhisiqWqQqggxkYQNunYNwbXnHBgOlGGgTCfMvRgcaeB2obtbbYPuZG0I/qUvfYmXXnqJjRs30tjYyI9//GMMY+L1hQ/Z+F2Dcf5y33+6uvB8/AoA6rxy293ff3cHi564n9wrvoYndxHDR69U9kzMy9YP/OytfhMNkFeEMevCwH3xDVf82dUXhDzv8FURTTxWVStVESLphA26ZbOzSXM0BbyZdesXeN/5GWrWhagpmVinDoHbBenTUHnFQceY7A3Bi4uLefnll7nrrrv4zne+w2OPPUZa2sS6u7J7nQG8VY9DWhZMyUCfOAB93ai5X8KYXRZ0jHSHwQPfup21F36T777wCUebw2fOtNZYNW8CYJZ/OWj7SMNSI1VF+Kd7b/u8mbdrW6UqQiSNsLdl65YGN/JWU3NQ2bPQjdVYde9C31nUvKU4rv8easrUoP1ToSF4Xl4eW7Zswe128/Wvf50zZ84k+pRGxe51BlC5hehTh3zN35WBcfENmJXftt1XA7ctLWTOnDlk5hbY7hOwf8N+6DoFGdmoksts9wk1LCVVEWIiC3unOyMrjWsWFATUb6rsWThWPxjRwZXyNZNJhXG1jIwMnnrqKf7qr/6Km2++mV/+8pdh208mE7vXGcC8fAPm5RtGfP7w1zmSdeqs6jcAMBZWokz7/e2GpUJVRVjHPsb69DV0RyMYJipnLua130Gl+Say+KsiKgpzQk7iECIezEceeeSRcDsU503l5U8a8VijX4Yhw2nyd7dWMCt74tbnjoZhGFRWVuJyufjhD3/IypUrKSgY+a4vGUTzda5vO8uH9afxhjiWbm/A2vsrMJ2YV30L5bC5KHvOcfqT12nc/y4ul4u8vDwyMjJ4+NUD1J4KHHawvvgA79tPQ18PqngxKq/Yt/5dyTKUM2NgP6/WtPec46sVc0L+Lq3dffxi11Gee/8oL3zUwPaDp6hvO8v8GZlMnZI8fUHExBXX3gup5JVXXuEnP/kJTz75JCtXrgzanowZ92i9zq3dfVz56Jshyw097/07um4nqnQljivusN1nisPgietzqP7kQ95//3327NnD7HmlHF/6bawho2Jaazy/+ks42475le9jzF4Y9nzTHAbvPXRt0N9YpiqLeBnxThegojCHnAwnu46cxjtCjFbKd+eTygEXoKysjEWLFnHvvfcya9Ysyst9Gf99xzt4+NUDPPzqZ+w+0sZnjZ180dpDTVMXe+pP8/OdX3Cg8QzFuVOZPT2+3xCi9TpPneLg0xNnONLaE/Q87erCeu/fQVs4rrwLlZFte+zry2fyFzcuZ/ny5dx6663ce++9HJ1SwoFmFxZDenZ0ncL69DXfZI7eDry7n8Oq2+kbg54xP+jYTkMxfaqTZfMGmyw9t7uev9j6CbWnuvBYOugO3f/YkdYeXv6kkZwMhwxRiDGLKOiC7wN59YUzaO85x/H2XpyGCvgqmu4wMA3Fl8tn8ne3VnD9RbNjdc4TRlFREddeey3f//736evr46B3ZtJ/uKP1OocarlCONMyKP8RcdKNtwAX7YSnDMHjl89N83tQdsK8+04yu2wnaAqVQcy6GliPohn2o3ELU9PMC9vdYmoKsNG64xHfeo72791iaXUfayMlwSuAVYzKqQaqKwhye2rhs1A3BU9nChQt55ZVXWPu9v+P0if14MUd8ztCMOxD3bwzReJ3Huk6db7iizDag2U7iGDJRw1z5TYwZJXgdU7AO7sA6vg+j+FKb4/iqIsY7VVmScmIsxpQZyM9KC1u0LgKd8mTQVfoVvEPrnb1urD0vYh3dA24XKq8YY9ltGAXnD+yT6A/3eF/nsa1TF7qe1rYqIjMfnOm+WnG//n9IOe2HZzKdvjFhu6nKftYXH+D19xUpvy5gUgfIVGUxdhNv+tQE9MSOOvq8w0qcPtyKdXC7b1JJ0WJ0yxG8VY+jXYGZef+He6LauKKErfesYPVFs0hzGKQ7At9y6Q6DNIfB6otmsfWeFWHv6n2TOIbNcjMdGP2TK7w7/w+enf8X6/B7oAzU/MuDjqEsD7/9j6dZ/41v8WZ1k+2FQPe0433/eVChPx6hOqsJMRKpgYkx27aJvZ39yR7lm1SSkY3HMNBH3seq2Y65eO3gvqPsQ5CMojUstW5pIY9V1QY9blT8IVgerMPvoY/uQeXMwVi0NuBbg9+UKVP472f+msdf2Y111AIVONyjtca781nImI6acxG6fk/I84lkqrIQw0nQjTG7ton6TCNYXsjKH0gmqfwS9JH30e3Hg/afLB/u8Q5XhJrEoQwTc8ktmEtuCft8/ySO8+cUYOQVYh1rDNrHqq5Cn6rDseZHeKurwh5vMnbQE7EnwwsxZtse098accikAOWYErhtCPlwD7q/spR0x8jJSDvpDpP7Kn0tSO2Scrr9BNbelzAWr0XlFUV0zMnYQU/ElgTdGLNtj+kvlfIMjgdqd1/gtqDjyIcbBqsiMpyje+sOr4qwS8pZx/aC5UU31+J541/QJ2t8jx/fh3fvr22PO1k76InYkeGFGLP7cKvpc8Awoec0urcTlZGNbqv3bcu1v8OSD/egaFRF2HZW0xrQvo5qQ3W3oluOBB17snfQE7EhQTfG7D7cKiMbdcEV6EPv4Hn9p6icOej6j8CRhlG2KugY8uEOtnFFCRWFOTy5o47tB1tQDLZzhMGpu6sWFnBfZWlQyZ1dUs5cvDYgienZ+Sz68C7bkjFIjQ56Ivok6MZYqIy7edl6LMPEqt+D7jyFKpiPsew2VHpwcJUPt73xVEWESspFKpU66InoiqjhjRgfu2XsIxVueXMxPvuOd7Dhmd30uu0nSIST4TTZes8KmZEmRk0SaXEQrYy7iK5oJeWEGA0JunEgH+7ktXFFCZvWlJPhNFEq/L7SQU9EgwwvxNFI63r5RdKHQETX/oaOMSflhBgNCbpxJh/u5CYd9ESsSdBNEPlwC5GaJOgKIUQcSSJNCCHiSIKuEELEkQRdIYSIIwm6QggRRxJ0hRAijv5/YaB70rh/Rq8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# mini-batch of the isolate\n",
    "clustering_machine = copy.deepcopy(check_clustering_machine)\n",
    "clustering_machine.general_isolate_clustering(2) \n",
    "check_clustering(clustering_machine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.75, 0.75)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check_clustering(clustering_machine, True)\n",
    "gcn_trainer_isolate = ClusterGCNTrainer_mini_Train(clustering_machine, 2, 2, input_layers = [16])\n",
    "gcn_trainer_isolate.train(1,  0.0001, 0.1)\n",
    "gcn_trainer_isolate.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use library data to check the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_clustering_machine(data, partition_num = 10, test_ratio = 0.05, validation_ratio = 0.75):\n",
    "    connect_edge_index, connect_features, connect_label = filter_out_isolate(data.edge_index, data.x, data.y)\n",
    "    clustering_machine = ClusteringMachine(connect_edge_index, connect_features, connect_label, partition_num = partition_num)\n",
    "    clustering_machine.decompose(test_ratio, validation_ratio)\n",
    "    return clustering_machine\n",
    "\n",
    "''' Draw the information about the GCN calculating batch and isolated cluster size '''\n",
    "def draw_cluster_info(pickle_filename, data_name, comments = '_cluster_node_distr'):\n",
    "    df = pd.read_pickle(pickle_filename)\n",
    "    df_reshape = df.melt('cluster_id', var_name = 'clusters', value_name = 'node_num')\n",
    "    \n",
    "    plt.clf()\n",
    "    plt.figure()\n",
    "    sns.set(style='whitegrid')\n",
    "    g = sns.catplot(x=\"cluster_id\", y=\"node_num\", hue='clusters', kind='bar', data=df_reshape)\n",
    "    g.despine(left=True)\n",
    "    g.fig.suptitle(data_name + comments)\n",
    "    g.set_xlabels(\"Cluster ID\")\n",
    "    g.set_ylabels(\"Number of nodes\")\n",
    "    \n",
    "    img_name = pickle_filename[:-4] + '_img'\n",
    "    os.makedirs(os.path.dirname(img_name), exist_ok=True)\n",
    "    g.savefig(img_name, bbox_inches='tight')\n",
    "    \n",
    "def store_cluster_trainbatch(clustering_machine, data_name, img_path, comments = '_cluster_node_distr'):\n",
    "    # store data for the train-batch size and cluster size\n",
    "    cluster_id = clustering_machine.clusters    # a list of cluster indices\n",
    "    cluster_datapoints = {'cluster_id': cluster_id,  \\\n",
    "                          'train_batch' : [clustering_machine.info_train_batch_size[idx] for idx in cluster_id], \\\n",
    "                          'cluster_size' : [clustering_machine.info_isolate_cluster_size[idx] for idx in cluster_id], \\\n",
    "                         }\n",
    "                         \n",
    "    df = pd.DataFrame(data=cluster_datapoints, dtype=np.int32)\n",
    "    \n",
    "    cluster_info_filename = img_path + data_name + '_' + comments + '.pkl'\n",
    "    os.makedirs(os.path.dirname(cluster_info_filename), exist_ok=True)\n",
    "    df.to_pickle(cluster_info_filename)\n",
    "    \n",
    "    # store data for the inter-trainbatch overlapping size info\n",
    "    batch_id = list(clustering_machine.info_overlap_batch_nodes.keys())   # a list of tuple\n",
    "    overlap_batch_nodes_data = {'batch_id' : batch_id, \\\n",
    "                                'overlap_nodes' : [clustering_machine.info_overlap_batch_nodes[idx] for idx in batch_id]\n",
    "                               }\n",
    "    \n",
    "    df = pd.DataFrame(data=overlap_batch_nodes_data, dtype=np.int32)\n",
    "    overlap_batch_nodes_filename = img_path + data_name + '_overlap_batch_nodes' + '.pkl'\n",
    "    os.makedirs(os.path.dirname(overlap_batch_nodes_filename), exist_ok=True)\n",
    "    df.to_pickle(overlap_batch_nodes_filename)\n",
    "    \n",
    "    overlap_batch_edges_data = {'batch_id' : batch_id, \\\n",
    "                                'overlap_edges' : [clustering_machine.info_overlap_batch_edges[idx] for idx in batch_id]\n",
    "                               }\n",
    "    \n",
    "    df = pd.DataFrame(data=overlap_batch_edges_data, dtype=np.int32)\n",
    "    overlap_batch_edges_filename = img_path + data_name + '_overlap_batch_edges' + '.pkl'\n",
    "    os.makedirs(os.path.dirname(overlap_batch_edges_filename), exist_ok=True)\n",
    "    df.to_pickle(overlap_batch_edges_filename)\n",
    "    \n",
    "    return cluster_info_filename, overlap_batch_nodes_filename, overlap_batch_edges_filename\n",
    "\n",
    "\n",
    "''' Draw the information about the GCN calculating overlapping train-batches information '''\n",
    "def draw_overlap_trainbatch_info(pickle_filename, data_name, comments, ylabel):\n",
    "    df = pd.read_pickle(pickle_filename)\n",
    "    df_reshape = df.melt('batch_id', var_name = 'overlapvals', value_name = ylabel)\n",
    "    \n",
    "    plt.clf()\n",
    "    plt.figure()\n",
    "    sns.set(style='whitegrid')\n",
    "    g = sns.catplot(x=\"batch_id\", y=ylabel, kind='bar', data=df_reshape)\n",
    "    g.despine(left=True)\n",
    "    g.fig.suptitle(data_name + ' ' + comments + ' ' + ylabel)\n",
    "    g.set_xlabels(\"Batch ID\")\n",
    "    g.set_ylabels(ylabel)\n",
    "    \n",
    "    img_name = pickle_filename[:-4] + '_img'\n",
    "    os.makedirs(os.path.dirname(img_name), exist_ok=True)\n",
    "    g.savefig(img_name, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formal Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def No_partition_run(local_clustering_machine, data_name, dataset, image_path, input_layer = [16, 16], epochs=300, \\\n",
    "                     dropout = 0.3, lr = 0.01, weight_decay = 0.01):\n",
    "    \"\"\"\n",
    "    # the partition num: will determine the training, testing and validation data\n",
    "    return: test F-1 value, validation F-1 value\n",
    "    \"\"\"\n",
    "    clustering_machine = copy.deepcopy(local_clustering_machine)\n",
    "    # the accumulating neighbor nodes only contain train nodes, no hop neighbors\n",
    "    clustering_machine.mini_batch_train_clustering(0)\n",
    "    # 0) train the data as a whole with no parition\n",
    "    gcn_trainer = wholeClusterGCNTrainer_sequence(clustering_machine, dataset.num_node_features, dataset.num_classes, input_layers = input_layer, dropout = dropout)\n",
    "    gcn_trainer.train(epoch_num=epochs, learning_rate=lr, weight_decay=weight_decay)\n",
    "    \n",
    "#     test_F1, test_accuracy = gcn_trainer.test()\n",
    "    validation_F1, validation_accuracy = gcn_trainer.validate()\n",
    "    time_train_total = gcn_trainer.time_train_total\n",
    "    time_data_load = gcn_trainer.time_train_load_data\n",
    "    return validation_accuracy, validation_F1, time_train_total, time_data_load, gcn_trainer\n",
    "\n",
    "\n",
    "def Cluster_train_batch_run(local_clustering_machine, data_name, dataset, image_path, input_layer = [16, 16], epochs=300, neigh_layer = 1, \\\n",
    "                           dropout = 0.3, lr = 0.01, weight_decay = 0.01):\n",
    "    \"\"\"\n",
    "    # the partition num: will determine the training, testing and validation data\n",
    "    Tuning parameters:  dropout, lr (learning rate), weight_decay: l2 regularization\n",
    "    return: validation accuracy value, validation F-1 value, time_training (ms), time_data_load (ms)\n",
    "    \"\"\"\n",
    "    clustering_machine = copy.deepcopy(local_clustering_machine)\n",
    "    # defalt to contain 1 layer of neighbors of train nodes\n",
    "    clustering_machine.mini_batch_train_clustering(neigh_layer)\n",
    "    \n",
    "    gcn_trainer = ClusterGCNTrainer_mini_Train(clustering_machine, dataset.num_node_features, dataset.num_classes, input_layers = input_layer, dropout = dropout)\n",
    "    gcn_trainer.train(epoch_num=epochs, learning_rate=lr, weight_decay=weight_decay)\n",
    "    \n",
    "#     test_F1, test_accuracy = gcn_trainer.test()\n",
    "    validation_F1, validation_accuracy = gcn_trainer.validate()\n",
    "    time_train_total = gcn_trainer.time_train_total\n",
    "    time_data_load = gcn_trainer.time_train_load_data\n",
    "    return validation_accuracy, validation_F1, time_train_total, time_data_load, gcn_trainer\n",
    "\n",
    "\n",
    "def Isolate_clustering_run(local_clustering_machine, data_name, dataset, image_path, input_layer = [16, 16], epochs=300, neigh_layer = 1, \\\n",
    "                           dropout = 0.3, lr = 0.01, weight_decay = 0.01):\n",
    "    \"\"\"\n",
    "    # the partition num: will determine the training, testing and validation data\n",
    "    return: test F-1 value, validation F-1 value\n",
    "    \"\"\"\n",
    "    clustering_machine = copy.deepcopy(local_clustering_machine)\n",
    "    # defalt to contain 1 layer of neighbors of train nodes\n",
    "    clustering_machine.general_isolate_clustering(neigh_layer)\n",
    "    gcn_trainer = ClusterGCNTrainer_mini_Train(clustering_machine, dataset.num_node_features, dataset.num_classes, input_layers = input_layer, dropout = dropout)\n",
    "    gcn_trainer.train(epoch_num=epochs, learning_rate=lr, weight_decay=weight_decay)\n",
    "    \n",
    "#     test_F1, test_accuracy = gcn_trainer.test()\n",
    "    validation_F1, validation_accuracy = gcn_trainer.validate()\n",
    "    time_train_total = gcn_trainer.time_train_total\n",
    "    time_data_load = gcn_trainer.time_train_load_data\n",
    "    return validation_accuracy, validation_F1, time_train_total, time_data_load, gcn_trainer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_train_loss_converge(clustering_machine, data_name, dataset, image_path,  comments, input_layer = [32, 16], epoch_num = 300, layer_num = 1, dropout = 0.3, lr = 0.0001, weight_decay = 0.01):\n",
    "    a0, v0, time0, load0, Cluster_train_batch_trainer = Cluster_train_batch_run(clustering_machine, data_name, dataset, image_path, input_layer = input_layer, epochs=epoch_num, neigh_layer = layer_num, \\\n",
    "                                                                               dropout = dropout, lr = lr, weight_decay = weight_decay)\n",
    "    draw_Cluster_train_batch = draw_trainer_info(data_name, Cluster_train_batch_trainer, image_path, 'train_batch_' + comments)\n",
    "    draw_Cluster_train_batch.draw_ave_loss_per_node()\n",
    "    \n",
    "    a1, v1, time1, load1, Isolate_clustering_trainer = Isolate_clustering_run(clustering_machine, data_name, dataset, image_path, input_layer = input_layer, epochs=epoch_num, neigh_layer = layer_num, \\\n",
    "                                                                             dropout = dropout, lr = lr, weight_decay = weight_decay)\n",
    "    draw_Isolate_clustering = draw_trainer_info(data_name, Isolate_clustering_trainer, image_path, 'Isolate_' + comments)\n",
    "    draw_Isolate_clustering.draw_ave_loss_per_node()\n",
    "    \n",
    "    # whole graph version, should not work for the large scale graph\n",
    "    a2, v2, time2, load2, No_partition_trainer = No_partition_run(clustering_machine, data_name, dataset, image_path, input_layer = input_layer, epochs=epoch_num, \\\n",
    "                                                                 dropout = dropout, lr = lr, weight_decay = weight_decay)\n",
    "    draw_No_partition = draw_trainer_info(data_name, No_partition_trainer, image_path, 'whole_' + comments)\n",
    "    draw_No_partition.draw_ave_loss_per_node()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Execute the testing program '''\n",
    "def execute_one(clustering_machine, image_path, repeate_time = 5, input_layer = [32, 16], epoch_num = 300, layer_num = 1, dropout = 0.3, lr = 0.0001, weight_decay = 0.01):\n",
    "    \"\"\"\n",
    "        return all 1) validation-accuracy, 2) validation-F1, 3) total training time, 4) data load onto GPU time, for all four models\n",
    "    \"\"\"\n",
    "#     test_f1 = {}\n",
    "    validation_accuracy = {}\n",
    "    validation_f1 = {}\n",
    "    time_total_train = {}\n",
    "    time_data_load = {}\n",
    "    \n",
    "    for i in range(repeate_time):\n",
    "        a0, v0, time0, load0, _ = Cluster_train_batch_run(clustering_machine, data_name, dataset, image_path, input_layer = input_layer, epochs=epoch_num, neigh_layer = layer_num, \\\n",
    "                                                         dropout = dropout, lr = lr, weight_decay = weight_decay)\n",
    "        a1, v1, time1, load1, _ = Isolate_clustering_run(clustering_machine, data_name, dataset, image_path, input_layer = input_layer, epochs=epoch_num, neigh_layer = layer_num, \\\n",
    "                                                        dropout = dropout, lr = lr, weight_decay = weight_decay)\n",
    "        a2, v2, time2, load2, _ = No_partition_run(clustering_machine, data_name, dataset, image_path, input_layer = input_layer, epochs=epoch_num, \n",
    "                                                  dropout = dropout, lr = lr, weight_decay = weight_decay)\n",
    "    \n",
    "#         test_f1[i] = [t0, t1, t2]\n",
    "        validation_accuracy[i] = [a0, a1, a2]\n",
    "        validation_f1[i] = [v0, v1, v2]\n",
    "        time_total_train[i] = [time0, time1, time2]\n",
    "        time_data_load[i] = [load0, load1, load2]\n",
    "    return validation_accuracy, validation_f1, time_total_train, time_data_load\n",
    "\n",
    "def store_data_multi_tests(f1_data, data_name, img_path, comments):\n",
    "    run_id = sorted(f1_data.keys())\n",
    "    run_data = {'run_id': run_id,  \\\n",
    "                'train_batch' : [f1_data[key][0] for key in run_id], \\\n",
    "                'isolate' : [f1_data[key][1] for key in run_id], \\\n",
    "                'whole_graph' : [f1_data[key][2] for key in run_id], \\\n",
    "               }\n",
    "    \n",
    "    pickle_filename = img_path + data_name + '_' + comments + '.pkl'\n",
    "    os.makedirs(os.path.dirname(pickle_filename), exist_ok=True)\n",
    "    df = pd.DataFrame(data=run_data, dtype=np.int32)\n",
    "    df.to_pickle(pickle_filename)\n",
    "    return pickle_filename\n",
    "\n",
    "def draw_data_multi_tests(pickle_filename, data_name, comments, ylabel):\n",
    "    df = pd.read_pickle(pickle_filename)\n",
    "    df_reshape = df.melt('run_id', var_name = 'model', value_name = ylabel)\n",
    "\n",
    "    plt.clf()\n",
    "    plt.figure()\n",
    "    sns.set(style='whitegrid')\n",
    "    g = sns.catplot(x=\"model\", y=ylabel, kind='box', data=df_reshape)\n",
    "    g.despine(left=True)\n",
    "    g.fig.suptitle(data_name + ' ' + ylabel + ' ' + comments)\n",
    "    g.set_xlabels(\"models\")\n",
    "    g.set_ylabels(ylabel)\n",
    "\n",
    "    img_name = pickle_filename[:-4] + '_img'\n",
    "    os.makedirs(os.path.dirname(img_name), exist_ok=True)\n",
    "    plt.savefig(img_name, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_tuning(tune_params, clustering_machine, image_path, repeate_time = 5, input_layer = [32, 32], epoch_num = 300, layer_num = 1):\n",
    "    \"\"\"\n",
    "        Tune all the hyperparameters\n",
    "        1) learning rate\n",
    "        2) dropout\n",
    "        3) layer unit number\n",
    "        4) weight decay\n",
    "    \"\"\"\n",
    "    validation_accuracy = {}\n",
    "    validation_f1 = {}\n",
    "    time_total_train = {}\n",
    "    time_data_load = {}\n",
    "    \n",
    "    res = [[Cluster_train_batch_run(clustering_machine, data_name, dataset, image_path, \\\n",
    "            input_layer = tune_val, epochs=epoch_num, neigh_layer = layer_num, \\\n",
    "            dropout = 0.3, lr = 10**(-4), weight_decay = 10**(-3))[:4] for tune_val in tune_params] for i in range(repeate_time)]\n",
    "    \n",
    "    for i, lst in enumerate(res):\n",
    "        validation_accuracy[i] = [val[0] for val in lst]\n",
    "        validation_f1[i] = [val[1] for val in lst]\n",
    "        time_total_train[i] = [val[2] for val in lst]\n",
    "        time_data_load[i] = [val[3] for val in lst]\n",
    "        \n",
    "    return validation_accuracy, validation_f1, time_total_train, time_data_load\n",
    "\n",
    "def store_data_multi_tuning(tune_params, target, data_name, img_path, comments):\n",
    "    run_ids = sorted(target.keys())\n",
    "    run_data = {'run_id': run_ids}\n",
    "    tmp = {str(key) : [target[run_id][i] for run_id in run_ids] for i, key in enumerate(tune_params)}\n",
    "    run_data.update(tmp)\n",
    "    \n",
    "    pickle_filename = img_path + data_name + '_' + comments + '.pkl'\n",
    "    os.makedirs(os.path.dirname(pickle_filename), exist_ok=True)\n",
    "    df = pd.DataFrame(data=run_data, dtype=np.int32)\n",
    "    df.to_pickle(pickle_filename)\n",
    "    return pickle_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use data from pytorch geometric datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_data_root = '/media/xiangli/storage1/projects/tmpdata/'\n",
    "test_folder_name = 'back_correct_valid_overlap/train_10%_hop_equal_net_layer/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cora dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "data_name = 'Cora'\n",
    "dataset = Planetoid(root = local_data_root + 'Planetoid/Cora', name=data_name)\n",
    "data = dataset[0]\n",
    "image_data_path = './results/' + data_name + '/' + test_folder_name\n",
    "\n",
    "partition_nums = [2, 4, 8]\n",
    "layers = [[32, 32]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start checking train loss for partition num: 2 hop layer: 3\n",
      "Start checking train loss for partition num: 4 hop layer: 3\n",
      "Start checking train loss for partition num: 8 hop layer: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/xiangli/storage1/projects/large_scale_GCN/handle_overlap/utils.py:25: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  plt.figure()\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:13: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  del sys.path[0]\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:68: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:68: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 464.35x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 464.35x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 464.35x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check convergence\n",
    "\n",
    "for partn in partition_nums:\n",
    "    for GCN_layer in layers:\n",
    "        net_layer = len(GCN_layer) + 1\n",
    "        hop_layer = net_layer\n",
    "        clustering_machine = set_clustering_machine(data, partition_num = partn, test_ratio = 0.05, validation_ratio = 0.85)\n",
    "        print('Start checking train loss for partition num: ' + str(partn) + ' hop layer: ' + str(hop_layer))\n",
    "        img_path = image_data_path + 'cluster_num_' + str(partn) + '/' + 'net_layer_' + str(net_layer) + '_hop_layer_' + str(hop_layer) + '/'\n",
    "        check_train_loss_converge(clustering_machine, data_name, dataset, img_path, 'part_num_' + str(partn), input_layer = GCN_layer, epoch_num = 400, layer_num = hop_layer, \\\n",
    "                                 dropout = 0.1, lr = 0.0001, weight_decay = 0.1)\n",
    "        clustering_machine.mini_batch_train_clustering(hop_layer)\n",
    "        cluster_info, overlap_batch_nodes, overlap_batch_edges = store_cluster_trainbatch(clustering_machine, data_name, img_path, comments = '_cluster_node_distr')\n",
    "        draw_cluster_info(cluster_info, data_name, comments = '_cluster_node_distr_' + str(hop_layer) + '_hops')\n",
    "        draw_overlap_trainbatch_info(overlap_batch_nodes, data_name, 'overlaping', 'number of nodes')\n",
    "        draw_overlap_trainbatch_info(overlap_batch_edges, data_name, 'overlaping', 'number of edges')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output accuracy, F1, time (train, load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start running for partition num: 2 hop layer 3\n",
      "Start running for partition num: 4 hop layer 3\n",
      "Start running for partition num: 8 hop layer 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:46: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:46: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for partn in partition_nums:\n",
    "    for GCN_layer in layers:\n",
    "        net_layer = len(GCN_layer) + 1\n",
    "        hop_layer = net_layer\n",
    "        clustering_machine = set_clustering_machine(data, partition_num = partn, test_ratio = 0.05, validation_ratio = 0.85)\n",
    "        print('Start running for partition num: ' + str(partn) + ' hop layer ' + str(hop_layer))\n",
    "        img_path = image_data_path + 'cluster_num_' + str(partn) + '/' + 'net_layer_' + str(net_layer) + '_hop_layer_' + str(hop_layer) + '/'\n",
    "\n",
    "        validation_accuracy, validation_f1, time_total_train, time_data_load = execute_one(clustering_machine, img_path, repeate_time = 7, input_layer = GCN_layer, epoch_num = 400, layer_num = hop_layer, \\\n",
    "                                                                                          dropout = 0.1, lr = 0.0001, weight_decay = 0.1)\n",
    "        \n",
    "        validation_accuracy = store_data_multi_tests(validation_accuracy, data_name, img_path, 'test_cluster_num_' + str(partn) + '_hops_' + str(hop_layer))\n",
    "        draw_data_multi_tests(validation_accuracy, data_name, 'vali_cluster_num_' + str(partn) + '_hop_' + str(hop_layer), 'Accuracy')\n",
    "        validation_f1 = store_data_multi_tests(validation_f1, data_name, img_path, 'validation_cluster_num_' + str(partn) + '_hops_' + str(hop_layer))\n",
    "        draw_data_multi_tests(validation_f1, data_name, 'vali_cluster_num_' + str(partn) + '_hop_' + str(hop_layer), 'F1 score')\n",
    "        \n",
    "        time_train = store_data_multi_tests(time_total_train, data_name, img_path, 'train_time_cluster_num_' + str(partn) + '_hops_' + str(hop_layer))\n",
    "        draw_data_multi_tests(time_train, data_name, 'train_time_cluster_num_' + str(partn) + '_hop_' + str(hop_layer), 'Train Time (ms)')\n",
    "        time_load = store_data_multi_tests(time_data_load, data_name, img_path, 'load_time_cluster_num_' + str(partn) + '_hops_' + str(hop_layer))\n",
    "        draw_data_multi_tests(time_load, data_name, 'load_time_cluster_num_' + str(partn) + '_hop_' + str(hop_layer), 'Load Time (ms)')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CiteSeer Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "data_name = 'CiteSeer'\n",
    "dataset = Planetoid(root = local_data_root + 'Planetoid/CiteSeer', name=data_name)\n",
    "data = dataset[0]\n",
    "image_data_path = './results/' + data_name + '/' + test_folder_name\n",
    "\n",
    "partition_nums = [2, 4, 8]\n",
    "layers = [[], [16], [16, 16], [16, 16, 16]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking train loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start checking train loss for partition num: 2 hop layer: 1\n",
      "Start checking train loss for partition num: 2 hop layer: 2\n",
      "Start checking train loss for partition num: 2 hop layer: 3\n",
      "Start checking train loss for partition num: 2 hop layer: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start checking train loss for partition num: 4 hop layer: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/xiangli/storage/projects/large_scale_GCN/neighbor_sampling/utils.py:25: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  plt.figure()\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:20: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start checking train loss for partition num: 4 hop layer: 2\n",
      "Start checking train loss for partition num: 4 hop layer: 3\n",
      "Start checking train loss for partition num: 4 hop layer: 4\n",
      "Start checking train loss for partition num: 8 hop layer: 1\n",
      "Start checking train loss for partition num: 8 hop layer: 2\n",
      "Start checking train loss for partition num: 8 hop layer: 3\n",
      "Start checking train loss for partition num: 8 hop layer: 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 464.35x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 464.35x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 464.35x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 464.35x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 464.35x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 464.35x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 464.35x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 464.35x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 464.35x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 464.35x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 464.35x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 464.35x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check convergence\n",
    "\n",
    "for partn in partition_nums:\n",
    "    for GCN_layer in layers:\n",
    "        net_layer = len(GCN_layer) + 1\n",
    "        hop_layer = net_layer\n",
    "        clustering_machine = set_clustering_machine(data, partition_num = partn, test_ratio = 0.05, validation_ratio = 0.85)\n",
    "        print('Start checking train loss for partition num: ' + str(partn) + ' hop layer: ' + str(hop_layer))\n",
    "        img_path = image_data_path + 'cluster_num_' + str(partn) + '/' + 'net_layer_' + str(net_layer) + '_hop_layer_' + str(hop_layer) + '/'\n",
    "        check_train_loss_converge(clustering_machine, data_name, dataset, img_path, 'part_num_' + str(partn), input_layer = GCN_layer, epoch_num = 400, layer_num = hop_layer, \\\n",
    "                                 dropout = 0.5, lr = 0.0001, weight_decay = 0.1)\n",
    "        clustering_machine.mini_batch_train_clustering(hop_layer)\n",
    "        draw_cluster_info(clustering_machine, data_name, img_path, comments = '_cluster_node_distr_' + str(hop_layer) + '_hops')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start running for partition num: 2 hop layer 1\n",
      "Start running for partition num: 2 hop layer 2\n",
      "Start running for partition num: 2 hop layer 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:45: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:45: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start running for partition num: 2 hop layer 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:45: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:45: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:45: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:45: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start running for partition num: 4 hop layer 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:45: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:45: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:45: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:45: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start running for partition num: 4 hop layer 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:45: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:45: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:45: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:45: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start running for partition num: 4 hop layer 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:45: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:45: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:45: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:45: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start running for partition num: 4 hop layer 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:45: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:45: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:45: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:45: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start running for partition num: 8 hop layer 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:45: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:45: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:45: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:45: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start running for partition num: 8 hop layer 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:45: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:45: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:45: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:45: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start running for partition num: 8 hop layer 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:45: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:45: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:45: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:45: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start running for partition num: 8 hop layer 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:45: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:45: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:45: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:45: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for partn in partition_nums:\n",
    "    for GCN_layer in layers:\n",
    "        net_layer = len(GCN_layer) + 1\n",
    "        hop_layer = net_layer\n",
    "        clustering_machine = set_clustering_machine(data, partition_num = partn, test_ratio = 0.05, validation_ratio = 0.85)\n",
    "        print('Start running for partition num: ' + str(partn) + ' hop layer ' + str(hop_layer))\n",
    "        img_path = image_data_path + 'cluster_num_' + str(partn) + '/' + 'net_layer_' + str(net_layer) + '_hop_layer_' + str(hop_layer) + '/'\n",
    "\n",
    "        validation_accuracy, validation_f1, time_total_train, time_data_load = execute_one(clustering_machine, img_path, repeate_time = 7, input_layer = GCN_layer, epoch_num = 400, layer_num = hop_layer, \\\n",
    "                                                                                          dropout = 0.5, lr = 0.0001, weight_decay = 0.1)\n",
    "        \n",
    "        validation_accuracy = store_data_multi_tests(validation_accuracy, data_name, img_path, 'test_cluster_num_' + str(partn) + '_hops_' + str(hop_layer))\n",
    "        draw_data_multi_tests(validation_accuracy, data_name, 'vali_cluster_num_' + str(partn) + '_hop_' + str(hop_layer), 'Accuracy')\n",
    "        validation_f1 = store_data_multi_tests(validation_f1, data_name, img_path, 'validation_cluster_num_' + str(partn) + '_hops_' + str(hop_layer))\n",
    "        draw_data_multi_tests(validation_f1, data_name, 'vali_cluster_num_' + str(partn) + '_hop_' + str(hop_layer), 'F1 score')\n",
    "        \n",
    "        time_train = store_data_multi_tests(time_total_train, data_name, img_path, 'train_time_cluster_num_' + str(partn) + '_hops_' + str(hop_layer))\n",
    "        draw_data_multi_tests(time_train, data_name, 'train_time_cluster_num_' + str(partn) + '_hop_' + str(hop_layer), 'Train Time (ms)')\n",
    "        time_load = store_data_multi_tests(time_data_load, data_name, img_path, 'load_time_cluster_num_' + str(partn) + '_hops_' + str(hop_layer))\n",
    "        draw_data_multi_tests(time_load, data_name, 'load_time_cluster_num_' + str(partn) + '_hop_' + str(hop_layer), 'Load Time (ms)')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PubMed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "data_name = 'PubMed'\n",
    "dataset = Planetoid(root = local_data_root + 'Planetoid/PubMed', name=data_name)\n",
    "data = dataset[0]\n",
    "image_data_path = './results/' + data_name + '/' + test_folder_name\n",
    "\n",
    "partition_nums = [2, 4, 8]\n",
    "layers = [[], [64], [64, 64], [64, 64, 64]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start checking train loss for partition num: 2 hop layer: 1\n",
      "Start checking train loss for partition num: 2 hop layer: 2\n",
      "Start checking train loss for partition num: 2 hop layer: 3\n",
      "Start checking train loss for partition num: 2 hop layer: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start checking train loss for partition num: 4 hop layer: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/xiangli/storage/projects/large_scale_GCN/neighbor_sampling/utils.py:25: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  plt.figure()\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:20: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start checking train loss for partition num: 4 hop layer: 2\n",
      "Start checking train loss for partition num: 4 hop layer: 3\n",
      "Start checking train loss for partition num: 4 hop layer: 4\n",
      "Start checking train loss for partition num: 8 hop layer: 1\n",
      "Start checking train loss for partition num: 8 hop layer: 2\n",
      "Start checking train loss for partition num: 8 hop layer: 3\n",
      "Start checking train loss for partition num: 8 hop layer: 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 464.35x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 464.35x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 464.35x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 464.35x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 464.35x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 464.35x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 464.35x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 464.35x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 464.35x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 464.35x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 464.35x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 464.35x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check convergence\n",
    "\n",
    "for partn in partition_nums:\n",
    "    for GCN_layer in layers:\n",
    "        net_layer = len(GCN_layer) + 1\n",
    "        hop_layer = net_layer\n",
    "        clustering_machine = set_clustering_machine(data, partition_num = partn, test_ratio = 0.05, validation_ratio = 0.85)\n",
    "        print('Start checking train loss for partition num: ' + str(partn) + ' hop layer: ' + str(hop_layer))\n",
    "        img_path = image_data_path + 'cluster_num_' + str(partn) + '/' + 'net_layer_' + str(net_layer) + '_hop_layer_' + str(hop_layer) + '/'\n",
    "        check_train_loss_converge(clustering_machine, data_name, dataset, img_path, 'part_num_' + str(partn), input_layer = GCN_layer, epoch_num = 400, layer_num = hop_layer, \\\n",
    "                                 dropout = 0.3, lr = 0.0001, weight_decay = 0.001)\n",
    "        clustering_machine.mini_batch_train_clustering(hop_layer)\n",
    "        draw_cluster_info(clustering_machine, data_name, img_path, comments = '_cluster_node_distr_' + str(hop_layer) + '_hops')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start running for partition num: 2 hop layer 1\n",
      "Start running for partition num: 2 hop layer 2\n",
      "Start running for partition num: 2 hop layer 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:45: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:45: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start running for partition num: 2 hop layer 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:45: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:45: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:45: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:45: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start running for partition num: 4 hop layer 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:45: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:45: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:45: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:45: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start running for partition num: 4 hop layer 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:45: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:45: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:45: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:45: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start running for partition num: 4 hop layer 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:45: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:45: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:45: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:45: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start running for partition num: 4 hop layer 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:45: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:45: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:45: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:45: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start running for partition num: 8 hop layer 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:45: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:45: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:45: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:45: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start running for partition num: 8 hop layer 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:45: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:45: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:45: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:45: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start running for partition num: 8 hop layer 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:45: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:45: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:45: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:45: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start running for partition num: 8 hop layer 4\n"
     ]
    }
   ],
   "source": [
    "for partn in partition_nums:\n",
    "    for GCN_layer in layers:\n",
    "        net_layer = len(GCN_layer) + 1\n",
    "        hop_layer = net_layer\n",
    "        clustering_machine = set_clustering_machine(data, partition_num = partn, test_ratio = 0.05, validation_ratio = 0.85)\n",
    "        print('Start running for partition num: ' + str(partn) + ' hop layer ' + str(hop_layer))\n",
    "        img_path = image_data_path + 'cluster_num_' + str(partn) + '/' + 'net_layer_' + str(net_layer) + '_hop_layer_' + str(hop_layer) + '/'\n",
    "\n",
    "        validation_accuracy, validation_f1, time_total_train, time_data_load = execute_one(clustering_machine, img_path, repeate_time = 7, input_layer = GCN_layer, epoch_num = 400, layer_num = hop_layer, \\\n",
    "                                                                                          dropout = 0.3, lr = 0.0001, weight_decay = 0.001)\n",
    "        \n",
    "        validation_accuracy = store_data_multi_tests(validation_accuracy, data_name, img_path, 'test_cluster_num_' + str(partn) + '_hops_' + str(hop_layer))\n",
    "        draw_data_multi_tests(validation_accuracy, data_name, 'vali_cluster_num_' + str(partn) + '_hop_' + str(hop_layer), 'Accuracy')\n",
    "        validation_f1 = store_data_multi_tests(validation_f1, data_name, img_path, 'validation_cluster_num_' + str(partn) + '_hops_' + str(hop_layer))\n",
    "        draw_data_multi_tests(validation_f1, data_name, 'vali_cluster_num_' + str(partn) + '_hop_' + str(hop_layer), 'F1 score')\n",
    "        \n",
    "        time_train = store_data_multi_tests(time_total_train, data_name, img_path, 'train_time_cluster_num_' + str(partn) + '_hops_' + str(hop_layer))\n",
    "        draw_data_multi_tests(time_train, data_name, 'train_time_cluster_num_' + str(partn) + '_hop_' + str(hop_layer), 'Train Time (ms)')\n",
    "        time_load = store_data_multi_tests(time_data_load, data_name, img_path, 'load_time_cluster_num_' + str(partn) + '_hops_' + str(hop_layer))\n",
    "        draw_data_multi_tests(time_load, data_name, 'load_time_cluster_num_' + str(partn) + '_hop_' + str(hop_layer), 'Load Time (ms)')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free GPU memory\n",
    "# !(nvidia-smi | grep 'python' | awk '{ print $3 }' | xargs -n1 kill -9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_geometric]",
   "language": "python",
   "name": "conda-env-pytorch_geometric-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
