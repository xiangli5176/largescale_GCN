{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-batch model for hpc run with input edge weights in csv file\n",
    "\n",
    "Comments:\n",
    "\n",
    "By using the read weighted edge list from a csv file, it saves much space on self.graph\n",
    "\n",
    "This will for specific batch number and hop-layer number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import csv\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import shutil\n",
    "\n",
    "from utils import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import torch\n",
    "from torch_geometric.utils import scatter_\n",
    "\n",
    "special_args = [\n",
    "    'edge_index', 'edge_index_i', 'edge_index_j', 'size', 'size_i', 'size_j'\n",
    "]\n",
    "__size_error_msg__ = ('All tensors which should get mapped to the same source '\n",
    "                      'or target nodes must be of same size in dimension 0.')\n",
    "\n",
    "is_python2 = sys.version_info[0] < 3\n",
    "getargspec = inspect.getargspec if is_python2 else inspect.getfullargspec\n",
    "\n",
    "\n",
    "class MessagePassing(torch.nn.Module):\n",
    "    r\"\"\"Base class for creating message passing layers\n",
    "\n",
    "    .. math::\n",
    "        \\mathbf{x}_i^{\\prime} = \\gamma_{\\mathbf{\\Theta}} \\left( \\mathbf{x}_i,\n",
    "        \\square_{j \\in \\mathcal{N}(i)} \\, \\phi_{\\mathbf{\\Theta}}\n",
    "        \\left(\\mathbf{x}_i, \\mathbf{x}_j,\\mathbf{e}_{i,j}\\right) \\right),\n",
    "\n",
    "    where :math:`\\square` denotes a differentiable, permutation invariant\n",
    "    function, *e.g.*, sum, mean or max, and :math:`\\gamma_{\\mathbf{\\Theta}}`\n",
    "    and :math:`\\phi_{\\mathbf{\\Theta}}` denote differentiable functions such as\n",
    "    MLPs.\n",
    "    See `here <https://pytorch-geometric.readthedocs.io/en/latest/notes/\n",
    "    create_gnn.html>`__ for the accompanying tutorial.\n",
    "\n",
    "    Args:\n",
    "        aggr (string, optional): The aggregation scheme to use\n",
    "            (:obj:`\"add\"`, :obj:`\"mean\"` or :obj:`\"max\"`).\n",
    "            (default: :obj:`\"add\"`)\n",
    "        flow (string, optional): The flow direction of message passing\n",
    "            (:obj:`\"source_to_target\"` or :obj:`\"target_to_source\"`).\n",
    "            (default: :obj:`\"source_to_target\"`)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, aggr='add', flow='source_to_target'):\n",
    "        super(MessagePassing, self).__init__()\n",
    "\n",
    "        self.aggr = aggr\n",
    "        assert self.aggr in ['add', 'mean', 'max']\n",
    "\n",
    "        self.flow = flow\n",
    "        # give a warning if the option is not valid\n",
    "        assert self.flow in ['source_to_target', 'target_to_source']\n",
    "\n",
    "        self.__message_args__ = getargspec(self.message)[0][1:]\n",
    "        # we will have [x_j, norm ] put into self.__message_args__\n",
    "        \n",
    "        self.__special_args__ = [(i, arg)\n",
    "                                 for i, arg in enumerate(self.__message_args__)\n",
    "                                 if arg in special_args]\n",
    "        \n",
    "        self.__message_args__ = [arg for arg in self.__message_args__ if arg not in special_args]\n",
    "        \n",
    "        self.__update_args__ = getargspec(self.update)[0][2:]\n",
    "        # empty, since there is nothing beyond: agg_out\n",
    "\n",
    "#     function call: res = self.propagate(edge_index, x=x, norm=norm)\n",
    "    def propagate(self, edge_index, size=None, **kwargs):\n",
    "        r\"\"\"The initial call to start propagating messages.\n",
    "\n",
    "        Args:\n",
    "            edge_index (Tensor): The indices of a general (sparse) assignment\n",
    "                matrix with shape :obj:`[N, M]` (can be directed or\n",
    "                undirected).\n",
    "            size (list or tuple, optional): The size :obj:`[N, M]` of the\n",
    "                assignment matrix. If set to :obj:`None`, the size is tried to\n",
    "                get automatically inferred. (default: :obj:`None`)\n",
    "            **kwargs: Any additional data which is needed to construct messages\n",
    "                and to update node embeddings.\n",
    "        \"\"\"\n",
    "        dim = 0\n",
    "        size = [None, None] if size is None else list(size)\n",
    "        assert len(size) == 2\n",
    "\n",
    "        i, j = (0, 1) if self.flow == 'target_to_source' else (1, 0)\n",
    "        # here (i, j) == (1, 0)\n",
    "        ij = {\"_i\": i, \"_j\": j}\n",
    "\n",
    "        message_args = []\n",
    "        \n",
    "        for arg in self.__message_args__:\n",
    "#             arg[-2] == '_j'\n",
    "            if arg[-2:] in ij.keys():\n",
    "#                 tmp == x, is inside the dwargs\n",
    "                tmp = kwargs.get(arg[:-2], None)   # get the value of the parameter\n",
    "                if tmp is None:  # pragma: no cover\n",
    "                    message_args.append(tmp)\n",
    "                else:\n",
    "                    idx = ij[arg[-2:]]    # idx == 0\n",
    "                    if isinstance(tmp, tuple) or isinstance(tmp, list):\n",
    "                        assert len(tmp) == 2\n",
    "                        if tmp[1 - idx] is not None:\n",
    "                            if size[1 - idx] is None:\n",
    "                                size[1 - idx] = tmp[1 - idx].size(dim)\n",
    "                            if size[1 - idx] != tmp[1 - idx].size(dim):\n",
    "                                raise ValueError(__size_error_msg__)\n",
    "                        tmp = tmp[idx]\n",
    "                    \n",
    "                    if tmp is None:\n",
    "                        message_args.append(tmp)\n",
    "                    else:\n",
    "                        if size[idx] is None:\n",
    "                            size[idx] = tmp.size(dim)\n",
    "                        if size[idx] != tmp.size(dim):\n",
    "                            raise ValueError(__size_error_msg__)\n",
    "                        # dim == 0, we duplicate part of the embeddings x by using the edge_index[idx]\n",
    "#                         print('Inside the propagate, edge_index[idx]: \\n', edge_index[idx].shape, '\\n', edge_index[idx])\n",
    "                        tmp = torch.index_select(tmp, dim, edge_index[idx])\n",
    "                        message_args.append(tmp)   # here we append x from the kwargs\n",
    "            else:\n",
    "                message_args.append(kwargs.get(arg, None))   # here we append norm\n",
    "        \n",
    "#         message_args are: x_j, norm \n",
    "#         size:  [8, None] \n",
    "#         kwargs:  dict_keys(['x', 'norm']) \n",
    "#         special keys:  []\n",
    "        \n",
    "        size[0] = size[1] if size[0] is None else size[0]\n",
    "        size[1] = size[0] if size[1] is None else size[1]\n",
    "\n",
    "        kwargs['edge_index'] = edge_index\n",
    "        kwargs['size'] = size\n",
    "        \n",
    "        # for now self.__special_args__ is empty\n",
    "        for (idx, arg) in self.__special_args__:\n",
    "            if arg[-2:] in ij.keys():\n",
    "                # here we will change the content of x (features)\n",
    "                # features will be corresponds to edge_index\n",
    "                message_args.insert(idx, kwargs[arg[:-2]][ij[arg[-2:]]])\n",
    "            else:\n",
    "                message_args.insert(idx, kwargs[arg])\n",
    "\n",
    "        update_args = [kwargs[arg] for arg in self.__update_args__]\n",
    "#         message_args are: x_j, norm \n",
    "        out = self.message(*message_args)\n",
    "        # here i = 1, edge_index is the target endpoint of an edge, size[i] is the size of target endpoints\n",
    "        out = scatter_(self.aggr, out, edge_index[i], dim_size=size[i])\n",
    "        out = self.update(out, *update_args)\n",
    "        return out\n",
    "\n",
    "    def message(self, x_j):  # pragma: no cover\n",
    "        r\"\"\"Constructs messages in analogy to :math:`\\phi_{\\mathbf{\\Theta}}`\n",
    "        for each edge in :math:`(i,j) \\in \\mathcal{E}`.\n",
    "        Can take any argument which was initially passed to :meth:`propagate`.\n",
    "        In addition, features can be lifted to the source node :math:`i` and\n",
    "        target node :math:`j` by appending :obj:`_i` or :obj:`_j` to the\n",
    "        variable name, *.e.g.* :obj:`x_i` and :obj:`x_j`.\"\"\"\n",
    "\n",
    "        return x_j\n",
    "\n",
    "    def update(self, aggr_out):  # pragma: no cover\n",
    "        r\"\"\"Updates node embeddings in analogy to\n",
    "        :math:`\\gamma_{\\mathbf{\\Theta}}` for each node\n",
    "        :math:`i \\in \\mathcal{V}`.\n",
    "        Takes in the output of aggregation as first argument and any argument\n",
    "        which was initially passed to :meth:`propagate`.\"\"\"\n",
    "\n",
    "        return aggr_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "from torch.nn import Parameter\n",
    "from torch_scatter import scatter_add\n",
    "# from torch_geometric.nn import MessagePassing\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import add_remaining_self_loops\n",
    "\n",
    "### ================== Definition of custom GCN\n",
    "\n",
    "def glorot(tensor):\n",
    "    if tensor is not None:\n",
    "        stdv = math.sqrt(6.0 / (tensor.size(-2) + tensor.size(-1)))\n",
    "        tensor.data.uniform_(-stdv, stdv)\n",
    "#         tensor.data.fill_(1.0)   # trivial example\n",
    "        \n",
    "def zeros(tensor):\n",
    "    if tensor is not None:\n",
    "        tensor.data.fill_(0)\n",
    "\n",
    "class custom_GCNConv(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels, improved=False, cached=False,\n",
    "                 bias=True, **kwargs):\n",
    "        super().__init__(aggr='add', **kwargs)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.improved = improved\n",
    "        self.cached = cached\n",
    "\n",
    "        self.weight = Parameter(torch.Tensor(in_channels, out_channels))\n",
    "\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        glorot(self.weight)\n",
    "        zeros(self.bias)\n",
    "        self.cached_result = None\n",
    "        self.cached_num_edges = None\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def norm(edge_index, num_nodes, edge_weight=None, improved=False, dtype=None):\n",
    "        \n",
    "        if edge_weight is None:\n",
    "            edge_weight = torch.ones((edge_index.size(1), ), dtype=dtype, device=edge_index.device)\n",
    "        \n",
    "        fill_value = 1 if not improved else 2\n",
    "        \n",
    "        edge_index, edge_weight = add_remaining_self_loops(\n",
    "            edge_index, edge_weight, fill_value, num_nodes)\n",
    "        \n",
    "        row, col = edge_index   \n",
    "        # row includes the starting points of the edges  (first row of edge_index)\n",
    "        # col includes the ending points of the edges   (second row of edge_index)\n",
    "\n",
    "        deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n",
    "        # row records the source nodes, which is the index we are trying to add\n",
    "        # deg will record the out-degree of each node of x_i in all edges (x_i, x_j) including self_loops\n",
    "        \n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "        normalized_edge_weight = deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n",
    "        \n",
    "#         print('whole GCN training normalized_edge_weight: \\n', normalized_edge_weight)\n",
    "        return edge_index, normalized_edge_weight\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight = None):\n",
    "        \"\"\"\"\"\"\n",
    "#         print('current weight is: ')\n",
    "#         print(self.weight)\n",
    "#         print('current bias is: ')\n",
    "#         print(self.bias)\n",
    "        \n",
    "        x = torch.matmul(x, self.weight)   # update x (embeddings)\n",
    "        \n",
    "#         print('inside custom_GCN, edge_index: ', edge_index.shape, '\\n', edge_index)\n",
    "        res = self.propagate(edge_index, x = x, norm = edge_weight)\n",
    "        return res\n",
    "\n",
    "    # self is the first parameter of the message func\n",
    "    def message(self, x_j, norm):\n",
    "        # in source code of the MessagePassing:\n",
    "#         self.__message_args__ = getargspec(self.message)[0][1:]  : will be initialized as [x_j, norm]\n",
    "        \n",
    "        # view is to reshape the tensor, here make it only a single column\n",
    "        # use the normalized weights multiplied by the feature of the target nodes\n",
    "        '''\n",
    "        For each of extended edge_index:(x_i, x_j), assume there is N such edges\n",
    "        x_j of shape (N, k) , assume there is k features, value along each row are the same\n",
    "        norm of shape (1, m), assume there is m edges (including self loops), 1-D tensor\n",
    "        '''\n",
    "#         print('inside the message custom_GCN: norm \\n', norm.shape, '\\n', norm)\n",
    "#         print('inside the message custom_GCN: x_j \\n', x_j.shape, '\\n', x_j)\n",
    "        res = norm.view(-1, 1) * x_j  # use the element wise multiplication\n",
    "        return res\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        # update the embeddings of each node\n",
    "        if self.bias is not None:\n",
    "            aggr_out = aggr_out + self.bias\n",
    "        return aggr_out\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}({}, {})'.format(self.__class__.__name__, self.in_channels,\n",
    "                                   self.out_channels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ====================== Establish a GCN based model ========================\n",
    "class ListModule(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Abstract list layer class.\n",
    "    \"\"\"\n",
    "    def __init__(self, *args):\n",
    "        \"\"\"\n",
    "        Module initializing.\n",
    "        \"\"\"\n",
    "        super(ListModule, self).__init__()\n",
    "        idx = 0\n",
    "        for module in args:\n",
    "            self.add_module(str(idx), module)\n",
    "            idx += 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Getting the indexed layer.\n",
    "        \"\"\"\n",
    "        if idx < 0 or idx >= len(self._modules):\n",
    "            raise IndexError('index {} is out of range'.format(idx))\n",
    "        it = iter(self._modules.values())\n",
    "        for i in range(idx):\n",
    "            next(it)\n",
    "        return next(it)\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Iterating on the layers.\n",
    "        \"\"\"\n",
    "        return iter(self._modules.values())\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Number of layers.\n",
    "        \"\"\"\n",
    "        return len(self._modules)\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, input_layers = [16, 16], dropout=0.3):\n",
    "        \"\"\"\n",
    "        input layers: list of integers\n",
    "        dropout: probability of droping out \n",
    "        \"\"\"\n",
    "        super(Net, self).__init__()\n",
    "        # one trivial example\n",
    "#         self.conv1 = custom_GCNConv(in_channels, out_channels)\n",
    "#         self.conv2 = GCNConv(16, dataset.num_classes)\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.input_layers = input_layers\n",
    "        self.dropout = dropout\n",
    "        self.setup_layers()\n",
    "\n",
    "    def setup_layers(self):\n",
    "        \"\"\"\n",
    "        Creating the layes based on the args.\n",
    "        \"\"\"\n",
    "        self.layers = []\n",
    "        self.input_layers = [self.in_channels] + self.input_layers + [self.out_channels]\n",
    "        for i, _ in enumerate(self.input_layers[:-1]):\n",
    "            self.layers.append(custom_GCNConv(self.input_layers[i],self.input_layers[i+1]))\n",
    "        self.layers = ListModule(*self.layers)\n",
    "\n",
    "    # change the dropout positions: \n",
    "    def forward(self, edge_index, features, edge_weights = None):\n",
    "        if len(self.layers) > 1:\n",
    "            for i in range(len(self.layers)-1):\n",
    "                features = F.relu(self.layers[i](features, edge_index, edge_weights))\n",
    "#                 if i>0:\n",
    "                features = F.dropout(features, p = self.dropout, training = self.training)\n",
    "                    \n",
    "            features = self.layers[len(self.layers)-1](features, edge_index, edge_weights)\n",
    "        else:\n",
    "            features = self.layers[0](features, edge_index, edge_weights)    # for a single layer case\n",
    "\n",
    "        predictions = F.log_softmax(features, dim=1)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edge_weight(edge_index, num_nodes, edge_weight=None, improved=False, dtype=None, store_path='./tmp/'):\n",
    "    \"\"\"\n",
    "        edge_index(ndarray): undirected edge index (two-directions both included)\n",
    "        num_nodes(int):  number of nodes inside the graph\n",
    "        edge_weight(ndarray): if any weights already assigned, otherwise will be generated \n",
    "        improved(boolean):   may assign 2 to the self loop weight if true\n",
    "        store_path(string): the path of the folder to contain all the clustering information files\n",
    "    \"\"\"\n",
    "    # calculate the global graph properties, global edge weights\n",
    "    if edge_weight is None:\n",
    "        edge_weight = torch.ones((edge_index.size(1), ), dtype=dtype, device=edge_index.device)\n",
    "\n",
    "    fill_value = 1 if not improved else 2\n",
    "    # there are num_nodes self-loop edges added after the edge_index\n",
    "    edge_index, edge_weight = add_remaining_self_loops(edge_index, edge_weight, fill_value, num_nodes)\n",
    "\n",
    "    row, col = edge_index   \n",
    "    # row includes the starting points of the edges  (first row of edge_index)\n",
    "    # col includes the ending points of the edges   (second row of edge_index)\n",
    "\n",
    "    deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n",
    "    # row records the source nodes, which is the index we are trying to add\n",
    "    # deg will record the out-degree of each node of x_i in all edges (x_i, x_j) including self_loops\n",
    "\n",
    "    deg_inv_sqrt = deg.pow(-0.5)\n",
    "    deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "    # normalize the edge weight\n",
    "    normalized_edge_weight = deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n",
    "\n",
    "    # transfer from tensor to the numpy to construct the dict for the edge_weights\n",
    "    edge_index = edge_index.t().numpy()\n",
    "    normalized_edge_weight = normalized_edge_weight.numpy()\n",
    "\n",
    "    num_edge = edge_index.shape[0]\n",
    "\n",
    "    output = ([edge_index[i][0], edge_index[i][1], normalized_edge_weight[i]] for i in range(num_edge))\n",
    "\n",
    "    # output the edge weights as the csv file\n",
    "    input_edge_weight_txt_file = store_path + 'input_edge_weight_list.csv'\n",
    "    os.makedirs(os.path.dirname(input_edge_weight_txt_file), exist_ok=True)\n",
    "    with open(input_edge_weight_txt_file, 'w', newline='\\n') as fp:\n",
    "        wr = csv.writer(fp, delimiter = ' ')\n",
    "        for line in output:\n",
    "            wr.writerow(line)\n",
    "    return input_edge_weight_txt_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partition GCN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Partition method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import metis\n",
    "import random\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import chain\n",
    "\n",
    "class ClusteringMachine(object):\n",
    "    \"\"\"\n",
    "    Clustering the graph, feature set and label. Performed on the CPU side\n",
    "    \"\"\"\n",
    "    def __init__(self, edge_index, features, label, tmp_folder = './tmp/'):\n",
    "        \"\"\"\n",
    "        :param edge_index: COO format of the edge indices.\n",
    "        :param features: Feature matrix (ndarray).\n",
    "        :param label: label vector (ndarray).\n",
    "        :tmp_folder(string): the path of the folder to contain all the clustering information files\n",
    "        \"\"\"\n",
    "        self.features = features\n",
    "        self.label = label\n",
    "        self._set_sizes()\n",
    "        self.edge_index = edge_index\n",
    "        # store the information folder for memory tracing\n",
    "        self.tmp_folder = tmp_folder\n",
    "        \n",
    "        edge_weight_file = self.tmp_folder + 'input_edge_weight_list.csv'\n",
    "        self.graph = nx.read_weighted_edgelist(edge_weight_file, create_using = nx.Graph, nodetype = int)\n",
    "\n",
    "    def _set_sizes(self):\n",
    "        \"\"\"\n",
    "        Setting the feature and class count.\n",
    "        \"\"\"\n",
    "        self.node_count = self.features.shape[0]\n",
    "        self.feature_count = self.features.shape[1]    # features all always in the columns\n",
    "        self.label_count = len(np.unique(self.label.numpy()) )\n",
    "    \n",
    "    # 2) first assign train, test, validation nodes, split edges; this is based on the assumption that the clustering is no longer that important\n",
    "    def split_whole_nodes_edges_then_cluster(self, test_ratio, validation_ratio, train_batch_num = 2, test_batch_num = 2):\n",
    "        \"\"\"\n",
    "            Only split nodes\n",
    "            First create train-test splits, then split train and validation into different batch seeds\n",
    "            Input:  \n",
    "                1) ratio of test, validation\n",
    "                2) partition number of train nodes, test nodes, validation nodes\n",
    "            Output:\n",
    "                1) self.sg_validation_nodes_global, self.sg_train_nodes_global, self.sg_test_nodes_global\n",
    "        \"\"\"\n",
    "        self.train_batch_num = train_batch_num\n",
    "        relative_test_ratio = (test_ratio) / (1 - validation_ratio)\n",
    "        \n",
    "        # first divide the nodes for the whole graph, result will always be a list of lists \n",
    "        model_nodes_global, self.valid_nodes_global = train_test_split(list(self.graph.nodes()), test_size = validation_ratio)\n",
    "        train_nodes_global, test_nodes_global = train_test_split(model_nodes_global, test_size = relative_test_ratio)\n",
    "        \n",
    "#         self.sg_train_nodes_global = self.random_clustering(train_nodes_global, train_batch_num)\n",
    "#         self.sg_validation_nodes_global = self.random_clustering(valid_nodes_global, valid_batch_num)\n",
    "#         self.sg_test_nodes_global = self.random_clustering(test_nodes_global, test_batch_num)\n",
    "\n",
    "        self.sg_train_nodes_global = self.metis_clustering(train_nodes_global, train_batch_num)\n",
    "        self.sg_test_nodes_global = self.metis_clustering(test_nodes_global, test_batch_num)\n",
    "        \n",
    "    # just allocate each node to arandom cluster, store the membership inside each dict\n",
    "    def random_clustering(self, target_nodes, partition_num):\n",
    "        \"\"\"\n",
    "            Random clustering the nodes.\n",
    "            Input: \n",
    "                1) target_nodes: list of node \n",
    "                2) partition_num: number of partition to be generated\n",
    "            Output: \n",
    "                1) membership of each node\n",
    "        \"\"\"\n",
    "        # randomly divide into two clusters\n",
    "        nodes_order = [node for node in target_nodes]\n",
    "        random.shuffle(nodes_order)\n",
    "        n = (len(nodes_order) + partition_num - 1) // partition_num\n",
    "        partition_list = [nodes_order[i * n:(i + 1) * n] for i in range(partition_num)]\n",
    "#         cluster_membership = {node : i for i, node_list in enumerate(partition_list) for node in node_list}\n",
    "        cluster_nodes_global = {i : node_list for i, node_list in enumerate(partition_list)}\n",
    "        \n",
    "        return cluster_nodes_global\n",
    "\n",
    "    def metis_clustering(self, target_nodes, partition_num):\n",
    "        \"\"\"\n",
    "            Random clustering the nodes.\n",
    "            Input: \n",
    "                1) target_nodes: list of node \n",
    "                2) partition_num: number of partition to be generated\n",
    "            Output: \n",
    "                1) membership of each node\n",
    "        \"\"\"\n",
    "        target_graph = self.graph.subgraph(target_nodes)\n",
    "        (st, parts) = metis.part_graph(target_graph, partition_num)\n",
    "        clusters = list(set(parts))\n",
    "        cluster_nodes_global = defaultdict(list)\n",
    "        for node, cluster_id in enumerate(parts):\n",
    "            cluster_nodes_global[cluster_id].append(node)\n",
    "        return cluster_nodes_global\n",
    "        \n",
    "    # select the training nodes as the mini-batch for each cluster\n",
    "    def mini_batch_sample(self, target_seed, k, frac = 1):\n",
    "        \"\"\"\n",
    "            This function is to generate the neighbors of the seed (either train nodes or validation nodes)\n",
    "            params: cluster index, number of layer k, fraction of sampling from each neighbor layer\n",
    "            input: \n",
    "                1) target_seed: this is the 0 layer inside self.neighbor\n",
    "            output:\n",
    "                1) neighbor: nodes global idx inside each layer of the batch\n",
    "                2) accum_neighbor: accumulating neighbors , i.e. the final batch nodes\n",
    "        \"\"\"\n",
    "        accum_neighbor = defaultdict(set)\n",
    "        for cluster in target_seed.keys():\n",
    "            neighbor = set(target_seed[cluster])  # first layer of the neighbor nodes of each cluster\n",
    "            for layer in range(k):\n",
    "                # first accumulate last layer\n",
    "                accum_neighbor[cluster] |= neighbor\n",
    "                tmp_level = set()\n",
    "                for node in neighbor:\n",
    "                    tmp_level |= set(self.graph.neighbors(node))  # the key here we are using self.graph, extract neighbor from the whole graph\n",
    "                # add the new layer of neighbors\n",
    "                tmp_level -= accum_neighbor[cluster]\n",
    "                # each layer will only contains partial nodes from the previous layer\n",
    "                neighbor = set(random.sample(tmp_level, int(len(tmp_level) * frac) ) ) if 0 < frac < 1 else tmp_level\n",
    "    #                 print('layer ' + str(layer + 1) + ' : ', self.neighbor[cluster][layer+1])\n",
    "            # the most outside layer: kth layer will be added:\n",
    "            accum_neighbor[cluster] |= neighbor\n",
    "        return accum_neighbor\n",
    "    \n",
    "    # this will sample the neighbor for use, these neighbor nodes will be combined with target_seed[cluster] to form a train batch\n",
    "    def mini_batch_neighbor_sample(self, target_seed, k, frac = 1):\n",
    "        \"\"\"\n",
    "            This function is to generate the neighbors of the seed (either train nodes or validation nodes)\n",
    "            params: cluster index, number of layer k, fraction of sampling from each neighbor layer\n",
    "            input: \n",
    "                1) target_seed: this is the 0 layer inside self.neighbor\n",
    "            output:\n",
    "                1) neighbor: nodes global idx inside each layer of the batch\n",
    "                2) accum_neighbor: accumulating neighbors , i.e. the final batch nodes\n",
    "        \"\"\"\n",
    "        accum_neighbor = defaultdict(set)\n",
    "        for cluster in target_seed.keys():\n",
    "            neighbor = set(target_seed[cluster])  # first layer of the neighbor nodes of each cluster\n",
    "            for layer in range(k):\n",
    "                # first accumulate last layer\n",
    "                accum_neighbor[cluster] |= neighbor\n",
    "                tmp_level = set()\n",
    "                for node in neighbor:\n",
    "                    tmp_level |= set(self.graph.neighbors(node))  # the key here we are using self.graph, extract neighbor from the whole graph\n",
    "                # add the new layer of neighbors\n",
    "                tmp_level -= accum_neighbor[cluster]\n",
    "                # each layer will only contains partial nodes from the previous layer\n",
    "                neighbor = set(random.sample(tmp_level, int(len(tmp_level) * frac) ) ) if 0 < frac < 1 else tmp_level\n",
    "    #                 print('layer ' + str(layer + 1) + ' : ', self.neighbor[cluster][layer+1])\n",
    "            # the most outside layer: kth layer will be added:\n",
    "            accum_neighbor[cluster] |= neighbor\n",
    "            accum_neighbor[cluster] -= set(target_seed[cluster])  # only contains neighor, does not include train seeds\n",
    "        return accum_neighbor\n",
    "    \n",
    "    # for use of validation on the whole graph as a whole in CPU-side memory\n",
    "    def whole_batch_generate(self, batch_file_folder, test_nodes):\n",
    "        \"\"\"\n",
    "            For use of testing the model: generate the needed tensors for testing in CPU-memory side\n",
    "        \"\"\"\n",
    "        # store the global edges\n",
    "        whole_nodes_global = sorted(self.graph.nodes())\n",
    "        whole_edges_global = {edge for edge in self.graph.edges()}\n",
    "        \n",
    "        whole_edge_weight_local = \\\n",
    "                        [ self.graph.edges[left, right]['weight'] for left, right in whole_edges_global ] + \\\n",
    "                        [ self.graph.edges[right, left]['weight'] for left, right in whole_edges_global ] + \\\n",
    "                        [ self.graph.edges[i, i]['weight'] for i in whole_nodes_global ]\n",
    "        \n",
    "        whole_edges_local = \\\n",
    "                       [ [ left, right ] for left, right in whole_edges_global ] + \\\n",
    "                       [ [ right, left ] for left, right in whole_edges_global ] + \\\n",
    "                       [ [i, i] for i in whole_nodes_global ]  \n",
    "        \n",
    "        # store local features and lables\n",
    "        whole_features_local = self.features\n",
    "        whole_labels_local = self.label\n",
    "\n",
    "        # transform all the data to the tensor form\n",
    "        whole_edges_local = torch.LongTensor(whole_edges_local).t()\n",
    "        whole_edge_weight_local = torch.FloatTensor(whole_edge_weight_local)\n",
    "        whole_features_local = torch.FloatTensor(whole_features_local)\n",
    "        whole_labels_local = torch.LongTensor(whole_labels_local)\n",
    "        whole_test_nodes_local = torch.LongTensor( sorted(test_nodes) )\n",
    "\n",
    "        whole_batch_data = [whole_test_nodes_local, whole_edges_local, whole_edge_weight_local, whole_features_local, whole_labels_local]\n",
    "\n",
    "        batch_file_name = batch_file_folder + 'batch_whole'\n",
    "\n",
    "        # store the batch files\n",
    "        t0 = time.time()\n",
    "        with open(batch_file_name, \"wb\") as fp:\n",
    "            pickle.dump(whole_batch_data, fp)\n",
    "        store_time = ((time.time() - t0) * 1000)\n",
    "        print('*** Generate batch file for # {0} batch, writing the batch file costed {1:.2f} ms ***'.format(\"whole graph\", store_time) )\n",
    "    \n",
    "    def mini_batch_generate_tensor(self, sampled_neighbor_nodes, train_seed_nodes):\n",
    "        \"\"\"\n",
    "            sampled_neighbor_nodes : selected neighbor nodes, to be combined with train seeds to form a train batch\n",
    "        \"\"\"\n",
    "        train_batch_nodes = list(sampled_neighbor_nodes) + list(train_seed_nodes)\n",
    "        batch_subgraph = self.graph.subgraph(train_batch_nodes)\n",
    "            \n",
    "         # first select all the overlapping nodes of the train nodes\n",
    "        mini_nodes_global = sorted(node for node in batch_subgraph.nodes())\n",
    "\n",
    "        # store the global edges\n",
    "        mini_edges_global = {edge for edge in batch_subgraph.edges()}\n",
    "\n",
    "        # map nodes from global index to local index\n",
    "        mini_mapper = {node: i for i, node in enumerate(mini_nodes_global)}\n",
    "\n",
    "        # store local index of batch nodes\n",
    "        mini_nodes_local = [ mini_mapper[global_idx] for global_idx in train_seed_nodes ]\n",
    "\n",
    "        # store local index of batch edges\n",
    "        mini_edges_local = \\\n",
    "                       [ [ mini_mapper[edge[0]], mini_mapper[edge[1]] ] for edge in mini_edges_global ] + \\\n",
    "                       [ [ mini_mapper[edge[1]], mini_mapper[edge[0]] ] for edge in mini_edges_global ] + \\\n",
    "                       [ [i, i] for i in sorted(mini_mapper.values()) ]  \n",
    "\n",
    "        mini_edge_weight_local = \\\n",
    "                        [ self.graph.edges[left, right]['weight'] for left, right in mini_edges_global ] + \\\n",
    "                        [ self.graph.edges[right, left]['weight'] for left, right in mini_edges_global ] + \\\n",
    "                        [ self.graph.edges[i, i]['weight'] for i in mini_nodes_global ]\n",
    "\n",
    "        # store local features and lables\n",
    "        mini_features = self.features[mini_nodes_global,:]\n",
    "        mini_labels = self.label[mini_nodes_global]\n",
    "        \n",
    "        # store the size of each sub-divided tiny batch\n",
    "        info_batch_node_size = len(mini_nodes_global)\n",
    "        info_batch_edge_size = len(mini_edges_local)\n",
    "        \n",
    "        # transform all the data to the tensor form\n",
    "        mini_nodes_local = torch.LongTensor(mini_nodes_local)\n",
    "        mini_edges_local = torch.LongTensor(mini_edges_local).t()\n",
    "        mini_edge_weight_local = torch.FloatTensor(mini_edge_weight_local)\n",
    "        mini_features = torch.FloatTensor(mini_features)\n",
    "        mini_labels = torch.LongTensor(mini_labels)\n",
    "\n",
    "        minibatch_data = [mini_nodes_local, mini_edges_local, mini_edge_weight_local, mini_features, mini_labels]\n",
    "        \n",
    "        return minibatch_data, info_batch_node_size, info_batch_edge_size\n",
    "    \n",
    "    def mini_batch_divide_neighbor(self, accum_neighbor, partition_size):\n",
    "        \"\"\"\n",
    "            accum_neighbor (set) : the total sampled neighbor of a specific cluster\n",
    "            partition_size: the size of each cluster\n",
    "            return  (a list of list ):  each cluster contains a certain number (partition_size) of nodes\n",
    "        \"\"\"\n",
    "        accum_neighbor = list(accum_neighbor)\n",
    "        random.shuffle(accum_neighbor)\n",
    "        sampled_neighbor_group = [accum_neighbor[i * partition_size : (i + 1) * partition_size] for i in range((len(accum_neighbor) + partition_size - 1) // partition_size )]  \n",
    "        return sampled_neighbor_group\n",
    "        \n",
    "    # main logic for generating overlapping batches from train nodes\n",
    "    def mini_batch_generate(self, batch_file_folder, target_seed, partition_size, k, fraction = 1.0, batch_range = (0, 1)):\n",
    "        \"\"\"\n",
    "            create the mini-batch focused on the train nodes only, include a total of k layers of neighbors of the original training nodes\n",
    "            k: number of layers of neighbors for each training node\n",
    "            fraction: fraction of neighbor nodes in each layer to be considered\n",
    "            Input:\n",
    "                1) target_seed: global ids of the nodes for seed to generate the batch\n",
    "                    usually one of (train_global, test_global_, validation_global)\n",
    "                2) partition_size: the partition size of each subset of train batch\n",
    "        \"\"\"\n",
    "        # these are currently believed to be the main memory cost, storing all overlapping batch information\n",
    "        # instead we store all the information inside one list to be stored in a pickle file as out-of-core mini-batch\n",
    "        \n",
    "        # this will get the edge weights in a complete graph\n",
    "        \n",
    "        info_batch_node_size = {}\n",
    "        info_batch_edge_size = {}\n",
    "                \n",
    "        accum_neighbor = self.mini_batch_neighbor_sample(target_seed, k, frac = fraction)\n",
    "        batch_start, batch_end = batch_range\n",
    "        \n",
    "        for cluster in range(batch_start, batch_end):\n",
    "            # main purpose is to avoid too large size of this batch_subgraph with too many intra-edges inside\n",
    "            sampled_neighbor_group = self.mini_batch_divide_neighbor(accum_neighbor[cluster], partition_size)\n",
    "            train_seed_nodes = target_seed[cluster]\n",
    "            train_batch_tensor_group = [self.mini_batch_generate_tensor(sampled_neighbor_nodes, train_seed_nodes) for sampled_neighbor_nodes in sampled_neighbor_group]\n",
    "            # record information \n",
    "            \n",
    "            info_batch_node_size[cluster] = [ele[1] for ele in train_batch_tensor_group]\n",
    "            info_batch_edge_size[cluster] = [ele[2] for ele in train_batch_tensor_group]\n",
    "            # store the batch files\n",
    "            t0 = time.time()\n",
    "            batch_file_name = batch_file_folder + 'batch_' + str(cluster)\n",
    "            # store a list of tensors\n",
    "            with open(batch_file_name, \"wb\") as fp:\n",
    "                pickle.dump([ele[0] for ele in train_batch_tensor_group], fp)\n",
    "            store_time = ((time.time() - t0) * 1000)\n",
    "            print('*** Generate batch file for # {0:3d} batch, writing the batch file costed {1:.2f} ms ***'.format(cluster, store_time) )\n",
    "            \n",
    "        return info_batch_node_size, info_batch_edge_size\n",
    "    \n",
    "    def save_info_dict(self, data, file_name, target_folder, header = 'key, value'):\n",
    "        # output the batch size information as the csv file\n",
    "#         os.makedirs(os.path.dirname(target_folder), exist_ok=True)\n",
    "        target_file = target_folder + file_name\n",
    "        \n",
    "        with open(target_file, 'a', newline='\\n') as fp:\n",
    "            wr = csv.writer(fp, delimiter = ',')\n",
    "            fp.write('\\n')\n",
    "            wr.writerow(header.split(','))\n",
    "            for key, val in data.items():\n",
    "                if isinstance(val, list):\n",
    "                    wr.writerow([key+1] + val)\n",
    "                else:\n",
    "                    wr.writerow([key+1, val])\n",
    "    \n",
    "    def mini_batch_train_clustering(self, batch_folder, partition_size, k, fraction = 1.0, batch_range = (0, 1), info_folder = './info/', info_file = 'train_batch_size_info.csv'):\n",
    "        \"\"\"\n",
    "            batch_folder : to save the batch file to this target folder \n",
    "            info_folder : to save the batch information as csv file to this folder\n",
    "            k:  number of hop of neighbor layer\n",
    "            fraction:  fraction of each neighbor layer\n",
    "            batch_range:   generate the batch from this range [start, end)  \n",
    "        \"\"\"\n",
    "        data_type = 'train'\n",
    "        batch_file_folder = batch_folder + data_type + '/'\n",
    "#         check_folder_exist(batch_file_folder)\n",
    "        os.makedirs(os.path.dirname(batch_file_folder), exist_ok=True)\n",
    "        \n",
    "        self.info_train_batch_node_size, self.info_train_batch_edge_size  = self.mini_batch_generate(batch_file_folder, self.sg_train_nodes_global, partition_size, k, fraction = fraction, batch_range = batch_range)\n",
    "        self.info_train_seed_size = {key : len(val) for key, val in self.sg_train_nodes_global.items()}\n",
    "        \n",
    "        self.save_info_dict(self.info_train_batch_node_size, info_file, info_folder, header = 'train_batch_node_id, train_batch_node_size')\n",
    "        self.save_info_dict(self.info_train_batch_edge_size, info_file, info_folder, header = 'train_batch_edge_id, train_batch_edge_size')\n",
    "        self.save_info_dict(self.info_train_seed_size, info_file, info_folder, header = 'train_seed_node_id, train_seed_node_size')\n",
    "        \n",
    "    def whole_validation_clustering(self, batch_folder, info_folder = './info/', info_file = 'validation_whole_size_info.csv'):\n",
    "        data_type = 'validation'\n",
    "        batch_file_folder = batch_folder + data_type + '/'\n",
    "#         check_folder_exist(batch_file_folder)\n",
    "        os.makedirs(os.path.dirname(batch_file_folder), exist_ok=True)\n",
    "        \n",
    "        self.whole_batch_generate(batch_file_folder, self.valid_nodes_global)\n",
    "        \n",
    "        \n",
    "    def mini_batch_test_clustering(self, batch_folder, k, fraction = 1.0, batch_range = (0, 1), info_folder = './info/', info_file = 'test_batch_size_info.csv'):\n",
    "        data_type = 'test'\n",
    "        batch_file_folder = batch_folder + data_type + '/'\n",
    "#         check_folder_exist(batch_file_folder)\n",
    "        os.makedirs(os.path.dirname(batch_file_folder), exist_ok=True)\n",
    "    \n",
    "        self.info_test_batch_node_size, self.info_test_batch_edge_size = self.mini_batch_generate(batch_file_folder, self.sg_test_nodes_global, k, fraction = fraction, batch_range = batch_range)\n",
    "        self.info_test_seed_size = {key : len(val) for key, val in self.sg_test_nodes_global.items()}\n",
    "        \n",
    "        self.save_info_dict(self.info_test_batch_node_size, info_file, info_folder, header = 'test_batch_node_id, test_batch_node_size')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Partition Graph with trainiing and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Custom_GCN_layer import Net\n",
    "import time\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class ClusterGCNTrainer_mini_Train(object):\n",
    "    \"\"\"\n",
    "    Training a ClusterGCN.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_folder, in_channels, out_channels, input_layers = [32, 16], dropout=0.3):\n",
    "        \"\"\"\n",
    "        :param in_channels, out_channels: input and output feature dimension\n",
    "        :param clustering_machine:\n",
    "        \"\"\"  \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.test_device = torch.device(\"cpu\")\n",
    "        \n",
    "        self.data_folder = data_folder\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.input_layers = input_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.create_model()\n",
    "\n",
    "    def create_model(self):\n",
    "        \"\"\"\n",
    "        Creating a StackedGCN and transferring to CPU/GPU.\n",
    "        \"\"\"\n",
    "#         print('used layers are: ', str(self.input_layers))\n",
    "        self.model = Net(self.in_channels, self.out_channels, input_layers = self.input_layers, dropout = self.dropout)\n",
    "        self.model = self.model.to(self.device)\n",
    "    \n",
    "    # call the forward function batch by batch\n",
    "    def do_forward_pass(self, tr_train_nodes, tr_edges, tr_edge_weights, tr_features, tr_target):\n",
    "        \"\"\"\n",
    "        Making a forward pass with data from a given partition.\n",
    "        :param cluster: Cluster index.\n",
    "        :return average_loss: Average loss on the cluster.\n",
    "        :return node_count: Number of nodes.\n",
    "        \"\"\"\n",
    "        \n",
    "        '''Target and features are one-one mapping'''\n",
    "        # calculate the probabilites from log_sofmax\n",
    "        predictions = self.model(tr_edges, tr_features, tr_edge_weights)\n",
    "        \n",
    "        ave_loss = torch.nn.functional.nll_loss(predictions[tr_train_nodes], tr_target[tr_train_nodes])\n",
    "        node_count = tr_train_nodes.shape[0]\n",
    "\n",
    "        # for each cluster keep track of the counts of the nodes\n",
    "        return ave_loss, node_count\n",
    "\n",
    "\n",
    "    def update_average_loss(self, batch_average_loss, node_count, isolate = True):\n",
    "        \"\"\"\n",
    "        Updating the average loss in the epoch.\n",
    "        :param batch_average_loss: Loss of the cluster. \n",
    "        :param node_count: Number of nodes in currently processed cluster.\n",
    "        :return average_loss: Average loss in the epoch.\n",
    "        \"\"\"\n",
    "        self.accumulated_training_loss = self.accumulated_training_loss + batch_average_loss.item() * node_count\n",
    "        if isolate:\n",
    "            self.node_count_seen = self.node_count_seen + node_count\n",
    "        average_loss = self.accumulated_training_loss / self.node_count_seen\n",
    "        return average_loss\n",
    "\n",
    "    # iterate through epoch and also the clusters\n",
    "    def train_investigate_F1(self, epoch_num=10, learning_rate=0.01, weight_decay = 0.01, mini_epoch_num = 1, output_period = 10, train_batch_num = 2, valid_batch_num = 2):\n",
    "        \"\"\"\n",
    "            *** Periodically output the F1 score during training. After certain number of epochs ***\n",
    "            epoch_num:  number of total training epoch number\n",
    "            learning rate: learning rate during training\n",
    "            weight_decay:  decay coefficients for the regularization\n",
    "            mini_epoch_num:  number of epochs of repeating training after loading data on the GPU\n",
    "            output_period:  number of epochs after which output the F1 and accuray to investigate the model refining process\n",
    "        \"\"\"\n",
    "        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        self.model.train()   #   set into train mode, only effective for certain modules such as dropout and batchNorm\n",
    "        self.record_ave_training_loss = []\n",
    "        self.time_train_load_data = 0\n",
    "        \n",
    "        epoch_partition = epoch_num // mini_epoch_num\n",
    "        investigate_f1 = {}\n",
    "        investigate_accuracy = {}\n",
    "        \n",
    "        t0 = time.time()\n",
    "        train_clusters = list(range(train_batch_num))\n",
    "        for epoch_part in range(epoch_partition):\n",
    "#             For test purpose, we let the clusters to follow specific order\n",
    "            random.shuffle(train_clusters)\n",
    "            self.node_count_seen = 0\n",
    "            self.accumulated_training_loss = 0\n",
    "            for cluster in train_clusters:\n",
    "                # for each batch, we load once and train it for multiple epochs:\n",
    "                # read in the train data from the pickle files\n",
    "                batch_file_name = self.data_folder + 'train/batch_' + str(cluster)\n",
    "                \n",
    "                t2 = time.time()\n",
    "                with open(batch_file_name, \"rb\") as fp:\n",
    "                    minibatch_data_train = pickle.load(fp)\n",
    "                read_time = (time.time() - t2) * 1000\n",
    "                print('*** During training for # {0:3d} batch, reading batch file costed {1:.2f} ms ***'.format(cluster, read_time) )\n",
    "                \n",
    "                tr_train_nodes, tr_edges, tr_edge_weights, tr_features, tr_target = minibatch_data_train\n",
    "                \n",
    "                t1 = time.time()\n",
    "                tr_train_nodes = tr_train_nodes.to(self.device)\n",
    "                tr_edges = tr_edges.to(self.device)\n",
    "                tr_edge_weights = tr_edge_weights.to(self.device)\n",
    "                tr_features = tr_features.to(self.device)\n",
    "                tr_target = tr_target.to(self.device)\n",
    "                \n",
    "                self.time_train_load_data += (time.time() - t1) * 1000\n",
    "                # train each batch for multiple epochs\n",
    "                for mini_epoch in range(mini_epoch_num):\n",
    "                    # record the current overall epoch index:\n",
    "                    real_epoch_num = 1 + mini_epoch + mini_epoch_num * epoch_part # real_epoch_num starts from 0, therefore we add 1\n",
    "                    \n",
    "                    self.optimizer.zero_grad()\n",
    "                    batch_ave_loss, node_count = self.do_forward_pass(tr_train_nodes, tr_edges, tr_edge_weights, tr_features, tr_target)\n",
    "                    batch_ave_loss.backward()\n",
    "                    self.optimizer.step()\n",
    "                    ave_loss = self.update_average_loss(batch_ave_loss, node_count)\n",
    "                    \n",
    "                    # at this point finish a single train duration: update the parameter and calcualte the loss function\n",
    "                    # periodically output the F1-score in the middle of the training process\n",
    "                    if real_epoch_num % output_period == 0:\n",
    "                        investigate_f1[real_epoch_num], investigate_accuracy[real_epoch_num] = self.batch_validate(valid_batch_num = valid_batch_num)\n",
    "                        self.model.train()    # reset to the train mode\n",
    "            \n",
    "            self.record_ave_training_loss.append(ave_loss)\n",
    "        # convert to ms\n",
    "        self.time_train_total = ((time.time() - t0) * 1000)\n",
    "        return investigate_f1, investigate_accuracy\n",
    "\n",
    "    # iterate through epoch and also the clusters\n",
    "    def train(self, epoch_num=10, learning_rate=0.01, weight_decay = 0.01, mini_epoch_num = 1, train_batch_num = 2):\n",
    "        \"\"\"\n",
    "            *** Training a model. ***\n",
    "            epoch_num:  number of total training epoch number\n",
    "            learning rate: learning rate during training\n",
    "            weight_decay:  decay coefficients for the regularization\n",
    "            mini_epoch_num:  number of epochs of repeating training after loading data on the GPU\n",
    "        \"\"\"\n",
    "        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        self.model.train()\n",
    "        self.record_ave_training_loss = []\n",
    "        self.time_train_load_data = 0\n",
    "        \n",
    "        epoch_partition = epoch_num // mini_epoch_num\n",
    "        t0 = time.time()\n",
    "        train_clusters = list(range(train_batch_num))\n",
    "        for epoch in range(epoch_partition):\n",
    "#             For test purpose, we let the clusters to follow specific order\n",
    "            random.shuffle(train_clusters)\n",
    "            self.node_count_seen = 0\n",
    "            self.accumulated_training_loss = 0\n",
    "            \n",
    "            for cluster in train_clusters:\n",
    "                # read in the train data from the pickle files\n",
    "                batch_file_name = self.data_folder + 'train/batch_' + str(cluster)\n",
    "                \n",
    "                t2 = time.time()\n",
    "                with open(batch_file_name, \"rb\") as fp:\n",
    "                    minibatch_data_train_group = pickle.load(fp)\n",
    "                read_time = (time.time() - t2) * 1000\n",
    "                print('*** During training for # {0:3d} batch, reading batch file costed {1:.2f} ms ***'.format(cluster, read_time) )\n",
    "                # for each train batch: we need to go through each subdivided-mini_train_subset\n",
    "                random.shuffle(minibatch_data_train_group)\n",
    "                for minibatch_data_train in minibatch_data_train_group:\n",
    "                    \n",
    "                    tr_train_nodes, tr_edges, tr_edge_weights, tr_features, tr_target = minibatch_data_train\n",
    "\n",
    "                    # for each cluster, we load once and train it for multiple epochs:\n",
    "                    t1 = time.time()\n",
    "                    tr_train_nodes = tr_train_nodes.to(self.device)\n",
    "                    tr_edges = tr_edges.to(self.device)\n",
    "                    tr_edge_weights = tr_edge_weights.to(self.device)\n",
    "                    tr_features = tr_features.to(self.device)\n",
    "                    tr_target = tr_target.to(self.device)\n",
    "\n",
    "\n",
    "                    self.time_train_load_data += (time.time() - t1) * 1000\n",
    "                    # train each batch for multiple epochs\n",
    "                    for mini_epoch in range(mini_epoch_num):\n",
    "                        self.optimizer.zero_grad()\n",
    "                        batch_ave_loss, node_count = self.do_forward_pass(tr_train_nodes, tr_edges, tr_edge_weights, tr_features, tr_target)\n",
    "                        batch_ave_loss.backward()\n",
    "                        self.optimizer.step()\n",
    "                        ave_loss = self.update_average_loss(batch_ave_loss, node_count)\n",
    "            \n",
    "            self.record_ave_training_loss.append(ave_loss)\n",
    "        # convert to ms\n",
    "        self.time_train_total = ((time.time() - t0) * 1000)\n",
    "\n",
    "    def whole_cpu_validate(self):\n",
    "        \"\"\"\n",
    "        Scoring the test and printing the F-1 score.\n",
    "        \"\"\"\n",
    "        self.test_device = torch.device(\"cpu\")\n",
    "        test_model = self.model.to(self.test_device)\n",
    "        test_model.eval()   # set into test mode, only effective for certain modules such as dropout and batchNorm\n",
    "        \n",
    "        batch_file_name = self.data_folder + 'validation/batch_whole'\n",
    "\n",
    "        t2 = time.time()\n",
    "        with open(batch_file_name, \"rb\") as fp:\n",
    "            minibatch_data_validation = pickle.load(fp)\n",
    "        read_time = (time.time() - t2) * 1000\n",
    "        print('*** During validation for # {0} batch, reading batch file costed {1:.2f} ms ***'.format(\"whole graph\", read_time) )\n",
    "\n",
    "        valid_validation_nodes, valid_edges, valid_edge_weights, valid_features, valid_target = minibatch_data_validation\n",
    "\n",
    "        prediction = test_model(valid_edges, valid_features, valid_edge_weights)\n",
    "        # select the testing nodes predictions and real labels\n",
    "        predictions = prediction[valid_validation_nodes].cpu().detach().numpy()\n",
    "        targets = valid_target[valid_validation_nodes].cpu().detach().numpy()\n",
    "        \n",
    "        # along axis:    axis == 1\n",
    "        predictions = predictions.argmax(1)  # return the indices of maximum probability \n",
    "        \n",
    "        f1 = f1_score(targets, predictions, average=\"micro\")\n",
    "        accuracy = accuracy_score(targets, predictions)\n",
    "#         print(\"\\nTest F-1 score: {:.4f}\".format(score))\n",
    "        return (f1, accuracy)\n",
    "\n",
    "# for cross-validation purpose: \n",
    "    def do_prediction(self, cluster):\n",
    "        \"\"\"\n",
    "        Scoring a cluster.\n",
    "        :param cluster: Cluster index.\n",
    "        :return prediction: Prediction matrix with probabilities.\n",
    "        :return target: Target vector.\n",
    "        \"\"\"\n",
    "        test_nodes = self.clustering_machine.sg_test_nodes_global[cluster].to(self.device)\n",
    "        prediction = self.model(self.edges, self.features, self.edge_weights)\n",
    "        \n",
    "        return prediction[test_nodes], self.label[test_nodes]\n",
    "    \n",
    "    def test(self):\n",
    "        \"\"\"\n",
    "        Scoring the test and printing the F-1 score.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        self.predictions = []\n",
    "        self.targets = []\n",
    "        \n",
    "        self.edges = self.clustering_machine.edge_index_global_self_loops.to(self.device)\n",
    "        self.features = self.clustering_machine.features.to(self.device)\n",
    "        self.edge_weights = self.clustering_machine.edge_weight_global.to(self.device)\n",
    "        self.label = self.clustering_machine.label.to(self.device)\n",
    "        \n",
    "        for cluster in self.clustering_machine.test_clusters:\n",
    "            prediction, target = self.do_prediction(cluster)\n",
    "\n",
    "            self.predictions.append(prediction.cpu().detach().numpy())\n",
    "            self.targets.append(target.cpu().detach().numpy())\n",
    "        \n",
    "        # concatenate all the ndarrays inside this list\n",
    "        self.targets = np.concatenate(self.targets)\n",
    "        # along axis:    axis == 1\n",
    "        self.predictions = np.concatenate(self.predictions).argmax(1)  # return the indices of maximum probability \n",
    "        \n",
    "        f1 = f1_score(self.targets, self.predictions, average=\"micro\")\n",
    "        accuracy = accuracy_score(self.targets, self.predictions)\n",
    "#         print(\"\\nTest F-1 score: {:.4f}\".format(score))\n",
    "        return (f1, accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Trivial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 2.],\n",
      "        [0., 3.],\n",
      "        [0., 4.],\n",
      "        [0., 5.],\n",
      "        [0., 6.],\n",
      "        [0., 7.],\n",
      "        [0., 8.],\n",
      "        [0., 9.]]) torch.Size([10, 2])\n"
     ]
    }
   ],
   "source": [
    "'''Trivial data'''\n",
    "edge_index = torch.tensor([[0, 1, 1, 3, 1, 2, 4, 2, 4, 6, 6, 7, 7, 9, 2, 5, 9, 8], \n",
    "                           [1, 0, 3, 1, 2, 1, 2, 4, 6, 4, 7, 6, 9, 7, 5, 2, 8, 9]])\n",
    "# features = torch.rand(10, 3)\n",
    "features = torch.tensor([[0, 0], [0, 1], [0, 2], [0, 3], [0, 4],  \n",
    "                           [0, 5], [0, 6], [0, 7], [0, 8], [0, 9]], dtype = torch.float)\n",
    "# label = torch.tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "label = torch.tensor([0, 1, 1, 0, 1, 1, 1, 0, 0, 0])\n",
    "print(features, features.shape)\n",
    "\n",
    "# set the tmp folder\n",
    "# tmp_folder = './tmp/'\n",
    "# check_folder_exist(tmp_folder)\n",
    "# os.makedirs(os.path.dirname(tmp_folder), exist_ok=True)\n",
    "# set the store clustering path\n",
    "tmp_folder = './res_save_batch/tmp/'\n",
    "check_folder_exist(tmp_folder)\n",
    "clustering_file_name = tmp_folder + 'check_clustering_machine.txt'\n",
    "os.makedirs(os.path.dirname(tmp_folder), exist_ok=True)\n",
    "info_folder = './res_save_batch/info/'\n",
    "check_folder_exist(info_folder)\n",
    "os.makedirs(os.path.dirname(info_folder), exist_ok=True)\n",
    "\n",
    "node_count = features.shape[0]\n",
    "get_edge_weight(edge_index, node_count, store_path = tmp_folder)\n",
    "clustering_machine = ClusteringMachine(edge_index, features, label, tmp_folder)\n",
    "\n",
    "with open(clustering_file_name, \"wb\") as fp:\n",
    "    pickle.dump(clustering_machine, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### minibatch train nodes and batch validatioin "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Generate batch file for #   0 batch, writing the batch file costed 0.22 ms ***\n",
      "*** Generate batch file for #   1 batch, writing the batch file costed 0.19 ms ***\n",
      "*** Generate batch file for # whole graph batch, writing the batch file costed 0.18 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 0.23 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 0.23 ms ***\n",
      "*** During validation for # whole graph batch, reading batch file costed 0.19 ms ***\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mini_batch_folder = './res_save_batch/mini_batch_files/'\n",
    "check_folder_exist(mini_batch_folder)\n",
    "\n",
    "with open(clustering_file_name, \"rb\") as fp:\n",
    "    clustering_machine = pickle.load(fp)\n",
    "\n",
    "clustering_machine.split_whole_nodes_edges_then_cluster(0.4, 0.4, train_batch_num = 2, test_batch_num = 2)\n",
    "\n",
    "output_GPU_memory_usage('Memory_use.txt', info_folder, comment ='after split: ')\n",
    "\n",
    "# generate the batches for train and validation\n",
    "# we know there is a totla of 10 nodes, here the partition size is : graph_node_number // train_batch_num\n",
    "clustering_machine.mini_batch_train_clustering(mini_batch_folder, 10 // 2, 1, batch_range = (0, 2), info_folder = info_folder) # include number of layers\n",
    "\n",
    "output_GPU_memory_usage('Memory_use.txt', info_folder, comment ='after train clustering: ')\n",
    "\n",
    "# clustering_machine.mini_batch_validation_clustering(mini_batch_folder, 1, batch_range = (0, 2), info_folder = info_folder)\n",
    "\n",
    "clustering_machine.whole_validation_clustering(mini_batch_folder, info_folder = info_folder)\n",
    "\n",
    "output_GPU_memory_usage('Memory_use.txt', info_folder, comment ='after validation clustering:  ')\n",
    "\n",
    "# construct the batch trainer\n",
    "gcn_trainer_batch = ClusterGCNTrainer_mini_Train(mini_batch_folder, 2, 2, input_layers = [16], dropout=0.3)\n",
    "\n",
    "output_GPU_memory_usage('Memory_use.txt', info_folder, comment = 'after generating trainer:  ')\n",
    "\n",
    "gcn_trainer_batch.train(1, 0.0001, 0.1, train_batch_num = 2)\n",
    "\n",
    "output_GPU_memory_usage('Memory_use.txt', info_folder, comment = 'after training the batch:  ')\n",
    "\n",
    "# gcn_trainer_batch.batch_validate(valid_batch_num = 2)\n",
    "\n",
    "gcn_trainer_batch.whole_cpu_validate()\n",
    "\n",
    "output_GPU_memory_usage('Memory_use.txt', info_folder, comment = 'after validating the batch:  ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the train loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_train_loss_converge(mini_batch_folder, data_name, dataset, image_path,  comments, input_layer = [32, 16], epoch_num = 300, \\\n",
    "                              dropout = 0.3, lr = 0.0001, weight_decay = 0.01, mini_epoch_num = 5, \\\n",
    "                               valid_part_num = 2, train_part_num = 2, test_part_num = 1):\n",
    "    # mini-batch, but valid also in batches\n",
    "    Trainer_folder = mini_batch_folder + 'GCNtrainer/'\n",
    "    check_folder_exist(Trainer_folder)\n",
    "    os.makedirs(os.path.dirname(Trainer_folder), exist_ok=True)\n",
    "    \n",
    "    trainer_id = 0\n",
    "    Cluster_train_batch_run(trainer_id, mini_batch_folder, data_name, dataset, image_path, input_layer = input_layer, epochs=epoch_num, \\\n",
    "                                                                               dropout = dropout, lr = lr, weight_decay = weight_decay, mini_epoch_num = mini_epoch_num, \\\n",
    "                                                                               train_part_num = train_part_num, test_part_num = test_part_num)\n",
    "    trainer_file_name = mini_batch_folder + 'GCNtrainer/GCN_trainer_' + str(trainer_id)\n",
    "    with open(trainer_file_name, \"rb\") as fp:\n",
    "        Cluster_train_batch_trainer = pickle.load(fp)\n",
    "    \n",
    "    draw_Cluster_train_valid_batch = draw_trainer_info(data_name, Cluster_train_batch_trainer, image_path, 'train_valid_batch_' + comments)\n",
    "    draw_Cluster_train_valid_batch.draw_ave_loss_per_node()\n",
    "    \n",
    "\n",
    "''' Draw the information about the GCN calculating batch size '''\n",
    "def draw_cluster_info(clustering_machine, data_name, img_path, comments = '_cluster_node_distr'):\n",
    "    \"\"\"\n",
    "        Won't call this for mini-batch with no clustering \n",
    "    \"\"\"\n",
    "    cluster_id = clustering_machine.train_clusters    # a list of cluster indices\n",
    "    cluster_datapoints = {'cluster_id': cluster_id,  \\\n",
    "                          'train_batch' : [clustering_machine.info_train_batch_size[idx] for idx in cluster_id], \\\n",
    "                          'cluster_size' : [clustering_machine.info_isolate_cluster_size[idx] for idx in cluster_id], \\\n",
    "                         }\n",
    "                         \n",
    "    df = pd.DataFrame(data=cluster_datapoints, dtype=np.int32)\n",
    "    # print(df)\n",
    "    df_reshape = df.melt('cluster_id', var_name = 'clusters', value_name = 'node_num')\n",
    "    \n",
    "    plt.clf()\n",
    "    plt.figure()\n",
    "    sns.set(style='whitegrid')\n",
    "    g = sns.catplot(x=\"cluster_id\", y=\"node_num\", hue='clusters', kind='bar', data=df_reshape)\n",
    "    g.despine(left=True)\n",
    "    g.fig.suptitle(data_name + comments)\n",
    "    g.set_xlabels(\"Cluster ID\")\n",
    "    g.set_ylabels(\"Number of nodes\")\n",
    "    \n",
    "    img_name = img_path + data_name + comments\n",
    "    os.makedirs(os.path.dirname(img_name), exist_ok=True)\n",
    "    g.savefig(img_name, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specific model run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Execute the testing program , set all the paths relative to intermediate_data_folder'''\n",
    "def set_clustering_machine(data, dataset, intermediate_data_folder, test_ratio = 0.05, validation_ratio = 0.85, train_batch_num = 2):\n",
    "    \"\"\"\n",
    "        Set the batch machine plus generate the training batches\n",
    "            1) data: the target dataset data\n",
    "            2) intermediate_data_folder: path to store the intermediate generated data\n",
    "            3) test_ratio, validation_ratio: data split ratio\n",
    "            4) neigh_layer: number of hops (layers) for the neighbor nodes \n",
    "            5) train_frac: each time including fraction of the neigbor nodes in each layer\n",
    "            6) valid_part_num, train_part_num, test_part_num :  batch number for validation, train and test data correspondingly\n",
    "    \"\"\"\n",
    "    # set the tmp file for garbage tmp files, just collect the info:\n",
    "    tmp_folder = intermediate_data_folder + 'tmp/'\n",
    "    check_folder_exist(tmp_folder)\n",
    "    os.makedirs(os.path.dirname(tmp_folder), exist_ok=True)\n",
    "    \n",
    "    # Set the clustering information storing path\n",
    "    clustering_file_folder = intermediate_data_folder + 'clustering/'\n",
    "    data_info_file_folder = intermediate_data_folder + 'data_info/'\n",
    "    check_folder_exist(clustering_file_folder)  # if exist then delete\n",
    "    check_folder_exist(data_info_file_folder)  # if exist then delete\n",
    "    \n",
    "    clustering_file_name = clustering_file_folder + 'clustering_machine.txt'\n",
    "    data_info_file_name = data_info_file_folder + 'data_info_file.txt'\n",
    "    os.makedirs(os.path.dirname(clustering_file_folder), exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(data_info_file_folder), exist_ok=True)\n",
    "    \n",
    "    # if we use the random assignment of the code, then filtering out the isolated data may not be necessary\n",
    "#     connect_edge_index, connect_features, connect_label = filter_out_isolate(data.edge_index, data.x, data.y)\n",
    "#     clustering_machine = ClusteringMachine(connect_edge_index, connect_features, connect_label)\n",
    "    print('\\n' + '=' * 100)\n",
    "    # start to generate the edge weights\n",
    "    print('Start to generate the global edge weights')\n",
    "    t00 = time.time()\n",
    "    node_count = data.x.shape[0]\n",
    "    get_edge_weight(data.edge_index, node_count, store_path = tmp_folder)\n",
    "    edge_weight_create = time.time() - t00\n",
    "    print('Edge weights creation costs a total of {0:.4f} seconds!'.format(edge_weight_create))\n",
    "    \n",
    "    print('Start to generate the clustering machine:')\n",
    "    t0 = time.time()\n",
    "    clustering_machine = ClusteringMachine(data.edge_index, data.x, data.y, tmp_folder)\n",
    "    batch_machine_create = time.time() - t0\n",
    "    print('Batch machine creation costs a total of {0:.4f} seconds!'.format(batch_machine_create))\n",
    "    \n",
    "    # at last output the information inside the folder:\n",
    "    print_dir_content_info(tmp_folder)\n",
    "    \n",
    "#     clustering_machine.split_cluster_nodes_edges(test_ratio, validation_ratio, partition_num = train_part_num)\n",
    "    # mini-batch only: split to train test valid before clustering\n",
    "    print('Start to split data into train, test, validation:')\n",
    "    t1 = time.time()\n",
    "    clustering_machine.split_whole_nodes_edges_then_cluster(test_ratio, validation_ratio, \\\n",
    "                                    train_batch_num = train_batch_num)\n",
    "    data_split_time = time.time() - t1\n",
    "    print('Data splitting costs a total of {0:.4f} seconds!'.format(data_split_time))\n",
    "    \n",
    "    print('Start to store the batch machine file:')\n",
    "    t3 = time.time()\n",
    "    with open(clustering_file_name, \"wb\") as fp:\n",
    "        pickle.dump(clustering_machine, fp)\n",
    "    data_info = (dataset.num_node_features, dataset.num_classes )\n",
    "    with open(data_info_file_name, \"wb\") as fp:\n",
    "        pickle.dump(data_info, fp)\n",
    "    \n",
    "    batch_machine_store_time = time.time() - t3\n",
    "    print('Storing batch machine after training batches generation costs a total of {0:.4f} seconds!'.format(batch_machine_store_time))\n",
    "    print('\\n' + '=' * 100)\n",
    "    # output the memory usage information\n",
    "    info_GPU_memory_folder = intermediate_data_folder + 'info_GPU_memory/'\n",
    "    output_GPU_memory_usage('Memory_use_setting_cluster.txt', info_GPU_memory_folder, comment ='after setting clustering machine: ')\n",
    "    \n",
    "    \n",
    "def set_clustering_machine_train_batch(intermediate_data_folder, neigh_layer = 1, train_frac = 1.0, \\\n",
    "                                       batch_range = (0, 1), info_folder = 'info_train_batch/', info_file = 'train_batch_size_info.csv'):\n",
    "    \"\"\"\n",
    "        Generate the train batches\n",
    "    \"\"\"\n",
    "    clustering_file_folder = intermediate_data_folder + 'clustering/'\n",
    "    clustering_file_name = clustering_file_folder + 'clustering_machine.txt'\n",
    "    print('\\n' + '=' * 100)\n",
    "    \n",
    "    t0 = time.time()\n",
    "    with open(clustering_file_name, \"rb\") as fp:\n",
    "        clustering_machine = pickle.load(fp)\n",
    "    batch_machine_read = time.time() - t0\n",
    "    print('Batch machine reading costs a total of {0:.4f} seconds!'.format(batch_machine_read))\n",
    "    \n",
    "#     check_folder_exist(intermediate_data_folder)  # if exist then delete\n",
    "    print('Start to generate the training batches:')\n",
    "    info_folder = intermediate_data_folder + info_folder\n",
    "    os.makedirs(os.path.dirname(info_folder), exist_ok=True)\n",
    "    t2 = time.time()\n",
    "    # here we keep each batch size as the average batch size divided from the whole graph : clustering_machine.node_count // clustering_machine.train_batch_num\n",
    "    clustering_machine.mini_batch_train_clustering(intermediate_data_folder, clustering_machine.node_count // clustering_machine.train_batch_num, \\\n",
    "                                                   neigh_layer, fraction = train_frac, \\\n",
    "                                                   batch_range = batch_range, info_folder = info_folder, info_file = info_file)\n",
    "    train_batch_production_time = time.time() - t2\n",
    "    print('Train batches production costs a total of {0:.4f} seconds!'.format(train_batch_production_time))\n",
    "    print_dir_content_info(intermediate_data_folder + 'train/')\n",
    "    print('=' * 100)\n",
    "    # output the memory usage information\n",
    "    info_GPU_memory_folder = intermediate_data_folder + 'info_GPU_memory/'\n",
    "    output_GPU_memory_usage('GPU_cost_setting_train_batch.txt', info_GPU_memory_folder, comment ='after generating train batches: ')\n",
    "    \n",
    "def set_clustering_machine_validation_whole_graph(intermediate_data_folder, \\\n",
    "                                                  info_folder = 'info_validation_whole_graph/', info_file = 'validation_whole_graph_size_info.csv'):\n",
    "    \"\"\"\n",
    "        Generate the validation batches\n",
    "    \"\"\"\n",
    "    clustering_file_folder = intermediate_data_folder + 'clustering/'\n",
    "    clustering_file_name = clustering_file_folder + 'clustering_machine.txt'\n",
    "    print('\\n' + '=' * 100)\n",
    "    \n",
    "    t0 = time.time()\n",
    "    with open(clustering_file_name, \"rb\") as fp:\n",
    "        clustering_machine = pickle.load(fp)\n",
    "    batch_machine_read = time.time() - t0\n",
    "    print('Batch machine reading costs a total of {0:.4f} seconds!'.format(batch_machine_read))\n",
    "    \n",
    "    print('Start to generate the validation whole graph:')\n",
    "    info_folder = intermediate_data_folder + info_folder\n",
    "    os.makedirs(os.path.dirname(info_folder), exist_ok=True)\n",
    "    t1 = time.time()\n",
    "    # create the validation batch for the whole graph\n",
    "    clustering_machine.whole_validation_clustering(intermediate_data_folder, info_folder = info_folder)\n",
    "    \n",
    "    validation_batch_production_time = time.time() - t1\n",
    "    print('Validation batches production costs a total of {0:.4f} seconds!'.format(validation_batch_production_time))\n",
    "    print_dir_content_info(intermediate_data_folder + 'validation/')\n",
    "    print('=' * 100)\n",
    "    # output the memory usage information\n",
    "    info_GPU_memory_folder = intermediate_data_folder + 'info_GPU_memory/'\n",
    "    output_GPU_memory_usage('GPU_memory_cost_generate_validatoin_data.txt', info_GPU_memory_folder, comment ='after generating validation for whole graph: ')\n",
    "\n",
    "def Cluster_train_batch_run(trainer_id, intermediate_data_folder, input_layer = [16, 16], epochs=300, \\\n",
    "                           dropout = 0.3, lr = 0.01, weight_decay = 0.01, mini_epoch_num = 5, \\\n",
    "                                 train_part_num = 2, test_part_num = 1):\n",
    "    \"\"\"\n",
    "    # Run the mini-batch model (train and validate both in batches)\n",
    "    Tuning parameters:  dropout, lr (learning rate), weight_decay: l2 regularization\n",
    "    return: validation accuracy value, validation F-1 value, time_training (ms), time_data_load (ms)\n",
    "    \"\"\"\n",
    "    \n",
    "    data_info_file_folder = intermediate_data_folder + 'data_info/'\n",
    "    data_info_file = data_info_file_folder + 'data_info_file.txt'\n",
    "    with open(data_info_file, \"rb\") as fp:\n",
    "        num_node_features, num_classes = pickle.load(fp)\n",
    "    \n",
    "    print('\\n' + '=' * 100)\n",
    "    print('Start generate the trainer:')\n",
    "    t0 = time.time()\n",
    "    gcn_trainer = ClusterGCNTrainer_mini_Train(intermediate_data_folder, num_node_features, num_classes, input_layers = input_layer, dropout = dropout)\n",
    "    train_create = time.time() - t0\n",
    "    print('Trainer creation costs a total of {0:.4f} seconds!'.format(train_create))\n",
    "    \n",
    "    print('Start train the model:')\n",
    "    t1 = time.time()\n",
    "    gcn_trainer.train(epoch_num=epochs, learning_rate=lr, weight_decay=weight_decay, mini_epoch_num = mini_epoch_num, train_batch_num = train_part_num)\n",
    "    train_period = time.time() - t1\n",
    "    print('Training costs a total of {0:.4f} seconds!'.format(train_period))\n",
    "    \n",
    "    print('Start to save the GCN trainer model (parameters: weights, bias):')\n",
    "    trainer_file_name = intermediate_data_folder + 'GCNtrainer/GCN_trainer_' + str(trainer_id)\n",
    "    t2 = time.time()\n",
    "    with open(trainer_file_name, \"wb\") as fp:\n",
    "        pickle.dump(gcn_trainer, fp)\n",
    "        \n",
    "    store_trainer = time.time() - t2\n",
    "    print('Storing the trainer costs a total of {0:.4f} seconds!'.format(store_trainer))\n",
    "    print('-' * 80)\n",
    "    info_GPU_memory_folder = intermediate_data_folder + 'info_GPU_memory/'\n",
    "    output_GPU_memory_usage('Memory_use_batch_train.txt', info_GPU_memory_folder, comment ='after generating trainer and train minibatches: ')\n",
    "\n",
    "def Cluster_valid_batch_run(trainer_id, intermediate_data_folder):\n",
    "    print('Start to read the GCN trainer model (parameters: weights, bias):')\n",
    "    trainer_file_name = intermediate_data_folder + 'GCNtrainer/GCN_trainer_' + str(trainer_id)\n",
    "    t1 = time.time()\n",
    "    with open(trainer_file_name, \"rb\") as fp:\n",
    "        gcn_trainer = pickle.load(fp)\n",
    "    read_trainer = (time.time() - t1) * 1000\n",
    "    print('Reading the trainer costs a total of {0:.4f} seconds!'.format(read_trainer))\n",
    "    \n",
    "    print('Start validate the model:')\n",
    "    t2 = time.time()\n",
    "#     validation_F1, validation_accuracy = gcn_trainer.batch_validate(valid_batch_num = valid_part_num)\n",
    "    validation_F1, validation_accuracy = gcn_trainer.whole_cpu_validate()\n",
    "    validation_period = time.time() - t2\n",
    "    print('Validatoin costs a total of {0:.4f} seconds!'.format(validation_period))\n",
    "    print('=' * 100)\n",
    "    time_train_total = gcn_trainer.time_train_total\n",
    "    time_data_load = gcn_trainer.time_train_load_data\n",
    "    \n",
    "    info_GPU_memory_folder = intermediate_data_folder + 'info_GPU_memory/'\n",
    "    output_GPU_memory_usage('Memory_use_run_cpu_validation.txt', info_GPU_memory_folder, comment ='after validating minibatches: ')\n",
    "    \n",
    "    return validation_accuracy, validation_F1, time_train_total, time_data_load\n",
    "\n",
    "\n",
    "def Cluster_tune_train_run(intermediate_data_folder, input_layer = [16, 16], epochs=300, \\\n",
    "                           dropout = 0.3, lr = 0.01, weight_decay = 0.01, mini_epoch_num = 5, \\\n",
    "                                 train_part_num = 2):\n",
    "    \"\"\"\n",
    "    # Run the mini-batch model (train and validate both in batches)\n",
    "    Tuning parameters:  dropout, lr (learning rate), weight_decay: l2 regularization\n",
    "    return: validation accuracy value, validation F-1 value, time_training (ms), time_data_load (ms)\n",
    "    \"\"\"\n",
    "    data_info_file_folder = intermediate_data_folder + 'data_info/'\n",
    "    data_info_file = data_info_file_folder + 'data_info_file.txt'\n",
    "    with open(data_info_file, \"rb\") as fp:\n",
    "        num_node_features, num_classes = pickle.load(fp)\n",
    "    \n",
    "    print('\\n' + '=' * 100)\n",
    "    print('Start generate the trainer:')\n",
    "    t0 = time.time()\n",
    "    gcn_trainer = ClusterGCNTrainer_mini_Train(intermediate_data_folder, num_node_features, num_classes, input_layers = input_layer, dropout = dropout)\n",
    "    train_create = time.time() - t0\n",
    "    print('Trainer creation costs a total of {0:.4f} seconds!'.format(train_create))\n",
    "    \n",
    "    print('Start train the model:')\n",
    "    t1 = time.time()\n",
    "    gcn_trainer.train(epoch_num=epochs, learning_rate=lr, weight_decay=weight_decay, mini_epoch_num = mini_epoch_num, train_batch_num = train_part_num)\n",
    "    train_period = time.time() - t1\n",
    "    print('Training costs a total of {0:.4f} seconds!'.format(train_period))\n",
    "    print('-' * 80)\n",
    "    info_GPU_memory_folder = intermediate_data_folder + 'info_GPU_memory/'\n",
    "    output_GPU_memory_usage('Memory_use_tune_batch_train.txt', info_GPU_memory_folder, comment ='after tune training: ')\n",
    "    return gcn_trainer\n",
    "    \n",
    "    \n",
    "def Cluster_tune_validation_run(gcn_trainer, valid_part_num = 2):\n",
    "    \n",
    "    print('Start validate the model:')\n",
    "    t2 = time.time()\n",
    "#     validation_F1, validation_accuracy = gcn_trainer.batch_validate(valid_batch_num = valid_part_num)\n",
    "    validation_F1, validation_accuracy = gcn_trainer.whole_cpu_validate()\n",
    "    \n",
    "    validation_period = time.time() - t2\n",
    "    print('Validatoin costs a total of {0:.4f} seconds!'.format(validation_period))\n",
    "    print('=' * 100)\n",
    "    time_train_total = gcn_trainer.time_train_total\n",
    "    time_data_load = gcn_trainer.time_train_load_data\n",
    "    \n",
    "    info_GPU_memory_folder = intermediate_data_folder + 'info_GPU_memory/'\n",
    "    output_GPU_memory_usage('Memory_use_tune_whole_validation.txt', info_GPU_memory_folder, comment ='after validation whole graph: ')\n",
    "    return validation_accuracy, validation_F1, time_train_total, time_data_load\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test and compare different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_one_train(mini_batch_folder, trainer_id = 0, input_layer = [32], epoch_num = 300, \\\n",
    "                dropout = 0.3, lr = 0.0001, weight_decay = 0.01, mini_epoch_num = 5, \\\n",
    "                train_part_num = 2, test_part_num = 1):\n",
    "    \"\"\"\n",
    "        Perform one train for one time, store the corresponding trainer for future testing\n",
    "    \"\"\"\n",
    "    Trainer_folder = mini_batch_folder + 'GCNtrainer/'\n",
    "#     check_folder_exist(Trainer_folder)\n",
    "    os.makedirs(os.path.dirname(Trainer_folder), exist_ok=True)\n",
    "#     graph_model = ['batch_valid', 'train_batch', 'whole_graph', 'isolate']\n",
    "    Cluster_train_batch_run(trainer_id, mini_batch_folder, input_layer = input_layer, epochs=epoch_num, \\\n",
    "                                                         dropout = dropout, lr = lr, weight_decay = weight_decay, mini_epoch_num = mini_epoch_num, \\\n",
    "                                                         train_part_num = train_part_num, test_part_num = test_part_num)\n",
    "\n",
    "def execute_one_validation(mini_batch_folder, trainer_list = [0]):\n",
    "    \"\"\"\n",
    "        return all test-F1 and validation-F1 for all four models\n",
    "    \"\"\"\n",
    "    validation_accuracy = {}\n",
    "    validation_f1 = {}\n",
    "    time_total_train = {}\n",
    "    time_data_load = {}\n",
    "    \n",
    "    # Each graph model corresponds to one function below\n",
    "#     graph_model = ['batch_valid', 'train_batch', 'whole_graph', 'isolate']\n",
    "    graph_model = ['batch_valid']\n",
    "    for trainer_id in trainer_list:\n",
    "        model_res = []\n",
    "        model_res.append(Cluster_valid_batch_run(trainer_id, mini_batch_folder))\n",
    "        \n",
    "        validation_accuracy[trainer_id], validation_f1[trainer_id], time_total_train[trainer_id], time_data_load[trainer_id] = zip(*model_res)\n",
    "    return graph_model, validation_accuracy, validation_f1, time_total_train, time_data_load\n",
    "\n",
    "def store_data_multi_tests(f1_data, data_name, graph_model, img_path, comments):\n",
    "    run_id = sorted(f1_data.keys())\n",
    "    run_data = {'run_id': run_id}\n",
    "    \n",
    "    run_data.update({model_name : [f1_data[key][idx] for key in run_id] for idx, model_name in enumerate(graph_model)})\n",
    "    \n",
    "    pickle_filename = img_path + data_name + '_' + comments + '.pkl'\n",
    "    os.makedirs(os.path.dirname(pickle_filename), exist_ok=True)\n",
    "    df = pd.DataFrame(data=run_data, dtype=np.int32)\n",
    "    df.to_pickle(pickle_filename)\n",
    "    return pickle_filename\n",
    "\n",
    "def draw_data_multi_tests(pickle_filename, data_name, comments, xlabel, ylabel):\n",
    "    df = pd.read_pickle(pickle_filename)\n",
    "    df_reshape = df.melt('run_id', var_name = 'model', value_name = ylabel)\n",
    "\n",
    "    plt.clf()\n",
    "    plt.figure()\n",
    "    sns.set(style='whitegrid')\n",
    "    g = sns.catplot(x=\"model\", y=ylabel, kind='box', data=df_reshape)\n",
    "    g.despine(left=True)\n",
    "    g.fig.suptitle(data_name + ' ' + ylabel + ' ' + comments)\n",
    "    g.set_xlabels(xlabel)\n",
    "    g.set_ylabels(ylabel)\n",
    "\n",
    "    img_name = pickle_filename[:-4] + '_img'\n",
    "    os.makedirs(os.path.dirname(img_name), exist_ok=True)\n",
    "    plt.savefig(img_name, bbox_inches='tight')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  To test one single model for different parameter values \n",
    "def execute_tuning_train(mini_batch_folder, tune_param_name, tune_val, trainer_id = 0, input_layer = [32], epoch_num = 400, \\\n",
    "                  dropout = 0.1, lr = 0.0001, weight_decay = 0.1, mini_epoch_num = 20, \\\n",
    "                  train_part_num = 2):\n",
    "    \"\"\"\n",
    "        Tune all the hyperparameters\n",
    "        1) learning rate\n",
    "        2) dropout\n",
    "        3) layer unit number\n",
    "        4) weight decay\n",
    "    \"\"\"\n",
    "    Trainer_folder = mini_batch_folder + 'GCN_tuning/tune_' + tune_param_name + '_' + str(tune_val) + '/'\n",
    "#     check_folder_exist(Trainer_folder)\n",
    "    os.makedirs(os.path.dirname(Trainer_folder), exist_ok=True)\n",
    "    \n",
    "    trainer_file_name = Trainer_folder + 'GCN_trainer_' + str(trainer_id)\n",
    "    \n",
    "    gcn_trainer = Cluster_tune_train_run(mini_batch_folder, input_layer = input_layer, epochs=epoch_num, \\\n",
    "                    dropout = dropout, lr = lr, weight_decay = weight_decay, mini_epoch_num = tune_val, \\\n",
    "                    train_part_num = train_part_num)\n",
    "    with open(trainer_file_name, \"wb\") as fp:\n",
    "        pickle.dump(gcn_trainer, fp)\n",
    "    \n",
    "# this tuning validation requires the image_path to store all the results as the picle file\n",
    "def execute_tuning_validation(image_path, mini_batch_folder, tune_param_name, tune_val, trainer_id = 0, valid_part_num = 2):\n",
    "    Trainer_folder = mini_batch_folder + 'GCN_tuning/tune_' + tune_param_name + '_' + str(tune_val) + '/'\n",
    "    trainer_file_name = Trainer_folder + 'GCN_trainer_' + str(trainer_id)\n",
    "    \n",
    "    print('Start to read the GCN trainer model (parameters: weights, bias):')\n",
    "    t1 = time.time()\n",
    "    with open(trainer_file_name, \"rb\") as fp:\n",
    "        gcn_trainer = pickle.load(fp)\n",
    "    read_trainer = (time.time() - t1) * 1000\n",
    "    print('Reading the trainer costs a total of {0:.4f} seconds!'.format(read_trainer))\n",
    "    # res are: validation_accuracy, validation_F1, time_train_total, time_data_load\n",
    "    res = Cluster_tune_validation_run(gcn_trainer, valid_part_num = valid_part_num)\n",
    "    \n",
    "    # store the resulting data on the disk\n",
    "    test_res_folder = image_path + 'test_res/tune_' + tune_param_name + '_' + str(tune_val) + '/'\n",
    "    os.makedirs(os.path.dirname(test_res_folder), exist_ok=True)\n",
    "    test_res_file = test_res_folder + 'res_trainer_' + str(trainer_id)\n",
    "    \n",
    "    with open(test_res_file, \"wb\") as fp:\n",
    "        pickle.dump(res, fp)\n",
    "    \n",
    "def summarize_tuning_res(image_path, mini_batch_folder, tune_param_name, tune_val_list, trainer_list):\n",
    "    validation_accuracy = {}\n",
    "    validation_f1 = {}\n",
    "    time_total_train = {}\n",
    "    time_data_load = {}\n",
    "    \n",
    "    res = []\n",
    "    for trainer_id in trainer_list:\n",
    "        ref = {}\n",
    "        for tune_val in tune_val_list:\n",
    "            test_res_folder = image_path + 'test_res/tune_' + tune_param_name + '_' + str(tune_val) + '/'\n",
    "            test_res_file = test_res_folder + 'res_trainer_' + str(trainer_id)\n",
    "            with open(test_res_file, \"rb\") as fp:\n",
    "                ref[tune_val] = pickle.load(fp)\n",
    "        res.append(ref)\n",
    "    \n",
    "    for i, ref in enumerate(res):\n",
    "        validation_accuracy[i] = {tune_val : res_lst[0] for tune_val, res_lst in ref.items()}\n",
    "        validation_f1[i] = {tune_val : res_lst[1] for tune_val, res_lst in ref.items()}\n",
    "        time_total_train[i] = {tune_val : res_lst[2] for tune_val, res_lst in ref.items()}\n",
    "        time_data_load[i] = {tune_val : res_lst[3] for tune_val, res_lst in ref.items()}\n",
    "        \n",
    "    return validation_accuracy, validation_f1, time_total_train, time_data_load\n",
    "\n",
    "def store_data_multi_tuning(tune_params, target, data_name, img_path, comments):\n",
    "    \"\"\"\n",
    "        tune_params: is the tuning parameter list\n",
    "        target: is the result, here should be F1-score, accuraycy, load time, train time\n",
    "    \"\"\"\n",
    "    run_ids = sorted(target.keys())   # key is the run_id\n",
    "    run_data = {'run_id': run_ids}\n",
    "    # the key can be converted to string or not: i.e. str(tune_val)\n",
    "    # here we keep it as integer such that we want it to follow order\n",
    "    tmp = {tune_val : [target[run_id][tune_val] for run_id in run_ids] for tune_val in tune_params}  # the value is list\n",
    "    run_data.update(tmp)\n",
    "    \n",
    "    pickle_filename = img_path + data_name + '_' + comments + '.pkl'\n",
    "    os.makedirs(os.path.dirname(pickle_filename), exist_ok=True)\n",
    "    df = pd.DataFrame(data=run_data, dtype=np.int32)\n",
    "    df.to_pickle(pickle_filename)\n",
    "    return pickle_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-test execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_train_loss(data, data_name, dataset, image_data_path, intermediate_data_path, partition_nums, layers, \\\n",
    "                      train_frac = 1.0, validation_frac = 1.0, valid_part_num = 2, \\\n",
    "                      dropout = 0.1, lr = 0.0001, weight_decay = 0.1, mini_epoch_num = 20):\n",
    "    for partn in partition_nums:\n",
    "        for GCN_layer in layers:\n",
    "            net_layer = len(GCN_layer) + 1\n",
    "            hop_layer = net_layer\n",
    "            print('Start checking train loss for partition num: ' + str(partn) + ' hop layer: ' + str(hop_layer))\n",
    "            img_path = image_data_path + 'cluster_num_' + str(partn) + '/' + 'net_layer_' + str(net_layer) + '_hop_layer_' + str(hop_layer) + '/'\n",
    "            img_path += 'output_train_loss/'  # further subfolder for different task\n",
    "            intermediate_data_folder = intermediate_data_path\n",
    "            \n",
    "            # set the batch for validation and train\n",
    "            mini_batch_folder = intermediate_data_folder\n",
    "            set_clustering_machine(data, dataset, img_path, intermediate_data_folder, test_ratio = 0.05, validation_ratio = 0.85, train_batch_num = partn, valid_batch_num = valid_part_num, test_batch_num = 2)\n",
    "            set_clustering_machine_train_batch(img_path, intermediate_data_folder, neigh_layer = hop_layer, train_frac = train_frac, batch_range = (0, partn))\n",
    "            set_clustering_machine_validation_batch(img_path, intermediate_data_folder, neigh_layer = hop_layer, validation_frac = validation_frac, batch_range = (0, valid_part_num))\n",
    "            \n",
    "            check_train_loss_converge(mini_batch_folder, data_name, dataset, img_path, 'part_num_' + str(partn), input_layer = GCN_layer, epoch_num = 400, \\\n",
    "                                     dropout = dropout, lr = lr, weight_decay = weight_decay, mini_epoch_num = mini_epoch_num, \n",
    "                                     valid_part_num = valid_part_num, train_part_num = partn, test_part_num = 1)\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate steps of HPC execution for one single task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step0_generate_clustering_machine(data, dataset, intermediate_data_path, train_batch_num, hop_layer):            \n",
    "        print('Start running for train batch num: ' + str(train_batch_num) + ' hop layer ' + str(hop_layer))\n",
    "\n",
    "        # set the basic settings for the future batches generation\n",
    "        set_clustering_machine(data, dataset, intermediate_data_path, test_ratio = 0.05, validation_ratio = 0.85, train_batch_num = train_batch_num)\n",
    "\n",
    "def step1_generate_train_batch(intermediate_data_path, hop_layer, train_frac = 1.0, \\\n",
    "                               batch_range = (0, 1), info_folder = 'info/'):            \n",
    "        # set the save path\n",
    "        info_file = 'train_batch_size_info_{}.csv'.format(str(batch_range))\n",
    "        print('Start running for train batch num range: ' + str(batch_range) + ' hop layer ' + str(hop_layer))\n",
    "\n",
    "        # generate the train batches\n",
    "        set_clustering_machine_train_batch(intermediate_data_path, neigh_layer = hop_layer, train_frac = train_frac, \\\n",
    "                                           batch_range = batch_range, info_folder = info_folder, info_file = info_file)\n",
    "\n",
    "def step2_generate_validation_whole_graph(intermediate_data_path, info_folder = 'info/'):            \n",
    "        info_file = 'validation_whole_graph_size_info.csv'\n",
    "        # generate all the tensors from the  whole graph for validation\n",
    "        set_clustering_machine_validation_whole_graph(intermediate_data_path, info_folder = info_folder, info_file = info_file)\n",
    "            \n",
    "def step3_run_train_batch(intermediate_data_path, train_batch_num, hop_layer, GCN_layer, \\\n",
    "                    trainer_id = 0, dropout = 0.1, lr = 0.0001, weight_decay = 0.1, mini_epoch_num = 20, epoch_num = 400):            \n",
    "        print('Start running training for train batch num: ' + str(train_batch_num) + ' hop layer ' + str(hop_layer))\n",
    "        # start to train the model\n",
    "        execute_one_train(intermediate_data_path, trainer_id = trainer_id, input_layer = GCN_layer, epoch_num = epoch_num, \n",
    "                                        dropout = dropout, lr = lr, weight_decay = weight_decay, mini_epoch_num = mini_epoch_num, \\\n",
    "                                         train_part_num = train_batch_num, test_part_num = 1)\n",
    "            \n",
    "\n",
    "def step4_run_validation_batch(image_data_path, intermediate_data_path, train_batch_num, hop_layer, net_layer_num, trainer_list = [0]):   \n",
    "        \"\"\"\n",
    "            image_data_path:  path of to be stored result images\n",
    "            intermediate_data_path :  path for all the intermediate results: clustering, train batches, trained trainers\n",
    "            train_batch_num:  number of generated overlapping batches\n",
    "            hop_layer:  number of to be included neighbor layers\n",
    "            trainer_list :  list of all the trained trainer (gcn model instance) indices\n",
    "        \"\"\"\n",
    "        # set the save path\n",
    "        print('Start running validation for train batch num: ' + str(train_batch_num) + ' hop layer ' + str(hop_layer))\n",
    "        img_path = image_data_path + 'cluster_num_' + str(train_batch_num) + '/' + 'net_layer_num_' + str(net_layer_num) + '_hop_layer_num_' + str(hop_layer) + '/'\n",
    "        img_path += 'output_f1_score/'  # further subfolder for different task\n",
    "\n",
    "        graph_model, validation_accuracy, validation_f1, time_total_train, time_data_load = \\\n",
    "            execute_one_validation(intermediate_data_path, trainer_list = trainer_list)\n",
    "\n",
    "        validation_accuracy = store_data_multi_tests(validation_accuracy, data_name, graph_model, img_path, 'test_cluster_num_' + str(train_batch_num) + '_hops_' + str(hop_layer))\n",
    "        draw_data_multi_tests(validation_accuracy, data_name, 'vali_cluster_num_' + str(train_batch_num) + '_hop_' + str(hop_layer), 'models', 'Accuracy')\n",
    "\n",
    "        validation_f1 = store_data_multi_tests(validation_f1, data_name, graph_model, img_path, 'validation_cluster_num_' + str(train_batch_num) + '_hops_' + str(hop_layer))\n",
    "        draw_data_multi_tests(validation_f1, data_name, 'vali_cluster_num_' + str(train_batch_num) + '_hop_' + str(hop_layer), 'models', 'F1 score')\n",
    "\n",
    "        time_train = store_data_multi_tests(time_total_train, data_name, graph_model, img_path, 'train_time_cluster_num_' + str(train_batch_num) + '_hops_' + str(hop_layer))\n",
    "        draw_data_multi_tests(time_train, data_name, 'train_time_cluster_num_' + str(train_batch_num) + '_hop_' + str(hop_layer), 'models', 'Train Time (ms)')\n",
    "\n",
    "        time_load = store_data_multi_tests(time_data_load, data_name, graph_model, img_path, 'load_time_cluster_num_' + str(train_batch_num) + '_hops_' + str(hop_layer))\n",
    "        draw_data_multi_tests(time_load, data_name, 'load_time_cluster_num_' + str(train_batch_num) + '_hop_' + str(hop_layer), 'models', 'Load Time (ms)')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seperate steps for parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step30_run_tune_train_batch(intermediate_data_path, tune_param_name, tune_val, train_batch_num, hop_layer_num, GCN_layer, \\\n",
    "                    trainer_id = 0, dropout = 0.1, lr = 0.0001, weight_decay = 0.1, mini_epoch_num = 20, epoch_num = 400):            \n",
    "        print('Start running training for partition num: ' + str(train_batch_num) + ' hop layer ' + str(hop_layer_num))\n",
    "        # start to tune the model, run different training \n",
    "        execute_tuning_train(intermediate_data_path, tune_param_name, tune_val, trainer_id = trainer_id, input_layer = GCN_layer, epoch_num = epoch_num, \\\n",
    "                                        dropout = dropout, lr = lr, weight_decay = weight_decay, mini_epoch_num = mini_epoch_num, \\\n",
    "                                      train_part_num = train_batch_num)\n",
    "            \n",
    "def step40_run_tune_validation_whole(image_data_path, intermediate_data_path, tune_param_name, tune_val, train_batch_num, hop_layer_num, net_layer_num, \\\n",
    "                    trainer_id = 0): \n",
    "        # set the save path\n",
    "        print('Start running training for partition num: ' + str(train_batch_num) + ' hop layer ' + str(hop_layer_num))\n",
    "        img_path = image_data_path + 'cluster_num_' + str(train_batch_num) + '/' + 'net_layer_num_' + str(net_layer_num) + '_hop_layer_num_' + str(hop_layer_num) + '/'\n",
    "        img_path += 'tuning_parameters/'  # further subfolder for different task\n",
    "        \n",
    "        # start to validate the model, with different tuning parameters\n",
    "        execute_tuning_validation(img_path, intermediate_data_path, tune_param_name, tune_val, trainer_id = trainer_id)\n",
    "\n",
    "            \n",
    "def step50_run_tune_summarize_whole(data_name, image_data_path, intermediate_data_path, tune_param_name, tune_val_list, \\\n",
    "                                    train_batch_num, hop_layer_num, net_layer_num, trainer_list): \n",
    "    \n",
    "        print('Start running training for partition num: ' + str(train_batch_num) + ' hop layer ' + str(hop_layer_num))\n",
    "        # set the batch for validation and train\n",
    "        img_path = image_data_path + 'cluster_num_' + str(train_batch_num) + '/' + 'net_layer_num_' + str(net_layer_num) + '_hop_layer_num_' + str(hop_layer_num) + '/'\n",
    "        img_path += 'tuning_parameters/'  # further subfolder for different task\n",
    "        \n",
    "        # start to summarize the results into images for output\n",
    "\n",
    "        validation_accuracy, validation_f1, time_total_train, time_data_load = summarize_tuning_res(img_path, intermediate_data_path, tune_param_name, tune_val_list, trainer_list)\n",
    "\n",
    "        validation_accuracy = store_data_multi_tuning(tune_val_list, validation_accuracy, data_name, img_path, 'accuracy_cluster_num_' + str(train_batch_num) + '_hops_' + str(hop_layer_num))\n",
    "        draw_data_multi_tests(validation_accuracy, data_name, 'vali_cluster_num_' + str(train_batch_num) + '_hop_' + str(hop_layer_num), 'epochs_per_batch', 'Accuracy')\n",
    "\n",
    "        validation_f1 = store_data_multi_tuning(tune_val_list, validation_f1, data_name, img_path, 'validation_cluster_num_' + str(train_batch_num) + '_hops_' + str(hop_layer_num))\n",
    "        draw_data_multi_tests(validation_f1, data_name, 'vali_cluster_num_' + str(train_batch_num) + '_hop_' + str(hop_layer_num), 'epochs_per_batch', 'F1 score')\n",
    "\n",
    "        time_train = store_data_multi_tuning(tune_val_list, time_total_train, data_name, img_path, 'train_time_cluster_num_' + str(train_batch_num) + '_hops_' + str(hop_layer_num))\n",
    "        draw_data_multi_tests(time_train, data_name, 'train_time_cluster_num_' + str(train_batch_num) + '_hop_' + str(hop_layer_num), 'epochs_per_batch', 'Train Time (ms)')\n",
    "\n",
    "        time_load = store_data_multi_tuning(tune_val_list, time_data_load, data_name, img_path, 'load_time_cluster_num_' + str(train_batch_num) + '_hops_' + str(hop_layer_num))\n",
    "        draw_data_multi_tests(time_load, data_name, 'load_time_cluster_num_' + str(train_batch_num) + '_hop_' + str(hop_layer_num), 'epochs_per_batch', 'Load Time (ms)')\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use data from pytorch geometric datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_data_root = '/media/xiangli/storage1/projects/tmpdata/'\n",
    "test_folder_name = 'test_subdivide_batches/metis_train_10%_full_neighbor/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cora dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "data_name = 'Cora'\n",
    "dataset = Planetoid(root = local_data_root + 'Planetoid/Cora', name=data_name)\n",
    "data = dataset[0]\n",
    "# set the current folder as the intermediate data folder so that we can easily copy either clustering \n",
    "intermediate_data_folder = './test_subdiv_f1_1/'   # path to store all the intermediate data \n",
    "image_data_path = './results/' + data_name + '/' + test_folder_name\n",
    "\n",
    "train_batch_num = 4\n",
    "GCN_layer = [32]\n",
    "net_layer_num = len(GCN_layer) + 1\n",
    "# for non-optimization: hop_layer_num == net_layer_num\n",
    "hop_layer_num = net_layer_num\n",
    "tune_param_name = 'batch_epoch_num'\n",
    "tune_val_list = [400, 200, 100, 50, 20, 10, 5]\n",
    "# tune_val_list = [10, 5]\n",
    "trainer_list = list(range(7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start running for train batch num: 4 hop layer 2\n",
      "\n",
      "====================================================================================================\n",
      "Start to generate the global edge weights\n",
      "Edge weights creation costs a total of 0.0275 seconds!\n",
      "Start to generate the clustering machine:\n",
      "Batch machine creation costs a total of 0.0328 seconds!\n",
      "\n",
      " Information about the content of ./test_subdiv_f1_1/tmp/\n",
      "File name: [ input_edge_weight_list.csv ]; with size: 267.9462890625 KB\n",
      "\n",
      "Start to split data into train, test, validation:\n",
      "Data splitting costs a total of 0.0065 seconds!\n",
      "Start to store the batch machine file:\n",
      "Storing batch machine after training batches generation costs a total of 0.0139 seconds!\n",
      "\n",
      "====================================================================================================\n",
      "Start running for train batch num range: (0, 4) hop layer 2\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0104 seconds!\n",
      "Start to generate the training batches:\n",
      "*** Generate batch file for #   0 batch, writing the batch file costed 3.72 ms ***\n",
      "*** Generate batch file for #   1 batch, writing the batch file costed 3.81 ms ***\n",
      "*** Generate batch file for #   2 batch, writing the batch file costed 3.48 ms ***\n",
      "*** Generate batch file for #   3 batch, writing the batch file costed 3.39 ms ***\n",
      "Train batches production costs a total of 0.0649 seconds!\n",
      "\n",
      " Information about the content of ./test_subdiv_f1_1/train/\n",
      "File name: [ batch_1 ]; with size: 7479.970703125 KB\n",
      "File name: [ batch_3 ]; with size: 6805.166015625 KB\n",
      "File name: [ batch_0 ]; with size: 7046.400390625 KB\n",
      "File name: [ batch_2 ]; with size: 6943.080078125 KB\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0073 seconds!\n",
      "Start to generate the validation whole graph:\n",
      "*** Generate batch file for # whole graph batch, writing the batch file costed 7.07 ms ***\n",
      "Validation batches production costs a total of 0.0247 seconds!\n",
      "\n",
      " Information about the content of ./test_subdiv_f1_1/validation/\n",
      "File name: [ batch_whole ]; with size: 15563.9873046875 KB\n",
      "\n",
      "====================================================================================================\n",
      "Start running training for train batch num: 4 hop layer 2\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0136 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   1 batch, reading batch file costed 2.71 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.36 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.41 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.42 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.56 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 3.02 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.70 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.57 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.34 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.57 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.71 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 3.27 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.31 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.38 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.78 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 4.29 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.36 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.71 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.74 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.94 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.34 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.22 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.54 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.38 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.57 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 4.38 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.79 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 4.22 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.85 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.63 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.72 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.58 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 3.18 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.39 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.31 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.37 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.22 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.87 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.59 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.80 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 3.80 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.42 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.92 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.64 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 3.93 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.72 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 3.36 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.73 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.33 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.54 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 3.99 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.64 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.32 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.39 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.34 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.65 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.37 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.35 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.44 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.84 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.54 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.31 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.29 ms ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** During training for #   3 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.44 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.86 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.99 ms ***\n",
      "Training costs a total of 4.4061 seconds!\n",
      "Start to save the GCN trainer model (parameters: weights, bias):\n",
      "Storing the trainer costs a total of 0.0015 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start running training for train batch num: 4 hop layer 2\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0010 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   3 batch, reading batch file costed 3.02 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.90 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 4.09 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.66 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.40 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.77 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.37 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.60 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 5.16 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 3.17 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.84 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.90 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.65 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.37 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.33 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.57 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.38 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.36 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.39 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.21 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.41 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.33 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.57 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.76 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.57 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 3.49 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 3.06 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.89 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 3.72 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 4.03 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.97 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.35 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.69 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.82 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.99 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.66 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.72 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.63 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.42 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.38 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.34 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.68 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.35 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.34 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.42 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.84 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.79 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.38 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.20 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.39 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.39 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.57 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.35 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.44 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.92 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.41 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.41 ms ***\n",
      "Training costs a total of 4.4962 seconds!\n",
      "Start to save the GCN trainer model (parameters: weights, bias):\n",
      "Storing the trainer costs a total of 0.0013 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start running validation for train batch num: 4 hop layer 2\n",
      "Start to read the GCN trainer model (parameters: weights, bias):\n",
      "Reading the trainer costs a total of 1.2887 seconds!\n",
      "Start validate the model:\n",
      "*** During validation for # whole graph batch, reading batch file costed 4.70 ms ***\n",
      "Validatoin costs a total of 0.0105 seconds!\n",
      "====================================================================================================\n",
      "Start to read the GCN trainer model (parameters: weights, bias):\n",
      "Reading the trainer costs a total of 1.1210 seconds!\n",
      "Start validate the model:\n",
      "*** During validation for # whole graph batch, reading batch file costed 4.81 ms ***\n",
      "Validatoin costs a total of 0.0104 seconds!\n",
      "====================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "step0_generate_clustering_machine(data, dataset, intermediate_data_folder, train_batch_num, hop_layer_num)\n",
    "\n",
    "step1_generate_train_batch(intermediate_data_folder, hop_layer_num, train_frac = 1.0, \\\n",
    "                           batch_range = (0, train_batch_num), info_folder = 'info_train_batch/' )\n",
    "\n",
    "step2_generate_validation_whole_graph(intermediate_data_folder, info_folder = 'info_validation_whole/')\n",
    "\n",
    "step3_run_train_batch(intermediate_data_folder, train_batch_num, hop_layer_num, GCN_layer, \\\n",
    "                trainer_id = 0, dropout = 0.1, lr = 0.0001, weight_decay = 0.1, mini_epoch_num = 20)\n",
    "\n",
    "step3_run_train_batch(intermediate_data_folder, train_batch_num, hop_layer_num, GCN_layer, \\\n",
    "                trainer_id = 1, dropout = 0.1, lr = 0.0001, weight_decay = 0.1, mini_epoch_num = 20)\n",
    "\n",
    "step4_run_validation_batch(image_data_path, intermediate_data_folder, train_batch_num, hop_layer_num, net_layer_num, trainer_list = list(range(2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune parameter (tune value, trainer_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start running for train batch num: 4 hop layer 2\n",
      "\n",
      "====================================================================================================\n",
      "Start to generate the global edge weights\n",
      "Edge weights creation costs a total of 0.0264 seconds!\n",
      "Start to generate the clustering machine:\n",
      "Batch machine creation costs a total of 0.0330 seconds!\n",
      "\n",
      " Information about the content of ./test_subdiv_tune_1/tmp/\n",
      "File name: [ input_edge_weight_list.csv ]; with size: 267.9462890625 KB\n",
      "\n",
      "Start to split data into train, test, validation:\n",
      "Data splitting costs a total of 0.0063 seconds!\n",
      "Start to store the batch machine file:\n",
      "Storing batch machine after training batches generation costs a total of 0.0098 seconds!\n",
      "\n",
      "====================================================================================================\n",
      "Start running for train batch num range: (0, 4) hop layer 2\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0096 seconds!\n",
      "Start to generate the training batches:\n",
      "*** Generate batch file for #   0 batch, writing the batch file costed 3.96 ms ***\n",
      "*** Generate batch file for #   1 batch, writing the batch file costed 3.56 ms ***\n",
      "*** Generate batch file for #   2 batch, writing the batch file costed 3.44 ms ***\n",
      "*** Generate batch file for #   3 batch, writing the batch file costed 4.09 ms ***\n",
      "Train batches production costs a total of 0.0664 seconds!\n",
      "\n",
      " Information about the content of ./test_subdiv_tune_1/train/\n",
      "File name: [ batch_1 ]; with size: 6888.212890625 KB\n",
      "File name: [ batch_3 ]; with size: 7645.689453125 KB\n",
      "File name: [ batch_0 ]; with size: 7456.478515625 KB\n",
      "File name: [ batch_2 ]; with size: 6699.705078125 KB\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0072 seconds!\n",
      "Start to generate the validation whole graph:\n",
      "*** Generate batch file for # whole graph batch, writing the batch file costed 7.03 ms ***\n",
      "Validation batches production costs a total of 0.0247 seconds!\n",
      "\n",
      " Information about the content of ./test_subdiv_tune_1/validation/\n",
      "File name: [ batch_whole ]; with size: 15563.9873046875 KB\n",
      "\n",
      "====================================================================================================\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0088 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   0 batch, reading batch file costed 3.01 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 3.55 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.57 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.95 ms ***\n",
      "Training costs a total of 4.0596 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0005 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   3 batch, reading batch file costed 2.58 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.57 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.78 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.70 ms ***\n",
      "Training costs a total of 3.9036 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0006 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   1 batch, reading batch file costed 2.19 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 3.06 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.75 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.54 ms ***\n",
      "Training costs a total of 3.8708 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0005 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   2 batch, reading batch file costed 2.31 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.87 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.86 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.79 ms ***\n",
      "Training costs a total of 3.9104 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0007 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   0 batch, reading batch file costed 2.94 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.59 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.81 ms ***\n",
      "Training costs a total of 3.9327 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0005 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   2 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.79 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 3.06 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.50 ms ***\n",
      "Training costs a total of 3.9233 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0006 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   0 batch, reading batch file costed 2.61 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.69 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.72 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.98 ms ***\n",
      "Training costs a total of 4.0237 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0009 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   0 batch, reading batch file costed 2.57 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.40 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.64 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.69 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.59 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.36 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.37 ms ***\n",
      "Training costs a total of 3.9795 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0005 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   3 batch, reading batch file costed 3.25 ms ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** During training for #   0 batch, reading batch file costed 2.86 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.39 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.82 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.56 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.36 ms ***\n",
      "Training costs a total of 3.9453 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0005 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   2 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.68 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.60 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.69 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.36 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.65 ms ***\n",
      "Training costs a total of 3.9302 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0005 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   3 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.42 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.36 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.73 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.42 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.33 ms ***\n",
      "Training costs a total of 3.8593 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0006 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   1 batch, reading batch file costed 2.33 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.63 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.36 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.78 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.82 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.36 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.41 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.70 ms ***\n",
      "Training costs a total of 3.9312 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0005 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   2 batch, reading batch file costed 2.16 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.65 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.66 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.62 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.88 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.75 ms ***\n",
      "Training costs a total of 3.9671 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0006 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   1 batch, reading batch file costed 2.40 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.79 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.79 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 3.00 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.37 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.73 ms ***\n",
      "Training costs a total of 3.9568 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0007 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   0 batch, reading batch file costed 2.72 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.37 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.32 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.58 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.31 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.64 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.52 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.72 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.70 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.36 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.52 ms ***\n",
      "Training costs a total of 4.0318 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0006 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   3 batch, reading batch file costed 2.58 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.57 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.44 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.33 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.81 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.77 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.33 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.37 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.65 ms ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** During training for #   0 batch, reading batch file costed 2.57 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.97 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.73 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.53 ms ***\n",
      "Training costs a total of 3.9278 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0008 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   1 batch, reading batch file costed 2.83 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.07 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.63 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.92 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.69 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.60 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.38 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.80 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.36 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.58 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.70 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.34 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.67 ms ***\n",
      "Training costs a total of 4.0345 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0006 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   2 batch, reading batch file costed 2.32 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.75 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.58 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.35 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.69 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.35 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.38 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.33 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.78 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.36 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.40 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.72 ms ***\n",
      "Training costs a total of 3.9776 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0007 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   3 batch, reading batch file costed 2.61 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.16 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.33 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.56 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.42 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.52 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.58 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.61 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.52 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.67 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.57 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 3.55 ms ***\n",
      "Training costs a total of 4.1083 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0006 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   2 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.57 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.71 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.39 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.40 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.60 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.54 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.80 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.44 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.33 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.86 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.33 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.75 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.83 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.89 ms ***\n",
      "Training costs a total of 4.2092 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0008 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   2 batch, reading batch file costed 2.83 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.56 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.37 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.66 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.42 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.36 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.41 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.61 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.40 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.79 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.31 ms ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** During training for #   3 batch, reading batch file costed 2.56 ms ***\n",
      "Training costs a total of 3.9448 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0006 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   3 batch, reading batch file costed 2.65 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.67 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.54 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.21 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.77 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.86 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.61 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.34 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.68 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.36 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.76 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.61 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.34 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.42 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.42 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.33 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.54 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.33 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.69 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.81 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.86 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.87 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.65 ms ***\n",
      "Training costs a total of 4.0983 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0007 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   0 batch, reading batch file costed 2.70 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.94 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.60 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.56 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.63 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.33 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.72 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.52 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.32 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.75 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.35 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.54 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.22 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.54 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.33 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.68 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.76 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.54 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.54 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.54 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.51 ms ***\n",
      "Training costs a total of 4.0109 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0006 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   3 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.52 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.57 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.58 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.68 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.70 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.71 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.41 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.65 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.20 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.34 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.65 ms ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** During training for #   1 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.52 ms ***\n",
      "Training costs a total of 4.0090 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0005 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   2 batch, reading batch file costed 2.37 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.52 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.31 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.70 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.70 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.62 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.21 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.60 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.61 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.57 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.57 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.59 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.56 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.58 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.56 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.38 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.18 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.58 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.37 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.70 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.26 ms ***\n",
      "Training costs a total of 4.0999 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0005 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   2 batch, reading batch file costed 2.61 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.35 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.33 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.63 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.31 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.71 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.36 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.61 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.31 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.68 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.64 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.58 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.38 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.42 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 4.25 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.73 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.59 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.62 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.85 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.31 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.35 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.71 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.54 ms ***\n",
      "Training costs a total of 4.1626 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0006 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   3 batch, reading batch file costed 2.77 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.32 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.44 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.32 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.36 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.42 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.76 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.39 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.34 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.85 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.76 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.69 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.34 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.59 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.95 ms ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** During training for #   3 batch, reading batch file costed 2.60 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.35 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.50 ms ***\n",
      "Training costs a total of 4.0001 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0006 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   1 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.65 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.56 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.58 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.74 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.57 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.52 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 3.30 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.34 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 3.07 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.95 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.31 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.66 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.60 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.34 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.82 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.33 ms ***\n",
      "Training costs a total of 4.0421 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0005 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   0 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.98 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.56 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.54 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.80 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.52 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.77 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.41 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.72 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.68 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.88 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.22 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.32 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.81 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.60 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.65 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.58 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.61 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.87 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.21 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.41 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.93 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.40 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.69 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.62 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.54 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.73 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.67 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.58 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.67 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.68 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.70 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.34 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.71 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.30 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.54 ms ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** During training for #   1 batch, reading batch file costed 2.44 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.80 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.38 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 3.08 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.73 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 3.69 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 3.01 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.44 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.35 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.20 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.45 ms ***\n",
      "Training costs a total of 4.3012 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0005 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   3 batch, reading batch file costed 2.72 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.85 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.88 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.35 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.52 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.54 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 3.06 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.56 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.72 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.37 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.38 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.85 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.40 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.52 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.34 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.65 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.36 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.67 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.93 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.77 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.35 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.59 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.17 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.41 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.84 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.56 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.66 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.85 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.63 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.44 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.40 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.54 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.40 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.89 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.40 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.62 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.80 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.22 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.44 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.21 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.65 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.56 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.19 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.58 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.39 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.33 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.94 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.19 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.44 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.22 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.56 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.10 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.61 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 3.41 ms ***\n",
      "Training costs a total of 4.2602 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0006 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   0 batch, reading batch file costed 2.59 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.22 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.52 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.67 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.44 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.67 ms ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** During training for #   3 batch, reading batch file costed 2.73 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.20 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.65 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.31 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.54 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.32 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.59 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.39 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.56 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.59 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.21 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.93 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.76 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.41 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.31 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.20 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.31 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.56 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.35 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.80 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.18 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.20 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.69 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.52 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.36 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.38 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.67 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.40 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.34 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.93 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.78 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.80 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.41 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.67 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.54 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.56 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.04 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.13 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.97 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 3.21 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.22 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.18 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.72 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.61 ms ***\n",
      "Training costs a total of 4.2242 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0006 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   3 batch, reading batch file costed 2.92 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.85 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.36 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.20 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 3.01 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.80 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.70 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.57 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.33 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.63 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.66 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.37 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 3.27 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.20 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.22 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.41 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.82 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.68 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.70 ms ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** During training for #   0 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.41 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.68 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.54 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.41 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.81 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.64 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.57 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.56 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.33 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.42 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.19 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.69 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.59 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.44 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.59 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.79 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.40 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.63 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 3.09 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.21 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.20 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.16 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.43 ms ***\n",
      "Training costs a total of 4.1476 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0006 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   2 batch, reading batch file costed 2.34 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.34 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.66 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.70 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.57 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.34 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.62 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.57 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.39 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.02 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.33 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.70 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.21 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.08 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.22 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.44 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.67 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 3.08 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.44 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.52 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.65 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.84 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.41 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.52 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.67 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.40 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.33 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.44 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.33 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.20 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.44 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.41 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.64 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.31 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.40 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.52 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.61 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.24 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.38 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.60 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.54 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.59 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.29 ms ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** During training for #   1 batch, reading batch file costed 2.36 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.37 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.69 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.75 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.32 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.22 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.61 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.94 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.58 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.65 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.54 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.41 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.22 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.57 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.21 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.39 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.13 ms ***\n",
      "Training costs a total of 4.2480 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0006 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   2 batch, reading batch file costed 2.37 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.58 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.95 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.52 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.79 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.81 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.40 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.59 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.40 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.40 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.69 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.44 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.22 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.69 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.67 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.81 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.40 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.98 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.65 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.44 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.62 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.75 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.34 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.44 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.20 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.40 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.20 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.71 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.42 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.42 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.20 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.41 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.36 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 3.03 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.59 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.60 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.73 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.62 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.39 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.78 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.37 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.71 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.64 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.62 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.31 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.69 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.29 ms ***\n",
      "Training costs a total of 4.2377 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0006 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   0 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.57 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.41 ms ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** During training for #   1 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.52 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.56 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.21 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.72 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.19 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 3.07 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.73 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.42 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.31 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.76 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.62 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.54 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.62 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.35 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.19 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.69 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.24 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.80 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.21 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.54 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.33 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.40 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.54 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.83 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.54 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.40 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.33 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.32 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.20 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.32 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.77 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.20 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.32 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 5.46 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.33 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.56 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.84 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.52 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.37 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.66 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.58 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.68 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.73 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.36 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.58 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.62 ms ***\n",
      "Training costs a total of 4.2329 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0006 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   3 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.32 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.20 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.40 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.44 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.37 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.39 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.63 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.38 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.75 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.90 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.42 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.60 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.83 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.54 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.46 ms ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** During training for #   2 batch, reading batch file costed 2.40 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.34 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.57 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.31 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.99 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.78 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.56 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.57 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.93 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.41 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.31 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.35 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.34 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.69 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.69 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.79 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.39 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.19 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.41 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.33 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.80 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.35 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.43 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.52 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.59 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 3.44 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.52 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.71 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.32 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.62 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.33 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.76 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.69 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.79 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.17 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 3.02 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.56 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.67 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.56 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.80 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.44 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.34 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.68 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.61 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.67 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.20 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.18 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.41 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.44 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.21 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.22 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.18 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.44 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.62 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.52 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.76 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.44 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.67 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.38 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.73 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.96 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.90 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.71 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.87 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.52 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.63 ms ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** During training for #   0 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.22 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.20 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.58 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.31 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.61 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.19 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.20 ms ***\n",
      "Training costs a total of 4.5377 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0006 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   2 batch, reading batch file costed 2.14 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.33 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.54 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.33 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.40 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.73 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.63 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.54 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.80 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.62 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.35 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.39 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.35 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.61 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.56 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.44 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.65 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.37 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.60 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.40 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.78 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.42 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.56 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.21 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.67 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.99 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 3.09 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.22 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.42 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.62 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.17 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.21 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.98 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.42 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.22 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.66 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.17 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.61 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.41 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.22 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.38 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.14 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.78 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.76 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.75 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.60 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.44 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.37 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.25 ms ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** During training for #   1 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.52 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.40 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.20 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.86 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.59 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.44 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.59 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.17 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.22 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.52 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.57 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.40 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.69 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.60 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.36 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.71 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.33 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.21 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.52 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.84 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.74 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.97 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.37 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.19 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.35 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.68 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.40 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.52 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.40 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.20 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.39 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.79 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.86 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.57 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.37 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.56 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.31 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.31 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.84 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.76 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.64 ms ***\n",
      "Training costs a total of 4.4920 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0005 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   0 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.36 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.52 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.71 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.88 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.58 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.20 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.91 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.68 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.35 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.39 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.82 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.70 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.71 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.64 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.33 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.20 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.83 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.83 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.69 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.65 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.44 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.34 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.19 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.57 ms ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** During training for #   2 batch, reading batch file costed 2.79 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.57 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.41 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.52 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.20 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.40 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.20 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.58 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.38 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.71 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.75 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.44 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 3.46 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.61 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.33 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.63 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.21 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.66 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.17 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.31 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.37 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.65 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.65 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.41 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.21 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.21 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.60 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.63 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.44 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.64 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.69 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.75 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.37 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.52 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.34 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.54 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.38 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.59 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.31 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.82 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.74 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.42 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.18 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.72 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.35 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.18 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.44 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.61 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.22 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.32 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.75 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.38 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.40 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.52 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.34 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.57 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.21 ms ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** During training for #   0 batch, reading batch file costed 2.44 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.33 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.50 ms ***\n",
      "Training costs a total of 4.4855 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0008 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   0 batch, reading batch file costed 2.72 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.60 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.69 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.21 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.42 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.21 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.39 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.21 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.56 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.31 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.41 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.03 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.56 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.32 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.67 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.64 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.44 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.34 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.62 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.94 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 3.15 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.35 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.35 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.68 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.44 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.60 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.41 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.42 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.34 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.57 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.59 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.66 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.37 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.10 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.63 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.68 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.34 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.31 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.54 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.57 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.41 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.57 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.69 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.22 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.63 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.22 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.79 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.33 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.56 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.77 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.57 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.34 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.66 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.74 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.35 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.19 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.27 ms ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** During training for #   3 batch, reading batch file costed 2.56 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.64 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.78 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.31 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.96 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.59 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.57 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.64 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.95 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.31 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 3.06 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.58 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.58 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.21 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.31 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.96 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.37 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.33 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.35 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.59 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.38 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.80 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.78 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.61 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.68 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.32 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.35 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 3.18 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.33 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.14 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.41 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.31 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.71 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.15 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.87 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.54 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 3.05 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.81 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.29 ms ***\n",
      "Training costs a total of 4.5395 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0005 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   3 batch, reading batch file costed 2.41 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.71 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.34 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.35 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.31 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.56 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.37 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.38 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.54 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.71 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.68 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.70 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.22 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.41 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.41 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.44 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.22 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.41 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.59 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.88 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.16 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.58 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.56 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.40 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.83 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.44 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.42 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.21 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.19 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.52 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.31 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.52 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.80 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.35 ms ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** During training for #   1 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.64 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.89 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.21 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.71 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.36 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.42 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.74 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.58 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.63 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.35 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.17 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.66 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.39 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.39 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.42 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.58 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.79 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.54 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.38 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.62 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.40 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.31 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.39 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.84 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.82 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.32 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.85 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.22 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.39 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.84 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.72 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.80 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.67 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.62 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.63 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.35 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.38 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.54 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.33 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.52 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.70 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.72 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.42 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.33 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.44 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.56 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.21 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.83 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.57 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.34 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.22 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.64 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.45 ms ***\n",
      "Training costs a total of 4.4574 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0007 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   3 batch, reading batch file costed 3.37 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.21 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.19 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.57 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.22 ms ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** During training for #   2 batch, reading batch file costed 2.40 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.54 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.71 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.58 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.38 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.14 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.62 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.60 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.42 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.35 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.63 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.31 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.57 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.83 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.17 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.70 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.42 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.38 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.34 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.44 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.67 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.22 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.56 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.59 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.42 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.56 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.64 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.67 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.21 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.36 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.54 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.35 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.44 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.41 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.12 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.19 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.68 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.44 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.22 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.40 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.42 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.66 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.80 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.57 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.56 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.39 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.54 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.32 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.33 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.19 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.40 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.58 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.66 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.99 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.57 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.59 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.68 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.77 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.58 ms ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** During training for #   0 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.42 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.35 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.67 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 3.15 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.21 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.56 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.38 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.18 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.39 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.44 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 3.16 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.83 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.61 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.56 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.31 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.54 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.77 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.17 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.39 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.19 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.51 ms ***\n",
      "Training costs a total of 4.4903 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0006 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   3 batch, reading batch file costed 2.41 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.32 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.34 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.36 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.75 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.52 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.77 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.31 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.54 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.37 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.78 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.66 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.62 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.20 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.78 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.62 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.31 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.60 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.20 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.60 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.78 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.74 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.44 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.54 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.56 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.52 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.22 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.34 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.57 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.41 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.56 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.57 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.56 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.42 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.57 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.60 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.85 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.93 ms ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** During training for #   3 batch, reading batch file costed 2.58 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.20 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.58 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.21 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.63 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.58 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.37 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.40 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.77 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.80 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.44 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.40 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.35 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.21 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.57 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.60 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.21 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.77 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.58 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.40 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.62 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.20 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.54 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.73 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.40 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.19 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.20 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.82 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.61 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.31 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.21 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.60 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.21 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.78 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.71 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.57 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.22 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.22 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.68 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.21 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.61 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.62 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.76 ms ***\n",
      "Training costs a total of 4.4711 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0008 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   2 batch, reading batch file costed 2.37 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.66 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.21 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.36 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.58 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.56 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.39 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.52 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.40 ms ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** During training for #   1 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.41 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.61 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 3.00 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.44 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.41 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.52 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.02 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.40 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.44 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.18 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.38 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.37 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.54 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.22 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.58 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.37 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.59 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.34 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.70 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.44 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.35 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.56 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.58 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.37 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.52 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.52 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.31 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.40 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.20 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.74 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.62 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.80 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.80 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.90 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.18 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.22 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.80 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.76 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.19 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.38 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.19 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.40 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.94 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.84 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.22 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.19 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.63 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.56 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.21 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.54 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.61 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.33 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.63 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.66 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.32 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.74 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.86 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.82 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.57 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.83 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.32 ms ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** During training for #   0 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.21 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.20 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.18 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.77 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.52 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.36 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.96 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.37 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.35 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.19 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.57 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.69 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.54 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.20 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.22 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.33 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.52 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.67 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.38 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.01 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.74 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.41 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.42 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.22 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.04 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.44 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.22 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.20 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.78 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.42 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.52 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.39 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.65 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.44 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.93 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.52 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.54 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.83 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.20 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.54 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.32 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.20 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.21 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.67 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.56 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.42 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.81 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.40 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.52 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 3.05 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.01 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.83 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.02 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.21 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.40 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.54 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.22 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.20 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.34 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.39 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.71 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.57 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.33 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.73 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.54 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.28 ms ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** During training for #   1 batch, reading batch file costed 2.93 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.67 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.20 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.71 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.36 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.61 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.58 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.54 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.60 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.87 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.76 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.42 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.58 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.33 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.36 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.84 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.36 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.59 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.54 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.34 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.33 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.42 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.54 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.74 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.60 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.87 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.58 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.56 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.22 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.40 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.34 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.44 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.86 ms ***\n",
      "Training costs a total of 5.0955 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0007 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   1 batch, reading batch file costed 2.59 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.31 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.31 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.36 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.57 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.59 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.20 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.41 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.63 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.62 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.35 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.59 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.41 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.66 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.52 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.36 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.38 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.72 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.95 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.52 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.89 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.11 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.72 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.32 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.59 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.69 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.92 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.77 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.33 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.17 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.39 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.78 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.54 ms ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** During training for #   1 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.84 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.79 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.64 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.72 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.60 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.38 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.20 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.42 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.64 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.20 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.41 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.32 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.52 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.85 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.36 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.64 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.32 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.56 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.60 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.18 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.42 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.39 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.19 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.35 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.69 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.39 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.54 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.59 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.18 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.35 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.52 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.39 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.19 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.60 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.74 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.52 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.20 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.20 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.62 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.37 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.42 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.86 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.97 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.65 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.79 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.44 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.62 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.20 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.75 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.20 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.40 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.56 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.73 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.89 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.91 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.60 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.85 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.73 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.31 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.98 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.32 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.56 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.71 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.37 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.52 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.83 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.48 ms ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** During training for #   3 batch, reading batch file costed 2.94 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.56 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.44 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.98 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.58 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.78 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.70 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.60 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.65 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.39 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.34 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.44 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.40 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.21 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.59 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.94 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.67 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.72 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.52 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.57 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.19 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.94 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.13 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.21 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.32 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.69 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.01 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.68 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.52 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.39 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.41 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.42 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.32 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.33 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.33 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.65 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.39 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.65 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.57 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.31 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.91 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.66 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.38 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.97 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.87 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.67 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.68 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.58 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.21 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.42 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.18 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.22 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.59 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.71 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.54 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.73 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.82 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.37 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.22 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.21 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.67 ms ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** During training for #   1 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.21 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.38 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.22 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.32 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.56 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.94 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.57 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.19 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.22 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 3.06 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.38 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.18 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.67 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.64 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.57 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.87 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.56 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.31 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.31 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.50 ms ***\n",
      "Training costs a total of 5.1101 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0005 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   0 batch, reading batch file costed 2.57 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.31 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.38 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.34 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.59 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.44 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.41 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.42 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.59 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.63 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.60 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.75 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.00 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.62 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.41 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.72 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.36 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.69 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.61 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.21 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.21 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.33 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.35 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.64 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.33 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.40 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.74 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.38 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.40 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.65 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.73 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.39 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.33 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.18 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.39 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.19 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.85 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.35 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.77 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.81 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.39 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.40 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.21 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.18 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.84 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.25 ms ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** During training for #   2 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.88 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.33 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.72 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.68 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.71 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.92 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.57 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.73 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.34 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.32 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.63 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.90 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.34 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.65 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.59 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.38 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.65 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.36 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.42 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.06 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.63 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.39 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.64 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.33 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.69 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.40 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.63 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.31 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.84 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.21 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.67 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.59 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.22 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.38 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.22 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.52 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.22 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.52 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.52 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.19 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.44 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.58 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.67 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.60 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.70 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.52 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.41 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.37 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.21 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.19 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.57 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.59 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.71 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 3.33 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.32 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.17 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.85 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.12 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.10 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.21 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.21 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.49 ms ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** During training for #   3 batch, reading batch file costed 3.01 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.33 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.31 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.60 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.35 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.56 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.31 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.57 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.61 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.68 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.41 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.64 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.17 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.17 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.78 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.57 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.62 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.32 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.44 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.57 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.34 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.44 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.85 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.44 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.54 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.54 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.41 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.22 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.40 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.32 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.61 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.34 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.85 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.21 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.71 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.76 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.32 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.22 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.60 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.67 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.22 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.61 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.22 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.42 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.42 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.21 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.20 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.19 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.56 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.42 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.20 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.44 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.39 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.56 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.44 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.44 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.39 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.54 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.36 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.80 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.65 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.97 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.82 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.59 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.62 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.17 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.19 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.36 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.19 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.32 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.47 ms ***\n",
      "Training costs a total of 5.0633 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0005 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   3 batch, reading batch file costed 2.56 ms ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** During training for #   0 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.22 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.42 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.33 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.66 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.67 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.68 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.86 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.22 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.80 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.41 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.41 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.22 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.87 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.54 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.62 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 3.22 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.35 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.20 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.56 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.66 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.52 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.34 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.22 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.61 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.37 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.65 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.31 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.19 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.37 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 3.04 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.92 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.34 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.66 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.18 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.40 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.98 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.57 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.76 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.20 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.19 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.70 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.41 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.57 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.18 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.58 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.37 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.66 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.40 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.61 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.42 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.84 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.71 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.44 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.32 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.57 ms ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** During training for #   2 batch, reading batch file costed 2.19 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.44 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.39 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.21 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.66 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.41 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.31 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.87 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.64 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.22 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.40 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.17 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.22 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.89 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.58 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.31 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.82 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.64 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.61 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.66 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.91 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.93 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.52 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.17 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.67 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.32 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.21 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.73 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.44 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.21 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.20 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.90 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.76 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.71 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.89 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.60 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.56 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.20 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.20 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.40 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.65 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.07 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.19 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.19 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.40 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.71 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.58 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.52 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.44 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.31 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.52 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.19 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.58 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.38 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.32 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.74 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.21 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.33 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.36 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.21 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.58 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.69 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.00 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.57 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.26 ms ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** During training for #   2 batch, reading batch file costed 2.20 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.58 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.65 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.37 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.42 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.44 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.58 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.22 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.59 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.22 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.88 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.68 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.71 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.98 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.39 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.34 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.58 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.21 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.20 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.63 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.52 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.44 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.87 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 3.02 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.35 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.71 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.41 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.41 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.19 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.57 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.60 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.90 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.18 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.72 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.40 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.35 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.62 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.36 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.50 ms ***\n",
      "Training costs a total of 5.0697 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0005 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   2 batch, reading batch file costed 2.63 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.22 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.42 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.19 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.64 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.58 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.35 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.22 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.84 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.41 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.52 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.89 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.35 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.52 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.63 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.37 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.38 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.22 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.32 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.42 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.38 ms ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** During training for #   3 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.18 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.41 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.22 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.59 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.65 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.96 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.08 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.34 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.39 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.52 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.57 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.35 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.64 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.64 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.57 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.82 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.58 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.80 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.52 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.44 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.40 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.56 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.40 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.84 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.22 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.58 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.60 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.20 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.44 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.57 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.67 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.67 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.33 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.56 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.42 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.13 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.52 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.69 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.65 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.96 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.77 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.44 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.52 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.33 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.58 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.54 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.60 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.38 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.93 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.35 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.56 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.60 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.75 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.31 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.59 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.41 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.11 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.76 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.75 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.33 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.61 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.54 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.76 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.16 ms ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** During training for #   1 batch, reading batch file costed 2.22 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.20 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.41 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.35 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.54 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.16 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.42 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.93 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.80 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.83 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.56 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.71 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.32 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.54 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.70 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.39 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.71 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.96 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.41 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.21 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.22 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.57 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.44 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.20 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.54 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.63 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.22 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.22 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.85 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.68 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.40 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.71 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.67 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.75 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.71 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.86 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.18 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.41 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.62 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.59 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.31 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 3.03 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.02 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.60 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.73 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.32 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.33 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.22 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.68 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.90 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.35 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.59 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.54 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.76 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.74 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.87 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.35 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.05 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.65 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.31 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.31 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.54 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.56 ms ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** During training for #   2 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.37 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.63 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.68 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.32 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.74 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.31 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.83 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.59 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.61 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.84 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.94 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.95 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.68 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.52 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.21 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.20 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.22 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.20 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.52 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.58 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.43 ms ***\n",
      "Training costs a total of 5.0876 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0005 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   3 batch, reading batch file costed 2.62 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.19 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.80 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.64 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.01 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.63 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.57 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.21 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.65 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.41 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.65 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.18 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.60 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.20 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.22 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.44 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.18 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.67 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.20 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.54 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.41 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.67 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.80 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.20 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.44 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.58 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.44 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.22 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.85 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.52 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.39 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.21 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.64 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.72 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.84 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.40 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.39 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.41 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.54 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.22 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.44 ms ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** During training for #   1 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.23 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.22 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.18 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.60 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.39 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.37 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.54 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.40 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.37 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.32 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.36 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.57 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.22 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.31 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.66 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.20 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.65 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.40 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.82 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 3.07 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.16 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.40 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.99 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.19 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.19 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.89 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.38 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.20 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.21 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.20 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.67 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.80 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.44 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.66 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.74 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.64 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.75 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.32 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.14 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.42 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.58 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.40 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.61 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.54 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.60 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.62 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.34 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.32 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.64 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.75 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.86 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.58 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.44 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.61 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.33 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.60 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.87 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.20 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.34 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.86 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 3.14 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.72 ms ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** During training for #   2 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.57 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.70 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.52 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.59 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.31 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 3.10 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.42 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.32 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.38 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.80 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.78 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.42 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.69 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.39 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.17 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.52 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.56 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.41 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.21 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.52 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.20 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.38 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.52 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.90 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.79 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.15 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.89 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.42 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.19 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.22 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.22 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.87 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.41 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.52 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.33 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.19 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.56 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.35 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.73 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.22 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.42 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.31 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.44 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.82 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.76 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.59 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.31 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.59 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.31 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.65 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.70 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.78 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.57 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.75 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.94 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.22 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.38 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.59 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.33 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.32 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.80 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.56 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.22 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.85 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.99 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.57 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.66 ms ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** During training for #   1 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.44 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.35 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.42 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.44 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.19 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 3.01 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.60 ms ***\n",
      "Training costs a total of 5.0772 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0007 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   1 batch, reading batch file costed 2.36 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.01 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.83 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.22 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.21 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.21 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.60 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.36 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.86 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.15 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.64 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.34 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.97 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.33 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.54 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.63 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.82 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.36 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.64 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 3.07 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.15 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.64 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.33 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.32 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.32 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.59 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.19 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.81 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.21 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.64 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.59 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.72 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.95 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.76 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.52 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.40 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.57 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.88 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.57 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.33 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.39 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.56 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.32 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.58 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.33 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.21 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.85 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.79 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.90 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.58 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.68 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.37 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.20 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.38 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.42 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.34 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.24 ms ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** During training for #   3 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.78 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.21 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.76 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.88 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.20 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.41 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.18 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.41 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.19 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.21 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.41 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.39 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.62 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.02 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.80 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.59 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.39 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.19 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.38 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.39 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.17 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.61 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.58 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.37 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.20 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.18 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.44 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.44 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.41 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.62 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.33 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.21 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.59 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.54 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.39 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.72 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.52 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.39 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.65 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.32 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.74 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.33 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.33 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.44 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.91 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.54 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.57 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.65 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.57 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.76 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.66 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.57 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.33 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.54 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.52 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.58 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.83 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.36 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.52 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.19 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.54 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.40 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.41 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.37 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 3.02 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.40 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.22 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.41 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.44 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.65 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.25 ms ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** During training for #   3 batch, reading batch file costed 2.66 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.65 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.63 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.78 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.58 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.85 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.65 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.54 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.67 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.42 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.41 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.94 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.42 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.37 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.61 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.68 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.63 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.18 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.58 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.33 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.41 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.18 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.13 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.09 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.71 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.42 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.14 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.48 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.50 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.55 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.44 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.66 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.71 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.41 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.59 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.56 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.19 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.30 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.19 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.36 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.31 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.47 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.77 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.59 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.59 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 3.12 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.84 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.54 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.29 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.54 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.53 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.25 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.20 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.54 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.54 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.60 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.59 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.51 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.56 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.56 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.27 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.26 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.23 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.43 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.28 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.69 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 3.04 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.07 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.40 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.45 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.24 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.46 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.20 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.39 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 2.49 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2.19 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.31 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.50 ms ***\n",
      "Training costs a total of 5.0734 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "Start to read the GCN trainer model (parameters: weights, bias):\n",
      "Reading the trainer costs a total of 1.0445 seconds!\n",
      "Start validate the model:\n",
      "*** During validation for # whole graph batch, reading batch file costed 4.67 ms ***\n",
      "Validatoin costs a total of 0.0101 seconds!\n",
      "====================================================================================================\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "Start to read the GCN trainer model (parameters: weights, bias):\n",
      "Reading the trainer costs a total of 1.0498 seconds!\n",
      "Start validate the model:\n",
      "*** During validation for # whole graph batch, reading batch file costed 5.24 ms ***\n",
      "Validatoin costs a total of 0.0106 seconds!\n",
      "====================================================================================================\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "Start to read the GCN trainer model (parameters: weights, bias):\n",
      "Reading the trainer costs a total of 1.1575 seconds!\n",
      "Start validate the model:\n",
      "*** During validation for # whole graph batch, reading batch file costed 5.12 ms ***\n",
      "Validatoin costs a total of 0.0103 seconds!\n",
      "====================================================================================================\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "Start to read the GCN trainer model (parameters: weights, bias):\n",
      "Reading the trainer costs a total of 1.2953 seconds!\n",
      "Start validate the model:\n",
      "*** During validation for # whole graph batch, reading batch file costed 4.86 ms ***\n",
      "Validatoin costs a total of 0.0099 seconds!\n",
      "====================================================================================================\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "Start to read the GCN trainer model (parameters: weights, bias):\n",
      "Reading the trainer costs a total of 1.1175 seconds!\n",
      "Start validate the model:\n",
      "*** During validation for # whole graph batch, reading batch file costed 4.78 ms ***\n",
      "Validatoin costs a total of 0.0098 seconds!\n",
      "====================================================================================================\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "Start to read the GCN trainer model (parameters: weights, bias):\n",
      "Reading the trainer costs a total of 1.1184 seconds!\n",
      "Start validate the model:\n",
      "*** During validation for # whole graph batch, reading batch file costed 4.82 ms ***\n",
      "Validatoin costs a total of 0.0099 seconds!\n",
      "====================================================================================================\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "Start to read the GCN trainer model (parameters: weights, bias):\n",
      "Reading the trainer costs a total of 1.1632 seconds!\n",
      "Start validate the model:\n",
      "*** During validation for # whole graph batch, reading batch file costed 4.85 ms ***\n",
      "Validatoin costs a total of 0.0100 seconds!\n",
      "====================================================================================================\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "Start to read the GCN trainer model (parameters: weights, bias):\n",
      "Reading the trainer costs a total of 1.1287 seconds!\n",
      "Start validate the model:\n",
      "*** During validation for # whole graph batch, reading batch file costed 4.71 ms ***\n",
      "Validatoin costs a total of 0.0103 seconds!\n",
      "====================================================================================================\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "Start to read the GCN trainer model (parameters: weights, bias):\n",
      "Reading the trainer costs a total of 1.1277 seconds!\n",
      "Start validate the model:\n",
      "*** During validation for # whole graph batch, reading batch file costed 4.72 ms ***\n",
      "Validatoin costs a total of 0.0097 seconds!\n",
      "====================================================================================================\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "Start to read the GCN trainer model (parameters: weights, bias):\n",
      "Reading the trainer costs a total of 1.1206 seconds!\n",
      "Start validate the model:\n",
      "*** During validation for # whole graph batch, reading batch file costed 4.83 ms ***\n",
      "Validatoin costs a total of 0.0099 seconds!\n",
      "====================================================================================================\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "Start to read the GCN trainer model (parameters: weights, bias):\n",
      "Reading the trainer costs a total of 1.1334 seconds!\n",
      "Start validate the model:\n",
      "*** During validation for # whole graph batch, reading batch file costed 4.74 ms ***\n",
      "Validatoin costs a total of 0.0204 seconds!\n",
      "====================================================================================================\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "Start to read the GCN trainer model (parameters: weights, bias):\n",
      "Reading the trainer costs a total of 1.1187 seconds!\n",
      "Start validate the model:\n",
      "*** During validation for # whole graph batch, reading batch file costed 4.97 ms ***\n",
      "Validatoin costs a total of 0.0100 seconds!\n",
      "====================================================================================================\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "Start to read the GCN trainer model (parameters: weights, bias):\n",
      "Reading the trainer costs a total of 1.1108 seconds!\n",
      "Start validate the model:\n",
      "*** During validation for # whole graph batch, reading batch file costed 4.73 ms ***\n",
      "Validatoin costs a total of 0.0106 seconds!\n",
      "====================================================================================================\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "Start to read the GCN trainer model (parameters: weights, bias):\n",
      "Reading the trainer costs a total of 1.1194 seconds!\n",
      "Start validate the model:\n",
      "*** During validation for # whole graph batch, reading batch file costed 4.73 ms ***\n",
      "Validatoin costs a total of 0.0098 seconds!\n",
      "====================================================================================================\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "Start to read the GCN trainer model (parameters: weights, bias):\n",
      "Reading the trainer costs a total of 1.1501 seconds!\n",
      "Start validate the model:\n",
      "*** During validation for # whole graph batch, reading batch file costed 4.72 ms ***\n",
      "Validatoin costs a total of 0.0098 seconds!\n",
      "====================================================================================================\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "Start to read the GCN trainer model (parameters: weights, bias):\n",
      "Reading the trainer costs a total of 1.1315 seconds!\n",
      "Start validate the model:\n",
      "*** During validation for # whole graph batch, reading batch file costed 4.78 ms ***\n",
      "Validatoin costs a total of 0.0099 seconds!\n",
      "====================================================================================================\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "Start to read the GCN trainer model (parameters: weights, bias):\n",
      "Reading the trainer costs a total of 1.5948 seconds!\n",
      "Start validate the model:\n",
      "*** During validation for # whole graph batch, reading batch file costed 4.73 ms ***\n",
      "Validatoin costs a total of 0.0099 seconds!\n",
      "====================================================================================================\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "Start to read the GCN trainer model (parameters: weights, bias):\n",
      "Reading the trainer costs a total of 1.1239 seconds!\n",
      "Start validate the model:\n",
      "*** During validation for # whole graph batch, reading batch file costed 5.16 ms ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validatoin costs a total of 0.0105 seconds!\n",
      "====================================================================================================\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "Start to read the GCN trainer model (parameters: weights, bias):\n",
      "Reading the trainer costs a total of 1.2126 seconds!\n",
      "Start validate the model:\n",
      "*** During validation for # whole graph batch, reading batch file costed 5.46 ms ***\n",
      "Validatoin costs a total of 0.0125 seconds!\n",
      "====================================================================================================\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "Start to read the GCN trainer model (parameters: weights, bias):\n",
      "Reading the trainer costs a total of 1.1220 seconds!\n",
      "Start validate the model:\n",
      "*** During validation for # whole graph batch, reading batch file costed 5.14 ms ***\n",
      "Validatoin costs a total of 0.0105 seconds!\n",
      "====================================================================================================\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "Start to read the GCN trainer model (parameters: weights, bias):\n",
      "Reading the trainer costs a total of 1.1182 seconds!\n",
      "Start validate the model:\n",
      "*** During validation for # whole graph batch, reading batch file costed 4.72 ms ***\n",
      "Validatoin costs a total of 0.0097 seconds!\n",
      "====================================================================================================\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "Start to read the GCN trainer model (parameters: weights, bias):\n",
      "Reading the trainer costs a total of 1.1265 seconds!\n",
      "Start validate the model:\n",
      "*** During validation for # whole graph batch, reading batch file costed 5.18 ms ***\n",
      "Validatoin costs a total of 0.0103 seconds!\n",
      "====================================================================================================\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "Start to read the GCN trainer model (parameters: weights, bias):\n",
      "Reading the trainer costs a total of 1.1184 seconds!\n",
      "Start validate the model:\n",
      "*** During validation for # whole graph batch, reading batch file costed 4.86 ms ***\n",
      "Validatoin costs a total of 0.0106 seconds!\n",
      "====================================================================================================\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "Start to read the GCN trainer model (parameters: weights, bias):\n",
      "Reading the trainer costs a total of 1.1177 seconds!\n",
      "Start validate the model:\n",
      "*** During validation for # whole graph batch, reading batch file costed 4.76 ms ***\n",
      "Validatoin costs a total of 0.0103 seconds!\n",
      "====================================================================================================\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "Start to read the GCN trainer model (parameters: weights, bias):\n",
      "Reading the trainer costs a total of 1.1320 seconds!\n",
      "Start validate the model:\n",
      "*** During validation for # whole graph batch, reading batch file costed 4.71 ms ***\n",
      "Validatoin costs a total of 0.0097 seconds!\n",
      "====================================================================================================\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "Start to read the GCN trainer model (parameters: weights, bias):\n",
      "Reading the trainer costs a total of 1.1275 seconds!\n",
      "Start validate the model:\n",
      "*** During validation for # whole graph batch, reading batch file costed 4.92 ms ***\n",
      "Validatoin costs a total of 0.0100 seconds!\n",
      "====================================================================================================\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "Start to read the GCN trainer model (parameters: weights, bias):\n",
      "Reading the trainer costs a total of 1.6675 seconds!\n",
      "Start validate the model:\n",
      "*** During validation for # whole graph batch, reading batch file costed 4.71 ms ***\n",
      "Validatoin costs a total of 0.0099 seconds!\n",
      "====================================================================================================\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "Start to read the GCN trainer model (parameters: weights, bias):\n",
      "Reading the trainer costs a total of 1.1177 seconds!\n",
      "Start validate the model:\n",
      "*** During validation for # whole graph batch, reading batch file costed 5.12 ms ***\n",
      "Validatoin costs a total of 0.0102 seconds!\n",
      "====================================================================================================\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "Start to read the GCN trainer model (parameters: weights, bias):\n",
      "Reading the trainer costs a total of 1.2248 seconds!\n",
      "Start validate the model:\n",
      "*** During validation for # whole graph batch, reading batch file costed 5.22 ms ***\n",
      "Validatoin costs a total of 0.0103 seconds!\n",
      "====================================================================================================\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "Start to read the GCN trainer model (parameters: weights, bias):\n",
      "Reading the trainer costs a total of 1.1170 seconds!\n",
      "Start validate the model:\n",
      "*** During validation for # whole graph batch, reading batch file costed 5.31 ms ***\n",
      "Validatoin costs a total of 0.0104 seconds!\n",
      "====================================================================================================\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "Start to read the GCN trainer model (parameters: weights, bias):\n",
      "Reading the trainer costs a total of 1.1163 seconds!\n",
      "Start validate the model:\n",
      "*** During validation for # whole graph batch, reading batch file costed 4.75 ms ***\n",
      "Validatoin costs a total of 0.0105 seconds!\n",
      "====================================================================================================\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "Start to read the GCN trainer model (parameters: weights, bias):\n",
      "Reading the trainer costs a total of 1.1239 seconds!\n",
      "Start validate the model:\n",
      "*** During validation for # whole graph batch, reading batch file costed 4.71 ms ***\n",
      "Validatoin costs a total of 0.0098 seconds!\n",
      "====================================================================================================\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "Start to read the GCN trainer model (parameters: weights, bias):\n",
      "Reading the trainer costs a total of 1.1060 seconds!\n",
      "Start validate the model:\n",
      "*** During validation for # whole graph batch, reading batch file costed 4.90 ms ***\n",
      "Validatoin costs a total of 0.0108 seconds!\n",
      "====================================================================================================\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "Start to read the GCN trainer model (parameters: weights, bias):\n",
      "Reading the trainer costs a total of 1.1241 seconds!\n",
      "Start validate the model:\n",
      "*** During validation for # whole graph batch, reading batch file costed 4.72 ms ***\n",
      "Validatoin costs a total of 0.0104 seconds!\n",
      "====================================================================================================\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "Start to read the GCN trainer model (parameters: weights, bias):\n",
      "Reading the trainer costs a total of 1.1308 seconds!\n",
      "Start validate the model:\n",
      "*** During validation for # whole graph batch, reading batch file costed 4.71 ms ***\n",
      "Validatoin costs a total of 0.0117 seconds!\n",
      "====================================================================================================\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "Start to read the GCN trainer model (parameters: weights, bias):\n",
      "Reading the trainer costs a total of 1.1671 seconds!\n",
      "Start validate the model:\n",
      "*** During validation for # whole graph batch, reading batch file costed 4.89 ms ***\n",
      "Validatoin costs a total of 0.0100 seconds!\n",
      "====================================================================================================\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "Start to read the GCN trainer model (parameters: weights, bias):\n",
      "Reading the trainer costs a total of 1.1206 seconds!\n",
      "Start validate the model:\n",
      "*** During validation for # whole graph batch, reading batch file costed 5.37 ms ***\n",
      "Validatoin costs a total of 0.0104 seconds!\n",
      "====================================================================================================\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "Start to read the GCN trainer model (parameters: weights, bias):\n",
      "Reading the trainer costs a total of 1.1148 seconds!\n",
      "Start validate the model:\n",
      "*** During validation for # whole graph batch, reading batch file costed 4.74 ms ***\n",
      "Validatoin costs a total of 0.0099 seconds!\n",
      "====================================================================================================\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "Start to read the GCN trainer model (parameters: weights, bias):\n",
      "Reading the trainer costs a total of 1.1230 seconds!\n",
      "Start validate the model:\n",
      "*** During validation for # whole graph batch, reading batch file costed 5.17 ms ***\n",
      "Validatoin costs a total of 0.0102 seconds!\n",
      "====================================================================================================\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "Start to read the GCN trainer model (parameters: weights, bias):\n",
      "Reading the trainer costs a total of 1.2703 seconds!\n",
      "Start validate the model:\n",
      "*** During validation for # whole graph batch, reading batch file costed 5.21 ms ***\n",
      "Validatoin costs a total of 0.0103 seconds!\n",
      "====================================================================================================\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "Start to read the GCN trainer model (parameters: weights, bias):\n",
      "Reading the trainer costs a total of 1.1241 seconds!\n",
      "Start validate the model:\n",
      "*** During validation for # whole graph batch, reading batch file costed 4.90 ms ***\n",
      "Validatoin costs a total of 0.0104 seconds!\n",
      "====================================================================================================\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "Start to read the GCN trainer model (parameters: weights, bias):\n",
      "Reading the trainer costs a total of 1.1232 seconds!\n",
      "Start validate the model:\n",
      "*** During validation for # whole graph batch, reading batch file costed 4.74 ms ***\n",
      "Validatoin costs a total of 0.0103 seconds!\n",
      "====================================================================================================\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "Start to read the GCN trainer model (parameters: weights, bias):\n",
      "Reading the trainer costs a total of 1.1258 seconds!\n",
      "Start validate the model:\n",
      "*** During validation for # whole graph batch, reading batch file costed 4.72 ms ***\n",
      "Validatoin costs a total of 0.0098 seconds!\n",
      "====================================================================================================\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "Start to read the GCN trainer model (parameters: weights, bias):\n",
      "Reading the trainer costs a total of 1.1170 seconds!\n",
      "Start validate the model:\n",
      "*** During validation for # whole graph batch, reading batch file costed 4.91 ms ***\n",
      "Validatoin costs a total of 0.0100 seconds!\n",
      "====================================================================================================\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "Start to read the GCN trainer model (parameters: weights, bias):\n",
      "Reading the trainer costs a total of 1.7765 seconds!\n",
      "Start validate the model:\n",
      "*** During validation for # whole graph batch, reading batch file costed 4.74 ms ***\n",
      "Validatoin costs a total of 0.0099 seconds!\n",
      "====================================================================================================\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "Start to read the GCN trainer model (parameters: weights, bias):\n",
      "Reading the trainer costs a total of 1.1144 seconds!\n",
      "Start validate the model:\n",
      "*** During validation for # whole graph batch, reading batch file costed 5.13 ms ***\n",
      "Validatoin costs a total of 0.0102 seconds!\n",
      "====================================================================================================\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "Start to read the GCN trainer model (parameters: weights, bias):\n",
      "Reading the trainer costs a total of 1.1137 seconds!\n",
      "Start validate the model:\n",
      "*** During validation for # whole graph batch, reading batch file costed 5.19 ms ***\n",
      "Validatoin costs a total of 0.0104 seconds!\n",
      "====================================================================================================\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "Start to read the GCN trainer model (parameters: weights, bias):\n",
      "Reading the trainer costs a total of 1.1170 seconds!\n",
      "Start validate the model:\n",
      "*** During validation for # whole graph batch, reading batch file costed 4.71 ms ***\n",
      "Validatoin costs a total of 0.0104 seconds!\n",
      "====================================================================================================\n",
      "Start running training for partition num: 4 hop layer 2\n",
      "Start to read the GCN trainer model (parameters: weights, bias):\n",
      "Reading the trainer costs a total of 1.1899 seconds!\n",
      "Start validate the model:\n",
      "*** During validation for # whole graph batch, reading batch file costed 4.76 ms ***\n",
      "Validatoin costs a total of 0.0100 seconds!\n",
      "====================================================================================================\n",
      "Start running training for partition num: 4 hop layer 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "step0_generate_clustering_machine(data, dataset, intermediate_data_folder, train_batch_num, hop_layer_num)\n",
    "\n",
    "step1_generate_train_batch(intermediate_data_folder, hop_layer_num, train_frac = 1.0, \\\n",
    "                           batch_range = (0, train_batch_num), info_folder = 'info_train_batch/' )\n",
    "\n",
    "step2_generate_validation_whole_graph(intermediate_data_folder, info_folder = 'info_validation_whole/')\n",
    "\n",
    "for tune_val in tune_val_list:\n",
    "    for tainer_id in trainer_list:\n",
    "        step30_run_tune_train_batch(intermediate_data_folder, tune_param_name, tune_val, train_batch_num, hop_layer_num, GCN_layer, \\\n",
    "                            trainer_id = tainer_id, dropout = 0.1, lr = 0.0001, weight_decay = 0.1, mini_epoch_num = 20, epoch_num = 400)\n",
    "        \n",
    "for tune_val in tune_val_list:\n",
    "    for tainer_id in trainer_list:\n",
    "        step40_run_tune_validation_whole(image_data_path, intermediate_data_folder, tune_param_name, tune_val, train_batch_num, hop_layer_num, net_layer_num, \\\n",
    "                            trainer_id = tainer_id)\n",
    "\n",
    "step50_run_tune_summarize_whole(data_name, image_data_path, intermediate_data_folder, tune_param_name, tune_val_list, \\\n",
    "                                train_batch_num, hop_layer_num, net_layer_num, trainer_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PubMed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "data_name = 'PubMed'\n",
    "dataset = Planetoid(root = local_data_root + 'Planetoid/PubMed', name=data_name)\n",
    "data = dataset[0]\n",
    "image_data_path = './results/' + data_name + '/' + test_folder_name\n",
    "# set the current folder as the intermediate data folder so that we can easily copy either clustering \n",
    "intermediate_data_folder = './'\n",
    "partition_nums = [2, 4, 8]\n",
    "layers = [[], [64], [64, 64]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CoraFull dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import CoraFull\n",
    "data_name = 'CoraFull'\n",
    "dataset = CoraFull(root = local_data_root + 'CoralFull')\n",
    "print('number of data: ', len(dataset))\n",
    "data = dataset[0]\n",
    "\n",
    "image_data_path = './results/' + data_name + '/' + test_folder_name\n",
    "# set the current folder as the intermediate data folder so that we can easily copy either clustering \n",
    "intermediate_data_folder = './'\n",
    "partition_nums = [2, 4, 8]\n",
    "layers = [[], [128], [128, 128]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check F1-score\n",
    "output_F1_score(data, data_name, dataset, image_data_path, intermediate_data_folder, partition_nums, layers, \\\n",
    "                train_frac = 0.5, validation_frac = 0.5, valid_part_num = 2, \\\n",
    "                dropout = 0.1, lr = 0.0001, weight_decay = 0.1, mini_epoch_num = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuning the mini_epoch\n",
    "output_tune_param(data, data_name, dataset, image_data_path, intermediate_data_folder, partition_nums, layers, \\\n",
    "                  train_frac = 1.0, validation_frac = 1.0, valid_part_num = 2, \\\n",
    "                  dropout = 0.3, lr = 0.0001, weight_decay = 0.001, mini_epoch_num = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check convergence\n",
    "output_train_loss(data, data_name, dataset, image_data_path, intermediate_data_folder, partition_nums, layers, \\\n",
    "                  train_frac = 1.0, validation_frac = 1.0, valid_part_num = 2, \\\n",
    "                  dropout = 0.3, lr = 0.0001, weight_decay = 0.001, mini_epoch_num = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check in_train performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_train_investigate(data, data_name, dataset, image_data_path, intermediate_data_folder, partition_nums, layers, \\\n",
    "                         train_frac = 1.0, validation_frac = 1.0, valid_part_num = 2, \\\n",
    "                         dropout = 0.3, lr = 0.0001, weight_decay = 0.001, mini_epoch_num = 20, output_period = 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CiteSeer Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "data_name = 'CiteSeer'\n",
    "dataset = Planetoid(root = local_data_root + 'Planetoid/CiteSeer', name=data_name)\n",
    "data = dataset[0]\n",
    "image_data_path = './results/' + data_name + '/' + test_folder_name\n",
    "\n",
    "partition_nums = [2, 4, 8]\n",
    "layers = [[], [16], [16, 16]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free GPU memory\n",
    "# !(nvidia-smi | grep 'python' | awk '{ print $3 }' | xargs -n1 kill -9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_geometric]",
   "language": "python",
   "name": "conda-env-pytorch_geometric-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
