{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrate model inter-cluster with three clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import csv\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pickle\n",
    "import shutil\n",
    "\n",
    "from utils import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import torch\n",
    "from torch_geometric.utils import scatter_\n",
    "\n",
    "special_args = [\n",
    "    'edge_index', 'edge_index_i', 'edge_index_j', 'size', 'size_i', 'size_j'\n",
    "]\n",
    "__size_error_msg__ = ('All tensors which should get mapped to the same source '\n",
    "                      'or target nodes must be of same size in dimension 0.')\n",
    "\n",
    "is_python2 = sys.version_info[0] < 3\n",
    "getargspec = inspect.getargspec if is_python2 else inspect.getfullargspec\n",
    "\n",
    "\n",
    "class MessagePassing(torch.nn.Module):\n",
    "    r\"\"\"Base class for creating message passing layers\n",
    "\n",
    "    .. math::\n",
    "        \\mathbf{x}_i^{\\prime} = \\gamma_{\\mathbf{\\Theta}} \\left( \\mathbf{x}_i,\n",
    "        \\square_{j \\in \\mathcal{N}(i)} \\, \\phi_{\\mathbf{\\Theta}}\n",
    "        \\left(\\mathbf{x}_i, \\mathbf{x}_j,\\mathbf{e}_{i,j}\\right) \\right),\n",
    "\n",
    "    where :math:`\\square` denotes a differentiable, permutation invariant\n",
    "    function, *e.g.*, sum, mean or max, and :math:`\\gamma_{\\mathbf{\\Theta}}`\n",
    "    and :math:`\\phi_{\\mathbf{\\Theta}}` denote differentiable functions such as\n",
    "    MLPs.\n",
    "    See `here <https://pytorch-geometric.readthedocs.io/en/latest/notes/\n",
    "    create_gnn.html>`__ for the accompanying tutorial.\n",
    "\n",
    "    Args:\n",
    "        aggr (string, optional): The aggregation scheme to use\n",
    "            (:obj:`\"add\"`, :obj:`\"mean\"` or :obj:`\"max\"`).\n",
    "            (default: :obj:`\"add\"`)\n",
    "        flow (string, optional): The flow direction of message passing\n",
    "            (:obj:`\"source_to_target\"` or :obj:`\"target_to_source\"`).\n",
    "            (default: :obj:`\"source_to_target\"`)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, aggr='add', flow='source_to_target'):\n",
    "        super(MessagePassing, self).__init__()\n",
    "\n",
    "        self.aggr = aggr\n",
    "        assert self.aggr in ['add', 'mean', 'max']\n",
    "\n",
    "        self.flow = flow\n",
    "        # give a warning if the option is not valid\n",
    "        assert self.flow in ['source_to_target', 'target_to_source']\n",
    "\n",
    "        self.__message_args__ = getargspec(self.message)[0][1:]\n",
    "        # we will have [x_j, norm ] put into self.__message_args__\n",
    "        \n",
    "        self.__special_args__ = [(i, arg)\n",
    "                                 for i, arg in enumerate(self.__message_args__)\n",
    "                                 if arg in special_args]\n",
    "        \n",
    "        self.__message_args__ = [arg for arg in self.__message_args__ if arg not in special_args]\n",
    "        \n",
    "        self.__update_args__ = getargspec(self.update)[0][2:]\n",
    "        # empty, since there is nothing beyond: agg_out\n",
    "\n",
    "#     function call: res = self.propagate(edge_index, x=x, norm=norm)\n",
    "    def propagate(self, edge_index, size=None, **kwargs):\n",
    "        r\"\"\"The initial call to start propagating messages.\n",
    "\n",
    "        Args:\n",
    "            edge_index (Tensor): The indices of a general (sparse) assignment\n",
    "                matrix with shape :obj:`[N, M]` (can be directed or\n",
    "                undirected).\n",
    "            size (list or tuple, optional): The size :obj:`[N, M]` of the\n",
    "                assignment matrix. If set to :obj:`None`, the size is tried to\n",
    "                get automatically inferred. (default: :obj:`None`)\n",
    "            **kwargs: Any additional data which is needed to construct messages\n",
    "                and to update node embeddings.\n",
    "        \"\"\"\n",
    "        dim = 0\n",
    "        size = [None, None] if size is None else list(size)\n",
    "        assert len(size) == 2\n",
    "\n",
    "        i, j = (0, 1) if self.flow == 'target_to_source' else (1, 0)\n",
    "        # here (i, j) == (1, 0)\n",
    "        ij = {\"_i\": i, \"_j\": j}\n",
    "\n",
    "        message_args = []\n",
    "        \n",
    "        for arg in self.__message_args__:\n",
    "#             arg[-2] == '_j'\n",
    "            if arg[-2:] in ij.keys():\n",
    "#                 tmp == x, is inside the dwargs\n",
    "                tmp = kwargs.get(arg[:-2], None)   # get the value of the parameter\n",
    "                if tmp is None:  # pragma: no cover\n",
    "                    message_args.append(tmp)\n",
    "                else:\n",
    "                    idx = ij[arg[-2:]]    # idx == 0\n",
    "                    if isinstance(tmp, tuple) or isinstance(tmp, list):\n",
    "                        assert len(tmp) == 2\n",
    "                        if tmp[1 - idx] is not None:\n",
    "                            if size[1 - idx] is None:\n",
    "                                size[1 - idx] = tmp[1 - idx].size(dim)\n",
    "                            if size[1 - idx] != tmp[1 - idx].size(dim):\n",
    "                                raise ValueError(__size_error_msg__)\n",
    "                        tmp = tmp[idx]\n",
    "                    \n",
    "                    if tmp is None:\n",
    "                        message_args.append(tmp)\n",
    "                    else:\n",
    "                        if size[idx] is None:\n",
    "                            size[idx] = tmp.size(dim)\n",
    "                        if size[idx] != tmp.size(dim):\n",
    "                            raise ValueError(__size_error_msg__)\n",
    "                        # dim == 0, we duplicate part of the embeddings x by using the edge_index[idx]\n",
    "#                         print('Inside the propagate, edge_index[idx]: \\n', edge_index[idx].shape, '\\n', edge_index[idx])\n",
    "                        tmp = torch.index_select(tmp, dim, edge_index[idx])\n",
    "                        message_args.append(tmp)   # here we append x from the kwargs\n",
    "            else:\n",
    "                message_args.append(kwargs.get(arg, None))   # here we append norm\n",
    "        \n",
    "#         message_args are: x_j, norm \n",
    "#         size:  [8, None] \n",
    "#         kwargs:  dict_keys(['x', 'norm']) \n",
    "#         special keys:  []\n",
    "        \n",
    "        size[0] = size[1] if size[0] is None else size[0]\n",
    "        size[1] = size[0] if size[1] is None else size[1]\n",
    "\n",
    "        kwargs['edge_index'] = edge_index\n",
    "        kwargs['size'] = size\n",
    "        \n",
    "        # for now self.__special_args__ is empty\n",
    "        for (idx, arg) in self.__special_args__:\n",
    "            if arg[-2:] in ij.keys():\n",
    "                # here we will change the content of x (features)\n",
    "                # features will be corresponds to edge_index\n",
    "                message_args.insert(idx, kwargs[arg[:-2]][ij[arg[-2:]]])\n",
    "            else:\n",
    "                message_args.insert(idx, kwargs[arg])\n",
    "\n",
    "        update_args = [kwargs[arg] for arg in self.__update_args__]\n",
    "#         message_args are: x_j, norm \n",
    "        out = self.message(*message_args)\n",
    "        # here i = 1, edge_index is the target endpoint of an edge, size[i] is the size of target endpoints\n",
    "        out = scatter_(self.aggr, out, edge_index[i], dim_size=size[i])\n",
    "        out = self.update(out, *update_args)\n",
    "        return out\n",
    "\n",
    "    def message(self, x_j):  # pragma: no cover\n",
    "        r\"\"\"Constructs messages in analogy to :math:`\\phi_{\\mathbf{\\Theta}}`\n",
    "        for each edge in :math:`(i,j) \\in \\mathcal{E}`.\n",
    "        Can take any argument which was initially passed to :meth:`propagate`.\n",
    "        In addition, features can be lifted to the source node :math:`i` and\n",
    "        target node :math:`j` by appending :obj:`_i` or :obj:`_j` to the\n",
    "        variable name, *.e.g.* :obj:`x_i` and :obj:`x_j`.\"\"\"\n",
    "\n",
    "        return x_j\n",
    "\n",
    "    def update(self, aggr_out):  # pragma: no cover\n",
    "        r\"\"\"Updates node embeddings in analogy to\n",
    "        :math:`\\gamma_{\\mathbf{\\Theta}}` for each node\n",
    "        :math:`i \\in \\mathcal{V}`.\n",
    "        Takes in the output of aggregation as first argument and any argument\n",
    "        which was initially passed to :meth:`propagate`.\"\"\"\n",
    "\n",
    "        return aggr_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "from torch.nn import Parameter\n",
    "from torch_scatter import scatter_add\n",
    "# from torch_geometric.nn import MessagePassing\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import add_remaining_self_loops\n",
    "\n",
    "### ================== Definition of custom GCN\n",
    "\n",
    "def glorot(tensor):\n",
    "    if tensor is not None:\n",
    "        stdv = math.sqrt(6.0 / (tensor.size(-2) + tensor.size(-1)))\n",
    "        tensor.data.uniform_(-stdv, stdv)\n",
    "#         tensor.data.fill_(1.0)   # trivial example\n",
    "        \n",
    "def zeros(tensor):\n",
    "    if tensor is not None:\n",
    "        tensor.data.fill_(0)\n",
    "\n",
    "class custom_GCNConv(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels, improved=False, cached=False,\n",
    "                 bias=True, **kwargs):\n",
    "        super().__init__(aggr='add', **kwargs)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.improved = improved\n",
    "        self.cached = cached\n",
    "\n",
    "        self.weight = Parameter(torch.Tensor(in_channels, out_channels))\n",
    "\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        glorot(self.weight)\n",
    "        zeros(self.bias)\n",
    "        self.cached_result = None\n",
    "        self.cached_num_edges = None\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def norm(edge_index, num_nodes, edge_weight=None, improved=False, dtype=None):\n",
    "        \n",
    "        if edge_weight is None:\n",
    "            edge_weight = torch.ones((edge_index.size(1), ), dtype=dtype, device=edge_index.device)\n",
    "        \n",
    "        fill_value = 1 if not improved else 2\n",
    "        \n",
    "        edge_index, edge_weight = add_remaining_self_loops(\n",
    "            edge_index, edge_weight, fill_value, num_nodes)\n",
    "        \n",
    "        row, col = edge_index   \n",
    "        # row includes the starting points of the edges  (first row of edge_index)\n",
    "        # col includes the ending points of the edges   (second row of edge_index)\n",
    "\n",
    "        deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n",
    "        # row records the source nodes, which is the index we are trying to add\n",
    "        # deg will record the out-degree of each node of x_i in all edges (x_i, x_j) including self_loops\n",
    "        \n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "        normalized_edge_weight = deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n",
    "        \n",
    "#         print('whole GCN training normalized_edge_weight: \\n', normalized_edge_weight)\n",
    "        return edge_index, normalized_edge_weight\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight = None):\n",
    "        \"\"\"\"\"\"\n",
    "#         print('current weight is: ')\n",
    "#         print(self.weight)\n",
    "#         print('current bias is: ')\n",
    "#         print(self.bias)\n",
    "        \n",
    "        x = torch.matmul(x, self.weight)   # update x (embeddings)\n",
    "        \n",
    "#         print('inside custom_GCN, edge_index: ', edge_index.shape, '\\n', edge_index)\n",
    "        res = self.propagate(edge_index, x = x, norm = edge_weight)\n",
    "        return res\n",
    "\n",
    "    # self is the first parameter of the message func\n",
    "    def message(self, x_j, norm):\n",
    "        # in source code of the MessagePassing:\n",
    "#         self.__message_args__ = getargspec(self.message)[0][1:]  : will be initialized as [x_j, norm]\n",
    "        \n",
    "        # view is to reshape the tensor, here make it only a single column\n",
    "        # use the normalized weights multiplied by the feature of the target nodes\n",
    "        '''\n",
    "        For each of extended edge_index:(x_i, x_j), assume there is N such edges\n",
    "        x_j of shape (N, k) , assume there is k features, value along each row are the same\n",
    "        norm of shape (1, m), assume there is m edges (including self loops), 1-D tensor\n",
    "        '''\n",
    "#         print('inside the message custom_GCN: norm \\n', norm.shape, '\\n', norm)\n",
    "#         print('inside the message custom_GCN: x_j \\n', x_j.shape, '\\n', x_j)\n",
    "        res = norm.view(-1, 1) * x_j  # use the element wise multiplication\n",
    "        return res\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        # update the embeddings of each node\n",
    "        if self.bias is not None:\n",
    "            aggr_out = aggr_out + self.bias\n",
    "        return aggr_out\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}({}, {})'.format(self.__class__.__name__, self.in_channels,\n",
    "                                   self.out_channels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ====================== Establish a GCN based model ========================\n",
    "class ListModule(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Abstract list layer class.\n",
    "    \"\"\"\n",
    "    def __init__(self, *args):\n",
    "        \"\"\"\n",
    "        Module initializing.\n",
    "        \"\"\"\n",
    "        super(ListModule, self).__init__()\n",
    "        idx = 0\n",
    "        for module in args:\n",
    "            self.add_module(str(idx), module)\n",
    "            idx += 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Getting the indexed layer.\n",
    "        \"\"\"\n",
    "        if idx < 0 or idx >= len(self._modules):\n",
    "            raise IndexError('index {} is out of range'.format(idx))\n",
    "        it = iter(self._modules.values())\n",
    "        for i in range(idx):\n",
    "            next(it)\n",
    "        return next(it)\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Iterating on the layers.\n",
    "        \"\"\"\n",
    "        return iter(self._modules.values())\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Number of layers.\n",
    "        \"\"\"\n",
    "        return len(self._modules)\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, input_layers = [16, 16], dropout=0.3):\n",
    "        \"\"\"\n",
    "        input layers: list of integers\n",
    "        dropout: probability of droping out \n",
    "        \"\"\"\n",
    "        super(Net, self).__init__()\n",
    "        # one trivial example\n",
    "#         self.conv1 = custom_GCNConv(in_channels, out_channels)\n",
    "#         self.conv2 = GCNConv(16, dataset.num_classes)\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.input_layers = input_layers\n",
    "        self.dropout = dropout\n",
    "        self.setup_layers()\n",
    "\n",
    "    def setup_layers(self):\n",
    "        \"\"\"\n",
    "        Creating the layes based on the args.\n",
    "        \"\"\"\n",
    "        self.layers = []\n",
    "        self.input_layers = [self.in_channels] + self.input_layers + [self.out_channels]\n",
    "        for i, _ in enumerate(self.input_layers[:-1]):\n",
    "            self.layers.append(custom_GCNConv(self.input_layers[i],self.input_layers[i+1]))\n",
    "        self.layers = ListModule(*self.layers)\n",
    "\n",
    "    # change the dropout positions: \n",
    "    def forward(self, edge_index, features, edge_weights = None):\n",
    "        if len(self.layers) > 1:\n",
    "            for i in range(len(self.layers)-1):\n",
    "                features = F.relu(self.layers[i](features, edge_index, edge_weights))\n",
    "#                 if i>0:\n",
    "                features = F.dropout(features, p = self.dropout, training = self.training)\n",
    "                    \n",
    "            features = self.layers[len(self.layers)-1](features, edge_index, edge_weights)\n",
    "        else:\n",
    "            features = self.layers[0](features, edge_index, edge_weights)    # for a single layer case\n",
    "\n",
    "        predictions = F.log_softmax(features, dim=1)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partition GCN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Partition method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import metis\n",
    "import random\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import chain\n",
    "\n",
    "class ClusteringMachine(object):\n",
    "    \"\"\"\n",
    "    Clustering the graph, feature set and label. Performed on the CPU side\n",
    "    \"\"\"\n",
    "    def __init__(self, edge_index, features, label):\n",
    "        \"\"\"\n",
    "        :param edge_index: COO format of the edge indices.\n",
    "        :param features: Feature matrix (ndarray).\n",
    "        :param label: label vector (ndarray).\n",
    "        \"\"\"\n",
    "        self.features = features\n",
    "        self.label = label\n",
    "        self._set_sizes()\n",
    "        self.edge_index = edge_index\n",
    "\n",
    "    def _set_sizes(self):\n",
    "        \"\"\"\n",
    "        Setting the feature and class count.\n",
    "        \"\"\"\n",
    "        self.node_count = self.features.shape[0]\n",
    "        self.feature_count = self.features.shape[1]    # features all always in the columns\n",
    "        self.label_count = len(np.unique(self.label.numpy()) )\n",
    "        \n",
    "    def get_edge_weight(self, edge_index, num_nodes, edge_weight=None, improved=False, dtype=None, clustering_file=None):\n",
    "        \"\"\"\n",
    "            To return a complete edges weights from the global graph\n",
    "            For large graph, this will yield a memory bottleneck, try to get around or decompose its functions\n",
    "        \"\"\"\n",
    "        # store all the files\n",
    "        cluster_feature_file = clustering_file + 'cluster_feature.txt'\n",
    "        os.makedirs(os.path.dirname(cluster_feature_file), exist_ok=True)\n",
    "        with open(cluster_feature_file, \"wb\") as fp:\n",
    "            pickle.dump(self.features, fp)\n",
    "        \n",
    "        cluster_label_file = clustering_file + 'cluster_label.txt'\n",
    "        with open(cluster_label_file, \"wb\") as fp:\n",
    "            pickle.dump(self.label, fp)\n",
    "        \n",
    "        cluster_edge_index_file = clustering_file + 'cluster_edge_index.txt'\n",
    "        with open(cluster_edge_index_file, \"wb\") as fp:\n",
    "            pickle.dump(self.edge_index, fp)\n",
    "        \n",
    "        # calculate the global graph properties, global edge weights\n",
    "        if edge_weight is None:\n",
    "            edge_weight = torch.ones((edge_index.size(1), ), dtype=dtype, device=edge_index.device)\n",
    "        \n",
    "        fill_value = 1 if not improved else 2\n",
    "        # there are num_nodes self-loop edges added after the edge_index\n",
    "        edge_index, edge_weight = add_remaining_self_loops(edge_index, edge_weight, fill_value, num_nodes)\n",
    "        \n",
    "        row, col = edge_index   \n",
    "        # row includes the starting points of the edges  (first row of edge_index)\n",
    "        # col includes the ending points of the edges   (second row of edge_index)\n",
    "\n",
    "        deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n",
    "        # row records the source nodes, which is the index we are trying to add\n",
    "        # deg will record the out-degree of each node of x_i in all edges (x_i, x_j) including self_loops\n",
    "        \n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "        # normalize the edge weight\n",
    "        normalized_edge_weight = deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n",
    "        \n",
    "        # transfer from tensor to the numpy to construct the dict for the edge_weights\n",
    "        edge_index = edge_index.t().numpy()\n",
    "        normalized_edge_weight = normalized_edge_weight.numpy()\n",
    "        \n",
    "        self.edge_index_noloop, self.edge_index_selfloop = edge_index[:-num_nodes], edge_index[-num_nodes:]\n",
    "        self.normalized_edge_weight_noloop, self.normalized_edge_weight_selfloop = normalized_edge_weight[:-num_nodes], normalized_edge_weight[-num_nodes:]\n",
    "        \n",
    "        calculate_edge_index_file = clustering_file + 'calculate_edge_index_noloop.txt'\n",
    "        with open(calculate_edge_index_file, \"wb\") as fp:\n",
    "            pickle.dump(self.edge_index_noloop, fp)\n",
    "        \n",
    "        calculate_normalized_edge_weight_file = clustering_file + 'calculate_normalized_edge_weight_noloop.txt'\n",
    "        with open(calculate_normalized_edge_weight_file, \"wb\") as fp:\n",
    "            pickle.dump(self.normalized_edge_weight_noloop, fp)\n",
    "        \n",
    "        num_edge = edge_index.shape[0]\n",
    "        # this info can also be stored as matrix considering the memory, depends whether the matrix is sparse or not\n",
    "        # after changed intot the numpy format\n",
    "        # cosntruct the dictionary\n",
    "        self.edge_weight_global_dict = {(edge_index[i][0], edge_index[i][1]) : normalized_edge_weight[i] for i in range(num_edge)}\n",
    "        self.edge_weight_global_list = [(edge_index[i][0], edge_index[i][1], normalized_edge_weight[i]) for i in range(num_edge)]\n",
    "        \n",
    "        cluster_edge_weight_dict_file = clustering_file + 'cluster_edge_weight_dict.txt'\n",
    "        with open(cluster_edge_weight_dict_file, \"wb\") as fp:\n",
    "            pickle.dump(self.edge_weight_global_dict, fp)\n",
    "            \n",
    "        cluster_edge_weight_list_file = clustering_file + 'cluster_edge_weight_list.txt'\n",
    "        with open(cluster_edge_weight_list_file, \"wb\") as fp:\n",
    "            pickle.dump(self.edge_weight_global_list, fp)\n",
    "        \n",
    "        # create a generator for the list\n",
    "        output = ([edge_index[i][0], edge_index[i][1], normalized_edge_weight[i]] for i in range(num_edge))\n",
    "        input_edge_weight_txt_file = clustering_file + 'input_edge_weight_list.csv'\n",
    "        with open(input_edge_weight_txt_file, 'w', newline='\\n') as fp:\n",
    "            wr = csv.writer(fp, delimiter = ' ')\n",
    "            for line in output:\n",
    "                wr.writerow(line)\n",
    "                \n",
    "        self.graph = nx.read_weighted_edgelist(input_edge_weight_txt_file, create_using = nx.Graph, nodetype = int)\n",
    "        cluster_graph_file = clustering_file + 'cluster_graph.txt'\n",
    "        with open(cluster_graph_file, \"wb\") as fp:\n",
    "            pickle.dump(self.graph, fp)\n",
    "            \n",
    "    \n",
    "    # 1) first use different clustering method, then split each cluster into train, test and validation nodes, split edges\n",
    "    def split_cluster_nodes_edges(self, test_ratio, validation_ratio, partition_num = 2):\n",
    "        \"\"\"\n",
    "        Decomposing the graph, partitioning the features and label, creating Torch arrays.\n",
    "        \"\"\"\n",
    "        # to keep the edge weights of the original whole graph:\n",
    "        self.train_clusters, self.sg_nodes_global = self.metis_clustering(self.graph, partition_num)\n",
    "#         self.train_clusters, self.sg_nodes_global = self.random_clustering(list(self.graph.nodes()), partition_num)\n",
    "        self.valid_clusters = self.train_clusters\n",
    "        \n",
    "        relative_test_ratio = (test_ratio) / (1 - validation_ratio)\n",
    "        self.sg_subgraph = {}\n",
    "        \n",
    "        self.sg_model_nodes_global = {}\n",
    "        self.sg_validation_nodes_global = {}\n",
    "        self.sg_train_nodes_global = {}\n",
    "        self.sg_test_nodes_global = {}\n",
    "        \n",
    "        # keep the info of each cluster:\n",
    "        self.info_isolate_cluster_size = {}\n",
    "        self.info_model_cluster_size = {}\n",
    "        self.info_validation_cluster_size = {}\n",
    "        self.info_train_cluster_size = {}\n",
    "        self.info_test_cluster_size = {}\n",
    "        \n",
    "        for cluster in self.train_clusters:\n",
    "            self.sg_model_nodes_global[cluster], self.sg_validation_nodes_global[cluster] = train_test_split(self.sg_nodes_global[cluster], test_size = validation_ratio)\n",
    "            self.sg_train_nodes_global[cluster], self.sg_test_nodes_global[cluster] = train_test_split(self.sg_model_nodes_global[cluster], test_size = relative_test_ratio)\n",
    "            \n",
    "            # record the information of each cluster:\n",
    "            self.info_isolate_cluster_size[cluster] = len(self.sg_nodes_global[cluster])\n",
    "            self.info_model_cluster_size[cluster] = len(self.sg_model_nodes_global[cluster])\n",
    "            self.info_validation_cluster_size[cluster] = len(self.sg_validation_nodes_global[cluster])\n",
    "            \n",
    "            self.info_train_cluster_size[cluster] = len(self.sg_train_nodes_global[cluster])\n",
    "            self.info_test_cluster_size[cluster] = len(self.sg_test_nodes_global[cluster])\n",
    "    \n",
    "    \n",
    "    # 2) first assign train, test, validation nodes, split edges; this is based on the assumption that the clustering is no longer that important\n",
    "    def split_whole_nodes_edges_then_cluster(self, test_ratio, validation_ratio):\n",
    "        \"\"\"\n",
    "            Only split nodes\n",
    "            First create train-test splits, then split train and validation into different batch seeds\n",
    "            Input:  \n",
    "                1) ratio of test, validation\n",
    "                2) partition number of train nodes, test nodes, validation nodes\n",
    "            Output:\n",
    "                1) sg_validation_nodes_global, sg_train_nodes_global, sg_test_nodes_global\n",
    "        \"\"\"\n",
    "        relative_test_ratio = (test_ratio) / (1 - validation_ratio)\n",
    "        \n",
    "        # first divide the nodes for the whole graph, result will always be a list of lists \n",
    "        model_nodes_global, self.valid_nodes_global = train_test_split(list(self.graph.nodes()), test_size = validation_ratio)\n",
    "        self.train_nodes_global, self.test_nodes_global = train_test_split(model_nodes_global, test_size = relative_test_ratio)\n",
    "        \n",
    "    # just allocate each node to arandom cluster, store the membership inside each dict\n",
    "    def random_clustering(self, target_nodes, partition_num):\n",
    "        \"\"\"\n",
    "            Random clustering the nodes.\n",
    "            Input: \n",
    "                1) target_nodes: list of node \n",
    "                2) number of partition to be generated\n",
    "            Output: \n",
    "                1) cluster idx\n",
    "                2) membership of each node\n",
    "                3) list of generated clusters, each cluster includes a bunch of global node idx\n",
    "        \"\"\"\n",
    "        # randomly divide into two clusters\n",
    "        nodes_order = [node for node in target_nodes]\n",
    "        random.shuffle(nodes_order)\n",
    "        n = (len(nodes_order) + partition_num - 1) // partition_num\n",
    "        partition_list = [nodes_order[i * n:(i + 1) * n] for i in range(partition_num)]\n",
    "#         cluster_membership = {node : i for i, node_list in enumerate(partition_list) for node in node_list}\n",
    "        cluster_nodes_global = {i : node_list for i, node_list in enumerate(partition_list)}\n",
    "        \n",
    "        return cluster_nodes_global\n",
    "\n",
    "    def metis_clustering(self, target_graph, partition_num):\n",
    "        \"\"\"\n",
    "        Clustering the graph with Metis. For details see:\n",
    "        Input: \n",
    "            1) target graph\n",
    "            2) number of parts\n",
    "        Output:\n",
    "            1) index of all the clusters\n",
    "            2) membership of each node\n",
    "        \"\"\"\n",
    "        (st, parts) = metis.part_graph(target_graph, partition_num)\n",
    "        clusters = list(set(parts))\n",
    "        cluster_nodes_global = defaultdict(list)\n",
    "        for node, cluster_id in enumerate(parts):\n",
    "            cluster_nodes_global[cluster_id].append(node)\n",
    "        return clusters, cluster_nodes_global\n",
    "        \n",
    "    # select the training nodes as the mini-batch for each cluster\n",
    "    def mini_batch_sample(self, target_seed, k, frac = 1):\n",
    "        \"\"\"\n",
    "            This function is to generate the neighbors of the seed (either train nodes or validation nodes)\n",
    "            params: cluster index, number of layer k, fraction of sampling from each neighbor layer\n",
    "            input: \n",
    "                1) target_seed: this is the 0 layer inside self.neighbor\n",
    "            output:\n",
    "                1) neighbor: nodes global idx inside each layer of the batch\n",
    "                2) accum_neighbor: accumulating neighbors , i.e. the final batch nodes\n",
    "        \"\"\"\n",
    "        accum_neighbor = defaultdict(set)\n",
    "        for cluster in target_seed.keys():\n",
    "            neighbor = set(target_seed[cluster])  # first layer of the neighbor nodes of each cluster\n",
    "            for layer in range(k):\n",
    "                # first accumulate last layer\n",
    "                accum_neighbor[cluster] |= neighbor\n",
    "                tmp_level = set()\n",
    "                for node in neighbor:\n",
    "                    tmp_level |= set(self.graph.neighbors(node))  # the key here we are using self.graph, extract neighbor from the whole graph\n",
    "                # add the new layer of neighbors\n",
    "                tmp_level -= accum_neighbor[cluster]\n",
    "                # each layer will only contains partial nodes from the previous layer\n",
    "                neighbor = set(random.sample(tmp_level, int(len(tmp_level) * frac) ) ) if 0 < frac < 1 else tmp_level\n",
    "    #                 print('layer ' + str(layer + 1) + ' : ', self.neighbor[cluster][layer+1])\n",
    "            # the most outside layer: kth layer will be added:\n",
    "            accum_neighbor[cluster] |= neighbor\n",
    "        return accum_neighbor\n",
    "        \n",
    "    def mini_batch_generate(self, batch_file_folder, target_seed, k, fraction = 1.0, data_type = 'train'):\n",
    "        \"\"\"\n",
    "            create the mini-batch focused on the train nodes only, include a total of k layers of neighbors of the original training nodes\n",
    "            k: number of layers of neighbors for each training node\n",
    "            fraction: fraction of neighbor nodes in each layer to be considered\n",
    "            Input:\n",
    "                1) target_seed: global ids of the nodes for seed to generate the batch\n",
    "                    usually one of (train_global, test_global_, validation_global)\n",
    "            Output: all tensors which are gonna be used in the train, forward procedure\n",
    "                local:\n",
    "                    1) sg_mini_edges_local\n",
    "                    2) self.sg_mini_train_edge_weight_local\n",
    "                    3) self.sg_mini_train_nodes_local\n",
    "                    4) self.sg_mini_train_features\n",
    "                    5) self.sg_mini_train_labels\n",
    "            \n",
    "        \"\"\"\n",
    "        # these are currently believed to be the main memory cost, storing all overlapping batch information\n",
    "        # instead we store all the information inside one list to be stored in a pickle file as out-of-core mini-batch\n",
    "        \n",
    "        # this will get the edge weights in a complete graph\n",
    "        \n",
    "        info_batch_size = {}\n",
    "                \n",
    "        accum_neighbor = self.mini_batch_sample(target_seed, k, frac = fraction)\n",
    "        \n",
    "        for cluster in target_seed.keys():\n",
    "            batch_subgraph = self.graph.subgraph(accum_neighbor[cluster])\n",
    "            \n",
    "             # first select all the overlapping nodes of the train nodes\n",
    "            mini_nodes_global = sorted(node for node in batch_subgraph.nodes())\n",
    "            \n",
    "            # store the global edges\n",
    "            mini_edges_global = {edge for edge in batch_subgraph.edges()}\n",
    "            \n",
    "            # map nodes from global index to local index\n",
    "            mini_mapper = {node: i for i, node in enumerate(mini_nodes_global)}\n",
    "            \n",
    "            # store local index of batch nodes\n",
    "            mini_nodes_local = [ mini_mapper[global_idx] for global_idx in target_seed[cluster] ]\n",
    "            \n",
    "            # store local index of batch edges\n",
    "            mini_edges_local = \\\n",
    "                           [ [ mini_mapper[edge[0]], mini_mapper[edge[1]] ] for edge in mini_edges_global ] + \\\n",
    "                           [ [ mini_mapper[edge[1]], mini_mapper[edge[0]] ] for edge in mini_edges_global ] + \\\n",
    "                           [ [i, i] for i in sorted(mini_mapper.values()) ]  \n",
    "#             self.edge_index_noloop, self.edge_index_selfloop\n",
    "#             self.normalized_edge_weight_noloop, self.normalized_edge_weight_selfloop \n",
    "#             # store local edge weights\n",
    "\n",
    "            mini_edge_weight_local = \\\n",
    "                            [ self.edge_weight_global_dict[(edge[0], edge[1])] for edge in mini_edges_global ] + \\\n",
    "                            [ self.edge_weight_global_dict[(edge[1], edge[0])] for edge in mini_edges_global ] + \\\n",
    "                            [ self.edge_weight_global_dict[(i, i)] for i in mini_nodes_global ]\n",
    "        \n",
    "            mini_edge_weight_local_old = \\\n",
    "                            [ self.graph.edges[left, right]['weight'] for left, right in mini_edges_global ] + \\\n",
    "                            [ self.graph.edges[right, left]['weight'] for left, right in mini_edges_global ] + \\\n",
    "                            [ self.graph.edges[i, i]['weight'] for i in mini_nodes_global ]\n",
    "            \n",
    "#             print('check the dictionary and the list weights is the same or not:')\n",
    "            diff_count = 0\n",
    "            for left, right in zip(mini_edge_weight_local_old, mini_edge_weight_local):\n",
    "                if abs(left - right) > 10**(-6):\n",
    "                    diff_count += 1\n",
    "                    print(left, right)\n",
    "            if diff_count > 0:\n",
    "                print('total number of different weight values is : ', diff_count)\n",
    "            \n",
    "            # store local features and lables\n",
    "            mini_features = self.features[mini_nodes_global,:]\n",
    "            mini_labels = self.label[mini_nodes_global]\n",
    "            \n",
    "            # record information \n",
    "            info_batch_size[cluster] = len(mini_nodes_global)\n",
    "            \n",
    "            mini_nodes_local = torch.LongTensor(mini_nodes_local)\n",
    "            mini_edges_local = torch.LongTensor(mini_edges_local).t()\n",
    "            mini_edge_weight_local = torch.FloatTensor(mini_edge_weight_local)\n",
    "            mini_features = torch.FloatTensor(mini_features)\n",
    "            mini_labels = torch.LongTensor(mini_labels)\n",
    "            \n",
    "            minibatch_data = [mini_nodes_local, mini_edges_local, mini_edge_weight_local, mini_features, mini_labels]\n",
    "            \n",
    "            batch_file_type_folder = batch_file_folder + data_type + '/'\n",
    "            batch_file_name = batch_file_type_folder + 'batch_' + str(cluster)\n",
    "            \n",
    "            # store the batch files\n",
    "            t0 = time.time()\n",
    "            os.makedirs(os.path.dirname(batch_file_name), exist_ok=True)\n",
    "            with open(batch_file_name, \"wb\") as fp:\n",
    "                pickle.dump(minibatch_data, fp)\n",
    "            store_time = ((time.time() - t0) * 1000)\n",
    "            print('*** Generate {0:s} batch file for # {1:3d} batch, writing the batch file costed {2:.2f} ms ***'.format(data_type, cluster, store_time) )\n",
    "#             print('writing to the path: ', batch_file_name)\n",
    "            \n",
    "        return info_batch_size\n",
    "        \n",
    "\n",
    "    def mini_batch_train_clustering(self, batch_file_folder, k, fraction = 1.0, train_batch_num = 2):\n",
    "        # special extreme case, each train node in one batch seed\n",
    "#         self.sg_train_nodes_global = self.random_clustering(train_nodes_global, len(train_nodes_global))\n",
    "        sg_train_nodes_global = self.random_clustering(self.train_nodes_global, train_batch_num)\n",
    "        \n",
    "        self.info_train_batch_size = self.mini_batch_generate(batch_file_folder, sg_train_nodes_global, k, fraction = 1.0, data_type = 'train')\n",
    "        self.info_train_cluster_size = {key : len(val) for key, val in sg_train_nodes_global.items()}\n",
    "        \n",
    "    def mini_batch_validation_clustering(self, batch_file_folder, k, fraction = 1.0, valid_batch_num = 2):\n",
    "        \n",
    "        sg_validation_nodes_global = self.random_clustering(self.valid_nodes_global, valid_batch_num)\n",
    "        self.info_validation_batch_size = self.mini_batch_generate(batch_file_folder, sg_validation_nodes_global, k, fraction = 1.0, data_type = 'validation')\n",
    "        \n",
    "        self.info_validation_cluster_size = {key : len(val) for key, val in sg_validation_nodes_global.items()}\n",
    "    \n",
    "    def mini_batch_test_clustering(self, batch_file_folder, k, fraction = 1.0, test_batch_num = 2):\n",
    "        \n",
    "        sg_test_nodes_global = self.random_clustering(self.test_nodes_global, test_batch_num)\n",
    "        self.info_test_batch_size = self.mini_batch_generate(batch_file_folder, sg_test_nodes_global, k, fraction = 1.0, data_type = 'test')\n",
    "        self.info_test_cluster_size = {key : len(val) for key, val in sg_test_nodes_global.items()}\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Partition Graph with trainiing and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Custom_GCN_layer import Net\n",
    "import time\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class ClusterGCNTrainer_mini_Train(object):\n",
    "    \"\"\"\n",
    "    Training a ClusterGCN.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_folder, in_channels, out_channels, input_layers = [32, 16], dropout=0.3):\n",
    "        \"\"\"\n",
    "        :param in_channels, out_channels: input and output feature dimension\n",
    "        :param clustering_machine:\n",
    "        \"\"\"  \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.data_folder = data_folder\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.input_layers = input_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.create_model()\n",
    "\n",
    "    def create_model(self):\n",
    "        \"\"\"\n",
    "        Creating a StackedGCN and transferring to CPU/GPU.\n",
    "        \"\"\"\n",
    "#         print('used layers are: ', str(self.input_layers))\n",
    "        self.model = Net(self.in_channels, self.out_channels, input_layers = self.input_layers, dropout = self.dropout)\n",
    "        self.model = self.model.to(self.device)\n",
    "    \n",
    "    # call the forward function batch by batch\n",
    "    def do_forward_pass(self, tr_train_nodes, tr_edges, tr_edge_weights, tr_features, tr_target):\n",
    "        \"\"\"\n",
    "        Making a forward pass with data from a given partition.\n",
    "        :param cluster: Cluster index.\n",
    "        :return average_loss: Average loss on the cluster.\n",
    "        :return node_count: Number of nodes.\n",
    "        \"\"\"\n",
    "        \n",
    "        '''Target and features are one-one mapping'''\n",
    "        # calculate the probabilites from log_sofmax\n",
    "        predictions = self.model(tr_edges, tr_features, tr_edge_weights)\n",
    "        \n",
    "        ave_loss = torch.nn.functional.nll_loss(predictions[tr_train_nodes], tr_target[tr_train_nodes])\n",
    "        node_count = tr_train_nodes.shape[0]\n",
    "\n",
    "        # for each cluster keep track of the counts of the nodes\n",
    "        return ave_loss, node_count\n",
    "\n",
    "\n",
    "    def update_average_loss(self, batch_average_loss, node_count, isolate = True):\n",
    "        \"\"\"\n",
    "        Updating the average loss in the epoch.\n",
    "        :param batch_average_loss: Loss of the cluster. \n",
    "        :param node_count: Number of nodes in currently processed cluster.\n",
    "        :return average_loss: Average loss in the epoch.\n",
    "        \"\"\"\n",
    "        self.accumulated_training_loss = self.accumulated_training_loss + batch_average_loss.item() * node_count\n",
    "        if isolate:\n",
    "            self.node_count_seen = self.node_count_seen + node_count\n",
    "        average_loss = self.accumulated_training_loss / self.node_count_seen\n",
    "        return average_loss\n",
    "\n",
    "    # iterate through epoch and also the clusters\n",
    "    def train_investigate_F1(self, epoch_num=10, learning_rate=0.01, weight_decay = 0.01, mini_epoch_num = 1, output_period = 10, train_batch_num = 2, valid_batch_num = 2):\n",
    "        \"\"\"\n",
    "            *** Periodically output the F1 score during training. After certain number of epochs ***\n",
    "            epoch_num:  number of total training epoch number\n",
    "            learning rate: learning rate during training\n",
    "            weight_decay:  decay coefficients for the regularization\n",
    "            mini_epoch_num:  number of epochs of repeating training after loading data on the GPU\n",
    "            output_period:  number of epochs after which output the F1 and accuray to investigate the model refining process\n",
    "        \"\"\"\n",
    "        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        self.model.train()   #   set into train mode, only effective for certain modules such as dropout and batchNorm\n",
    "        self.record_ave_training_loss = []\n",
    "        self.time_train_load_data = 0\n",
    "        \n",
    "        epoch_partition = epoch_num // mini_epoch_num\n",
    "        investigate_f1 = {}\n",
    "        investigate_accuracy = {}\n",
    "        \n",
    "        t0 = time.time()\n",
    "        train_clusters = list(range(train_batch_num))\n",
    "        for epoch_part in range(epoch_partition):\n",
    "#             For test purpose, we let the clusters to follow specific order\n",
    "            random.shuffle(train_clusters)\n",
    "            self.node_count_seen = 0\n",
    "            self.accumulated_training_loss = 0\n",
    "            for cluster in train_clusters:\n",
    "                # for each batch, we load once and train it for multiple epochs:\n",
    "                # read in the train data from the pickle files\n",
    "                batch_file_name = self.data_folder + 'train/batch_' + str(cluster)\n",
    "                \n",
    "                t2 = time.time()\n",
    "                with open(batch_file_name, \"rb\") as fp:\n",
    "                    minibatch_data_train = pickle.load(fp)\n",
    "                read_time = (time.time() - t2) * 1000\n",
    "                print('*** During training for # {0:3d} batch, reading batch file costed {1:.2f} ms ***'.format(cluster, read_time) )\n",
    "                \n",
    "                tr_train_nodes, tr_edges, tr_edge_weights, tr_features, tr_target = minibatch_data_train\n",
    "                \n",
    "                t1 = time.time()\n",
    "                tr_train_nodes = tr_train_nodes.to(self.device)\n",
    "                tr_edges = tr_edges.to(self.device)\n",
    "                tr_edge_weights = tr_edge_weights.to(self.device)\n",
    "                tr_features = tr_features.to(self.device)\n",
    "                tr_target = tr_target.to(self.device)\n",
    "                \n",
    "                self.time_train_load_data += (time.time() - t1) * 1000\n",
    "                # train each batch for multiple epochs\n",
    "                for mini_epoch in range(mini_epoch_num):\n",
    "                    # record the current overall epoch index:\n",
    "                    real_epoch_num = 1 + mini_epoch + mini_epoch_num * epoch_part # real_epoch_num starts from 0, therefore we add 1\n",
    "                    \n",
    "                    self.optimizer.zero_grad()\n",
    "                    batch_ave_loss, node_count = self.do_forward_pass(tr_train_nodes, tr_edges, tr_edge_weights, tr_features, tr_target)\n",
    "                    batch_ave_loss.backward()\n",
    "                    self.optimizer.step()\n",
    "                    ave_loss = self.update_average_loss(batch_ave_loss, node_count)\n",
    "                    \n",
    "                    # at this point finish a single train duration: update the parameter and calcualte the loss function\n",
    "                    # periodically output the F1-score in the middle of the training process\n",
    "                    if real_epoch_num % output_period == 0:\n",
    "                        investigate_f1[real_epoch_num], investigate_accuracy[real_epoch_num] = self.batch_validate(valid_batch_num = valid_batch_num)\n",
    "                        self.model.train()    # reset to the train mode\n",
    "            \n",
    "            self.record_ave_training_loss.append(ave_loss)\n",
    "        # convert to ms\n",
    "        self.time_train_total = ((time.time() - t0) * 1000)\n",
    "        return investigate_f1, investigate_accuracy\n",
    "\n",
    "    # iterate through epoch and also the clusters\n",
    "    def train(self, epoch_num=10, learning_rate=0.01, weight_decay = 0.01, mini_epoch_num = 1, train_batch_num = 2):\n",
    "        \"\"\"\n",
    "            *** Training a model. ***\n",
    "            epoch_num:  number of total training epoch number\n",
    "            learning rate: learning rate during training\n",
    "            weight_decay:  decay coefficients for the regularization\n",
    "            mini_epoch_num:  number of epochs of repeating training after loading data on the GPU\n",
    "        \"\"\"\n",
    "        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        self.model.train()\n",
    "        self.record_ave_training_loss = []\n",
    "        self.time_train_load_data = 0\n",
    "        \n",
    "        epoch_partition = epoch_num // mini_epoch_num\n",
    "        t0 = time.time()\n",
    "        train_clusters = list(range(train_batch_num))\n",
    "        for epoch in range(epoch_partition):\n",
    "#             For test purpose, we let the clusters to follow specific order\n",
    "            random.shuffle(train_clusters)\n",
    "            self.node_count_seen = 0\n",
    "            self.accumulated_training_loss = 0\n",
    "            \n",
    "            for cluster in train_clusters:\n",
    "                # read in the train data from the pickle files\n",
    "                batch_file_name = self.data_folder + 'train/batch_' + str(cluster)\n",
    "                \n",
    "                t2 = time.time()\n",
    "                with open(batch_file_name, \"rb\") as fp:\n",
    "                    minibatch_data_train = pickle.load(fp)\n",
    "                read_time = (time.time() - t2) * 1000\n",
    "                print('*** During training for # {0:3d} batch, reading batch file costed {1:.2f} ms ***'.format(cluster, read_time) )\n",
    "                \n",
    "                tr_train_nodes, tr_edges, tr_edge_weights, tr_features, tr_target = minibatch_data_train\n",
    "                \n",
    "                # for each cluster, we load once and train it for multiple epochs:\n",
    "                t1 = time.time()\n",
    "                tr_train_nodes = tr_train_nodes.to(self.device)\n",
    "                tr_edges = tr_edges.to(self.device)\n",
    "                tr_edge_weights = tr_edge_weights.to(self.device)\n",
    "                tr_features = tr_features.to(self.device)\n",
    "                tr_target = tr_target.to(self.device)\n",
    "                \n",
    "                \n",
    "                self.time_train_load_data += (time.time() - t1) * 1000\n",
    "                # train each batch for multiple epochs\n",
    "                for mini_epoch in range(mini_epoch_num):\n",
    "                    self.optimizer.zero_grad()\n",
    "                    batch_ave_loss, node_count = self.do_forward_pass(tr_train_nodes, tr_edges, tr_edge_weights, tr_features, tr_target)\n",
    "                    batch_ave_loss.backward()\n",
    "                    self.optimizer.step()\n",
    "                    ave_loss = self.update_average_loss(batch_ave_loss, node_count)\n",
    "            \n",
    "            self.record_ave_training_loss.append(ave_loss)\n",
    "        # convert to ms\n",
    "        self.time_train_total = ((time.time() - t0) * 1000)\n",
    "    \n",
    "\n",
    "    def do_batch_validation_prediction(self, valid_validation_nodes, valid_edges, valid_edge_weights, valid_features, valid_target):\n",
    "        \"\"\"\n",
    "        Scoring a cluster.\n",
    "        :param cluster: Cluster index.\n",
    "        :return prediction: Prediction matrix with probabilities.\n",
    "        :return target: Target vector.\n",
    "        \"\"\"\n",
    "        predictions = self.model(valid_edges, valid_features, valid_edge_weights)\n",
    "        return predictions[valid_validation_nodes], valid_target[valid_validation_nodes]\n",
    "\n",
    "    def batch_validate(self, valid_batch_num = 2):\n",
    "        \"\"\"\n",
    "        Scoring the test and printing the F-1 score.\n",
    "        \"\"\"\n",
    "        self.model.eval()   # set into test mode, only effective for certain modules such as dropout and batchNorm\n",
    "        \n",
    "        predictions = []\n",
    "        targets = []\n",
    "        valid_clusters = list(range(valid_batch_num))\n",
    "        for cluster in valid_clusters:\n",
    "            # read in the train data from the pickle files\n",
    "            batch_file_name = self.data_folder + 'validation/batch_' + str(cluster)\n",
    "            \n",
    "            t2 = time.time()\n",
    "            with open(batch_file_name, \"rb\") as fp:\n",
    "                minibatch_data_validation = pickle.load(fp)\n",
    "            read_time = (time.time() - t2) * 1000\n",
    "            print('*** During validation for # {0:3d} batch, reading batch file costed {1:.2f} ms ***'.format(cluster, read_time) )\n",
    "\n",
    "            valid_validation_nodes, valid_edges, valid_edge_weights, valid_features, valid_target = minibatch_data_validation\n",
    "            \n",
    "            valid_validation_nodes = valid_validation_nodes.to(self.device)\n",
    "            valid_edges = valid_edges.to(self.device)\n",
    "            valid_edge_weights = valid_edge_weights.to(self.device)\n",
    "            valid_features = valid_features.to(self.device)\n",
    "            valid_target = valid_target.to(self.device)\n",
    "            \n",
    "            \n",
    "            prediction, target = self.do_batch_validation_prediction(valid_validation_nodes, valid_edges, valid_edge_weights, valid_features, valid_target)\n",
    "\n",
    "            predictions.append(prediction.cpu().detach().numpy())\n",
    "            targets.append(target.cpu().detach().numpy())\n",
    "        \n",
    "        # concatenate all the ndarrays inside this list\n",
    "        targets = np.concatenate(targets)\n",
    "        # along axis:    axis == 1\n",
    "        predictions = np.concatenate(predictions).argmax(1)  # return the indices of maximum probability \n",
    "#         print('shape of the targets and predictions are: ', self.targets.shape, self.predictions.shape)\n",
    "        \n",
    "        f1 = f1_score(targets, predictions, average=\"micro\")\n",
    "        accuracy = accuracy_score(targets, predictions)\n",
    "#         print(\"\\nTest F-1 score: {:.4f}\".format(score))\n",
    "        return (f1, accuracy)\n",
    "\n",
    "# for cross-validation purpose: \n",
    "    def do_prediction(self, cluster):\n",
    "        \"\"\"\n",
    "        Scoring a cluster.\n",
    "        :param cluster: Cluster index.\n",
    "        :return prediction: Prediction matrix with probabilities.\n",
    "        :return target: Target vector.\n",
    "        \"\"\"\n",
    "        test_nodes = self.clustering_machine.sg_test_nodes_global[cluster].to(self.device)\n",
    "        prediction = self.model(self.edges, self.features, self.edge_weights)\n",
    "        \n",
    "        return prediction[test_nodes], self.label[test_nodes]\n",
    "    \n",
    "    def test(self):\n",
    "        \"\"\"\n",
    "        Scoring the test and printing the F-1 score.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        self.predictions = []\n",
    "        self.targets = []\n",
    "        \n",
    "        self.edges = self.clustering_machine.edge_index_global_self_loops.to(self.device)\n",
    "        self.features = self.clustering_machine.features.to(self.device)\n",
    "        self.edge_weights = self.clustering_machine.edge_weight_global.to(self.device)\n",
    "        self.label = self.clustering_machine.label.to(self.device)\n",
    "        \n",
    "        for cluster in self.clustering_machine.test_clusters:\n",
    "            prediction, target = self.do_prediction(cluster)\n",
    "\n",
    "            self.predictions.append(prediction.cpu().detach().numpy())\n",
    "            self.targets.append(target.cpu().detach().numpy())\n",
    "        \n",
    "        # concatenate all the ndarrays inside this list\n",
    "        self.targets = np.concatenate(self.targets)\n",
    "        # along axis:    axis == 1\n",
    "        self.predictions = np.concatenate(self.predictions).argmax(1)  # return the indices of maximum probability \n",
    "#         print('shape of the targets and predictions are: ', self.targets.shape, self.predictions.shape)\n",
    "        \n",
    "        f1 = f1_score(self.targets, self.predictions, average=\"micro\")\n",
    "        accuracy = accuracy_score(self.targets, self.predictions)\n",
    "#         print(\"\\nTest F-1 score: {:.4f}\".format(score))\n",
    "        return (f1, accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Trivial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 2.],\n",
      "        [0., 3.],\n",
      "        [0., 4.],\n",
      "        [0., 5.],\n",
      "        [0., 6.],\n",
      "        [0., 7.],\n",
      "        [0., 8.],\n",
      "        [0., 9.]]) torch.Size([10, 2])\n"
     ]
    }
   ],
   "source": [
    "'''Trivial data'''\n",
    "edge_index = torch.tensor([[0, 1, 1, 3, 1, 2, 4, 2, 4, 6, 6, 7, 7, 9, 2, 5, 9, 8], \n",
    "                           [1, 0, 3, 1, 2, 1, 2, 4, 6, 4, 7, 6, 9, 7, 5, 2, 8, 9]])\n",
    "# features = torch.rand(10, 3)\n",
    "features = torch.tensor([[0, 0], [0, 1], [0, 2], [0, 3], [0, 4],  \n",
    "                           [0, 5], [0, 6], [0, 7], [0, 8], [0, 9]], dtype = torch.float)\n",
    "# label = torch.tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "label = torch.tensor([0, 1, 1, 0, 1, 1, 1, 0, 0, 0])\n",
    "print(features, features.shape)\n",
    "\n",
    "# set the store clustering path\n",
    "clustering_folder = './res_save_batch/clustering/'\n",
    "check_folder_exist(clustering_folder)\n",
    "clustering_file_name = clustering_folder + 'check_clustering_machine.txt'\n",
    "os.makedirs(os.path.dirname(clustering_file_name), exist_ok=True)\n",
    "\n",
    "clustering_machine = ClusteringMachine(edge_index, features, label)\n",
    "\n",
    "clustering_machine.get_edge_weight(clustering_machine.edge_index, clustering_machine.node_count, clustering_file = clustering_folder)\n",
    "\n",
    "with open(clustering_file_name, \"wb\") as fp:\n",
    "    pickle.dump(clustering_machine, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### minibatch train nodes and batch validatioin "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Generate train batch file for #   0 batch, writing the batch file costed 0.27 ms ***\n",
      "*** Generate train batch file for #   1 batch, writing the batch file costed 0.21 ms ***\n",
      "*** Generate validation batch file for #   0 batch, writing the batch file costed 0.30 ms ***\n",
      "*** Generate validation batch file for #   1 batch, writing the batch file costed 0.20 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 0.25 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 0.24 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 0.23 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 0.22 ms ***\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5, 0.5)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "mini_batch_folder = './res_save_batch/mini_batch_files/'\n",
    "check_folder_exist(mini_batch_folder)\n",
    "\n",
    "with open(clustering_file_name, \"rb\") as fp:\n",
    "    clustering_machine = pickle.load(fp)\n",
    "\n",
    "clustering_machine.split_whole_nodes_edges_then_cluster(0.4, 0.4)\n",
    "# generate the batches for train and validation\n",
    "clustering_machine.mini_batch_train_clustering(mini_batch_folder, 1, train_batch_num = 2) # include number of layers\n",
    "\n",
    "clustering_machine.mini_batch_validation_clustering(mini_batch_folder, 1, valid_batch_num = 2)\n",
    "\n",
    "# construct the batch trainer\n",
    "gcn_trainer_batch = ClusterGCNTrainer_mini_Train(mini_batch_folder, 2, 2, input_layers = [16], dropout=0.3)\n",
    "\n",
    "gcn_trainer_batch.train(1, 0.0001, 0.1, train_batch_num = 2)\n",
    "gcn_trainer_batch.batch_validate(valid_batch_num = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the train loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_train_loss_converge(mini_batch_folder, data_name, dataset, image_path,  comments, input_layer = [32, 16], epoch_num = 300, \\\n",
    "                              dropout = 0.3, lr = 0.0001, weight_decay = 0.01, mini_epoch_num = 5, \\\n",
    "                               valid_part_num = 2, train_part_num = 2, test_part_num = 1):\n",
    "    # mini-batch, but valid also in batches\n",
    "    a3, v3, time3, load3, Cluster_train_valid_batch_trainer = Cluster_train_valid_batch_run(mini_batch_folder, data_name, dataset, image_path, input_layer = input_layer, epochs=epoch_num, \\\n",
    "                                                                               dropout = dropout, lr = lr, weight_decay = weight_decay, mini_epoch_num = mini_epoch_num, \\\n",
    "                                                                               valid_part_num = valid_part_num, train_part_num = train_part_num, test_part_num = test_part_num)\n",
    "    \n",
    "    draw_Cluster_train_valid_batch = draw_trainer_info(data_name, Cluster_train_valid_batch_trainer, image_path, 'train_valid_batch_' + comments)\n",
    "    draw_Cluster_train_valid_batch.draw_ave_loss_per_node()\n",
    "    \n",
    "\n",
    "''' Draw the information about the GCN calculating batch size '''\n",
    "def draw_cluster_info(clustering_machine, data_name, img_path, comments = '_cluster_node_distr'):\n",
    "    \"\"\"\n",
    "        Won't call this for mini-batch with no clustering \n",
    "    \"\"\"\n",
    "    cluster_id = clustering_machine.train_clusters    # a list of cluster indices\n",
    "    cluster_datapoints = {'cluster_id': cluster_id,  \\\n",
    "                          'train_batch' : [clustering_machine.info_train_batch_size[idx] for idx in cluster_id], \\\n",
    "                          'cluster_size' : [clustering_machine.info_isolate_cluster_size[idx] for idx in cluster_id], \\\n",
    "                         }\n",
    "                         \n",
    "    df = pd.DataFrame(data=cluster_datapoints, dtype=np.int32)\n",
    "    # print(df)\n",
    "    df_reshape = df.melt('cluster_id', var_name = 'clusters', value_name = 'node_num')\n",
    "    \n",
    "    plt.clf()\n",
    "    plt.figure()\n",
    "    sns.set(style='whitegrid')\n",
    "    g = sns.catplot(x=\"cluster_id\", y=\"node_num\", hue='clusters', kind='bar', data=df_reshape)\n",
    "    g.despine(left=True)\n",
    "    g.fig.suptitle(data_name + comments)\n",
    "    g.set_xlabels(\"Cluster ID\")\n",
    "    g.set_ylabels(\"Number of nodes\")\n",
    "    \n",
    "    img_name = img_path + data_name + comments\n",
    "    os.makedirs(os.path.dirname(img_name), exist_ok=True)\n",
    "    g.savefig(img_name, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specific model run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Execute the testing program '''\n",
    "def set_clustering_machine(data, intermediate_data_folder, test_ratio = 0.05, validation_ratio = 0.85, neigh_layer = 1, train_frac = 1.0, train_part_num = 2):\n",
    "    \"\"\"\n",
    "        Set the batch machine plus generate the training batches\n",
    "            1) data: the target dataset data\n",
    "            2) intermediate_data_folder: path to store the intermediate generated data\n",
    "            3) test_ratio, validation_ratio: data split ratio\n",
    "            4) neigh_layer: number of hops (layers) for the neighbor nodes \n",
    "            5) train_frac: each time including fraction of the neigbor nodes in each layer\n",
    "            6) valid_part_num, train_part_num, test_part_num :  batch number for validation, train and test data correspondingly\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set the clustering information storing path\n",
    "    clustering_file_folder = intermediate_data_folder + 'clustering/'\n",
    "    check_folder_exist(clustering_file_folder)  # if exist then delete\n",
    "    clustering_file_name = clustering_file_folder + 'clustering_machine.txt'\n",
    "    os.makedirs(os.path.dirname(clustering_file_name), exist_ok=True)\n",
    "    \n",
    "    # if we use the random assignment of the code, then filtering out the isolated data may not be necessary\n",
    "#     connect_edge_index, connect_features, connect_label = filter_out_isolate(data.edge_index, data.x, data.y)\n",
    "#     clustering_machine = ClusteringMachine(connect_edge_index, connect_features, connect_label)\n",
    "    print('\\n' + '=' * 100)\n",
    "    print('Start to generate the clustering machine:')\n",
    "    t0 = time.time()\n",
    "    clustering_machine = ClusteringMachine(data.edge_index, data.x, data.y)\n",
    "    batch_machine_create = time.time() - t0\n",
    "    print('Batch machine creation costs a total of {0:.4f} seconds!'.format(batch_machine_create))\n",
    "    \n",
    "    # start to generate the edge weights\n",
    "    print('Start to generate the global edge weights')\n",
    "    t11 = time.time()\n",
    "    clustering_machine.get_edge_weight(clustering_machine.edge_index, clustering_machine.node_count, clustering_file = clustering_file_folder) \n",
    "    edge_weight_create = time.time() - t11\n",
    "    print('Edge weights creation costs a total of {0:.4f} seconds!'.format(edge_weight_create))\n",
    "    \n",
    "#     clustering_machine.split_cluster_nodes_edges(test_ratio, validation_ratio, partition_num = train_part_num)\n",
    "    # mini-batch only: split to train test valid before clustering\n",
    "    print('Start to split data into train, test, validation:')\n",
    "    t1 = time.time()\n",
    "    clustering_machine.split_whole_nodes_edges_then_cluster(test_ratio, validation_ratio)\n",
    "    data_split_time = time.time() - t1\n",
    "    print('Data splitting costs a total of {0:.4f} seconds!'.format(data_split_time))\n",
    "    \n",
    "    # generate mini-batches\n",
    "    mini_batch_folder = intermediate_data_folder + 'mini_batch_files/'\n",
    "    check_folder_exist(mini_batch_folder)  # if exist then delete\n",
    "    print('Start to generate the training batches:')\n",
    "    t2 = time.time()\n",
    "    clustering_machine.mini_batch_train_clustering(mini_batch_folder, neigh_layer, fraction = train_frac, train_batch_num = train_part_num)\n",
    "    train_batch_production_time = time.time() - t2\n",
    "    print('Train batches production costs a total of {0:.4f} seconds!'.format(train_batch_production_time))\n",
    "    \n",
    "    \n",
    "    \n",
    "    print('Start to store the batch machine file:')\n",
    "    t3 = time.time()\n",
    "    with open(clustering_file_name, \"wb\") as fp:\n",
    "        pickle.dump(clustering_machine, fp)\n",
    "    batch_machine_store_time = time.time() - t3\n",
    "    print('Storing batch machine after training batches generation costs a total of {0:.4f} seconds!'.format(batch_machine_store_time))\n",
    "    print('\\n' + '=' * 100)\n",
    "    \n",
    "    return mini_batch_folder\n",
    "    \n",
    "\n",
    "def set_clustering_machine_validation_batch(intermediate_data_folder, neigh_layer = 1, valid_part_num = 1):\n",
    "    \"\"\"\n",
    "        Generate the validation batches\n",
    "    \"\"\"\n",
    "    \n",
    "    clustering_file_folder = intermediate_data_folder + 'clustering/'\n",
    "    clustering_file_name = clustering_file_folder + 'clustering_machine.txt'\n",
    "    print('\\n' + '=' * 100)\n",
    "    \n",
    "    t0 = time.time()\n",
    "    with open(clustering_file_name, \"rb\") as fp:\n",
    "        clustering_machine = pickle.load(fp)\n",
    "    batch_machine_read = time.time() - t0\n",
    "    print('Batch machine reading costs a total of {0:.4f} seconds!'.format(batch_machine_read))\n",
    "    \n",
    "    print('Start to generate the validation batches:')\n",
    "    t1 = time.time()\n",
    "    mini_batch_folder = intermediate_data_folder + 'mini_batch_files/'\n",
    "    # for validation , fraction has to be 1.0 so that to include the information form original graph\n",
    "    clustering_machine.mini_batch_validation_clustering(mini_batch_folder, neigh_layer, fraction = 1.0, valid_batch_num = valid_part_num)\n",
    "    validation_batch_production_time = time.time() - t1\n",
    "    print('Validation batches production costs a total of {0:.4f} seconds!'.format(validation_batch_production_time))\n",
    "    print('=' * 100)\n",
    "    \n",
    "    # can off-line load the clustering model with train-batch generated\n",
    "#     with open(clustering_file_name, \"rb\") as fp:\n",
    "#         clustering_machine = pickle.load(fp)\n",
    "\n",
    "    return mini_batch_folder\n",
    "\n",
    "def Cluster_train_valid_batch_run(mini_batch_folder, data_name, dataset, image_path, input_layer = [16, 16], epochs=300, \\\n",
    "                           dropout = 0.3, lr = 0.01, weight_decay = 0.01, mini_epoch_num = 5, \\\n",
    "                                 valid_part_num = 2, train_part_num = 2, test_part_num = 1):\n",
    "    \"\"\"\n",
    "    # Run the mini-batch model (train and validate both in batches)\n",
    "    Tuning parameters:  dropout, lr (learning rate), weight_decay: l2 regularization\n",
    "    return: validation accuracy value, validation F-1 value, time_training (ms), time_data_load (ms)\n",
    "    \"\"\"\n",
    "#     gcn_trainer_batch = ClusterGCNTrainer_mini_Train(mini_batch_folder, 2, 2, 2, 2, 2, input_layers = [16], dropout=0.3)\n",
    "    print('\\n' + '=' * 100)\n",
    "    print('Start generate the trainer:')\n",
    "    t0 = time.time()\n",
    "    gcn_trainer = ClusterGCNTrainer_mini_Train(mini_batch_folder, dataset.num_node_features, dataset.num_classes, input_layers = input_layer, dropout = dropout)\n",
    "    train_create = time.time() - t0\n",
    "    print('Trainer creation costs a total of {0:.4f} seconds!'.format(train_create))\n",
    "    \n",
    "    print('Start train the model:')\n",
    "    t1 = time.time()\n",
    "    gcn_trainer.train(epoch_num=epochs, learning_rate=lr, weight_decay=weight_decay, mini_epoch_num = mini_epoch_num, train_batch_num = train_part_num)\n",
    "    train_period = time.time() - t1\n",
    "    print('Training costs a total of {0:.4f} seconds!'.format(train_period))\n",
    "    print('-' * 80)\n",
    "    \n",
    "    print('Start validate the model:')\n",
    "    t2 = time.time()\n",
    "    validation_F1, validation_accuracy = gcn_trainer.batch_validate(valid_batch_num = valid_part_num)\n",
    "    validation_period = time.time() - t2\n",
    "    print('Validatoin costs a total of {0:.4f} seconds!'.format(validation_period))\n",
    "    print('Finish train and validate the model:')\n",
    "    print('=' * 100)\n",
    "    time_train_total = gcn_trainer.time_train_total\n",
    "    time_data_load = gcn_trainer.time_train_load_data\n",
    "    return validation_accuracy, validation_F1, time_train_total, time_data_load, gcn_trainer\n",
    "\n",
    "\n",
    "def Cluster_train_valid_batch_investigate(mini_batch_folder, data_name, dataset, image_path, input_layer = [16, 16], epochs=300, \\\n",
    "                           dropout = 0.3, lr = 0.01, weight_decay = 0.01, mini_epoch_num = 5, output_period = 10, \n",
    "                                         valid_part_num = 2, train_part_num = 2, test_part_num = 1):\n",
    "    \"\"\"\n",
    "        *** dynamically investigate the F1 score in the middle of the training after certain period ***\n",
    "        output: two dict containing F1-score and accuracy of a certain epoch index\n",
    "    \"\"\"\n",
    "    print('\\n' + '=' * 100)\n",
    "    print('Start generate the trainer:')\n",
    "    t0 = time.time()\n",
    "    gcn_trainer = ClusterGCNTrainer_mini_Train(mini_batch_folder, dataset.num_node_features, dataset.num_classes, input_layers = input_layer, dropout = dropout)\n",
    "    train_create = time.time() - t0\n",
    "    print('Trainer creation costs a total of {0:.4f} seconds!'.format(train_create))\n",
    "    \n",
    "    print('Start train the model:')\n",
    "    t1 = time.time()\n",
    "    Train_period_F1, Train_period_accuracy = gcn_trainer.train_investigate_F1(epoch_num=epochs, learning_rate=lr, weight_decay=weight_decay, mini_epoch_num = mini_epoch_num, \\\n",
    "                                                            output_period = output_period, train_batch_num = train_part_num, valid_batch_num = valid_part_num)\n",
    "    train_period = time.time() - t1\n",
    "    print('In-process Training costs a total of {0:.4f} seconds!'.format(train_period))\n",
    "    print('Finish train and validate the model:')\n",
    "    print('=' * 100)\n",
    "    \n",
    "    return Train_period_F1, Train_period_accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test and compare different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_one(mini_batch_folder, image_path, repeate_time = 5, input_layer = [32], epoch_num = 300, \\\n",
    "                dropout = 0.3, lr = 0.0001, weight_decay = 0.01, mini_epoch_num = 5, \\\n",
    "                valid_part_num = 2, train_part_num = 2, test_part_num = 1):\n",
    "    \"\"\"\n",
    "        return all test-F1 and validation-F1 for all four models\n",
    "    \"\"\"\n",
    "    validation_accuracy = {}\n",
    "    validation_f1 = {}\n",
    "    time_total_train = {}\n",
    "    time_data_load = {}\n",
    "    \n",
    "    # Each graph model corresponds to one function below\n",
    "#     graph_model = ['batch_valid', 'train_batch', 'whole_graph', 'isolate']\n",
    "    graph_model = ['batch_valid']\n",
    "    for i in range(repeate_time):\n",
    "        model_res = []\n",
    "        \n",
    "        model_res.append(Cluster_train_valid_batch_run(mini_batch_folder, data_name, dataset, image_path, input_layer = input_layer, epochs=epoch_num, \\\n",
    "                                                         dropout = dropout, lr = lr, weight_decay = weight_decay, mini_epoch_num = mini_epoch_num, \\\n",
    "                                                      valid_part_num = valid_part_num, train_part_num = train_part_num, test_part_num = test_part_num)[:4])\n",
    "        \n",
    "#         model_res.append(Isolate_clustering_run(clustering_machine, data_name, dataset, image_path, input_layer = input_layer, epochs=epoch_num, neigh_layer = layer_num, frac = frac, \\\n",
    "#                                                         dropout = dropout, lr = lr, weight_decay = weight_decay)[:4])\n",
    "        \n",
    "        validation_accuracy[i], validation_f1[i], time_total_train[i], time_data_load[i] = zip(*model_res)\n",
    "    return graph_model, validation_accuracy, validation_f1, time_total_train, time_data_load\n",
    "\n",
    "def store_data_multi_tests(f1_data, data_name, graph_model, img_path, comments):\n",
    "    run_id = sorted(f1_data.keys())\n",
    "    run_data = {'run_id': run_id}\n",
    "    \n",
    "    run_data.update({model_name : [f1_data[key][idx] for key in run_id] for idx, model_name in enumerate(graph_model)})\n",
    "    \n",
    "    pickle_filename = img_path + data_name + '_' + comments + '.pkl'\n",
    "    os.makedirs(os.path.dirname(pickle_filename), exist_ok=True)\n",
    "    df = pd.DataFrame(data=run_data, dtype=np.int32)\n",
    "    df.to_pickle(pickle_filename)\n",
    "    return pickle_filename\n",
    "\n",
    "def draw_data_multi_tests(pickle_filename, data_name, comments, xlabel, ylabel):\n",
    "    df = pd.read_pickle(pickle_filename)\n",
    "    df_reshape = df.melt('run_id', var_name = 'model', value_name = ylabel)\n",
    "\n",
    "    plt.clf()\n",
    "    plt.figure()\n",
    "    sns.set(style='whitegrid')\n",
    "    g = sns.catplot(x=\"model\", y=ylabel, kind='box', data=df_reshape)\n",
    "    g.despine(left=True)\n",
    "    g.fig.suptitle(data_name + ' ' + ylabel + ' ' + comments)\n",
    "    g.set_xlabels(xlabel)\n",
    "    g.set_ylabels(ylabel)\n",
    "\n",
    "    img_name = pickle_filename[:-4] + '_img'\n",
    "    os.makedirs(os.path.dirname(img_name), exist_ok=True)\n",
    "    plt.savefig(img_name, bbox_inches='tight')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate performance in the middle of the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_investigate(mini_batch_folder, image_path, repeate_time = 5, input_layer = [32], epoch_num = 300, \\\n",
    "                        dropout = 0.3, lr = 0.0001, weight_decay = 0.01, mini_epoch_num = 5, output_period = 10, \\\n",
    "                         valid_part_num = 2, train_part_num = 2, test_part_num = 1):\n",
    "    \"\"\"\n",
    "        return all test-F1 and validation-F1 for all four models\n",
    "    \"\"\"\n",
    "    \n",
    "    Train_peroid_f1 = {}\n",
    "    Train_peroid_accuracy = {}\n",
    "    \n",
    "    for i in range(repeate_time):\n",
    "        Train_peroid_f1[i], Train_peroid_accuracy[i] = Cluster_train_valid_batch_investigate(mini_batch_folder, data_name, dataset, image_path, input_layer = input_layer, epochs=epoch_num, \\\n",
    "                                            dropout = dropout, lr = lr, weight_decay = weight_decay, mini_epoch_num = mini_epoch_num, output_period = output_period, \\\n",
    "                                                                    valid_part_num = valid_part_num, train_part_num = train_part_num, test_part_num = test_part_num)\n",
    "        \n",
    "    return Train_peroid_f1, Train_peroid_accuracy\n",
    "\n",
    "def store_data_multi_investigate(investigate_res, data_name, res_name, img_path, comments):\n",
    "    \"\"\"\n",
    "        investigate_res: currently either F1-score or accuracy a dict {epoch num : value}\n",
    "    \"\"\"\n",
    "    run_id = sorted(investigate_res.keys())\n",
    "    run_data = {'run_id': run_id}\n",
    "    \n",
    "    epoch_num_range = sorted(investigate_res[0].keys())  # at least one entry exists inside the dictionary and the epoch range is fixed\n",
    "    run_data.update({epoch_num : [investigate_res[key][epoch_num] for key in run_id] for epoch_num in epoch_num_range})\n",
    "    \n",
    "    pickle_filename = img_path + data_name + '_' + res_name + '_' + comments + '.pkl'\n",
    "    os.makedirs(os.path.dirname(pickle_filename), exist_ok=True)\n",
    "    df = pd.DataFrame(data=run_data, dtype=np.int32)\n",
    "    df.to_pickle(pickle_filename)\n",
    "    return pickle_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"To test one single model for different parameter values\"\"\"\n",
    "def execute_tuning(tune_params, mini_batch_folder, image_path, repeate_time = 7, input_layer = [32], epoch_num = 400, \\\n",
    "                  dropout = 0.1, lr = 0.0001, weight_decay = 0.1, mini_epoch_num = 20, \\\n",
    "                  valid_part_num = 2, train_part_num = 2, test_part_num = 1):\n",
    "    \"\"\"\n",
    "        Tune all the hyperparameters\n",
    "        1) learning rate\n",
    "        2) dropout\n",
    "        3) layer unit number\n",
    "        4) weight decay\n",
    "    \"\"\"\n",
    "    validation_accuracy = {}\n",
    "    validation_f1 = {}\n",
    "    time_total_train = {}\n",
    "    time_data_load = {}\n",
    "    \n",
    "    res = [{tune_val : Cluster_train_valid_batch_run(mini_batch_folder, data_name, dataset, image_path, input_layer = input_layer, epochs=epoch_num, \\\n",
    "            dropout = dropout, lr = lr, weight_decay = weight_decay, mini_epoch_num = tune_val, \\\n",
    "            valid_part_num = valid_part_num, train_part_num = train_part_num, test_part_num = test_part_num)[:4] for tune_val in tune_params} for i in range(repeate_time)]\n",
    "    \n",
    "    for i, ref in enumerate(res):\n",
    "        validation_accuracy[i] = {tune_val : res_lst[0] for tune_val, res_lst in ref.items()}\n",
    "        validation_f1[i] = {tune_val : res_lst[1] for tune_val, res_lst in ref.items()}\n",
    "        time_total_train[i] = {tune_val : res_lst[2] for tune_val, res_lst in ref.items()}\n",
    "        time_data_load[i] = {tune_val : res_lst[3] for tune_val, res_lst in ref.items()}\n",
    "        \n",
    "    return validation_accuracy, validation_f1, time_total_train, time_data_load\n",
    "\n",
    "def store_data_multi_tuning(tune_params, target, data_name, img_path, comments):\n",
    "    \"\"\"\n",
    "        tune_params: is the tuning parameter list\n",
    "        target: is the result, here should be F1-score, accuraycy, load time, train time\n",
    "    \"\"\"\n",
    "    run_ids = sorted(target.keys())   # key is the run_id\n",
    "    run_data = {'run_id': run_ids}\n",
    "    # the key can be converted to string or not: i.e. str(tune_val)\n",
    "    # here we keep it as integer such that we want it to follow order\n",
    "    tmp = {tune_val : [target[run_id][tune_val] for run_id in run_ids] for tune_val in tune_params}  # the value is list\n",
    "    run_data.update(tmp)\n",
    "    \n",
    "    pickle_filename = img_path + data_name + '_' + comments + '.pkl'\n",
    "    os.makedirs(os.path.dirname(pickle_filename), exist_ok=True)\n",
    "    df = pd.DataFrame(data=run_data, dtype=np.int32)\n",
    "    df.to_pickle(pickle_filename)\n",
    "    return pickle_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-test execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_train_loss(data, data_name, dataset, image_data_path, intermediate_data_path, partition_nums, layers, \\\n",
    "                      dropout = 0.1, lr = 0.0001, weight_decay = 0.1, mini_epoch_num = 20, valid_part_num = 2):\n",
    "    for partn in partition_nums:\n",
    "        for GCN_layer in layers:\n",
    "            net_layer = len(GCN_layer) + 1\n",
    "            hop_layer = net_layer\n",
    "            print('Start checking train loss for partition num: ' + str(partn) + ' hop layer: ' + str(hop_layer))\n",
    "            img_path = image_data_path + 'cluster_num_' + str(partn) + '/' + 'net_layer_' + str(net_layer) + '_hop_layer_' + str(hop_layer) + '/'\n",
    "            intermediate_data_folder = intermediate_data_path + 'cluster_num_' + str(partn) + '/' + 'net_layer_' + str(net_layer) + '_hop_layer_' + str(hop_layer) + '/'\n",
    "            \n",
    "            # set the batch for validation and train\n",
    "            mini_batch_folder = set_clustering_machine(data, intermediate_data_folder, test_ratio = 0.05, validation_ratio = 0.85, \\\n",
    "                                                       neigh_layer = hop_layer, train_frac = 1.0, train_part_num = partn)\n",
    "            set_clustering_machine_validation_batch(intermediate_data_folder, neigh_layer = hop_layer, valid_part_num = valid_part_num)\n",
    "            \n",
    "            check_train_loss_converge(mini_batch_folder, data_name, dataset, img_path, 'part_num_' + str(partn), input_layer = GCN_layer, epoch_num = 400, \\\n",
    "                                     dropout = dropout, lr = lr, weight_decay = weight_decay, mini_epoch_num = mini_epoch_num, \n",
    "                                     valid_part_num = valid_part_num, train_part_num = partn, test_part_num = 1)\n",
    "            \n",
    "#             # for the large dataset and split first case, the cluster info cannot be generated\n",
    "#             clustering_machine.mini_batch_train_clustering(hop_layer)\n",
    "#             draw_cluster_info(clustering_machine, data_name, img_path, comments = '_cluster_node_distr_' + str(hop_layer) + '_hops')\n",
    "            \n",
    "def output_F1_score(data, data_name, dataset, image_data_path, intermediate_data_path, partition_nums, layers, \\\n",
    "                    dropout = 0.1, lr = 0.0001, weight_decay = 0.1, mini_epoch_num = 20, valid_part_num = 2):            \n",
    "    for partn in partition_nums:\n",
    "        for GCN_layer in layers:\n",
    "            net_layer = len(GCN_layer) + 1\n",
    "            hop_layer = net_layer\n",
    "            \n",
    "            # set the save path\n",
    "            print('Start running for partition num: ' + str(partn) + ' hop layer ' + str(hop_layer))\n",
    "            img_path = image_data_path + 'cluster_num_' + str(partn) + '/' + 'net_layer_' + str(net_layer) + '_hop_layer_' + str(hop_layer) + '/'\n",
    "            intermediate_data_folder = intermediate_data_path + 'cluster_num_' + str(partn) + '/' + 'net_layer_' + str(net_layer) + '_hop_layer_' + str(hop_layer) + '/'\n",
    "            \n",
    "            # set the batch for validation and train\n",
    "            mini_batch_folder = set_clustering_machine(data, intermediate_data_folder, test_ratio = 0.05, validation_ratio = 0.85, \\\n",
    "                                                       neigh_layer = hop_layer, train_frac = 1.0, train_part_num = partn)\n",
    "            set_clustering_machine_validation_batch(intermediate_data_folder, neigh_layer = hop_layer, valid_part_num = valid_part_num)\n",
    "            \n",
    "            # start to run the model, train and validation \n",
    "            graph_model, validation_accuracy, validation_f1, time_total_train, time_data_load = \\\n",
    "                execute_one(mini_batch_folder, img_path, repeate_time = 7, input_layer = GCN_layer, epoch_num = 400, \n",
    "                                            dropout = dropout, lr = lr, weight_decay = weight_decay, mini_epoch_num = mini_epoch_num, \\\n",
    "                                             valid_part_num = valid_part_num, train_part_num = partn, test_part_num = 1)\n",
    "            \n",
    "            \n",
    "            validation_accuracy = store_data_multi_tests(validation_accuracy, data_name, graph_model, img_path, 'test_cluster_num_' + str(partn) + '_hops_' + str(hop_layer))\n",
    "            draw_data_multi_tests(validation_accuracy, data_name, 'vali_cluster_num_' + str(partn) + '_hop_' + str(hop_layer), 'models', 'Accuracy')\n",
    "\n",
    "            validation_f1 = store_data_multi_tests(validation_f1, data_name, graph_model, img_path, 'validation_cluster_num_' + str(partn) + '_hops_' + str(hop_layer))\n",
    "            draw_data_multi_tests(validation_f1, data_name, 'vali_cluster_num_' + str(partn) + '_hop_' + str(hop_layer), 'models', 'F1 score')\n",
    "\n",
    "            time_train = store_data_multi_tests(time_total_train, data_name, graph_model, img_path, 'train_time_cluster_num_' + str(partn) + '_hops_' + str(hop_layer))\n",
    "            draw_data_multi_tests(time_train, data_name, 'train_time_cluster_num_' + str(partn) + '_hop_' + str(hop_layer), 'models', 'Train Time (ms)')\n",
    "\n",
    "            time_load = store_data_multi_tests(time_data_load, data_name, graph_model, img_path, 'load_time_cluster_num_' + str(partn) + '_hops_' + str(hop_layer))\n",
    "            draw_data_multi_tests(time_load, data_name, 'load_time_cluster_num_' + str(partn) + '_hop_' + str(hop_layer), 'models', 'Load Time (ms)')\n",
    "\n",
    "def output_train_investigate(data, data_name, dataset, image_data_path, intermediate_data_path, partition_nums, layers, \\\n",
    "                             dropout = 0.1, lr = 0.0001, weight_decay = 0.1, mini_epoch_num = 20, output_period = 40, valid_part_num = 2):            \n",
    "    for partn in partition_nums:\n",
    "        for GCN_layer in layers:\n",
    "            net_layer = len(GCN_layer) + 1\n",
    "            hop_layer = net_layer\n",
    "            # set the save path\n",
    "            print('Start running for partition num: ' + str(partn) + ' hop layer ' + str(hop_layer))\n",
    "            img_path = image_data_path + 'cluster_num_' + str(partn) + '/' + 'net_layer_' + str(net_layer) + '_hop_layer_' + str(hop_layer) + '/'\n",
    "            intermediate_data_folder = intermediate_data_path + 'cluster_num_' + str(partn) + '/' + 'net_layer_' + str(net_layer) + '_hop_layer_' + str(hop_layer) + '/'\n",
    "            \n",
    "            # set the batch for validation and train\n",
    "            mini_batch_folder = set_clustering_machine(data, intermediate_data_folder, test_ratio = 0.05, validation_ratio = 0.85, \\\n",
    "                                                       neigh_layer = hop_layer, train_frac = 1.0, train_part_num = partn)\n",
    "            set_clustering_machine_validation_batch(intermediate_data_folder, neigh_layer = hop_layer, valid_part_num = valid_part_num)\n",
    "\n",
    "            Train_peroid_f1, Train_peroid_accuracy = execute_investigate(mini_batch_folder, img_path, repeate_time = 7, input_layer = GCN_layer, epoch_num = 400, \\\n",
    "                                            dropout = dropout, lr = lr, weight_decay = weight_decay, mini_epoch_num = mini_epoch_num, output_period = output_period, \\\n",
    "                                            valid_part_num = valid_part_num, train_part_num = partn, test_part_num = 1)\n",
    "            \n",
    "            Train_peroid_f1 = store_data_multi_investigate(Train_peroid_f1, data_name, 'F1_score', img_path, 'invest_batch_num_' + str(partn) + '_hops_' + str(hop_layer))\n",
    "            draw_data_multi_tests(Train_peroid_f1, data_name, 'Train_process_batch_num_' + str(partn) + '_hop_' + str(hop_layer), 'epoch number', 'F1 score')\n",
    "\n",
    "            Train_peroid_accuracy = store_data_multi_investigate(Train_peroid_accuracy, data_name, 'Accuracy', img_path, 'invest_batch_num_' + str(partn) + '_hops_' + str(hop_layer))\n",
    "            draw_data_multi_tests(Train_peroid_accuracy, data_name, 'Train_process_batch_num_' + str(partn) + '_hop_' + str(hop_layer), 'epoch number', 'Accuracy')\n",
    "            \n",
    "            \n",
    "            \n",
    "def output_tune_param(data, data_name, dataset, image_data_path, intermediate_data_path, partition_nums, layers, dropout = 0.1, lr = 0.0001, weight_decay = 0.1, mini_epoch_num = 20, valid_part_num = 2):\n",
    "    for partn in partition_nums:\n",
    "        for GCN_layer in layers:\n",
    "            net_layer = len(GCN_layer) + 1\n",
    "            hop_layer = net_layer\n",
    "            # Set the tune parameters and name\n",
    "            tune_name = 'batch_epoch_num'\n",
    "            tune_params = [400, 200, 100, 50, 20, 10, 5]\n",
    "#             tune_name = 'weight_decay'\n",
    "#             tune_params = [0.0001, 0.001, 0.01, 0.1]\n",
    "\n",
    "            img_path = image_data_path + 'cluster_num_' + str(partn) + '/' + 'net_layer_' + str(net_layer) + '_hop_layer_' + str(hop_layer) + '/' + 'tune_' + tune_name + '/'\n",
    "            intermediate_data_folder = intermediate_data_path + 'cluster_num_' + str(partn) + '/' + 'net_layer_' + str(net_layer) + '_hop_layer_' + str(hop_layer) + '/' + 'tune_' + tune_name + '/'\n",
    "            print('Start tuning for tuning param: ' + tune_name + ' partition num: ' + str(partn) + ' hop layer ' + str(hop_layer))\n",
    "            \n",
    "            # set the batch for validation and train\n",
    "            mini_batch_folder = set_clustering_machine(data, intermediate_data_folder, test_ratio = 0.05, validation_ratio = 0.85, \\\n",
    "                                                       neigh_layer = hop_layer, train_frac = 1.0, train_part_num = partn)\n",
    "            set_clustering_machine_validation_batch(intermediate_data_folder, neigh_layer = hop_layer, valid_part_num = valid_part_num)\n",
    "\n",
    "            validation_accuracy, validation_f1, time_total_train, time_data_load = execute_tuning(tune_params, mini_batch_folder, img_path, repeate_time = 7, \\\n",
    "                                                input_layer = GCN_layer, epoch_num = 400, dropout = dropout, lr = lr, weight_decay = weight_decay, mini_epoch_num = mini_epoch_num, \\\n",
    "                                                valid_part_num = valid_part_num, train_part_num = partn, test_part_num = 1)\n",
    "\n",
    "            validation_accuracy = store_data_multi_tuning(tune_params,validation_accuracy, data_name, img_path, 'accuracy_cluster_num_' + str(partn) + '_hops_' + str(hop_layer))\n",
    "            draw_data_multi_tests(validation_accuracy, data_name, 'vali_cluster_num_' + str(partn) + '_hop_' + str(hop_layer), 'epochs_per_batch', 'Accuracy')\n",
    "\n",
    "            validation_f1 = store_data_multi_tuning(tune_params, validation_f1, data_name, img_path, 'validation_cluster_num_' + str(partn) + '_hops_' + str(hop_layer))\n",
    "            draw_data_multi_tests(validation_f1, data_name, 'vali_cluster_num_' + str(partn) + '_hop_' + str(hop_layer), 'epochs_per_batch', 'F1 score')\n",
    "\n",
    "            time_train = store_data_multi_tuning(tune_params, time_total_train, data_name, img_path, 'train_time_cluster_num_' + str(partn) + '_hops_' + str(hop_layer))\n",
    "            draw_data_multi_tests(time_train, data_name, 'train_time_cluster_num_' + str(partn) + '_hop_' + str(hop_layer), 'epochs_per_batch', 'Train Time (ms)')\n",
    "\n",
    "            time_load = store_data_multi_tuning(tune_params, time_data_load, data_name, img_path, 'load_time_cluster_num_' + str(partn) + '_hops_' + str(hop_layer))\n",
    "            draw_data_multi_tests(time_load, data_name, 'load_time_cluster_num_' + str(partn) + '_hop_' + str(hop_layer), 'epochs_per_batch', 'Load Time (ms)')\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use data from pytorch geometric datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_data_root = '/media/xiangli/storage1/projects/tmpdata/'\n",
    "test_folder_name = 'Use_array_size_comparison/train_10%_full_neigh_random_check_graphF1/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cora dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "data_name = 'Cora'\n",
    "dataset = Planetoid(root = local_data_root + 'Planetoid/Cora', name=data_name)\n",
    "data = dataset[0]\n",
    "image_data_path = './results/' + data_name + '/' + test_folder_name\n",
    "intermediate_data_folder = './intermediate_data/' + data_name + '/' + test_folder_name\n",
    "\n",
    "partition_nums = [2]\n",
    "layers = [[32]]\n",
    "tune_lr = 0.0001\n",
    "check_mini_epoch = 20\n",
    "# partition_nums = [2, 4, 8]\n",
    "# layers = [[], [32], [32, 32], [32, 32, 32]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CoraFull dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of data:  1\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import CoraFull\n",
    "data_name = 'CoraFull'\n",
    "dataset = CoraFull(root = local_data_root + 'CoralFull')\n",
    "print('number of data: ', len(dataset))\n",
    "data = dataset[0]\n",
    "\n",
    "image_data_path = './results/' + data_name + '/' + test_folder_name\n",
    "intermediate_data_folder = './intermediate_data/' + data_name + '/' + test_folder_name\n",
    "\n",
    "partition_nums = [4]\n",
    "layers = [[128, 128]]\n",
    "tune_lr = 0.0001\n",
    "check_mini_epoch = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check generation of the batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start running for partition num: 4 hop layer 3\n",
      "\n",
      "====================================================================================================\n",
      "Start to generate the clustering machine:\n",
      "Batch machine creation costs a total of 0.0006 seconds!\n",
      "Start to generate the global edge weights\n",
      "Edge weights creation costs a total of 2.9930 seconds!\n",
      "Start to split data into train, test, validation:\n",
      "Data splitting costs a total of 0.0040 seconds!\n",
      "Start to generate the training batches:\n",
      "*** Generate train batch file for #   0 batch, writing the batch file costed 410.01 ms ***\n",
      "*** Generate train batch file for #   1 batch, writing the batch file costed 1306.98 ms ***\n",
      "*** Generate train batch file for #   2 batch, writing the batch file costed 3407.73 ms ***\n",
      "*** Generate train batch file for #   3 batch, writing the batch file costed 5679.44 ms ***\n",
      "Train batches production costs a total of 12.8563 seconds!\n",
      "Start to store the batch machine file:\n",
      "Storing batch machine after training batches generation costs a total of 9.6199 seconds!\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.8458 seconds!\n",
      "Start to generate the validation batches:\n",
      "*** Generate validation batch file for #   0 batch, writing the batch file costed 7485.01 ms ***\n",
      "*** Generate validation batch file for #   1 batch, writing the batch file costed 7024.04 ms ***\n",
      "Validation batches production costs a total of 15.6539 seconds!\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "def generate_train_batch(data, data_name, dataset, image_data_path, intermediate_data_path, partition_nums, layers, \\\n",
    "                    dropout = 0.1, lr = 0.0001, weight_decay = 0.1, mini_epoch_num = 20, valid_part_num = 2):            \n",
    "    for partn in partition_nums:\n",
    "        for GCN_layer in layers:\n",
    "            net_layer = len(GCN_layer) + 1\n",
    "            hop_layer = net_layer\n",
    "            \n",
    "            # set the save path\n",
    "            print('Start running for partition num: ' + str(partn) + ' hop layer ' + str(hop_layer))\n",
    "            img_path = image_data_path + 'cluster_num_' + str(partn) + '/' + 'net_layer_' + str(net_layer) + '_hop_layer_' + str(hop_layer) + '/'\n",
    "            intermediate_data_folder = intermediate_data_path + 'cluster_num_' + str(partn) + '/' + 'net_layer_' + str(net_layer) + '_hop_layer_' + str(hop_layer) + '/'\n",
    "            \n",
    "            # set the batch for validation and train\n",
    "            mini_batch_folder = set_clustering_machine(data, intermediate_data_folder, test_ratio = 0.05, validation_ratio = 0.85, \\\n",
    "                                                       neigh_layer = hop_layer, train_frac = 1.0, train_part_num = partn)\n",
    "            \n",
    "            set_clustering_machine_validation_batch(intermediate_data_folder, neigh_layer = hop_layer, valid_part_num = valid_part_num)\n",
    "\n",
    "generate_train_batch(data, data_name, dataset, image_data_path, intermediate_data_folder, partition_nums, layers, \\\n",
    "                dropout = 0.1, lr = tune_lr, weight_decay = 0.1, mini_epoch_num = check_mini_epoch, valid_part_num = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickle save Output_F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start running for partition num: 4 hop layer 3\n",
      "\n",
      "====================================================================================================\n",
      "Start to generate the clustering machine:\n",
      "Batch machine creation costs a total of 0.0005 seconds!\n",
      "Start to generate the global edge weights\n",
      "Edge weights creation costs a total of 3.0279 seconds!\n",
      "Start to split data into train, test, validation:\n",
      "Data splitting costs a total of 0.0033 seconds!\n",
      "Start to generate the training batches:\n",
      "*** Generate train batch file for #   0 batch, writing the batch file costed 2119.35 ms ***\n",
      "*** Generate train batch file for #   1 batch, writing the batch file costed 5209.54 ms ***\n",
      "*** Generate train batch file for #   2 batch, writing the batch file costed 6444.48 ms ***\n",
      "*** Generate train batch file for #   3 batch, writing the batch file costed 6364.36 ms ***\n",
      "Train batches production costs a total of 22.1461 seconds!\n",
      "Start to store the batch machine file:\n",
      "Storing batch machine after training batches generation costs a total of 8.1329 seconds!\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.8451 seconds!\n",
      "Start to generate the validation batches:\n",
      "*** Generate validation batch file for #   0 batch, writing the batch file costed 5881.54 ms ***\n",
      "*** Generate validation batch file for #   1 batch, writing the batch file costed 7105.31 ms ***\n",
      "Validation batches production costs a total of 14.1140 seconds!\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0156 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   0 batch, reading batch file costed 13722.38 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 551.17 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 6484.86 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 6635.12 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 545.27 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 6528.57 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 543.90 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 532.58 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 548.78 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 532.79 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 546.40 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 531.40 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 546.55 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 536.73 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 552.27 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 537.66 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 539.28 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 547.63 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 540.46 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 549.23 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 547.22 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 545.13 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 537.45 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 541.94 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 540.78 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 541.25 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 538.29 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 540.44 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 542.16 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 551.19 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 554.44 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 542.23 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 549.98 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 542.18 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 552.35 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 540.01 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 542.43 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 569.36 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 558.44 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 565.15 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 560.67 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 534.94 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 555.63 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 551.62 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 564.55 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 563.71 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 554.79 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 571.66 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 537.39 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 546.90 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 545.91 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 547.75 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 546.45 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 522.52 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 555.20 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 547.69 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 548.88 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 530.36 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 556.34 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 539.89 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 555.84 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 544.64 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 552.42 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 560.76 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 548.85 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 552.36 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 544.17 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 551.61 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 543.17 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 532.13 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 533.86 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 540.28 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 553.25 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 548.00 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 526.42 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 549.67 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 528.56 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 570.45 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 562.45 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 533.24 ms ***\n",
      "Training costs a total of 137.0938 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start validate the model:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** During validation for #   0 batch, reading batch file costed 1855.91 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 633.21 ms ***\n",
      "Validatoin costs a total of 2.6976 seconds!\n",
      "Finish train and validate the model:\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0054 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   2 batch, reading batch file costed 535.83 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 561.13 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 538.68 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 547.28 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 538.37 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 524.31 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 540.31 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 537.01 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 537.10 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 549.38 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 529.90 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 535.50 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 545.45 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 542.21 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 548.71 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 535.19 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 568.35 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 546.19 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 565.01 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 546.23 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 566.53 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 542.94 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 571.03 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 550.02 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 551.01 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 532.79 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 556.66 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 542.16 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 554.04 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 520.66 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 547.25 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 532.72 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 534.42 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 532.74 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 540.76 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 546.49 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 529.30 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 535.86 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 522.41 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 535.81 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 529.82 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 536.08 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 539.21 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 563.37 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 553.83 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 533.68 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 573.67 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 539.52 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 545.60 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 550.45 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 553.95 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 535.70 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 565.99 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 549.20 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 538.98 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 548.82 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 559.30 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 559.43 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 548.36 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 543.73 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 555.42 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 540.59 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 537.46 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 526.83 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 549.49 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 537.26 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 538.23 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 537.62 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 548.11 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 541.18 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 538.55 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 541.75 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 562.31 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 533.93 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 536.92 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 531.77 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 534.72 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 536.71 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 520.92 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 531.24 ms ***\n",
      "Training costs a total of 104.1024 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start validate the model:\n",
      "*** During validation for #   0 batch, reading batch file costed 4145.35 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 671.24 ms ***\n",
      "Validatoin costs a total of 5.0241 seconds!\n",
      "Finish train and validate the model:\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0054 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   1 batch, reading batch file costed 532.30 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 532.09 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 538.63 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 538.47 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 544.70 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 551.83 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 569.08 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 550.42 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 551.12 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 565.65 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 556.97 ms ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** During training for #   2 batch, reading batch file costed 549.27 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 554.38 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 521.58 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 536.97 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 537.76 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 539.12 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 521.04 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 532.74 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 537.13 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 540.65 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 524.62 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 538.19 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 534.44 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 537.56 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 523.09 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 538.26 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 552.86 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 540.53 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 540.76 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 568.22 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 531.40 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 542.41 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 538.50 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 562.85 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 551.04 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 574.23 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 548.04 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 544.76 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 546.44 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 542.89 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 552.66 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 567.40 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 562.37 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 546.20 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 561.99 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 561.89 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 559.92 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 559.85 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 539.78 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 560.14 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 555.92 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 566.48 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 552.52 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 552.40 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 562.89 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 580.28 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 557.14 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 563.34 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 555.81 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 574.14 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 542.46 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 564.10 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 538.21 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 547.62 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 551.33 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 562.65 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 572.70 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 553.55 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 542.19 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 528.47 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 541.38 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 541.33 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 535.80 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 565.33 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 552.60 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 571.81 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 545.51 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 559.20 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 529.82 ms ***\n",
      "Training costs a total of 105.5005 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start validate the model:\n",
      "*** During validation for #   0 batch, reading batch file costed 615.56 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 620.85 ms ***\n",
      "Validatoin costs a total of 1.4375 seconds!\n",
      "Finish train and validate the model:\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0055 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   1 batch, reading batch file costed 540.93 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 543.89 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 541.79 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 548.99 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 560.10 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 550.99 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 548.96 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 562.07 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 571.18 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 553.61 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 573.78 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 558.38 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 552.40 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 543.85 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 537.26 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 568.06 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 563.61 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 541.47 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 553.45 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 521.37 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 539.11 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 536.69 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 559.02 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 553.25 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 541.57 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 541.80 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 548.68 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 535.29 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 533.50 ms ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** During training for #   3 batch, reading batch file costed 539.43 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 552.70 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 528.90 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 545.08 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 533.82 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 553.25 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 543.75 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 532.79 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 536.03 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 537.51 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 522.01 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 526.09 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 535.01 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 531.92 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 532.86 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 531.10 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 535.59 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 536.47 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 522.07 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 524.75 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 530.04 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 539.09 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 535.13 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 539.43 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 531.23 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 526.42 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 535.05 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 525.80 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 557.10 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 543.23 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 559.92 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 530.92 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 553.70 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 546.65 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 535.70 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 539.77 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 539.82 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 546.23 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 529.45 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 532.20 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 535.28 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 543.19 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 537.34 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 540.15 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 523.15 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 531.86 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 535.81 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 539.67 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 523.94 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 533.34 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 537.80 ms ***\n",
      "Training costs a total of 103.7413 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start validate the model:\n",
      "*** During validation for #   0 batch, reading batch file costed 602.30 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 609.90 ms ***\n",
      "Validatoin costs a total of 1.4054 seconds!\n",
      "Finish train and validate the model:\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0054 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   1 batch, reading batch file costed 525.80 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 534.96 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 540.49 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 522.08 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 537.81 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 529.28 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 541.61 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 522.58 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 540.18 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 522.62 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 533.87 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 535.87 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 527.33 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 535.14 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 541.37 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 530.69 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 538.78 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 536.76 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 533.62 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 522.72 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 525.95 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 536.00 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 539.70 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 530.69 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 539.88 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 535.67 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 533.82 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 523.57 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 534.35 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 536.92 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 526.51 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 536.21 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 539.38 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 530.37 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 526.61 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 535.80 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 538.21 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 536.62 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 534.01 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 522.80 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 537.88 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 535.94 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 533.48 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 522.57 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 541.02 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 530.01 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 528.48 ms ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** During training for #   3 batch, reading batch file costed 538.23 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 539.57 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 523.37 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 534.40 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 535.01 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 533.02 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 535.81 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 525.93 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 535.06 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 538.34 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 523.32 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 534.62 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 536.91 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 533.86 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 537.36 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 527.22 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 533.89 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 534.43 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 537.85 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 526.70 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 533.78 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 539.02 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 522.32 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 540.61 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 528.38 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 541.05 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 533.54 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 527.14 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 528.95 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 534.68 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 521.79 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 541.88 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 535.53 ms ***\n",
      "Training costs a total of 101.9751 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start validate the model:\n",
      "*** During validation for #   0 batch, reading batch file costed 604.32 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 609.07 ms ***\n",
      "Validatoin costs a total of 1.4070 seconds!\n",
      "Finish train and validate the model:\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0054 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   1 batch, reading batch file costed 526.29 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 521.71 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 541.37 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 533.40 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 533.41 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 532.56 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 525.68 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 535.14 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 532.83 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 535.33 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 537.58 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 522.48 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 534.23 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 533.96 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 526.76 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 536.59 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 533.58 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 520.96 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 536.77 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 536.17 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 537.19 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 527.88 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 525.62 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 535.27 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 539.21 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 527.83 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 540.16 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 522.27 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 541.09 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 533.42 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 533.91 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 523.19 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 537.07 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 521.02 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 540.23 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 528.06 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 526.35 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 532.73 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 540.22 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 528.01 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 537.86 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 533.11 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 540.51 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 521.08 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 542.80 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 534.99 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 534.23 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 521.99 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 539.71 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 521.26 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 539.73 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 527.93 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 533.17 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 532.71 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 525.69 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 535.20 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 526.20 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 534.28 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 533.32 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 543.88 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 540.36 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 538.68 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 533.98 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 522.53 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 534.14 ms ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** During training for #   2 batch, reading batch file costed 535.72 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 540.06 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 522.53 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 538.13 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 522.62 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 535.83 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 540.40 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 542.64 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 533.47 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 532.24 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 539.65 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 545.18 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 533.02 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 543.64 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 526.72 ms ***\n",
      "Training costs a total of 101.9629 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start validate the model:\n",
      "*** During validation for #   0 batch, reading batch file costed 606.79 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 614.45 ms ***\n",
      "Validatoin costs a total of 1.4138 seconds!\n",
      "Finish train and validate the model:\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0054 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   3 batch, reading batch file costed 536.49 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 539.76 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 531.61 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 532.78 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 535.32 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 539.39 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 540.19 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 540.20 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 545.48 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 538.94 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 539.50 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 526.49 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 537.18 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 540.94 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 531.02 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 538.63 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 537.49 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 539.92 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 542.82 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 525.65 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 532.20 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 541.78 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 546.37 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 536.28 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 533.98 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 536.92 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 546.89 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 540.90 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 540.52 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 528.68 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 545.50 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 541.44 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 540.65 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 527.64 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 544.59 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 542.73 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 533.52 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 541.01 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 547.51 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 537.24 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 544.73 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 543.15 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 532.77 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 535.60 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 545.69 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 536.09 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 544.99 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 529.13 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 532.19 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 541.77 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 539.93 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 545.99 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 533.24 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 542.54 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 547.35 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 536.22 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 546.47 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 541.87 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 540.42 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 529.21 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 537.84 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 535.44 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 527.22 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 539.06 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 539.83 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 522.10 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 535.20 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 538.84 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 540.48 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 537.57 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 535.80 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 526.17 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 543.73 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 530.01 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 531.29 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 536.00 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 535.13 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 535.22 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 541.42 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 524.60 ms ***\n",
      "Training costs a total of 102.3437 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start validate the model:\n",
      "*** During validation for #   0 batch, reading batch file costed 602.86 ms ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** During validation for #   1 batch, reading batch file costed 609.75 ms ***\n",
      "Validatoin costs a total of 1.4057 seconds!\n",
      "Finish train and validate the model:\n",
      "====================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check F1-score\n",
    "output_F1_score(data, data_name, dataset, image_data_path, intermediate_data_folder, partition_nums, layers, \\\n",
    "                dropout = 0.1, lr = tune_lr, weight_decay = 0.1, mini_epoch_num = check_mini_epoch, valid_part_num = 2)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start checking train loss for partition num: 4 hop layer: 3\n",
      "\n",
      "====================================================================================================\n",
      "Start to generate the clustering machine:\n",
      "Batch machine creation costs a total of 0.0005 seconds!\n",
      "Start to generate the global edge weights\n",
      "Edge weights creation costs a total of 2.9631 seconds!\n",
      "Start to split data into train, test, validation:\n",
      "Data splitting costs a total of 0.0070 seconds!\n",
      "Start to generate the training batches:\n",
      "*** Generate train batch file for #   0 batch, writing the batch file costed 2507.60 ms ***\n",
      "*** Generate train batch file for #   1 batch, writing the batch file costed 5755.15 ms ***\n",
      "*** Generate train batch file for #   2 batch, writing the batch file costed 5684.02 ms ***\n",
      "*** Generate train batch file for #   3 batch, writing the batch file costed 5692.33 ms ***\n",
      "Train batches production costs a total of 21.6400 seconds!\n",
      "Start to store the batch machine file:\n",
      "Storing batch machine after training batches generation costs a total of 8.8508 seconds!\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.8314 seconds!\n",
      "Start to generate the validation batches:\n",
      "*** Generate validation batch file for #   0 batch, writing the batch file costed 6186.30 ms ***\n",
      "*** Generate validation batch file for #   1 batch, writing the batch file costed 7175.13 ms ***\n",
      "Validation batches production costs a total of 14.5282 seconds!\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0192 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   2 batch, reading batch file costed 539.03 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 544.40 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 554.94 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2310.00 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 560.50 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 551.46 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 549.60 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 552.26 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 554.00 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 553.97 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 541.12 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 534.99 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 542.30 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 538.86 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 539.60 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 538.17 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 547.83 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 539.33 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 558.63 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 551.08 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 551.92 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 552.16 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 547.28 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 548.54 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 549.86 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 539.11 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 551.61 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 552.90 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 546.61 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 552.48 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 558.06 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 548.41 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 554.45 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 543.91 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 543.28 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 542.90 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 546.69 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 550.13 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 536.61 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 559.65 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 557.53 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 565.90 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 561.70 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 543.95 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 567.21 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 550.76 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 564.37 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 543.00 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 540.04 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 553.55 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 550.66 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 563.80 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 561.77 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 541.03 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 564.68 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 534.65 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 539.57 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 537.89 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 537.07 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 536.11 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 539.02 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 534.26 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 543.76 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 534.18 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 538.53 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 532.81 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 539.03 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 539.00 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 538.58 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 533.61 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 542.97 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 560.44 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 564.32 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 532.74 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 551.77 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 560.71 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 547.57 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 547.51 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 543.23 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 536.27 ms ***\n",
      "Training costs a total of 106.3869 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start validate the model:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** During validation for #   0 batch, reading batch file costed 600.69 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 639.59 ms ***\n",
      "Validatoin costs a total of 1.4504 seconds!\n",
      "Finish train and validate the model:\n",
      "====================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check convergence\n",
    "output_train_loss(data, data_name, dataset, image_data_path, intermediate_data_folder, partition_nums, layers, \\\n",
    "                  dropout = 0.1, lr = tune_lr, weight_decay = 0.1, mini_epoch_num = check_mini_epoch, valid_part_num = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check in_train performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start running for partition num: 4 hop layer 3\n",
      "\n",
      "====================================================================================================\n",
      "Start to generate the clustering machine:\n",
      "Batch machine creation costs a total of 0.0005 seconds!\n",
      "Start to generate the global edge weights\n",
      "Edge weights creation costs a total of 3.0649 seconds!\n",
      "Start to split data into train, test, validation:\n",
      "Data splitting costs a total of 0.0039 seconds!\n",
      "Start to generate the training batches:\n",
      "*** Generate train batch file for #   0 batch, writing the batch file costed 2627.54 ms ***\n",
      "*** Generate train batch file for #   1 batch, writing the batch file costed 5749.88 ms ***\n",
      "*** Generate train batch file for #   2 batch, writing the batch file costed 5603.28 ms ***\n",
      "*** Generate train batch file for #   3 batch, writing the batch file costed 6094.34 ms ***\n",
      "Train batches production costs a total of 22.1514 seconds!\n",
      "Start to store the batch machine file:\n",
      "Storing batch machine after training batches generation costs a total of 9.1861 seconds!\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.8872 seconds!\n",
      "Start to generate the validation batches:\n",
      "*** Generate validation batch file for #   0 batch, writing the batch file costed 6094.69 ms ***\n",
      "*** Generate validation batch file for #   1 batch, writing the batch file costed 7479.54 ms ***\n",
      "Validation batches production costs a total of 14.7485 seconds!\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0194 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   1 batch, reading batch file costed 521.59 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 539.78 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 4585.88 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 550.44 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 555.22 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 619.78 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 635.11 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 7650.02 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 632.10 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 653.60 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 2008.05 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 625.82 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 635.85 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 6881.77 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 626.00 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 649.65 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 573.20 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 568.56 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 548.51 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 547.49 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 555.61 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 618.69 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 625.37 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 550.47 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 646.63 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 634.77 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 573.75 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 626.90 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 646.23 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 567.67 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 623.44 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 639.63 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 559.26 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 530.81 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 564.91 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 566.28 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 549.03 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 619.18 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 618.16 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 549.43 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 603.12 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 611.02 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 550.62 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 610.57 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 614.20 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 514.19 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 614.10 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 639.42 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 561.09 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 548.34 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 514.90 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 528.92 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 515.31 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 602.03 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 616.01 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 540.32 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 610.39 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 618.22 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 549.08 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 604.04 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 618.02 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 540.86 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 607.52 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 619.60 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 524.57 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 537.69 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 544.70 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 535.30 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 545.89 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 603.36 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 615.73 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 540.64 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 609.24 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 619.85 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 523.87 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 603.28 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 614.87 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 539.02 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 612.49 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 617.48 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 541.69 ms ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** During training for #   1 batch, reading batch file costed 516.60 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 545.96 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 536.61 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 540.29 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 602.12 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 616.04 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 538.34 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 608.25 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 617.07 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 550.80 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 603.68 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 613.54 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 521.05 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 608.97 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 617.16 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 541.33 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 536.43 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 519.49 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 541.28 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 541.99 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 606.87 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 619.43 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 522.66 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 612.31 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 618.99 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 544.78 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 605.45 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 617.89 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 547.76 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 611.61 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 618.78 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 543.07 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 518.01 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 546.53 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 538.63 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 540.24 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 607.12 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 617.04 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 543.47 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 610.90 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 619.17 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 551.24 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 611.44 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 617.70 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 522.12 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 610.62 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 619.09 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 526.33 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 539.15 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 547.39 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 537.29 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 522.62 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 604.27 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 616.22 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 542.81 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 609.86 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 618.75 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 551.26 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 604.26 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 615.64 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 540.33 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 610.34 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 617.25 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 545.20 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 545.52 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 540.73 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 517.26 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 520.83 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 605.58 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 617.13 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 540.23 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 611.47 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 618.67 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 546.76 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 605.81 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 615.51 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 548.09 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 612.50 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 619.43 ms ***\n",
      "In-process Training costs a total of 180.1472 seconds!\n",
      "Finish train and validate the model:\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0054 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   1 batch, reading batch file costed 514.69 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 538.51 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 545.63 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 533.42 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 540.60 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 603.01 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 615.23 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 541.06 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 611.51 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 618.77 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 551.29 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 601.89 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 615.25 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 520.67 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 609.01 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 618.93 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 526.27 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 536.98 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 549.03 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 528.31 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 515.47 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 596.87 ms ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** During validation for #   1 batch, reading batch file costed 608.74 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 531.93 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 603.61 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 614.76 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 538.97 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 595.63 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 610.56 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 540.67 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 604.61 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 611.14 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 538.44 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 531.44 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 540.93 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 511.21 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 515.19 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 597.41 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 611.58 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 532.51 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 604.55 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 612.71 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 544.93 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 596.56 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 608.93 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 535.34 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 604.88 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 610.84 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 540.09 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 510.24 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 540.05 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 528.64 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 541.23 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 595.03 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 611.00 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 535.80 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 612.07 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 611.91 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 522.20 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 597.64 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 609.00 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 532.04 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 603.62 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 611.54 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 519.52 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 529.47 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 532.87 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 535.03 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 537.01 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 595.85 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 608.70 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 541.01 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 604.78 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 612.21 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 538.50 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 596.84 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 608.40 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 514.48 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 605.98 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 612.58 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 545.16 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 530.63 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 532.71 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 510.45 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 538.62 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 598.51 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 609.21 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 533.59 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 605.63 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 612.18 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 545.74 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 595.15 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 608.89 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 515.94 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 604.02 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 611.28 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 540.11 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 535.95 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 534.34 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 510.79 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 515.09 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 595.90 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 608.42 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 539.99 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 605.47 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 612.57 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 541.19 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 595.44 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 610.47 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 532.98 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 605.12 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 612.62 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 541.70 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 510.21 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 541.84 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 529.74 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 534.70 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 595.92 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 610.29 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 515.16 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 602.58 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 612.50 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 545.44 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 595.91 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 608.79 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 533.14 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 603.86 ms ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** During validation for #   1 batch, reading batch file costed 611.22 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 519.12 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 534.82 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 533.04 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 530.37 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 532.82 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 596.03 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 609.49 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 514.83 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 604.47 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 610.40 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 545.46 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 597.14 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 607.78 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 535.20 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 602.99 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 612.28 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 537.49 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 509.28 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 539.82 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 530.55 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 533.69 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 598.58 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 610.19 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 517.58 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 605.12 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 612.17 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 545.50 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 597.58 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 609.99 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 536.88 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 603.90 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 614.86 ms ***\n",
      "In-process Training costs a total of 158.5406 seconds!\n",
      "Finish train and validate the model:\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0054 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   0 batch, reading batch file costed 535.35 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 528.15 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 535.97 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 510.64 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 543.14 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 596.64 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 611.32 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 515.18 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 605.39 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 613.87 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 539.88 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 596.95 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 610.52 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 531.59 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 605.10 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 613.26 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 521.79 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 537.08 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 534.77 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 532.75 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 541.91 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 598.06 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 611.81 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 532.23 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 609.63 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 621.61 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 526.32 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 600.03 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 613.54 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 540.71 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 610.36 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 618.23 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 545.33 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 534.14 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 545.46 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 516.08 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 548.45 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 606.47 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 615.83 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 537.39 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 610.00 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 616.73 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 524.75 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 604.06 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 617.06 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 541.94 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 618.83 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 621.71 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 552.83 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 538.34 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 522.18 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 534.28 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 541.35 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 603.73 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 616.70 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 522.28 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 611.86 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 618.01 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 546.07 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 603.17 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 617.08 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 547.49 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 610.52 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 619.62 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 552.18 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 537.87 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 540.63 ms ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** During training for #   1 batch, reading batch file costed 516.96 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 548.55 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 604.93 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 616.97 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 538.96 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 612.43 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 618.98 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 526.13 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 603.42 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 617.03 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 542.26 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 611.03 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 619.36 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 555.04 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 517.96 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 543.65 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 533.75 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 533.97 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 597.63 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 610.16 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 535.61 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 603.44 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 612.67 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 546.56 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 597.01 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 608.71 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 516.40 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 605.71 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 613.02 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 538.21 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 540.54 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 515.84 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 532.28 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 542.34 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 596.94 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 611.40 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 536.92 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 605.83 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 612.48 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 522.94 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 598.49 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 609.12 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 533.97 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 605.21 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 614.39 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 540.86 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 536.82 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 538.68 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 515.47 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 544.82 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 602.13 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 614.61 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 521.72 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 609.97 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 616.68 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 545.41 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 601.38 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 615.36 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 538.62 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 609.97 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 617.97 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 544.23 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 516.36 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 547.27 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 536.30 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 547.55 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 603.49 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 613.80 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 538.90 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 611.11 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 620.30 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 526.97 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 604.79 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 617.84 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 543.67 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 612.80 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 619.09 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 545.17 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 543.22 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 522.75 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 539.17 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 542.09 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 604.05 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 617.73 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 548.36 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 611.91 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 617.92 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 545.49 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 606.34 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 616.00 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 522.13 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 612.38 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 620.77 ms ***\n",
      "In-process Training costs a total of 159.3432 seconds!\n",
      "Finish train and validate the model:\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0054 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   1 batch, reading batch file costed 515.19 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 542.43 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 542.00 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 536.16 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 547.62 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 603.74 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 615.99 ms ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** During training for #   3 batch, reading batch file costed 545.81 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 614.14 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 619.75 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 527.99 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 604.58 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 616.18 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 540.95 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 610.99 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 620.24 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 553.44 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 537.61 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 522.31 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 535.97 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 543.09 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 604.71 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 616.21 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 521.53 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 613.20 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 620.90 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 552.66 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 602.70 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 616.72 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 539.22 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 612.45 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 618.23 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 544.58 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 518.49 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 548.38 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 537.14 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 521.59 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 609.48 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 615.29 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 546.66 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 611.31 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 618.00 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 544.53 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 604.23 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 614.58 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 541.31 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 610.90 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 620.36 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 546.14 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 543.72 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 541.78 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 516.66 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 546.48 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 603.13 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 614.83 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 522.91 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 611.27 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 618.29 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 545.88 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 604.48 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 616.73 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 538.96 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 611.04 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 620.60 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 553.18 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 536.99 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 540.13 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 517.99 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 541.80 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 604.29 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 617.24 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 540.91 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 611.26 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 618.44 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 525.74 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 603.33 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 616.12 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 546.42 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 611.48 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 619.25 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 545.43 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 516.66 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 549.06 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 531.16 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 541.63 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 596.94 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 608.04 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 535.00 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 603.45 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 612.40 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 537.66 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 598.05 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 608.60 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 516.70 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 605.70 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 611.00 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 521.67 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 530.23 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 534.45 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 535.18 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 534.64 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 595.27 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 609.78 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 515.21 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 605.44 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 612.21 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 540.53 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 596.99 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 608.23 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 540.52 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 605.03 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 612.91 ms ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** During training for #   3 batch, reading batch file costed 538.24 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 510.25 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 534.64 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 535.72 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 515.72 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 596.33 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 609.82 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 540.88 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 605.14 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 612.07 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 541.16 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 597.88 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 609.53 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 532.06 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 604.87 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 612.89 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 545.28 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 513.54 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 537.35 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 529.25 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 541.82 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 595.89 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 608.90 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 532.42 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 605.54 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 613.49 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 539.93 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 598.06 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 609.15 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 515.26 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 603.91 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 614.48 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 540.23 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 529.10 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 517.25 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 536.32 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 516.13 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 598.16 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 607.97 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 533.31 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 603.96 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 614.57 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 545.04 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 595.73 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 610.57 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 534.54 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 605.22 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 611.94 ms ***\n",
      "In-process Training costs a total of 159.1503 seconds!\n",
      "Finish train and validate the model:\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0053 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   2 batch, reading batch file costed 530.07 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 538.70 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 517.12 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 528.47 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 516.90 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 597.21 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 610.73 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 535.39 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 606.43 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 612.21 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 547.10 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 596.18 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 607.69 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 532.62 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 604.88 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 611.38 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 519.54 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 536.59 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 533.93 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 535.26 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 538.59 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 601.68 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 614.69 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 540.45 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 610.48 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 616.60 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 526.01 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 602.37 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 613.52 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 545.46 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 610.48 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 618.13 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 542.47 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 536.56 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 547.10 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 515.21 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 521.97 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 603.39 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 614.01 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 546.37 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 611.03 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 617.63 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 543.84 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 601.18 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 614.89 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 539.51 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 612.32 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 622.71 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 526.93 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 543.06 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 544.38 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 536.75 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 543.69 ms ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** During validation for #   0 batch, reading batch file costed 604.15 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 616.84 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 538.98 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 612.32 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 618.80 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 551.41 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 602.14 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 609.34 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 516.94 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 609.09 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 614.04 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 546.99 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 511.65 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 535.96 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 531.75 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 542.23 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 596.69 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 610.69 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 536.53 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 605.28 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 612.17 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 538.47 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 598.70 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 609.32 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 516.47 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 604.73 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 614.35 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 540.55 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 528.26 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 541.84 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 511.09 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 516.03 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 596.69 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 609.98 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 535.71 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 606.16 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 611.15 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 552.17 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 598.19 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 610.55 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 532.53 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 604.56 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 613.40 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 520.54 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 527.51 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 541.19 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 531.46 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 535.30 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 596.19 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 610.43 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 535.44 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 605.44 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 612.87 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 520.14 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 597.91 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 609.68 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 541.05 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 604.27 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 614.32 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 542.56 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 512.91 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 536.24 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 537.00 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 537.16 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 598.46 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 609.55 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 534.04 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 606.53 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 614.65 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 546.63 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 596.18 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 610.84 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 516.05 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 604.81 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 612.58 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 541.01 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 528.96 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 516.97 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 536.21 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 536.48 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 596.30 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 609.58 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 540.73 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 605.43 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 613.42 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 521.28 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 597.40 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 607.48 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 532.91 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 605.94 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 611.48 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 544.94 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 527.98 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 515.72 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 531.01 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 533.59 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 595.01 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 609.65 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 535.50 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 606.17 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 611.73 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 546.62 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 598.26 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 608.41 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 516.25 ms ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** During validation for #   0 batch, reading batch file costed 604.37 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 611.78 ms ***\n",
      "In-process Training costs a total of 158.9402 seconds!\n",
      "Finish train and validate the model:\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0054 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   3 batch, reading batch file costed 526.65 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 535.79 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 516.98 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 530.17 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 541.20 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 602.67 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 613.73 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 520.94 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 612.23 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 619.06 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 544.70 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 605.48 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 617.62 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 542.21 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 611.53 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 620.24 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 553.00 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 538.44 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 522.78 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 536.15 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 548.23 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 606.61 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 615.70 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 521.88 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 612.70 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 619.82 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 548.35 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 603.28 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 617.33 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 539.09 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 611.74 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 618.14 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 526.44 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 536.37 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 548.93 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 536.94 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 546.42 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 603.83 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 615.98 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 541.07 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 610.73 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 618.47 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 526.74 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 602.92 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 615.28 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 539.01 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 612.51 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 619.04 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 544.39 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 541.54 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 521.50 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 541.28 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 541.18 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 602.36 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 616.41 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 548.48 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 612.12 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 617.96 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 546.04 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 603.17 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 618.44 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 521.32 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 605.03 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 613.37 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 541.52 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 536.06 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 534.11 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 511.94 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 517.37 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 596.11 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 608.17 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 535.47 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 605.46 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 613.73 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 545.27 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 594.78 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 608.87 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 532.77 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 605.30 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 611.82 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 541.95 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 536.12 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 536.61 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 512.45 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 534.46 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 595.73 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 609.12 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 541.15 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 604.81 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 612.57 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 539.45 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 597.64 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 608.85 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 515.33 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 604.82 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 611.99 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 540.45 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 537.48 ms ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** During training for #   3 batch, reading batch file costed 534.84 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 510.02 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 540.03 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 595.99 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 609.69 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 532.48 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 603.47 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 613.04 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 520.73 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 598.00 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 608.13 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 535.14 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 606.07 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 611.97 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 537.03 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 536.27 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 515.62 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 530.38 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 541.45 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 597.88 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 610.05 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 537.11 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 607.31 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 612.86 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 520.60 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 598.42 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 610.33 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 533.46 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 605.00 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 614.19 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 521.66 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 529.85 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 536.84 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 535.93 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 539.27 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 599.82 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 609.80 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 535.57 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 606.31 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 613.15 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 545.59 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 597.18 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 611.03 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 516.48 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 606.15 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 614.46 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 539.55 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 532.92 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 518.07 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 537.21 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 517.55 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 598.15 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 609.70 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 534.14 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 607.58 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 612.79 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 547.88 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 596.87 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 610.45 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 535.61 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 605.54 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 613.62 ms ***\n",
      "In-process Training costs a total of 159.0771 seconds!\n",
      "Finish train and validate the model:\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0054 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   3 batch, reading batch file costed 528.74 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 533.09 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 516.92 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 537.66 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 542.61 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 597.78 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 610.94 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 533.85 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 606.61 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 613.97 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 541.90 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 602.82 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 615.67 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 525.62 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 613.46 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 616.33 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 545.10 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 532.87 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 548.10 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 516.86 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 541.55 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 603.42 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 617.89 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 521.16 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 614.32 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 618.72 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 553.46 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 604.84 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 615.73 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 542.60 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 611.57 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 619.56 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 545.93 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 544.13 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 522.29 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 534.41 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 547.57 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 604.65 ms ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** During validation for #   1 batch, reading batch file costed 615.90 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 540.35 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 611.90 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 619.07 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 545.59 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 598.18 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 609.61 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 515.33 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 604.64 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 612.29 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 541.39 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 536.94 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 517.74 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 529.33 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 517.96 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 597.78 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 609.29 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 541.81 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 605.12 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 612.76 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 540.73 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 596.40 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 610.04 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 533.05 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 605.40 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 612.88 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 521.72 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 530.19 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 536.87 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 536.28 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 534.98 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 597.17 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 610.10 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 541.43 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 606.60 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 614.27 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 521.39 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 597.93 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 609.05 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 536.10 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 606.23 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 611.75 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 546.60 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 529.51 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 536.87 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 511.19 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 540.82 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 595.26 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 609.73 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 535.48 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 606.41 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 613.43 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 538.90 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 597.79 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 608.00 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 515.17 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 603.72 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 611.26 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 520.90 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 530.10 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 533.18 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 535.07 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 516.46 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 597.60 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 610.65 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 541.70 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 604.93 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 611.11 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 540.96 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 597.45 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 609.63 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 532.65 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 605.63 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 612.51 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 537.18 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 536.35 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 535.92 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 512.04 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 540.70 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 597.10 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 609.97 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 536.22 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 605.64 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 612.56 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 520.07 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 595.75 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 609.72 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 532.22 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 605.14 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 610.86 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 521.78 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 537.70 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 539.12 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 528.19 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 516.44 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 602.80 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 609.34 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 533.65 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 604.64 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 615.32 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 540.92 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 596.12 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 610.08 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 541.87 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 606.47 ms ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** During validation for #   1 batch, reading batch file costed 610.60 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 540.89 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 527.33 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 515.85 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 536.04 ms ***\n",
      "*** During training for #   3 batch, reading batch file costed 533.67 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 595.76 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 608.89 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 540.63 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 605.38 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 612.54 ms ***\n",
      "*** During training for #   2 batch, reading batch file costed 542.83 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 597.49 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 608.31 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 516.83 ms ***\n",
      "*** During validation for #   0 batch, reading batch file costed 605.08 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 612.12 ms ***\n",
      "In-process Training costs a total of 158.8575 seconds!\n",
      "Finish train and validate the model:\n",
      "====================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_train_investigate(data, data_name, dataset, image_data_path, intermediate_data_folder, partition_nums, layers, \\\n",
    "                         dropout = 0.1, lr = tune_lr, weight_decay = 0.1, mini_epoch_num = check_mini_epoch, output_period = 40, valid_part_num = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tuning epoch number for each train-batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start tuning for tuning param: batch_epoch_num partition num: 2 hop layer 2\n",
      "\n",
      "====================================================================================================\n",
      "Start to generate the clustering machine:\n",
      "Batch machine creation costs a total of 0.0490 seconds!\n",
      "Start to split data into train, test, validation:\n",
      "Data splitting costs a total of 0.0006 seconds!\n",
      "Start to generate the training batches:\n",
      "*** Generate train batch file for #   0 batch, writing the batch file costed 4.85 ms ***\n",
      "*** Generate train batch file for #   1 batch, writing the batch file costed 6.43 ms ***\n",
      "Train batches production costs a total of 0.0483 seconds!\n",
      "Start to store the batch machine file:\n",
      "Storing batch machine after training batches generation costs a total of 0.0104 seconds!\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0062 seconds!\n",
      "Start to generate the validation batches:\n",
      "*** Generate validation batch file for #   0 batch, writing the batch file costed 8.58 ms ***\n",
      "*** Generate validation batch file for #   1 batch, writing the batch file costed 8.97 ms ***\n",
      "Validation batches production costs a total of 0.0764 seconds!\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0008 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   0 batch, reading batch file costed 3.02 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 4.58 ms ***\n",
      "Training costs a total of 0.9896 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start validate the model:\n",
      "*** During validation for #   0 batch, reading batch file costed 5.54 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 5.15 ms ***\n",
      "Validatoin costs a total of 0.0178 seconds!\n",
      "Finish train and validate the model:\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0012 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   1 batch, reading batch file costed 4.36 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.67 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 5.98 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.58 ms ***\n",
      "Training costs a total of 1.1121 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start validate the model:\n",
      "*** During validation for #   0 batch, reading batch file costed 5.31 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 4.90 ms ***\n",
      "Validatoin costs a total of 0.0170 seconds!\n",
      "Finish train and validate the model:\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0009 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   1 batch, reading batch file costed 3.40 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.23 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 4.60 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 5.18 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 5.48 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.13 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.72 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.18 ms ***\n",
      "Training costs a total of 1.0989 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start validate the model:\n",
      "*** During validation for #   0 batch, reading batch file costed 4.73 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 4.64 ms ***\n",
      "Validatoin costs a total of 0.0154 seconds!\n",
      "Finish train and validate the model:\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0009 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   0 batch, reading batch file costed 2.77 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.04 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.88 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 5.26 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.99 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.61 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.33 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.78 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.20 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.99 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.02 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.43 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.13 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.51 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.75 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.94 ms ***\n",
      "Training costs a total of 1.1037 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start validate the model:\n",
      "*** During validation for #   0 batch, reading batch file costed 5.02 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 4.43 ms ***\n",
      "Validatoin costs a total of 0.0157 seconds!\n",
      "Finish train and validate the model:\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0009 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   1 batch, reading batch file costed 3.59 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.90 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.91 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.94 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.01 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.20 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.41 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.07 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.24 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.09 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.04 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 4.12 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.64 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.87 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.20 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.96 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 4.03 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.16 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.32 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.16 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.28 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.93 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.85 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.46 ms ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** During training for #   1 batch, reading batch file costed 3.27 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.89 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.81 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.04 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.83 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.96 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.71 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.92 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.51 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.09 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.98 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.36 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.98 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.28 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.45 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.55 ms ***\n",
      "Training costs a total of 1.1680 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start validate the model:\n",
      "*** During validation for #   0 batch, reading batch file costed 5.28 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 4.24 ms ***\n",
      "Validatoin costs a total of 0.0154 seconds!\n",
      "Finish train and validate the model:\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0008 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   1 batch, reading batch file costed 3.30 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.03 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.94 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.86 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.51 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.15 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.41 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.29 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.16 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.60 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.72 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 4.50 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 4.51 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.70 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.83 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.28 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.17 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.85 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.21 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.48 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.63 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.06 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.68 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.12 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 4.48 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 4.57 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.91 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.08 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.09 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.99 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.33 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.95 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.20 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.87 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.08 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.38 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.23 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.68 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.67 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.71 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.67 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.63 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 4.34 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.06 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.69 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.99 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.44 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.98 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.20 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.95 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.81 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.24 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.18 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.25 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.24 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.49 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.69 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.81 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 4.40 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.11 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.65 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.38 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 4.08 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.38 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 4.30 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 4.96 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.75 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.25 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.69 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.01 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.87 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.19 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.98 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.12 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.25 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.89 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.35 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.88 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.32 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.93 ms ***\n",
      "Training costs a total of 1.4276 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start validate the model:\n",
      "*** During validation for #   0 batch, reading batch file costed 4.76 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 4.69 ms ***\n",
      "Validatoin costs a total of 0.0156 seconds!\n",
      "Finish train and validate the model:\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0009 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   1 batch, reading batch file costed 3.16 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.06 ms ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** During training for #   1 batch, reading batch file costed 3.77 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.07 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.14 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.42 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.03 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.20 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.83 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.39 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.40 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.81 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.97 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.37 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.54 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.97 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.47 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.00 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.20 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.39 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.45 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.18 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.85 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.20 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.35 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.14 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.13 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.06 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.47 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.92 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.85 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.31 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.91 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.26 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.00 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.24 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.90 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.94 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.87 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.50 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.84 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.93 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.36 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.89 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.22 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.17 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.86 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.53 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.99 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.44 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.57 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.88 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.13 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.92 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.51 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.99 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.96 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.26 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.71 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.38 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.04 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.88 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.47 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.87 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.44 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.26 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.80 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.51 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.25 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.99 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.35 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.96 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.93 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.32 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.52 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.88 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.69 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.19 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.92 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.54 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.44 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.02 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.97 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.39 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.94 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.07 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.15 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.27 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.20 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.99 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.31 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.97 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.32 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.13 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.95 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.52 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.52 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.88 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.50 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.34 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.84 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.28 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.95 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.30 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.92 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.24 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.91 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.50 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.81 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.22 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.05 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.04 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.88 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.23 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.00 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.20 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.33 ms ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** During training for #   0 batch, reading batch file costed 3.57 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.52 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.31 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.86 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.91 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.82 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.42 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.24 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.04 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.86 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.58 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.21 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.05 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.28 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.84 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.01 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.19 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.83 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.02 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.19 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.02 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.05 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.75 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.12 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.86 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 5.32 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.63 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.00 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.43 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.89 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.98 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.12 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.85 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.05 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.66 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 5.02 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.99 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.67 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 4.52 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 4.28 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.67 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.82 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 4.17 ms ***\n",
      "Training costs a total of 1.7397 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start validate the model:\n",
      "*** During validation for #   0 batch, reading batch file costed 7.24 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 4.75 ms ***\n",
      "Validatoin costs a total of 0.0185 seconds!\n",
      "Finish train and validate the model:\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0018 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   1 batch, reading batch file costed 3.51 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.32 ms ***\n",
      "Training costs a total of 1.0051 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start validate the model:\n",
      "*** During validation for #   0 batch, reading batch file costed 6.15 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 5.71 ms ***\n",
      "Validatoin costs a total of 0.0194 seconds!\n",
      "Finish train and validate the model:\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0012 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   1 batch, reading batch file costed 3.65 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.22 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.73 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.25 ms ***\n",
      "Training costs a total of 1.0412 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start validate the model:\n",
      "*** During validation for #   0 batch, reading batch file costed 4.88 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 4.37 ms ***\n",
      "Validatoin costs a total of 0.0160 seconds!\n",
      "Finish train and validate the model:\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0010 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   0 batch, reading batch file costed 3.02 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.19 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.49 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.91 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.41 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.12 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.33 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.91 ms ***\n",
      "Training costs a total of 0.9780 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start validate the model:\n",
      "*** During validation for #   0 batch, reading batch file costed 4.77 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 4.45 ms ***\n",
      "Validatoin costs a total of 0.0152 seconds!\n",
      "Finish train and validate the model:\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0007 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   1 batch, reading batch file costed 3.25 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.95 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.51 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 4.42 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 4.35 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.12 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 4.25 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.03 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.26 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.99 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.27 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.38 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.53 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.46 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.60 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.36 ms ***\n",
      "Training costs a total of 1.0744 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start validate the model:\n",
      "*** During validation for #   0 batch, reading batch file costed 4.88 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 4.73 ms ***\n",
      "Validatoin costs a total of 0.0160 seconds!\n",
      "Finish train and validate the model:\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0011 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   1 batch, reading batch file costed 3.25 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.05 ms ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** During training for #   0 batch, reading batch file costed 3.63 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.45 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.71 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.16 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.32 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.36 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.33 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.89 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.93 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.25 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.27 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.86 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.28 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.08 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.48 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.39 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.00 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.27 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.98 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.67 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.28 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.00 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.17 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 4.00 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.25 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.92 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.07 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.94 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.28 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.02 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.63 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.98 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.92 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.36 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.12 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.38 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.11 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 4.03 ms ***\n",
      "Training costs a total of 1.1620 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start validate the model:\n",
      "*** During validation for #   0 batch, reading batch file costed 4.98 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 4.94 ms ***\n",
      "Validatoin costs a total of 0.0168 seconds!\n",
      "Finish train and validate the model:\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0010 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   0 batch, reading batch file costed 3.47 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.26 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.43 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.16 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.21 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.89 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.98 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.14 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.75 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.35 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.65 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.00 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.99 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.16 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.06 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.52 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.49 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.02 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.01 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.39 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.11 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.91 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 4.06 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.51 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.16 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.36 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.78 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.93 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.01 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.58 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.83 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.92 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.15 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.35 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.44 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.55 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.88 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.07 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.17 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.87 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.91 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.70 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.81 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.23 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.57 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.10 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.34 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.47 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.78 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 4.14 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.95 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.49 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.50 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 4.06 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 4.61 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.13 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.31 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 4.02 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.45 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.75 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.37 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.35 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.23 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.00 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.34 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.91 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.34 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.96 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.89 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.27 ms ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** During training for #   1 batch, reading batch file costed 3.47 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.96 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.07 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 4.25 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.32 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 4.16 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.54 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.91 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.34 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.52 ms ***\n",
      "Training costs a total of 1.3849 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start validate the model:\n",
      "*** During validation for #   0 batch, reading batch file costed 5.09 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 4.25 ms ***\n",
      "Validatoin costs a total of 0.0159 seconds!\n",
      "Finish train and validate the model:\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0008 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   0 batch, reading batch file costed 2.98 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 4.76 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.86 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.47 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.60 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.78 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.95 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 4.16 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.87 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.45 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.96 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.83 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.95 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.86 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.43 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.02 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.02 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.20 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.86 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.02 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.33 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.21 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.88 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.26 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.93 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.03 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.82 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.19 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.93 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.08 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.14 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.93 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.92 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.55 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.98 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 4.01 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.88 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.68 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 4.73 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.16 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.13 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.43 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.82 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.43 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.26 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.26 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.96 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 4.83 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.96 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.24 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.53 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.43 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.29 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.00 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.23 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.04 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.77 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.11 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.39 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.53 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.09 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.88 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.17 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.04 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.80 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.15 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.90 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.18 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.85 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.17 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.07 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.02 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.23 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.88 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.82 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.16 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.83 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.21 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.66 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.86 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.19 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.24 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.70 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.87 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 4.09 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.15 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.97 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 4.03 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.91 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.15 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.20 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.31 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.10 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.36 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.28 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.98 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.55 ms ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** During training for #   0 batch, reading batch file costed 3.03 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 4.40 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 4.14 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.36 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.02 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.29 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.60 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.04 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.91 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.33 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.03 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.38 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.03 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.64 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.05 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.03 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.56 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.84 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.94 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.61 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.89 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.33 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.92 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.55 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.91 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.11 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.91 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.87 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.18 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.96 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.96 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.98 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.52 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.37 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.66 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.85 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.83 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.95 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.34 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.80 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.24 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.95 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.03 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.13 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.84 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.34 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.49 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.83 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.08 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.46 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.03 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.04 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.82 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.22 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.97 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.79 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.99 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.01 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.22 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.32 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.05 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.12 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.91 ms ***\n",
      "Training costs a total of 1.7390 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start validate the model:\n",
      "*** During validation for #   0 batch, reading batch file costed 4.64 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 4.20 ms ***\n",
      "Validatoin costs a total of 0.0153 seconds!\n",
      "Finish train and validate the model:\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0008 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   0 batch, reading batch file costed 2.80 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.66 ms ***\n",
      "Training costs a total of 1.0114 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start validate the model:\n",
      "*** During validation for #   0 batch, reading batch file costed 4.82 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 4.42 ms ***\n",
      "Validatoin costs a total of 0.0154 seconds!\n",
      "Finish train and validate the model:\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0007 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   1 batch, reading batch file costed 3.32 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.37 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.72 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.34 ms ***\n",
      "Training costs a total of 1.0007 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start validate the model:\n",
      "*** During validation for #   0 batch, reading batch file costed 4.73 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 4.70 ms ***\n",
      "Validatoin costs a total of 0.0165 seconds!\n",
      "Finish train and validate the model:\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0009 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   0 batch, reading batch file costed 2.99 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.95 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.59 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.31 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.19 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.93 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.19 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.63 ms ***\n",
      "Training costs a total of 1.0453 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start validate the model:\n",
      "*** During validation for #   0 batch, reading batch file costed 5.24 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 4.24 ms ***\n",
      "Validatoin costs a total of 0.0156 seconds!\n",
      "Finish train and validate the model:\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0009 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   1 batch, reading batch file costed 3.21 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.04 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.29 ms ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** During training for #   0 batch, reading batch file costed 2.91 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.93 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.51 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.32 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.13 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.25 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.89 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.96 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.15 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.48 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.44 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.87 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.18 ms ***\n",
      "Training costs a total of 1.0197 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start validate the model:\n",
      "*** During validation for #   0 batch, reading batch file costed 4.69 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 4.51 ms ***\n",
      "Validatoin costs a total of 0.0159 seconds!\n",
      "Finish train and validate the model:\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0009 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   1 batch, reading batch file costed 3.13 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.84 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 4.42 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 4.09 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.28 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.35 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.58 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.23 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.27 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.21 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.93 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.24 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 4.27 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.80 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.38 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.90 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.96 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.18 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.26 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.74 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.41 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.09 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.45 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.02 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.60 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.34 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.14 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.47 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.47 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.98 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.40 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.06 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.91 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.31 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.18 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.52 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.01 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.24 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.36 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.04 ms ***\n",
      "Training costs a total of 1.1943 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start validate the model:\n",
      "*** During validation for #   0 batch, reading batch file costed 5.38 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 4.51 ms ***\n",
      "Validatoin costs a total of 0.0160 seconds!\n",
      "Finish train and validate the model:\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0009 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   0 batch, reading batch file costed 3.24 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.71 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.30 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.34 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.48 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.17 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.21 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.36 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.11 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.50 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.01 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.32 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.48 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.93 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.36 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.95 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.94 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.12 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 4.00 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.57 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.94 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.77 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.21 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 4.26 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.59 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.97 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.06 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.81 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.46 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.99 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.86 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.22 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.84 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.59 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.21 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.84 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.87 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.77 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.86 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.26 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.69 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.82 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.26 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.93 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.55 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.92 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.74 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.20 ms ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** During training for #   1 batch, reading batch file costed 3.73 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.21 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.40 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.85 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.63 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.96 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.83 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.29 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.01 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.00 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.89 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.29 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.03 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.13 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.35 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.89 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.04 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.81 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.24 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.00 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.78 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.00 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.25 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.90 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.90 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.22 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 4.35 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.20 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.95 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.21 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.47 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.11 ms ***\n",
      "Training costs a total of 1.3524 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start validate the model:\n",
      "*** During validation for #   0 batch, reading batch file costed 4.66 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 4.40 ms ***\n",
      "Validatoin costs a total of 0.0150 seconds!\n",
      "Finish train and validate the model:\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0012 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   1 batch, reading batch file costed 3.31 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.88 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.91 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 4.16 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.86 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.08 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.93 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.81 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.09 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.91 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.18 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.08 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.06 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.17 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.97 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.91 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.85 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.24 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.33 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.05 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.29 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.83 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.36 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.02 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.04 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.12 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.40 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.94 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.15 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.59 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.07 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.35 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.12 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.25 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.15 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.75 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.85 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.33 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.48 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.18 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.30 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 4.20 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.28 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.05 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.09 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.09 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.19 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.76 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.90 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.51 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.98 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.50 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.71 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.05 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.82 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.06 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.84 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.55 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.03 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.03 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.07 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.45 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.91 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.06 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.58 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.93 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.14 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.44 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.98 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.00 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.84 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.49 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.13 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.00 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.94 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.38 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.59 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.93 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.82 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.01 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.96 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.17 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.18 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.99 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.20 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.00 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.90 ms ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** During training for #   0 batch, reading batch file costed 2.90 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.07 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.17 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.57 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.02 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.78 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.08 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.89 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.78 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.03 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.76 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.85 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.07 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.78 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.34 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.01 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.00 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.49 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.90 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.93 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.76 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.18 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.24 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.81 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.16 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.88 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.57 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.22 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.87 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.78 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.23 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.76 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.85 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.81 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.29 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.19 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.20 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.16 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.88 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.86 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.14 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.41 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.11 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.11 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.81 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.15 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.86 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.86 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.02 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.59 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.96 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.78 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.20 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.89 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.82 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.73 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.01 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.35 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.06 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.93 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.36 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.07 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.03 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.27 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.14 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.86 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.44 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.64 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.89 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.72 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.49 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.03 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.36 ms ***\n",
      "Training costs a total of 1.7145 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start validate the model:\n",
      "*** During validation for #   0 batch, reading batch file costed 4.81 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 4.25 ms ***\n",
      "Validatoin costs a total of 0.0152 seconds!\n",
      "Finish train and validate the model:\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0008 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   0 batch, reading batch file costed 2.99 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 4.03 ms ***\n",
      "Training costs a total of 0.9790 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start validate the model:\n",
      "*** During validation for #   0 batch, reading batch file costed 4.68 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 4.27 ms ***\n",
      "Validatoin costs a total of 0.0153 seconds!\n",
      "Finish train and validate the model:\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0008 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   1 batch, reading batch file costed 3.03 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.18 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.46 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.04 ms ***\n",
      "Training costs a total of 0.9780 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start validate the model:\n",
      "*** During validation for #   0 batch, reading batch file costed 4.69 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 4.35 ms ***\n",
      "Validatoin costs a total of 0.0149 seconds!\n",
      "Finish train and validate the model:\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0007 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   0 batch, reading batch file costed 2.94 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.16 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.28 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.04 ms ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** During training for #   1 batch, reading batch file costed 3.44 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.88 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.30 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.13 ms ***\n",
      "Training costs a total of 0.9720 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start validate the model:\n",
      "*** During validation for #   0 batch, reading batch file costed 4.84 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 4.56 ms ***\n",
      "Validatoin costs a total of 0.0157 seconds!\n",
      "Finish train and validate the model:\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0009 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   1 batch, reading batch file costed 3.06 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.04 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.91 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.37 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.87 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.05 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.31 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.07 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.97 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.21 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.49 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.08 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.91 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.26 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.24 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.94 ms ***\n",
      "Training costs a total of 1.0418 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start validate the model:\n",
      "*** During validation for #   0 batch, reading batch file costed 4.64 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 4.21 ms ***\n",
      "Validatoin costs a total of 0.0148 seconds!\n",
      "Finish train and validate the model:\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0009 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   0 batch, reading batch file costed 2.84 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.06 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.80 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.19 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.28 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.85 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.93 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.25 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.90 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.28 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.06 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.12 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.81 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.59 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.48 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.86 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.89 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.09 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.40 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.88 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.83 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.63 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.31 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.94 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.08 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.08 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.81 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.20 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.25 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.83 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.27 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.60 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.58 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.86 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.91 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.91 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.21 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.84 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.98 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.06 ms ***\n",
      "Training costs a total of 1.1549 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start validate the model:\n",
      "*** During validation for #   0 batch, reading batch file costed 4.61 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 4.43 ms ***\n",
      "Validatoin costs a total of 0.0151 seconds!\n",
      "Finish train and validate the model:\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0008 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   1 batch, reading batch file costed 3.13 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.26 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.83 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.28 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.14 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.18 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.01 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.05 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.84 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.79 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.93 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.03 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.12 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.96 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.28 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.94 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.97 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.22 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.92 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.18 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.82 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.42 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.02 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.77 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.80 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.98 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.64 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.88 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.03 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.83 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.70 ms ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** During training for #   0 batch, reading batch file costed 3.04 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.06 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.85 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.74 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.40 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.93 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.09 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.02 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.58 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.26 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.06 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.86 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.44 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.15 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.21 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.86 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.02 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.89 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.19 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.37 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.84 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.96 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.89 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.30 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.00 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.24 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.20 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.17 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.24 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.89 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.49 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.39 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.86 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.23 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.88 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.84 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.04 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.23 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.89 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.11 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.75 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.39 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.89 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.83 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.05 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.83 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.18 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.27 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.79 ms ***\n",
      "Training costs a total of 1.3204 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start validate the model:\n",
      "*** During validation for #   0 batch, reading batch file costed 5.05 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 5.00 ms ***\n",
      "Validatoin costs a total of 0.0168 seconds!\n",
      "Finish train and validate the model:\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0009 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   1 batch, reading batch file costed 3.48 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.38 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.97 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.16 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.34 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.86 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.29 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.94 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.37 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.09 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.34 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.15 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.06 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.93 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 4.10 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.87 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.85 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.01 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.30 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.26 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.01 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.94 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.32 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.17 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.51 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.19 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.09 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.92 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.08 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.48 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.29 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.87 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.83 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.46 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.24 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.84 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.93 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.00 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.14 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.22 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.84 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.82 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.13 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.95 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.93 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.03 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.84 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.47 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.24 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.80 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.28 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.90 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.21 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.04 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.20 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.13 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.05 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.22 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.21 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.85 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.18 ms ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** During training for #   0 batch, reading batch file costed 2.88 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.74 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.62 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.86 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.17 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.50 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.78 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.83 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.07 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.94 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.76 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.85 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.92 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.06 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.60 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.95 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.10 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.07 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.26 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.46 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.14 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.18 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.02 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.39 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.44 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.74 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.33 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.26 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.40 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.90 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.57 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.84 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.21 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.83 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.23 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.84 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.24 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.94 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.66 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.88 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.84 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.25 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.10 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.39 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.29 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.09 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.09 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.13 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.12 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.42 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.47 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.96 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.38 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.81 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.03 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.36 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.76 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.78 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.63 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.14 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.07 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.96 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.48 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.48 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.95 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.61 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.94 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.41 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.87 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.50 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.25 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.92 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.73 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.30 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.25 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.60 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.26 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.29 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.48 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.36 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 4.36 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.39 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.08 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.92 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.27 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.57 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.86 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 5.30 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.20 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.73 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.75 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.76 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.23 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.63 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.79 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.75 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 5.15 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.32 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.33 ms ***\n",
      "Training costs a total of 1.7435 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start validate the model:\n",
      "*** During validation for #   0 batch, reading batch file costed 4.82 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 4.33 ms ***\n",
      "Validatoin costs a total of 0.0161 seconds!\n",
      "Finish train and validate the model:\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0008 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   0 batch, reading batch file costed 2.96 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.42 ms ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training costs a total of 0.9797 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start validate the model:\n",
      "*** During validation for #   0 batch, reading batch file costed 5.29 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 4.74 ms ***\n",
      "Validatoin costs a total of 0.0165 seconds!\n",
      "Finish train and validate the model:\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0011 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   0 batch, reading batch file costed 3.25 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.42 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.25 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.59 ms ***\n",
      "Training costs a total of 1.0208 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start validate the model:\n",
      "*** During validation for #   0 batch, reading batch file costed 4.73 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 4.77 ms ***\n",
      "Validatoin costs a total of 0.0155 seconds!\n",
      "Finish train and validate the model:\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0009 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   0 batch, reading batch file costed 2.94 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.15 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.34 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.96 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.96 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.38 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.45 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.28 ms ***\n",
      "Training costs a total of 1.1063 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start validate the model:\n",
      "*** During validation for #   0 batch, reading batch file costed 4.95 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 4.65 ms ***\n",
      "Validatoin costs a total of 0.0163 seconds!\n",
      "Finish train and validate the model:\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0008 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   0 batch, reading batch file costed 3.04 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.09 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.91 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.40 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.44 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.89 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.03 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.50 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.37 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.50 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.67 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.82 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 4.11 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.45 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 4.78 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.89 ms ***\n",
      "Training costs a total of 1.1213 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start validate the model:\n",
      "*** During validation for #   0 batch, reading batch file costed 5.88 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 4.30 ms ***\n",
      "Validatoin costs a total of 0.0168 seconds!\n",
      "Finish train and validate the model:\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0012 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   1 batch, reading batch file costed 4.08 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 4.18 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.89 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.34 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.79 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.32 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 4.69 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 4.08 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 4.96 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.63 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.65 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 4.09 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.26 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 4.65 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 4.87 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.06 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.46 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.81 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 4.27 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.62 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 4.93 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 4.04 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 4.44 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.33 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 4.18 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.46 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.29 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.68 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.50 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.91 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.87 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.33 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.24 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.50 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.23 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.05 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.29 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.88 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.82 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.19 ms ***\n",
      "Training costs a total of 1.2029 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start validate the model:\n",
      "*** During validation for #   0 batch, reading batch file costed 4.97 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 4.33 ms ***\n",
      "Validatoin costs a total of 0.0152 seconds!\n",
      "Finish train and validate the model:\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0011 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   1 batch, reading batch file costed 3.32 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.08 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.90 ms ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** During training for #   1 batch, reading batch file costed 3.34 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.77 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.14 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.01 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.16 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.05 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.44 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.20 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.80 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.88 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.38 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.54 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.70 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.48 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.90 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.93 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.07 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.30 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.92 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.11 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.76 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.94 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 4.09 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 5.62 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.61 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.38 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.60 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.26 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.52 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.31 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.54 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.52 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.24 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.34 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.27 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.64 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.91 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.13 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.80 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.86 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.05 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.85 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.27 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.22 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.85 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.36 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.36 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.37 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.89 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.47 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.99 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.09 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.79 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.24 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.86 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.78 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.00 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.34 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.03 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.34 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.93 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.50 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.01 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.11 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.98 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.48 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.89 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.13 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.76 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.47 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.89 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.79 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.02 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.26 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.33 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.19 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.83 ms ***\n",
      "Training costs a total of 1.3572 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start validate the model:\n",
      "*** During validation for #   0 batch, reading batch file costed 4.69 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 4.58 ms ***\n",
      "Validatoin costs a total of 0.0152 seconds!\n",
      "Finish train and validate the model:\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0009 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   0 batch, reading batch file costed 2.93 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.07 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.09 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.30 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.35 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.22 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.26 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.99 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.74 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.32 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.89 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 5.09 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.91 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.69 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 4.89 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 4.75 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 4.07 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.65 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 5.11 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 4.83 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.60 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 4.21 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 4.52 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 5.88 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 4.57 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.17 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.84 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 4.60 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.54 ms ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** During training for #   1 batch, reading batch file costed 3.97 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.61 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 5.30 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.23 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.89 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.90 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.49 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.51 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.94 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.67 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.26 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.96 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.41 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.80 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.47 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 4.06 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.88 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.81 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.40 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.76 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 4.56 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.69 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.81 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.91 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.02 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.30 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.99 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.02 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.82 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.95 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.02 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.94 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.18 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.88 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 4.05 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.08 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.69 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.46 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.93 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.83 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.98 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.80 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.38 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.89 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.03 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.09 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.24 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.37 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.00 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.75 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.01 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.00 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.10 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.46 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.03 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.78 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.42 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 4.33 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.98 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.19 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.23 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.44 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.84 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.04 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.47 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.95 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.59 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.25 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.91 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.87 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.79 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.33 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.08 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.65 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.23 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.80 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.88 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.90 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 4.39 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.03 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.18 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.88 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.27 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.48 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.01 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.16 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.91 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.85 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.33 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 4.06 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.26 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.18 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.96 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.88 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.66 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 5.17 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.96 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.25 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.86 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.82 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.07 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.56 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.20 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.20 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.03 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.33 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.95 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.78 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.45 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.17 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.38 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.53 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.65 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.93 ms ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** During training for #   1 batch, reading batch file costed 3.09 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.95 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.96 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.28 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.30 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.94 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.18 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.61 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.44 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.47 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.49 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.01 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.28 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 4.52 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.13 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.17 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.99 ms ***\n",
      "Training costs a total of 1.7826 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start validate the model:\n",
      "*** During validation for #   0 batch, reading batch file costed 4.95 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 4.77 ms ***\n",
      "Validatoin costs a total of 0.0161 seconds!\n",
      "Finish train and validate the model:\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0009 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   1 batch, reading batch file costed 3.34 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.27 ms ***\n",
      "Training costs a total of 1.0714 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start validate the model:\n",
      "*** During validation for #   0 batch, reading batch file costed 4.68 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 4.28 ms ***\n",
      "Validatoin costs a total of 0.0149 seconds!\n",
      "Finish train and validate the model:\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0008 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   1 batch, reading batch file costed 3.21 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.21 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.41 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.75 ms ***\n",
      "Training costs a total of 1.0854 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start validate the model:\n",
      "*** During validation for #   0 batch, reading batch file costed 4.95 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 4.32 ms ***\n",
      "Validatoin costs a total of 0.0155 seconds!\n",
      "Finish train and validate the model:\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0007 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   0 batch, reading batch file costed 3.31 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.75 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.00 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.67 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.09 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.23 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.05 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.29 ms ***\n",
      "Training costs a total of 1.0254 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start validate the model:\n",
      "*** During validation for #   0 batch, reading batch file costed 4.66 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 4.39 ms ***\n",
      "Validatoin costs a total of 0.0151 seconds!\n",
      "Finish train and validate the model:\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0008 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   0 batch, reading batch file costed 2.77 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.34 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.46 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 4.07 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.98 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 4.41 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.91 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.16 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.13 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.69 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.50 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.04 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.15 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.91 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.04 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.75 ms ***\n",
      "Training costs a total of 1.0979 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start validate the model:\n",
      "*** During validation for #   0 batch, reading batch file costed 5.90 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 5.13 ms ***\n",
      "Validatoin costs a total of 0.0179 seconds!\n",
      "Finish train and validate the model:\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0009 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   1 batch, reading batch file costed 3.18 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.06 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.14 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 4.10 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.93 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.14 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.45 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.04 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.92 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.28 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.79 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.80 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.29 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.01 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.47 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.63 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 5.02 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.17 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.00 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.99 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.89 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.05 ms ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** During training for #   1 batch, reading batch file costed 4.12 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.00 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.04 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.90 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.91 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.26 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.95 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 4.48 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.19 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 4.09 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.30 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.63 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 4.08 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.62 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 4.16 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 5.25 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 4.62 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.28 ms ***\n",
      "Training costs a total of 1.2030 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start validate the model:\n",
      "*** During validation for #   0 batch, reading batch file costed 5.50 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 4.78 ms ***\n",
      "Validatoin costs a total of 0.0173 seconds!\n",
      "Finish train and validate the model:\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0008 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   0 batch, reading batch file costed 3.22 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 4.78 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.25 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.88 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.82 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.12 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.90 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.39 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.28 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.46 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.61 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 4.06 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 4.54 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.42 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.22 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 5.72 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.53 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.13 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.51 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.08 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.09 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.43 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.62 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.10 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 4.07 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.58 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.30 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 4.58 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.60 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 4.89 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.33 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.65 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.13 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.41 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 4.12 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.30 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.05 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.56 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.41 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.12 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.12 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.41 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.10 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.63 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.56 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 5.22 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.70 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 4.27 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.14 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.42 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.30 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.75 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.52 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.06 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.65 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.33 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.81 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.58 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.99 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.24 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.24 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.22 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.02 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.00 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.89 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.99 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.85 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.70 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 4.22 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.54 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.64 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.37 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.60 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 4.02 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.68 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.07 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.65 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.27 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.69 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.96 ms ***\n",
      "Training costs a total of 1.4163 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start validate the model:\n",
      "*** During validation for #   0 batch, reading batch file costed 4.58 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 4.18 ms ***\n",
      "Validatoin costs a total of 0.0146 seconds!\n",
      "Finish train and validate the model:\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0007 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   1 batch, reading batch file costed 3.03 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.92 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.16 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.31 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.19 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.95 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.20 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.84 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.67 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.01 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.46 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.86 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.05 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.89 ms ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** During training for #   0 batch, reading batch file costed 2.84 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.37 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.37 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.88 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.09 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.62 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.08 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.18 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.42 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.26 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.01 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.27 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.31 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.93 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.89 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.72 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.49 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.89 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.03 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.32 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.97 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.43 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.27 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.94 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.36 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.81 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.09 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.01 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.51 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.16 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.90 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.81 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.79 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.12 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.85 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.26 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.22 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.22 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 4.33 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.91 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.31 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.93 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.85 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.68 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 4.05 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.81 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.14 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 4.14 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.96 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.67 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 4.10 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.05 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.39 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.32 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.94 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.47 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.85 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.91 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.31 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.22 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.56 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.36 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.46 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.85 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.08 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.27 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.21 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.04 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.94 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.88 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.27 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.46 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.85 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.06 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.30 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.85 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.11 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.23 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.94 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.23 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.91 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.82 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.97 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.03 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.17 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.66 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.82 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.04 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.44 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.83 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.12 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.77 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.22 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.88 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.82 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.19 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.74 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.87 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.82 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.11 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.26 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.06 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.21 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.91 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.66 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.79 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.21 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.95 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.03 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.25 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.73 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.22 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.31 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.01 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.05 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.63 ms ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** During training for #   1 batch, reading batch file costed 3.23 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.15 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.94 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.84 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.71 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.46 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.24 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.31 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.92 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.36 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.13 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.27 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.06 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.87 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.29 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.78 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.75 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.08 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.46 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.51 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.28 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.04 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.33 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.39 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.68 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.30 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.11 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.12 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.53 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.01 ms ***\n",
      "Training costs a total of 1.7243 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start validate the model:\n",
      "*** During validation for #   0 batch, reading batch file costed 4.84 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 4.29 ms ***\n",
      "Validatoin costs a total of 0.0159 seconds!\n",
      "Finish train and validate the model:\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0010 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   0 batch, reading batch file costed 3.01 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.77 ms ***\n",
      "Training costs a total of 0.9774 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start validate the model:\n",
      "*** During validation for #   0 batch, reading batch file costed 5.56 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 4.96 ms ***\n",
      "Validatoin costs a total of 0.0172 seconds!\n",
      "Finish train and validate the model:\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0011 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   0 batch, reading batch file costed 3.58 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.99 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.29 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.91 ms ***\n",
      "Training costs a total of 1.0396 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start validate the model:\n",
      "*** During validation for #   0 batch, reading batch file costed 4.85 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 4.43 ms ***\n",
      "Validatoin costs a total of 0.0153 seconds!\n",
      "Finish train and validate the model:\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0007 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   1 batch, reading batch file costed 3.13 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.00 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.87 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.30 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 4.04 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.94 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.22 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.81 ms ***\n",
      "Training costs a total of 1.0383 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start validate the model:\n",
      "*** During validation for #   0 batch, reading batch file costed 4.90 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 5.22 ms ***\n",
      "Validatoin costs a total of 0.0167 seconds!\n",
      "Finish train and validate the model:\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0009 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   1 batch, reading batch file costed 3.55 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.64 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 4.01 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 4.23 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.99 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.20 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 4.50 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.15 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.98 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.17 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.10 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.49 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.87 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.64 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.19 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.82 ms ***\n",
      "Training costs a total of 1.0766 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start validate the model:\n",
      "*** During validation for #   0 batch, reading batch file costed 4.69 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 4.72 ms ***\n",
      "Validatoin costs a total of 0.0159 seconds!\n",
      "Finish train and validate the model:\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0010 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   1 batch, reading batch file costed 3.20 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.93 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.73 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.23 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.86 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.00 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.88 ms ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** During training for #   1 batch, reading batch file costed 3.84 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.24 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.03 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.95 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.80 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.17 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.87 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.91 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.18 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.85 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.13 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.22 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.08 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.45 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.86 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.89 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.35 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.21 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.38 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.32 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.85 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.50 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.02 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.23 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.93 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.23 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.93 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.04 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.25 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.41 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.12 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.06 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.95 ms ***\n",
      "Training costs a total of 1.1315 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start validate the model:\n",
      "*** During validation for #   0 batch, reading batch file costed 5.07 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 4.26 ms ***\n",
      "Validatoin costs a total of 0.0156 seconds!\n",
      "Finish train and validate the model:\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0007 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   1 batch, reading batch file costed 3.48 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.04 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.81 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.24 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.31 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.10 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.96 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.29 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.21 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.09 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.51 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.99 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.20 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.43 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.20 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.86 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.23 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.29 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.90 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.31 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.67 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.84 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.99 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.24 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.59 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.29 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.87 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.96 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.00 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.25 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.26 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.89 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.19 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.14 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.15 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.80 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.21 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.44 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.30 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.91 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.01 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.97 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.86 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.29 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.14 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.84 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.61 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.97 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.91 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.76 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 4.12 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.89 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.05 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.82 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.03 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.07 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.30 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.90 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.21 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.82 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.81 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.09 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.80 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.33 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.15 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.82 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.87 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.18 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.26 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.85 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.76 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.26 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.95 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.04 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.87 ms ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** During training for #   1 batch, reading batch file costed 3.41 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.88 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.04 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.88 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.48 ms ***\n",
      "Training costs a total of 1.3180 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start validate the model:\n",
      "*** During validation for #   0 batch, reading batch file costed 4.68 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 4.22 ms ***\n",
      "Validatoin costs a total of 0.0152 seconds!\n",
      "Finish train and validate the model:\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0009 seconds!\n",
      "Start train the model:\n",
      "*** During training for #   0 batch, reading batch file costed 2.96 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.37 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.16 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.29 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.20 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.83 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.13 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.11 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.29 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.03 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.21 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.87 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.37 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.38 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.94 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.57 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.10 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.40 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.89 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.86 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.33 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.13 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.99 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.43 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.96 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.07 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.36 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.11 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.40 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.19 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.31 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.86 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.96 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.24 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.95 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.59 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.11 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.78 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.47 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.03 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.52 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.09 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.94 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.58 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.03 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.11 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.91 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.10 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.89 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 4.06 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.85 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.12 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.86 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.89 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.99 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.79 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.27 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.06 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.99 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.38 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.28 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.91 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.87 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.47 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.90 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.02 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.46 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.92 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.21 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.70 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.97 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.08 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.90 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.30 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.11 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 2.98 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.93 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.32 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.88 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.04 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.44 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.89 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.93 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.70 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.17 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.91 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.03 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.54 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.98 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.88 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.06 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.82 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.25 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.17 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.11 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.16 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.22 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.11 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.23 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.20 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.43 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.92 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.07 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.02 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.81 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.36 ms ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** During training for #   1 batch, reading batch file costed 3.76 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.88 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.69 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.22 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.16 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.30 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.91 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.44 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.45 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.22 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.37 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.09 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.88 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.55 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.24 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.90 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.11 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.92 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.31 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.66 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.30 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.87 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.28 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.24 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.77 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.07 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.06 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.87 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.75 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.37 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.16 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.10 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.90 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.48 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.28 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.87 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.86 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.19 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.46 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.43 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.23 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.88 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.69 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.15 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.25 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.84 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.72 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.15 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.89 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.16 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 2.87 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.05 ms ***\n",
      "*** During training for #   0 batch, reading batch file costed 3.05 ms ***\n",
      "*** During training for #   1 batch, reading batch file costed 3.24 ms ***\n",
      "Training costs a total of 1.7145 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start validate the model:\n",
      "*** During validation for #   0 batch, reading batch file costed 4.55 ms ***\n",
      "*** During validation for #   1 batch, reading batch file costed 4.23 ms ***\n",
      "Validatoin costs a total of 0.0148 seconds!\n",
      "Finish train and validate the model:\n",
      "====================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tuning the mini_epoch\n",
    "output_tune_param(data, data_name, dataset, image_data_path, intermediate_data_folder, partition_nums, layers, \\\n",
    "                  dropout = 0.1, lr = tune_lr, weight_decay = 0.1, valid_part_num = 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CoraFull dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import CoraFull\n",
    "data_name = 'CoraFull'\n",
    "dataset = CoraFull(root = local_data_root + 'CoralFull')\n",
    "print('number of data: ', len(dataset))\n",
    "data = dataset[0]\n",
    "\n",
    "image_data_path = './results/' + data_name + '/' + test_folder_name\n",
    "intermediate_data_folder = './intermediate_data/' + data_name + '/' + test_folder_name\n",
    "\n",
    "partition_nums = [4]\n",
    "layers = [[128, 128]]\n",
    "# partition_nums = [2, 4, 8]\n",
    "# layers = [[], [32], [32, 32], [32, 32, 32]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune the parameter\n",
    "\n",
    "output_tune_param(data, data_name, dataset, image_data_path, intermediate_data_folder, partition_nums, layers, \\\n",
    "                  dropout = 0.5, lr = 0.0001, weight_decay = 0.001, mini_epoch_num = 20, valid_part_num = 2)\n",
    "\n",
    "# in-train process\n",
    "# output_train_investigate(data, data_name, dataset, image_data_path, intermediate_data_folder, partition_nums, layers, \\\n",
    "#                          dropout = 0.5, lr = 0.0001, weight_decay = 0.001, mini_epoch_num = 20, output_period = 40, valid_part_num = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check F1-score\n",
    "# output_F1_score(data, data_name, dataset, image_data_path, intermediate_data_folder, partition_nums, layers, \\\n",
    "#                 dropout = 0.1, lr = 0.0001, weight_decay = 0.1, mini_epoch_num = 20, valid_part_num = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CiteSeer Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "data_name = 'CiteSeer'\n",
    "dataset = Planetoid(root = local_data_root + 'Planetoid/CiteSeer', name=data_name)\n",
    "data = dataset[0]\n",
    "image_data_path = './results/' + data_name + '/' + test_folder_name\n",
    "\n",
    "partition_nums = [2, 4, 8]\n",
    "layers = [[], [16], [16, 16]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning the epoch number per batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking train loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PubMed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "data_name = 'PubMed'\n",
    "dataset = Planetoid(root = local_data_root + 'Planetoid/PubMed', name=data_name)\n",
    "data = dataset[0]\n",
    "image_data_path = './results/' + data_name + '/' + test_folder_name\n",
    "\n",
    "partition_nums = [2, 4, 8]\n",
    "layers = [[], [64], [64, 64], [64, 64, 64]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tune epoch number per batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the train error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free GPU memory\n",
    "# !(nvidia-smi | grep 'python' | awk '{ print $3 }' | xargs -n1 kill -9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_geometric]",
   "language": "python",
   "name": "conda-env-pytorch_geometric-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
