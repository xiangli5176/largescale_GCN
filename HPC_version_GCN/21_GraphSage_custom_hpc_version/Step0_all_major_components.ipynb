{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Post_utils import *\n",
    "# from multi_exec import *\n",
    "import torch\n",
    "import time\n",
    "\n",
    "import pickle\n",
    "import dill\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "from metric import *\n",
    "from model_graphsaint import GraphSAINT\n",
    "\n",
    "from utils import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minibatch machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import scipy.sparse as sp\n",
    "import scipy\n",
    "import numpy as np\n",
    "\n",
    "from graphsaint_cython.norm_aggr import *\n",
    "from samplers import *\n",
    "\n",
    "def _coo_scipy2torch(adj):\n",
    "    \"\"\"\n",
    "    convert a scipy sparse COO matrix to torch\n",
    "    \n",
    "    Torch supports sparse tensors in COO(rdinate) format, which can efficiently store and process tensors \n",
    "    for which the majority of elements are zeros.\n",
    "    A sparse tensor is represented as a pair of dense tensors: a tensor of values and a 2D tensor of indices. \n",
    "    A sparse tensor can be constructed by providing these two tensors, as well as the size of the sparse tensor \n",
    "    \"\"\"\n",
    "    values = adj.data\n",
    "    indices = np.vstack((adj.row, adj.col))\n",
    "    i = torch.LongTensor(indices)\n",
    "    v = torch.FloatTensor(values)\n",
    "    \n",
    "    return torch.sparse.FloatTensor(i,v, torch.Size(adj.shape))\n",
    "\n",
    "\n",
    "class Minibatch:\n",
    "    \"\"\"\n",
    "        This minibatch iterator iterates over nodes for supervised learning.\n",
    "        Data transferred to GPU:     A  init: 1) self.adj_full_norm;  2) self.norm_loss_test;\n",
    "                                     B  set_sampler:  1) self.norm_loss_train\n",
    "                                     C  one_batch : 1) subgraph adjacency matrix (adj)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, adj_full_norm, adj_train, role, train_params, cpu_eval = False, num_cpu_core = 1):\n",
    "        \"\"\"\n",
    "        role:       array of string (length |V|)\n",
    "                    storing role of the node ('tr'/'va'/'te')\n",
    "        \"\"\"\n",
    "        self.num_cpu_core = num_cpu_core\n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "        if cpu_eval:\n",
    "            self.use_cuda = False\n",
    "        \n",
    "        # store all the node roles as the numpy array:\n",
    "        self.node_train = np.array(role['tr'])\n",
    "        self.node_val = np.array(role['va'])\n",
    "        self.node_test = np.array(role['te'])\n",
    "\n",
    "        # self.adj_full_norm : torch sparse tensor\n",
    "        self.adj_full_norm = _coo_scipy2torch(adj_full_norm.tocoo())\n",
    "        self.adj_train = adj_train\n",
    "\n",
    "        # below: book-keeping for mini-batch\n",
    "        self.node_subgraph = None\n",
    "        self.batch_num = -1\n",
    "\n",
    "        # all the subgraph attributes should be used for the training process\n",
    "        self.method_sample = None\n",
    "        self.subgraphs_remaining_indptr = []\n",
    "        self.subgraphs_remaining_indices = []\n",
    "        self.subgraphs_remaining_data = []\n",
    "        self.subgraphs_remaining_nodes = []\n",
    "        self.subgraphs_remaining_edge_index = []\n",
    "        \n",
    "        # What is this norm_loss aimed at?\n",
    "        self.norm_loss_train = np.zeros(self.adj_train.shape[0])\n",
    "        # norm_loss_test is used in full batch evaluation (without sampling). so neighbor features are simply averaged.\n",
    "        self.norm_loss_test = np.zeros(self.adj_full_norm.shape[0])\n",
    "        \n",
    "        _denom = len(self.node_train) + len(self.node_val) +  len(self.node_test)\n",
    "        \n",
    "        # instead of assign all elements of self.norm_loss_test to the same averaged denominator, separately assingment instead. \n",
    "        # does this mean there are other meaningless roles beyond: test, train and validation?\n",
    "        self.norm_loss_test[self.node_train] = 1./_denom     \n",
    "        self.norm_loss_test[self.node_val] = 1./_denom\n",
    "        self.norm_loss_test[self.node_test] = 1./_denom\n",
    "        self.norm_loss_test = torch.from_numpy(self.norm_loss_test.astype(np.float32))\n",
    "        \n",
    "            \n",
    "        self.norm_aggr_train = np.zeros(self.adj_train.size)\n",
    "        \n",
    "        self.sample_coverage = train_params['sample_coverage']\n",
    "        self.deg_train = np.array(self.adj_train.sum(1)).flatten()   # sum the degree of each train node, here sum along column for adjacency matrix\n",
    "\n",
    "\n",
    "    def set_sampler(self, train_phases, input_neigh_deg = [10, 5], core_par_sampler = 1, samples_per_processor = 200):\n",
    "        \"\"\"\n",
    "            Train_phases (a dict defined in the .yml file) : usually including : end, smapler, size_subg_edge\n",
    "            end:  number of total epochs to stop\n",
    "            sampler: category for sampler (e.g. edge)\n",
    "            size_subg_edge:  size of the subgraph in number of edges\n",
    "        \"\"\"\n",
    "        \n",
    "        self.subgraphs_remaining_indptr = list()\n",
    "        self.subgraphs_remaining_indices = list()\n",
    "        self.subgraphs_remaining_data = list()\n",
    "        self.subgraphs_remaining_nodes = list()\n",
    "        self.subgraphs_remaining_edge_index = list()\n",
    "        \n",
    "        self.method_sample = train_phases['sampler']   # one of the string indicators regarding sampler methods\n",
    "        if self.method_sample == 'mrw':\n",
    "            if 'deg_clip' in train_phases:\n",
    "                _deg_clip = int(train_phases['deg_clip'])\n",
    "            else:\n",
    "                _deg_clip = 100000      # setting this to a large number so essentially there is no clipping in probability\n",
    "            self.size_subg_budget = train_phases['size_subgraph']\n",
    "            self.graph_sampler = mrw_sampling(self.adj_train, self.node_train,\n",
    "                                self.size_subg_budget, train_phases['size_frontier'], _deg_clip, \n",
    "                                        core_par_sampler = core_par_sampler, samples_per_processor = samples_per_processor)\n",
    "        elif self.method_sample == 'rw':\n",
    "            self.size_subg_budget = train_phases['num_root'] * train_phases['depth']\n",
    "            self.graph_sampler = rw_sampling(self.adj_train, self.node_train,\n",
    "                                self.size_subg_budget, int(train_phases['num_root']), int(train_phases['depth']), \n",
    "                                        core_par_sampler = core_par_sampler, samples_per_processor = samples_per_processor)\n",
    "        elif self.method_sample == 'edge':\n",
    "            self.size_subg_budget = train_phases['size_subg_edge'] * 2\n",
    "            self.graph_sampler = edge_sampling(self.adj_train, self.node_train, train_phases['size_subg_edge'], \n",
    "                                        core_par_sampler = core_par_sampler, samples_per_processor = samples_per_processor)\n",
    "        elif self.method_sample == 'node':\n",
    "            self.size_subg_budget = train_phases['size_subgraph']\n",
    "            self.graph_sampler = node_sampling(self.adj_train,self.node_train, self.size_subg_budget, \n",
    "                                        core_par_sampler = core_par_sampler, samples_per_processor = samples_per_processor)\n",
    "        elif self.method_sample == 'full_batch':\n",
    "            self.size_subg_budget = self.node_train.size\n",
    "            self.graph_sampler = full_batch_sampling(self.adj_train,self.node_train, self.size_subg_budget, \n",
    "                                        core_par_sampler = core_par_sampler, samples_per_processor = samples_per_processor)\n",
    "        elif self.method_sample == 'sage_node':\n",
    "            self.size_subg_budget = train_phases['size_subgraph']\n",
    "            self.graph_sampler = sage_sampling(self.adj_train,self.node_train, self.size_subg_budget, input_neigh_deg = input_neigh_deg,\n",
    "                                        core_par_sampler = core_par_sampler, samples_per_processor = samples_per_processor)\n",
    "            print(\"using sage node sampler! \")\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        self.norm_loss_train = np.zeros(self.adj_train.shape[0])\n",
    "        self.norm_aggr_train = np.zeros(self.adj_train.size).astype(np.float32)\n",
    "\n",
    "        # For edge sampler, no need to estimate norm factors, we can calculate directly.\n",
    "        # However, for integrity of the framework, we decide to follow the same procedure for all samplers: \n",
    "        # 1. sample enough number of subgraphs\n",
    "        # 2. estimate norm factor alpha and lambda\n",
    "        tot_sampled_nodes = 0\n",
    "        while True:\n",
    "            self.par_graph_sample('train')\n",
    "            tot_sampled_nodes = sum([len(n) for n in self.subgraphs_remaining_nodes])\n",
    "            if tot_sampled_nodes > self.sample_coverage * self.node_train.size:\n",
    "                break\n",
    "        print()\n",
    "        num_subg = len(self.subgraphs_remaining_nodes)  # each subgraph nodes are stored as one list inside the self.subgraphs_remaining_nodes\n",
    "        for i in range(num_subg):\n",
    "            self.norm_aggr_train[self.subgraphs_remaining_edge_index[i]] += 1\n",
    "            self.norm_loss_train[self.subgraphs_remaining_nodes[i]] += 1\n",
    "        assert self.norm_loss_train[self.node_val].sum() + self.norm_loss_train[self.node_test].sum() == 0\n",
    "        for v in range(self.adj_train.shape[0]):\n",
    "            i_s = self.adj_train.indptr[v]\n",
    "            i_e = self.adj_train.indptr[v+1]\n",
    "            val = np.clip(self.norm_loss_train[v]/self.norm_aggr_train[i_s:i_e], 0, 1e4)\n",
    "            val[np.isnan(val)] = 0.1\n",
    "            self.norm_aggr_train[i_s:i_e] = val\n",
    "        \n",
    "        # normalize the self.norm_loss_train:\n",
    "        self.norm_loss_train[np.where(self.norm_loss_train==0)[0]] = 0.1\n",
    "        self.norm_loss_train[self.node_val] = 0\n",
    "        self.norm_loss_train[self.node_test] = 0\n",
    "        self.norm_loss_train[self.node_train] = num_subg/self.norm_loss_train[self.node_train]/self.node_train.size\n",
    "        self.norm_loss_train = torch.from_numpy(self.norm_loss_train.astype(np.float32))\n",
    "\n",
    "    # each time finish one-time sampling: generate a single sample subgraph\n",
    "    def par_graph_sample(self, phase):\n",
    "        \"\"\"\n",
    "           Phase: can be a string \"train\"\n",
    "        \"\"\"\n",
    "        t0 = time.time()\n",
    "        _indptr, _indices, _data, _v, _edge_index = self.graph_sampler.par_sample(phase)\n",
    "        t1 = time.time()\n",
    "        # create 200 subgraphs per CPU, these 200 graphs may be generated by different cores, but 200 each time, not to exceed the memory limit\n",
    "        print('sampling 200 subgraphs:   time = {:.3f} sec'.format(t1 - t0), end=\"\\r\")\n",
    "        self.subgraphs_remaining_indptr.extend(_indptr)   # add lists into the subgraphs_remaining_indptr, each list is a subgraph\n",
    "        self.subgraphs_remaining_indices.extend(_indices)\n",
    "        self.subgraphs_remaining_data.extend(_data)\n",
    "        self.subgraphs_remaining_nodes.extend(_v)\n",
    "        self.subgraphs_remaining_edge_index.extend(_edge_index)\n",
    "\n",
    "    def one_batch(self, mode='train'):\n",
    "        \"\"\"\n",
    "            self.batch_num : for train mode, create one batch and the batch number will be increased by 1\n",
    "        \"\"\"\n",
    "        if mode in ['val','test']:\n",
    "            self.node_subgraph = np.arange(self.adj_full_norm.shape[0])  # include all the nodes inside the graph\n",
    "            adj = self.adj_full_norm\n",
    "        else:\n",
    "            assert mode == 'train'\n",
    "            \n",
    "            \n",
    "            if len(self.subgraphs_remaining_nodes) == 0:\n",
    "                self.par_graph_sample('train')   # if there is no sampled subgraphs, then make one\n",
    "                print()\n",
    "\n",
    "            self.node_subgraph = self.subgraphs_remaining_nodes.pop()\n",
    "            self.size_subgraph = len(self.node_subgraph)\n",
    "            adj = sp.csr_matrix((self.subgraphs_remaining_data.pop(),\\\n",
    "                                 self.subgraphs_remaining_indices.pop(),\\\n",
    "                                 self.subgraphs_remaining_indptr.pop()),\\\n",
    "                                 shape=(self.size_subgraph,self.size_subgraph))\n",
    "            adj_edge_index = self.subgraphs_remaining_edge_index.pop()\n",
    "            #print(\"{} nodes, {} edges, {} degree\".format(self.node_subgraph.size,adj.size,adj.size/self.node_subgraph.size))\n",
    "            norm_aggr(adj.data, adj_edge_index, self.norm_aggr_train, num_proc = self.num_cpu_core)\n",
    "            adj = adj_norm(adj, deg = self.deg_train[self.node_subgraph])\n",
    "            adj = _coo_scipy2torch(adj.tocoo())\n",
    "            \n",
    "            self.batch_num += 1          # create one batch\n",
    "            \n",
    "        norm_loss = self.norm_loss_test if mode in ['val','test'] else self.norm_loss_train\n",
    "        norm_loss = norm_loss[self.node_subgraph]\n",
    "        # this self.node_subgraph is to select the target nodes, can be left on the CPU\n",
    "        \n",
    "        # for evaluation: all nodes, its adj_full_norm and norm_loss_test \n",
    "        return self.node_subgraph, adj, norm_loss\n",
    "\n",
    "\n",
    "    def num_training_batches(self):\n",
    "        return math.ceil(self.node_train.shape[0] / float(self.size_subg_budget))\n",
    "\n",
    "    def shuffle(self):\n",
    "        self.node_train = np.random.permutation(self.node_train)\n",
    "        self.batch_num = -1\n",
    "\n",
    "    def end(self):\n",
    "        return (self.batch_num + 1) * self.size_subg_budget >= self.node_train.shape[0]   # greater or equal to the number of train nodes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic execution components "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a lambda func\n",
    "f_mean = lambda l: sum(l)/len(l)\n",
    "\n",
    "def evaluate_full_batch(model, minibatch, mode='val'):\n",
    "    \"\"\"\n",
    "        Full batch evaluation: for validation and test sets only.\n",
    "        When calculating the F1 score, we will mask the relevant root nodes.\n",
    "        mode: can be val or test\n",
    "    \"\"\"\n",
    "    loss, preds, labels = model.eval_step(*minibatch.one_batch(mode = mode))\n",
    "    node_val_test = minibatch.node_val if mode=='val' else minibatch.node_test\n",
    "    # may not be necessary \n",
    "    f1_scores = calc_f1(to_numpy(labels[node_val_test]), to_numpy(preds[node_val_test]), model.sigmoid_loss)\n",
    "    return loss, f1_scores[0], f1_scores[1]\n",
    "\n",
    "\n",
    "def train_setting(dataname, datapath, train_config_file):\n",
    "    \"\"\"\n",
    "        YAML (a recursive acronym for \"YAML Ain't Markup Language\") is a human-readable data-serialization language. \n",
    "        It is commonly used for configuration files and in applications where data is being stored or transmitted.\n",
    "    \n",
    "        yaml.load is as powerful as pickle.load and so may call any Python function. Check the yaml.safe_load function though.\n",
    "        The function yaml.load converts a YAML document to a Python object.\n",
    "    \"\"\"\n",
    "    with open(train_config_file) as f_train_config:\n",
    "        train_config = yaml.load(f_train_config)\n",
    "        \n",
    "    arch_gcn = {'dim':-1,\n",
    "                'aggr':'concat',\n",
    "                'loss':'softmax',\n",
    "                'arch':'1',\n",
    "                'act':'I',\n",
    "                'bias':'norm'}\n",
    "    # check the loss:  default to be softmax, multi-class problem, each node can only belong to just one class at last\n",
    "    arch_gcn.update(train_config['network'][0])   # train_config['network'] is a list of dict\n",
    "    \n",
    "    \n",
    "    train_params = {'lr' : 0.01, 'weight_decay' : 0., 'norm_loss':True, 'norm_aggr':True, 'q_threshold' : 50, 'q_offset':0}\n",
    "    train_params.update(train_config['params'][0])\n",
    "    train_phases = train_config['phase']\n",
    "    for ph in train_phases:\n",
    "        assert 'end' in ph\n",
    "        assert 'sampler' in ph\n",
    "    print(\"Loading training data..\")\n",
    "    temp_data = load_data(dataname, datapath = datapath)\n",
    "    train_data = process_graph_data(*temp_data)\n",
    "    print(\"Done loading training data..\")\n",
    "    \n",
    "    # train_data is a tuple: adj_full, adj_train, feats, class_arr, role\n",
    "    return train_params, train_phases, train_data, arch_gcn\n",
    "\n",
    "def prepare(working_dir, train_data, train_params, arch_gcn):\n",
    "    \"\"\"\n",
    "        working_dir: main working dir for experiments\n",
    "        train_params: contain settings for the mini-batch setting\n",
    "        arch_gcn: contain all the settings \n",
    "    \"\"\"\n",
    "    adj_full, adj_train, feat_full, class_arr, role = train_data\n",
    "    adj_full = adj_full.astype(np.int32)\n",
    "    adj_train = adj_train.astype(np.int32)\n",
    "    adj_full_norm = adj_norm(adj_full)\n",
    "    num_classes = class_arr.shape[1]\n",
    "    \n",
    "    # key switch :  cpu_eval (bool)\n",
    "    # establish two models, one for train, one for evaluation, because later the model_eval will load the trained model parameters\n",
    "    \n",
    "    # for training process: on GPU\n",
    "    minibatch = Minibatch(adj_full_norm, adj_train, role, train_params)\n",
    "    model = GraphSAINT(num_classes, arch_gcn, train_params, feat_full, class_arr)\n",
    "    # for evaluation: validaiton/test  : on CPU\n",
    "    minibatch_eval = Minibatch(adj_full_norm, adj_train,role, train_params, cpu_eval=True)\n",
    "    model_eval = GraphSAINT(num_classes, arch_gcn, train_params, feat_full, class_arr, cpu_eval=True)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "        \n",
    "    # model, model_eval, mini_batch_eval can be saved as pickle file for use later\n",
    "\n",
    "    ### but cannot pickle lambda func for now\n",
    "\n",
    "    prepare_data_folder = working_dir + 'prepare_data/'\n",
    "    os.makedirs(os.path.dirname(prepare_data_folder), exist_ok=True)\n",
    "\n",
    "    train_input_file_name = prepare_data_folder + 'model_train_input'\n",
    "    with open(train_input_file_name, \"wb\") as fp:\n",
    "        dill.dump((minibatch, model), fp)\n",
    "\n",
    "    evaluation_input_file_name = prepare_data_folder + 'model_eval_input'\n",
    "    with open(evaluation_input_file_name, \"wb\") as fp:\n",
    "        dill.dump((minibatch_eval, model_eval), fp)\n",
    "    \n",
    "    # return model, minibatch, minibatch_eval, model_eval\n",
    "\n",
    "\n",
    "def train_investigate(snap_model_folder, train_phases, model, minibatch, eval_train_every, snapshot_every = 10,\n",
    "          mini_epoch_num = 5, multilabel = True, input_neigh_deg = [10, 5], core_par_sampler = 1, samples_per_processor = 200):\n",
    "    \"\"\"\n",
    "    PURPOSE:  to go through each training phase and take a snapshot of current mode and saved as pickle files\n",
    "        snap_model_folder : folder to save the model snapshots during training\n",
    "        train_phases:  use defined train fases defined in the .yml file\n",
    "        model :  graphsaint model for training\n",
    "        minibatch:   minibatch for training, usually with batches pool\n",
    "        eval_train_every :  periodically store the train loss during the training process\n",
    "        snapshot_every :  periodically store the states of the trained model for later evaluation\n",
    "        mini_epoch_num :  how long the training will focus on one single batch\n",
    "        multilabel : True if a multi-label task, otherwise a multi-class case\n",
    "        core_par_sampler : how many CPU cores  will be used on each CPU\n",
    "        samples_per_processor : how many samples will be generated from each CPU\n",
    "    return: 1) total training time;  2) data uploading time\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    epoch_ph_start = 0\n",
    "    time_train, time_upload, pure_time_train = 0, 0, 0\n",
    "    \n",
    "    # establish a new folder if it does not exist\n",
    "#     os.makedirs(snap_model_folder, exist_ok = True)\n",
    "    \n",
    "    for ip, phase in enumerate(train_phases):\n",
    "        printf('START PHASE {:4d}'.format(ip),style='underline')\n",
    "        \n",
    "        minibatch.set_sampler(phase, input_neigh_deg = input_neigh_deg, core_par_sampler = core_par_sampler, samples_per_processor = samples_per_processor)  # one of the phases defined in the phase section of the .yml file, here phase is a dict\n",
    "        num_batches = minibatch.num_training_batches()\n",
    "#         print('calculated batch number is: ', num_batches)\n",
    "        \n",
    "        \n",
    "        epoch_ph_end = int(phase['end'])\n",
    "        macro_epoch_part_num = (epoch_ph_end - epoch_ph_start) // mini_epoch_num\n",
    "        for macro_epoch_idx in range(macro_epoch_part_num):\n",
    "            \n",
    "            minibatch.shuffle()  # each time shuffle, restore the self.batch_num back to -1\n",
    "            l_loss_tr, l_f1mic_tr, l_f1mac_tr = [], [], []\n",
    "            \n",
    "            actual_batch_num = 0\n",
    "            \n",
    "#             while not minibatch.end():\n",
    "            for batch_idx in range(num_batches):\n",
    "                \n",
    "                node_subgraph, adj_subgraph, norm_loss_subgraph = minibatch.one_batch(mode='train')   # here minibatch should be an interator\n",
    "                # redefine all the function interfaces to explicitely upload and run the training\n",
    "                \n",
    "                ### =========== prepare for all the data to be used ==============\n",
    "                feat_subg = model.feat_full[node_subgraph]\n",
    "                label_subg = model.label_full[node_subgraph]\n",
    "                \n",
    "                if not multilabel:\n",
    "                    # for the multi-class, need type conversion\n",
    "                    label_full_cat = torch.from_numpy(model.label_full.numpy().argmax(axis=1).astype(np.int64))\n",
    "\n",
    "                label_subg_converted = label_subg if multilabel else label_full_cat[node_subgraph]\n",
    "                \n",
    "                t0 = time.time()\n",
    "                # transfer data to the GPU\n",
    "                feat_subg = feat_subg.cuda()\n",
    "                label_subg = label_subg.cuda()\n",
    "                adj_subgraph = adj_subgraph.cuda()\n",
    "                norm_loss_subgraph = norm_loss_subgraph.cuda()\n",
    "                label_subg_converted = label_subg_converted.cuda()\n",
    "                time_upload += time.time() - t0\n",
    "                \n",
    "                \n",
    "                for micro_epoch_idx in range(mini_epoch_num):\n",
    "                    real_epoch_idx = 1 + micro_epoch_idx + macro_epoch_idx * mini_epoch_num + epoch_ph_start\n",
    "                    printf('Epoch {:4d}, Batch ID {}'.format(real_epoch_idx, batch_idx),style='bold')\n",
    "                    # pure training process:\n",
    "                    t1 = time.time()\n",
    "                    loss_train, preds_train = \\\n",
    "                            model.train_step(node_subgraph, adj_subgraph, norm_loss_subgraph, feat_subg, label_subg_converted)\n",
    "                    \n",
    "                    pure_time_train += time.time() - t1\n",
    "                    \n",
    "                    # take a snapshot of current model\n",
    "                    if batch_idx == num_batches - 1 and real_epoch_idx % snapshot_every == 0:\n",
    "                        snap_model_file = snap_model_folder + 'snapshot_epoch_' + str(real_epoch_idx) + '.pkl'\n",
    "                        torch.save(model.state_dict(), snap_model_file)  # store the current state_dict() into the file: 'tmp.pkl'\n",
    "                    \n",
    "                    labels_train = label_subg\n",
    "                    # periodically calculate all the statistics and store them\n",
    "                    if not minibatch.batch_num % eval_train_every:\n",
    "                        # to_numpy already convert tensor onto CPU\n",
    "                        f1_mic, f1_mac = calc_f1(to_numpy(labels_train), to_numpy(preds_train), model.sigmoid_loss)\n",
    "                        l_loss_tr.append(loss_train)\n",
    "                        l_f1mic_tr.append(f1_mic)\n",
    "                        l_f1mac_tr.append(f1_mac)\n",
    "            \n",
    "            \n",
    "        # for different training phase, train it continuously \n",
    "        epoch_ph_start = int(phase['end'])\n",
    "        printf(\"Optimization Finished!\", style=\"yellow\")\n",
    "    \n",
    "    time_train = pure_time_train + time_upload\n",
    "    # after going through all the training phases, print out the total time\n",
    "    printf(\"Total training time: {:6.2f} ms\".format(time_train * 1000), style='red')\n",
    "    printf(\"Total train data uploading time: {:6.2f} ms\".format(time_upload * 1000), style='red')\n",
    "    \n",
    "    return time_train * 1000, time_upload * 1000\n",
    "            \n",
    "\n",
    "def evaluate(snap_model_folder, minibatch_eval, model_eval, epoch_idx, mode='val'):\n",
    "    \"\"\"\n",
    "        Perform the evaluation: either validaiton or test offline from saved snapshot of the models\n",
    "        generate evaluation results from a single timepoint snapshot of the trained model\n",
    "        return : micro_f1 score, macro_f1 score\n",
    "        \n",
    "    \"\"\"\n",
    "    ### location to output the evaluation result\n",
    "    \n",
    "    snap_model_file = snap_model_folder + 'snapshot_epoch_' + str(epoch_idx) + '.pkl'\n",
    "\n",
    "    model_eval.load_state_dict(torch.load(snap_model_file, map_location=lambda storage, loc: storage))\n",
    "\n",
    "    loss_val, f1mic_val, f1mac_val = evaluate_full_batch(model_eval, minibatch_eval, mode = mode)\n",
    "\n",
    "    return f1mic_val, f1mac_val\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### multi-execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiexec module:\n",
    "def execute_train_investigate(image_path, work_dir, train_phases, model, minibatch, eval_train_every, \n",
    "                              tune_param_name, tune_val_label, tune_val, trainer_id = 0,\n",
    "                         snapshot_every = 5, mini_epoch_num = 5, multilabel = True, input_neigh_deg = [10, 5],\n",
    "                              core_par_sampler = 1, samples_per_processor = 200):\n",
    "    \"\"\"\n",
    "        return all validation-F1 for all four models\n",
    "    \"\"\"\n",
    "    # run the training process for the model\n",
    "    tune_model_folder = work_dir + 'model_snapshot/tune_' + tune_param_name + '_' + str(tune_val_label) + \\\n",
    "                        '/model_trainer_' + str(trainer_id) + '/'\n",
    "    \n",
    "    os.makedirs(os.path.dirname(tune_model_folder), exist_ok=True)\n",
    "    \n",
    "    # to apply any tuning values\n",
    "    total_time_train, time_upload = train_investigate(tune_model_folder, train_phases, model, minibatch, eval_train_every, \n",
    "                                    snapshot_every = snapshot_every, mini_epoch_num = tune_val, multilabel = multilabel, input_neigh_deg = input_neigh_deg,\n",
    "                              core_par_sampler = core_par_sampler, samples_per_processor = samples_per_processor)\n",
    "    \n",
    "    time_info_folder = image_path + 'train_res/tune_' + tune_param_name + '_' + str(tune_val_label) + \\\n",
    "                                    '/model_trainer_' + str(trainer_id) + '/'\n",
    "    \n",
    "    os.makedirs(os.path.dirname(time_info_folder), exist_ok=True)\n",
    "    \n",
    "    time_info_file_name = time_info_folder + 'train_time'\n",
    "    with open(time_info_file_name, \"wb\") as fp:\n",
    "        pickle.dump((total_time_train, time_upload), fp)\n",
    "            \n",
    "    \n",
    "def execute_validation_investigate(image_path, work_dir, minibatch_eval, model_eval, snapshot_epoch_list, \n",
    "                                 tune_param_name, tune_val_label, tune_val, trainer_id = 0):\n",
    "    \"\"\"\n",
    "        Perform the validaiton offline from saved snapshot of the models\n",
    "        snapshot_epoch_list :  a list of the saved models to perform evaluation\n",
    "    \"\"\"\n",
    "    \n",
    "    tune_model_folder = work_dir + 'model_snapshot/tune_' + tune_param_name + '_' + str(tune_val_label) + \\\n",
    "                                    '/model_trainer_' + str(trainer_id) + '/'\n",
    "    \n",
    "    validation_res_folder = image_path + 'validation_res/tune_' + tune_param_name + '_' + str(tune_val_label) + \\\n",
    "                                    '/validation_trainer_' + str(trainer_id) + '/'\n",
    "    \n",
    "    os.makedirs(os.path.dirname(validation_res_folder), exist_ok=True)\n",
    "    \n",
    "    # start evaluation:\n",
    "    for validation_epoch in snapshot_epoch_list:\n",
    "        \n",
    "        res = evaluate(tune_model_folder, minibatch_eval, model_eval, validation_epoch)\n",
    "        \n",
    "        validation_res_file_name = validation_res_folder + 'model_epoch_' + str(validation_epoch)\n",
    "        with open(validation_res_file_name, \"wb\") as fp:\n",
    "            pickle.dump(res, fp)\n",
    "\n",
    "            \n",
    "    \n",
    "def execute_test_tuning(image_path, work_dir, minibatch_eval, model_eval, snapshot_epoch_list, \n",
    "                                 tune_param_name, tune_val_label, tune_val, trainer_id = 0):\n",
    "    \"\"\"\n",
    "        1) After the validation, select the epoch with the best validation score\n",
    "        2) use the trained model at the selected optimal epoch of validation\n",
    "        3) perform the evaluate func for the test data\n",
    "    \"\"\"\n",
    "    # start to search for the trained model epoch with the best validation f1 socre\n",
    "    f1mic_best, ep_best = 0, -1\n",
    "    validation_res_folder = image_path + 'validation_res/tune_' + tune_param_name + '_' + str(tune_val_label) + \\\n",
    "                                    '/validation_trainer_' + str(trainer_id) + '/'\n",
    "    \n",
    "    for validation_epoch in snapshot_epoch_list:\n",
    "        validation_res_file_name = validation_res_folder + 'model_epoch_' + str(validation_epoch)\n",
    "        with open(validation_res_file_name, \"rb\") as fp:\n",
    "            f1mic_val, f1mac_val = pickle.load(fp)\n",
    "        \n",
    "        if f1mic_val > f1mic_best:\n",
    "            f1mic_best, ep_best = f1mic_val, validation_epoch\n",
    "        \n",
    "    # use the selected model to perform on the test\n",
    "    tune_model_folder = work_dir + 'model_snapshot/tune_' + tune_param_name + '_' + str(tune_val_label) + \\\n",
    "                                    '/model_trainer_' + str(trainer_id) + '/'\n",
    "    \n",
    "    # return 1) micro-f1 ;  2) macro-f1\n",
    "    res = evaluate(tune_model_folder, minibatch_eval, model_eval, ep_best, mode = 'test')\n",
    "    \n",
    "    # save the selected best saved snapshot\n",
    "    best_model_file = tune_model_folder + 'snapshot_epoch_' + str(ep_best) + '.pkl'\n",
    "    shutil.copy2(best_model_file, tune_model_folder + 'best_saved_snapshot.pkl')\n",
    "    \n",
    "    # store the resulting data on the disk\n",
    "    test_res_folder = image_path + 'test_res/tune_' + tune_param_name + '_' + str(tune_val_label) + '/'\n",
    "    os.makedirs(os.path.dirname(test_res_folder), exist_ok=True)\n",
    "    test_res_file = test_res_folder + 'res_trainer_' + str(trainer_id)\n",
    "    \n",
    "    with open(test_res_file, \"wb\") as fp:\n",
    "        pickle.dump(res, fp)\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = '/home/xiangli/projects/tmpdata/GCN/GraphSaint/'\n",
    "\n",
    "working_dir = './res_step0_sage_all_in_one/'\n",
    "prepare_data_folder = working_dir + 'prepare_data/'\n",
    "img_path = working_dir + 'result/'\n",
    "\n",
    "core_par_sampler = 1\n",
    "samples_per_processor = -(-200 // core_par_sampler) # round up division\n",
    "eval_train_every = 5  # period to record the train loss\n",
    "\n",
    "### ================ Start to do flexible settings according to different dataset: \n",
    "# read the total epoch number from the yml file to determine the mini_epoch_num and eval_train_every\n",
    "data_name = 'Flickr'\n",
    "# train_config_yml = './table2/flickr2_e.yml'\n",
    "train_config_yml = './table2/flickr2_sage.yml'\n",
    "multilabel_tag = False\n",
    "\n",
    "# data_name = 'PPI_small'\n",
    "# train_config_yml = './table2/ppi2_e.yml'\n",
    "\n",
    "\n",
    "tune_param_name = 'mini_epoch_num'\n",
    "tune_val_label_list = [1, 5] \n",
    "tune_val_list = [val for val in tune_val_label_list]\n",
    "\n",
    "snapshot_period = 5   # period when to take a snapshot of the model for validation later\n",
    "\n",
    "# refer to the yml file to decide the training period:\n",
    "model_epoch_list = list(range(snapshot_period, 31, snapshot_period))    # snapshot epoch list for validation\n",
    "\n",
    "trainer_list = list(range(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangli/anaconda3/envs/pytorch_1_4_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:26: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data..\n",
      "Done loading training data..\n"
     ]
    }
   ],
   "source": [
    "# =============== Step1 *** prepare for the batches, models, model_evaluation\n",
    "train_params, train_phases, train_data, arch_gcn = train_setting(data_name, datapath, train_config_yml)\n",
    "prepare(working_dir, train_data, train_params, arch_gcn)\n",
    "train_phase_file_name = prepare_data_folder + 'model_train_phase'\n",
    "with open(train_phase_file_name, \"wb\") as fp:\n",
    "    dill.dump(train_phases, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Study using the adj_train to generate the train neighbor nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_full, adj_train, feat_full, class_arr, role = train_data\n",
    "adj_full = adj_full.astype(np.int32)\n",
    "adj_train = adj_train.astype(np.int32)\n",
    "node_train = np.array(role['tr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"node_train Type is: {} ; and Shape is {} \".format(type(node_train), node_train.shape) )\n",
    "print(\"adj_train.indptr info:  \", type(adj_train.indptr), adj_train.indptr.shape, adj_train.indptr[:5])\n",
    "print(\"adj_full.indptr info:  \", type(adj_full.indptr), adj_full.indptr.shape, adj_full.indptr[:5])\n",
    "v = 0\n",
    "print( sum(adj_train.data[adj_train.indptr[v]:adj_train.indptr[v+1] ] ) )\n",
    "print( adj_train.indices[adj_train.indptr[v]:adj_train.indptr[v+1] ]  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_neigh_deg = [10, 5]  # first neigh layer choose 10; second neigh layer choose 5...\n",
    "max_deg = sum(input_neigh_deg)\n",
    "neigh_adj_train = np.zeros((adj_train.indptr.shape[0], max_deg)) \n",
    "\n",
    "for train_id in node_train:\n",
    "    neighbors = [np.array([train_id])]\n",
    "    \n",
    "    for deg in input_neigh_deg:\n",
    "        neigh_layer = np.unique( np.concatenate([adj_train.indices[adj_train.indptr[train_idx]:adj_train.indptr[train_idx+1] ] for train_idx in neighbors[-1]] ) )\n",
    "        if len(neigh_layer) >= deg:\n",
    "            neighbors.append(np.random.choice(neigh_layer, deg, replace=False) )\n",
    "        elif len(neigh_layer) > 0:\n",
    "            neighbors.append( neigh_layer )\n",
    "        else:\n",
    "            break\n",
    "    # no neighbors for the current train node, then skip:\n",
    "    if len(neighbors) == 1:\n",
    "        continue\n",
    "    # otherwise: store all the neighbor nodes of each train node inside the array:\n",
    "    neighbors = np.unique(np.concatenate(neighbors[1:]))\n",
    "    neigh_adj_train[train_id][:len(neighbors)] = neighbors[:]\n",
    "    \n",
    "neigh_adj_train_csr = sp.csr_matrix(neigh_adj_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dense form of the sparse matrix\n",
    "print(neigh_adj_train[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the sparse form, investigate how to retrieve a specific train node's neighbors:\n",
    "print(type(neigh_adj_train_csr.data), neigh_adj_train_csr.data.shape)\n",
    "\n",
    "neigh_adj_train_csr_data = neigh_adj_train_csr.data.astype(np.int32)\n",
    "#\n",
    "target_train_node = 0\n",
    "print( neigh_adj_train_csr_data[neigh_adj_train_csr.indptr[target_train_node]:neigh_adj_train_csr.indptr[target_train_node+1] ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique( np.concatenate([neigh_adj_train.indices[neigh_adj_train.indptr[train_idx]:neigh_adj_train.indptr[train_idx+1] ] for train_idx in node_train] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[4mSTART PHASE    0\u001b[0m\n",
      "Start shuffling the node_train...\n",
      "using sage node sampler! \n",
      "sampling 200 subgraphs:   time = 0.285 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangli/anaconda3/envs/pytorch_1_4_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:157: RuntimeWarning: invalid value encountered in true_divide\n",
      "/home/xiangli/anaconda3/envs/pytorch_1_4_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:157: RuntimeWarning: divide by zero encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mEpoch    1, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    1, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    1, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    1, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    1, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch    1, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch    2, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    2, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    2, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    2, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    2, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch    2, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch    3, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    3, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    3, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    3, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    3, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch    3, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch    4, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    4, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    4, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    4, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    4, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch    4, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch    5, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    5, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    5, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    5, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    5, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch    5, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch    6, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    6, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    6, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    6, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    6, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch    6, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch    7, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    7, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    7, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    7, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    7, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch    7, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch    8, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    8, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    8, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    8, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    8, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch    8, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch    9, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    9, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    9, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    9, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    9, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch    9, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   10, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   10, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   10, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   10, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   10, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   10, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   11, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   11, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   11, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   11, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   11, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   11, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   12, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   12, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   12, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   12, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   12, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   12, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   13, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   13, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   13, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   13, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   13, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   13, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   14, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   14, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   14, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   14, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   14, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   14, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   15, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   15, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   15, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   15, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   15, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   15, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   16, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   16, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   16, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   16, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   16, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   16, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   17, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   17, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   17, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   17, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   17, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   17, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   18, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   18, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   18, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   18, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   18, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   18, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   19, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   19, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   19, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   19, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   19, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   19, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   20, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   20, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   20, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   20, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   20, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   20, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   21, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   21, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   21, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   21, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   21, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   21, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   22, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   22, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   22, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   22, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   22, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   22, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   23, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   23, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   23, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   23, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   23, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   23, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   24, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   24, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   24, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   24, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   24, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   24, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   25, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   25, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   25, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   25, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   25, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   25, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   26, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   26, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   26, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   26, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   26, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   26, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   27, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   27, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   27, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   27, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   27, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   27, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   28, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   28, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   28, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   28, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   28, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   28, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   29, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   29, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   29, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   29, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   29, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   29, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   30, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   30, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   30, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   30, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   30, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   30, Batch ID 5\u001b[0m\n",
      "\u001b[93mOptimization Finished!\u001b[0m\n",
      "\u001b[91mTotal training time: 4292.02 ms\u001b[0m\n",
      "\u001b[91mTotal train data uploading time: 317.33 ms\u001b[0m\n",
      "\u001b[4mSTART PHASE    0\u001b[0m\n",
      "Start shuffling the node_train...\n",
      "using sage node sampler! \n",
      "sampling 200 subgraphs:   time = 0.298 sec\n",
      "\u001b[1mEpoch    1, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    1, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    1, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    1, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    1, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch    1, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch    2, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    2, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    2, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    2, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    2, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch    2, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch    3, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    3, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    3, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    3, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    3, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch    3, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch    4, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    4, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    4, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    4, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    4, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch    4, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch    5, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    5, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    5, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    5, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    5, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch    5, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch    6, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    6, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    6, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    6, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    6, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch    6, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch    7, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    7, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    7, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    7, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    7, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch    7, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch    8, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    8, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    8, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    8, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    8, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch    8, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch    9, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    9, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    9, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    9, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    9, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch    9, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   10, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   10, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   10, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   10, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   10, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   10, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   11, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   11, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   11, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   11, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   11, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   11, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   12, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   12, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   12, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   12, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   12, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   12, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   13, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   13, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   13, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   13, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   13, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   13, Batch ID 5\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mEpoch   14, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   14, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   14, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   14, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   14, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   14, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   15, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   15, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   15, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   15, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   15, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   15, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   16, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   16, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   16, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   16, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   16, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   16, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   17, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   17, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   17, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   17, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   17, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   17, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   18, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   18, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   18, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   18, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   18, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   18, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   19, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   19, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   19, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   19, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   19, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   19, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   20, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   20, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   20, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   20, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   20, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   20, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   21, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   21, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   21, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   21, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   21, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   21, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   22, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   22, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   22, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   22, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   22, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   22, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   23, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   23, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   23, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   23, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   23, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   23, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   24, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   24, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   24, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   24, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   24, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   24, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   25, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   25, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   25, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   25, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   25, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   25, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   26, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   26, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   26, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   26, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   26, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   26, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   27, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   27, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   27, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   27, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   27, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   27, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   28, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   28, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   28, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   28, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   28, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   28, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   29, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   29, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   29, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   29, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   29, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   29, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   30, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   30, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   30, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   30, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   30, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   30, Batch ID 5\u001b[0m\n",
      "\u001b[93mOptimization Finished!\u001b[0m\n",
      "\u001b[91mTotal training time: 3965.21 ms\u001b[0m\n",
      "\u001b[91mTotal train data uploading time: 314.66 ms\u001b[0m\n",
      "\u001b[4mSTART PHASE    0\u001b[0m\n",
      "Start shuffling the node_train...\n",
      "using sage node sampler! \n",
      "sampling 200 subgraphs:   time = 0.269 sec\n",
      "\u001b[1mEpoch    1, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    1, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    1, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    1, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    1, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch    1, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch    2, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    2, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    2, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    2, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    2, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch    2, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch    3, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    3, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    3, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    3, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    3, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch    3, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch    4, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    4, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    4, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    4, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    4, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch    4, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch    5, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    5, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    5, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    5, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    5, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch    5, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch    6, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    6, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    6, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    6, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    6, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch    6, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch    7, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    7, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    7, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    7, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    7, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch    7, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch    8, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    8, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    8, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    8, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    8, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch    8, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch    9, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    9, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    9, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    9, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    9, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch    9, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   10, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   10, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   10, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   10, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   10, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   10, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   11, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   11, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   11, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   11, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   11, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   11, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   12, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   12, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   12, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   12, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   12, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   12, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   13, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   13, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   13, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   13, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   13, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   13, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   14, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   14, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   14, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   14, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   14, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   14, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   15, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   15, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   15, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   15, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   15, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   15, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   16, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   16, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   16, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   16, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   16, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   16, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   17, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   17, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   17, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   17, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   17, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   17, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   18, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   18, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   18, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   18, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   18, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   18, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   19, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   19, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   19, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   19, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   19, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   19, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   20, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   20, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   20, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   20, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   20, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   20, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   21, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   21, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   21, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   21, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   21, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   21, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   22, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   22, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   22, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   22, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   22, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   22, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   23, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   23, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   23, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   23, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   23, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   23, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   24, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   24, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   24, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   24, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   24, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   24, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   25, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   25, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   25, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   25, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   25, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   25, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   26, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   26, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   26, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   26, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   26, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   26, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   27, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   27, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   27, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   27, Batch ID 3\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mEpoch   27, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   27, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   28, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   28, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   28, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   28, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   28, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   28, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   29, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   29, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   29, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   29, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   29, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   29, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   30, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   30, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   30, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   30, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   30, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   30, Batch ID 5\u001b[0m\n",
      "\u001b[93mOptimization Finished!\u001b[0m\n",
      "\u001b[91mTotal training time: 4081.48 ms\u001b[0m\n",
      "\u001b[91mTotal train data uploading time: 318.22 ms\u001b[0m\n",
      "\u001b[4mSTART PHASE    0\u001b[0m\n",
      "Start shuffling the node_train...\n",
      "using sage node sampler! \n",
      "sampling 200 subgraphs:   time = 0.269 sec\n",
      "\u001b[1mEpoch    1, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    2, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    3, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    4, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    5, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    1, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    2, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    3, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    4, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    5, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    1, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    2, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    3, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    4, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    5, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    1, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    2, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    3, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    4, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    5, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    1, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch    2, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch    3, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch    4, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch    5, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch    1, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch    2, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch    3, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch    4, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch    5, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch    6, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    7, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    8, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    9, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   10, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    6, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    7, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    8, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    9, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   10, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    6, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    7, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    8, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    9, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   10, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    6, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    7, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    8, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    9, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   10, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    6, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch    7, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch    8, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch    9, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   10, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch    6, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch    7, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch    8, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch    9, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   10, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   11, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   12, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   13, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   14, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   15, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   11, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   12, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   13, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   14, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   15, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   11, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   12, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   13, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   14, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   15, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   11, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   12, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   13, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   14, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   15, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   11, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   12, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   13, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   14, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   15, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   11, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   12, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   13, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   14, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   15, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   16, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   17, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   18, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   19, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   20, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   16, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   17, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   18, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   19, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   20, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   16, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   17, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   18, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   19, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   20, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   16, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   17, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   18, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   19, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   20, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   16, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   17, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   18, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   19, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   20, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   16, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   17, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   18, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   19, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   20, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   21, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   22, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   23, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   24, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   25, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   21, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   22, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   23, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   24, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   25, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   21, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   22, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   23, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   24, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   25, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   21, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   22, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   23, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   24, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   25, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   21, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   22, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   23, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   24, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   25, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   21, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   22, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   23, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   24, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   25, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   26, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   27, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   28, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   29, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   30, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   26, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   27, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   28, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   29, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   30, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   26, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   27, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   28, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   29, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   30, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   26, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   27, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   28, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   29, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   30, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   26, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   27, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   28, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   29, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   30, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   26, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   27, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   28, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   29, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   30, Batch ID 5\u001b[0m\n",
      "\u001b[93mOptimization Finished!\u001b[0m\n",
      "\u001b[91mTotal training time: 3803.47 ms\u001b[0m\n",
      "\u001b[91mTotal train data uploading time:  62.56 ms\u001b[0m\n",
      "\u001b[4mSTART PHASE    0\u001b[0m\n",
      "Start shuffling the node_train...\n",
      "using sage node sampler! \n",
      "sampling 200 subgraphs:   time = 0.281 sec\n",
      "\u001b[1mEpoch    1, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    2, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    3, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    4, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    5, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    1, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    2, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    3, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    4, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    5, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    1, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    2, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    3, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    4, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    5, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    1, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    2, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    3, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    4, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    5, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    1, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch    2, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch    3, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch    4, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch    5, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch    1, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch    2, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch    3, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch    4, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch    5, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch    6, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    7, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    8, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    9, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   10, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    6, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    7, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    8, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    9, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   10, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    6, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    7, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    8, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    9, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   10, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    6, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    7, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    8, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    9, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   10, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    6, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch    7, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch    8, Batch ID 4\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mEpoch    9, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   10, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch    6, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch    7, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch    8, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch    9, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   10, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   11, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   12, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   13, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   14, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   15, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   11, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   12, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   13, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   14, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   15, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   11, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   12, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   13, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   14, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   15, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   11, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   12, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   13, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   14, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   15, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   11, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   12, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   13, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   14, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   15, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   11, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   12, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   13, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   14, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   15, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   16, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   17, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   18, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   19, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   20, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   16, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   17, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   18, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   19, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   20, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   16, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   17, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   18, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   19, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   20, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   16, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   17, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   18, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   19, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   20, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   16, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   17, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   18, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   19, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   20, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   16, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   17, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   18, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   19, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   20, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   21, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   22, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   23, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   24, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   25, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   21, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   22, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   23, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   24, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   25, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   21, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   22, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   23, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   24, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   25, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   21, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   22, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   23, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   24, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   25, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   21, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   22, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   23, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   24, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   25, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   21, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   22, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   23, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   24, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   25, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   26, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   27, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   28, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   29, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   30, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   26, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   27, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   28, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   29, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   30, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   26, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   27, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   28, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   29, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   30, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   26, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   27, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   28, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   29, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   30, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   26, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   27, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   28, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   29, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   30, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   26, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   27, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   28, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   29, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   30, Batch ID 5\u001b[0m\n",
      "\u001b[93mOptimization Finished!\u001b[0m\n",
      "\u001b[91mTotal training time: 3794.20 ms\u001b[0m\n",
      "\u001b[91mTotal train data uploading time:  62.80 ms\u001b[0m\n",
      "\u001b[4mSTART PHASE    0\u001b[0m\n",
      "Start shuffling the node_train...\n",
      "using sage node sampler! \n",
      "sampling 200 subgraphs:   time = 0.265 sec\n",
      "\u001b[1mEpoch    1, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    2, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    3, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    4, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    5, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    1, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    2, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    3, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    4, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    5, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    1, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    2, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    3, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    4, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    5, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    1, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    2, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    3, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    4, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    5, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    1, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch    2, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch    3, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch    4, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch    5, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch    1, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch    2, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch    3, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch    4, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch    5, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch    6, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    7, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    8, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    9, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   10, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    6, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    7, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    8, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    9, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   10, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    6, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    7, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    8, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    9, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   10, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    6, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    7, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    8, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    9, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   10, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    6, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch    7, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch    8, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch    9, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   10, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch    6, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch    7, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch    8, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch    9, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   10, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   11, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   12, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   13, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   14, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   15, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   11, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   12, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   13, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   14, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   15, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   11, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   12, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   13, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   14, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   15, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   11, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   12, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   13, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   14, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   15, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   11, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   12, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   13, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   14, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   15, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   11, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   12, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   13, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   14, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   15, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   16, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   17, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   18, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   19, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   20, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   16, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   17, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   18, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   19, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   20, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   16, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   17, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   18, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   19, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   20, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   16, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   17, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   18, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   19, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   20, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   16, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   17, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   18, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   19, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   20, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   16, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   17, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   18, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   19, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   20, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   21, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   22, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   23, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   24, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   25, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   21, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   22, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   23, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   24, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   25, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   21, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   22, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   23, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   24, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   25, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   21, Batch ID 3\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mEpoch   22, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   23, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   24, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   25, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   21, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   22, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   23, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   24, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   25, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   21, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   22, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   23, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   24, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   25, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   26, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   27, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   28, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   29, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   30, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   26, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   27, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   28, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   29, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   30, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   26, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   27, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   28, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   29, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   30, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   26, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   27, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   28, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   29, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   30, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   26, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   27, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   28, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   29, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   30, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   26, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   27, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   28, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   29, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   30, Batch ID 5\u001b[0m\n",
      "\u001b[93mOptimization Finished!\u001b[0m\n",
      "\u001b[91mTotal training time: 3711.99 ms\u001b[0m\n",
      "\u001b[91mTotal train data uploading time:  61.83 ms\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# ============== Step2 *** conduct the training process\n",
    "train_input_file_name = prepare_data_folder + 'model_train_input'\n",
    "with open(train_input_file_name, \"rb\") as fp:\n",
    "    minibatch, model = dill.load(fp)\n",
    "\n",
    "\n",
    "train_phase_file_name = prepare_data_folder + 'model_train_phase'\n",
    "with open(train_phase_file_name, \"rb\") as fp:\n",
    "    train_phases = dill.load(fp)\n",
    "\n",
    "for tune_val_label, tune_val in zip(tune_val_label_list, tune_val_list):\n",
    "    for trainer_id in trainer_list:\n",
    "        execute_train_investigate(img_path, working_dir, train_phases, model, minibatch, eval_train_every, \n",
    "                                  tune_param_name, tune_val_label, tune_val, trainer_id = trainer_id,\n",
    "                                  snapshot_every = snapshot_period, mini_epoch_num = 5, multilabel = multilabel_tag, input_neigh_deg = [10, 5],\n",
    "                                  core_par_sampler = core_par_sampler, samples_per_processor = samples_per_processor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================ Step3*** investigate validation:\n",
    "evaluation_input_file_name = prepare_data_folder + 'model_eval_input'\n",
    "with open(evaluation_input_file_name, \"rb\") as fp:\n",
    "    minibatch_eval, model_eval = dill.load(fp)\n",
    "\n",
    "for tune_val_label, tune_val in zip(tune_val_label_list, tune_val_list):\n",
    "    for trainer_id in trainer_list:\n",
    "        execute_validation_investigate(img_path, working_dir, minibatch_eval, model_eval, model_epoch_list, \n",
    "                                tune_param_name, tune_val_label, tune_val, trainer_id = trainer_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ================= Step4*** investigate test:\n",
    "evaluation_input_file_name = prepare_data_folder + 'model_eval_input'\n",
    "with open(evaluation_input_file_name, \"rb\") as fp:\n",
    "    minibatch_eval, model_eval = dill.load(fp)\n",
    "\n",
    "for tune_val_label, tune_val in zip(tune_val_label_list, tune_val_list):\n",
    "    for trainer_id in trainer_list:\n",
    "        execute_test_tuning(img_path, working_dir, minibatch_eval, model_eval, model_epoch_list, \n",
    "                                tune_param_name, tune_val_label, tune_val, trainer_id = trainer_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start summarizing for dataset : Flickr\n",
      "Start summarizing for dataset : Flickr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangli/anaconda3/envs/pytorch_1_4_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:324: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/projects/GCN_program/largescale_GCN/HPC_version_GCN/21_GraphSage_custom_hpc_version/Post_utils.py:76: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  plt.figure()\n",
      "/home/xiangli/anaconda3/envs/pytorch_1_4_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:324: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/projects/GCN_program/largescale_GCN/HPC_version_GCN/21_GraphSage_custom_hpc_version/Post_utils.py:76: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  plt.figure()\n",
      "/home/xiangli/anaconda3/envs/pytorch_1_4_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:324: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start summarizing for dataset : Flickr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangli/projects/GCN_program/largescale_GCN/HPC_version_GCN/21_GraphSage_custom_hpc_version/Post_utils.py:76: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  plt.figure()\n",
      "/home/xiangli/anaconda3/envs/pytorch_1_4_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:324: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/projects/GCN_program/largescale_GCN/HPC_version_GCN/21_GraphSage_custom_hpc_version/Post_utils.py:76: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  plt.figure()\n",
      "/home/xiangli/anaconda3/envs/pytorch_1_4_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:324: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/projects/GCN_program/largescale_GCN/HPC_version_GCN/21_GraphSage_custom_hpc_version/Post_utils.py:76: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  plt.figure()\n",
      "/home/xiangli/anaconda3/envs/pytorch_1_4_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:324: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/projects/GCN_program/largescale_GCN/HPC_version_GCN/21_GraphSage_custom_hpc_version/Post_utils.py:76: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  plt.figure()\n",
      "/home/xiangli/anaconda3/envs/pytorch_1_4_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:324: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/projects/GCN_program/largescale_GCN/HPC_version_GCN/21_GraphSage_custom_hpc_version/Post_utils.py:76: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  plt.figure()\n",
      "/home/xiangli/anaconda3/envs/pytorch_1_4_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:324: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/projects/GCN_program/largescale_GCN/HPC_version_GCN/21_GraphSage_custom_hpc_version/Post_utils.py:76: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  plt.figure()\n",
      "/home/xiangli/anaconda3/envs/pytorch_1_4_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:324: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start summarizing for dataset : Flickr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangli/projects/GCN_program/largescale_GCN/HPC_version_GCN/21_GraphSage_custom_hpc_version/Post_utils.py:76: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  plt.figure()\n",
      "/home/xiangli/anaconda3/envs/pytorch_1_4_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:324: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/projects/GCN_program/largescale_GCN/HPC_version_GCN/21_GraphSage_custom_hpc_version/Post_utils.py:76: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  plt.figure()\n",
      "/home/xiangli/anaconda3/envs/pytorch_1_4_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:324: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/projects/GCN_program/largescale_GCN/HPC_version_GCN/21_GraphSage_custom_hpc_version/Post_utils.py:76: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  plt.figure()\n",
      "/home/xiangli/anaconda3/envs/pytorch_1_4_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:324: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/projects/GCN_program/largescale_GCN/HPC_version_GCN/21_GraphSage_custom_hpc_version/Post_utils.py:76: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  plt.figure()\n",
      "/home/xiangli/anaconda3/envs/pytorch_1_4_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:324: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/projects/GCN_program/largescale_GCN/HPC_version_GCN/21_GraphSage_custom_hpc_version/Post_utils.py:76: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  plt.figure()\n",
      "/home/xiangli/anaconda3/envs/pytorch_1_4_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:324: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/projects/GCN_program/largescale_GCN/HPC_version_GCN/21_GraphSage_custom_hpc_version/Post_utils.py:76: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  plt.figure()\n",
      "/home/xiangli/anaconda3/envs/pytorch_1_4_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:324: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start summarizing for dataset : Flickr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangli/projects/GCN_program/largescale_GCN/HPC_version_GCN/21_GraphSage_custom_hpc_version/Post_utils.py:76: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  plt.figure()\n",
      "/home/xiangli/anaconda3/envs/pytorch_1_4_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:324: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/projects/GCN_program/largescale_GCN/HPC_version_GCN/21_GraphSage_custom_hpc_version/Post_utils.py:76: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  plt.figure()\n",
      "/home/xiangli/anaconda3/envs/pytorch_1_4_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:324: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/projects/GCN_program/largescale_GCN/HPC_version_GCN/21_GraphSage_custom_hpc_version/Post_utils.py:76: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  plt.figure()\n",
      "/home/xiangli/anaconda3/envs/pytorch_1_4_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:324: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/projects/GCN_program/largescale_GCN/HPC_version_GCN/21_GraphSage_custom_hpc_version/Post_utils.py:76: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  plt.figure()\n",
      "/home/xiangli/anaconda3/envs/pytorch_1_4_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:324: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/projects/GCN_program/largescale_GCN/HPC_version_GCN/21_GraphSage_custom_hpc_version/Post_utils.py:76: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  plt.figure()\n",
      "/home/xiangli/anaconda3/envs/pytorch_1_4_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:324: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/projects/GCN_program/largescale_GCN/HPC_version_GCN/21_GraphSage_custom_hpc_version/Post_utils.py:76: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  plt.figure()\n",
      "/home/xiangli/anaconda3/envs/pytorch_1_4_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:324: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start summarizing for dataset : Flickr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangli/projects/GCN_program/largescale_GCN/HPC_version_GCN/21_GraphSage_custom_hpc_version/Post_utils.py:76: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  plt.figure()\n",
      "/home/xiangli/anaconda3/envs/pytorch_1_4_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:324: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/projects/GCN_program/largescale_GCN/HPC_version_GCN/21_GraphSage_custom_hpc_version/Post_utils.py:76: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  plt.figure()\n",
      "/home/xiangli/anaconda3/envs/pytorch_1_4_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:324: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/projects/GCN_program/largescale_GCN/HPC_version_GCN/21_GraphSage_custom_hpc_version/Post_utils.py:76: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  plt.figure()\n",
      "/home/xiangli/anaconda3/envs/pytorch_1_4_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:324: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/projects/GCN_program/largescale_GCN/HPC_version_GCN/21_GraphSage_custom_hpc_version/Post_utils.py:76: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  plt.figure()\n",
      "/home/xiangli/anaconda3/envs/pytorch_1_4_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:324: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/projects/GCN_program/largescale_GCN/HPC_version_GCN/21_GraphSage_custom_hpc_version/Post_utils.py:76: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  plt.figure()\n",
      "/home/xiangli/anaconda3/envs/pytorch_1_4_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:324: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/projects/GCN_program/largescale_GCN/HPC_version_GCN/21_GraphSage_custom_hpc_version/Post_utils.py:76: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  plt.figure()\n",
      "/home/xiangli/anaconda3/envs/pytorch_1_4_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:324: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start running training for dataset: Flickr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangli/projects/GCN_program/largescale_GCN/HPC_version_GCN/21_GraphSage_custom_hpc_version/Post_utils.py:178: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  plt.figure()\n",
      "/home/xiangli/anaconda3/envs/pytorch_1_4_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:324: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/projects/GCN_program/largescale_GCN/HPC_version_GCN/21_GraphSage_custom_hpc_version/Post_utils.py:178: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  plt.figure()\n",
      "/home/xiangli/anaconda3/envs/pytorch_1_4_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:324: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/projects/GCN_program/largescale_GCN/HPC_version_GCN/21_GraphSage_custom_hpc_version/Post_utils.py:178: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  plt.figure()\n",
      "/home/xiangli/anaconda3/envs/pytorch_1_4_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:324: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/projects/GCN_program/largescale_GCN/HPC_version_GCN/21_GraphSage_custom_hpc_version/Post_utils.py:178: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  plt.figure()\n",
      "/home/xiangli/anaconda3/envs/pytorch_1_4_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:324: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start running training for dataset: Flickr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangli/projects/GCN_program/largescale_GCN/HPC_version_GCN/21_GraphSage_custom_hpc_version/Post_utils.py:178: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  plt.figure()\n",
      "/home/xiangli/anaconda3/envs/pytorch_1_4_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:324: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/projects/GCN_program/largescale_GCN/HPC_version_GCN/21_GraphSage_custom_hpc_version/Post_utils.py:178: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  plt.figure()\n",
      "/home/xiangli/anaconda3/envs/pytorch_1_4_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:324: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/projects/GCN_program/largescale_GCN/HPC_version_GCN/21_GraphSage_custom_hpc_version/Post_utils.py:178: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  plt.figure()\n",
      "/home/xiangli/anaconda3/envs/pytorch_1_4_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:324: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/projects/GCN_program/largescale_GCN/HPC_version_GCN/21_GraphSage_custom_hpc_version/Post_utils.py:178: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  plt.figure()\n",
      "/home/xiangli/anaconda3/envs/pytorch_1_4_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:324: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start running training for dataset: Flickr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangli/projects/GCN_program/largescale_GCN/HPC_version_GCN/21_GraphSage_custom_hpc_version/Post_utils.py:178: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  plt.figure()\n",
      "/home/xiangli/anaconda3/envs/pytorch_1_4_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:324: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/projects/GCN_program/largescale_GCN/HPC_version_GCN/21_GraphSage_custom_hpc_version/Post_utils.py:178: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  plt.figure()\n",
      "/home/xiangli/anaconda3/envs/pytorch_1_4_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:324: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/projects/GCN_program/largescale_GCN/HPC_version_GCN/21_GraphSage_custom_hpc_version/Post_utils.py:178: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  plt.figure()\n",
      "/home/xiangli/anaconda3/envs/pytorch_1_4_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:324: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/projects/GCN_program/largescale_GCN/HPC_version_GCN/21_GraphSage_custom_hpc_version/Post_utils.py:178: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  plt.figure()\n",
      "/home/xiangli/anaconda3/envs/pytorch_1_4_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:324: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/projects/GCN_program/largescale_GCN/HPC_version_GCN/21_GraphSage_custom_hpc_version/Post_utils.py:178: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  plt.figure()\n",
      "/home/xiangli/anaconda3/envs/pytorch_1_4_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:324: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start running training for dataset: Flickr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangli/projects/GCN_program/largescale_GCN/HPC_version_GCN/21_GraphSage_custom_hpc_version/Post_utils.py:178: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  plt.figure()\n",
      "/home/xiangli/anaconda3/envs/pytorch_1_4_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:324: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/projects/GCN_program/largescale_GCN/HPC_version_GCN/21_GraphSage_custom_hpc_version/Post_utils.py:178: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  plt.figure()\n",
      "/home/xiangli/anaconda3/envs/pytorch_1_4_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:324: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/projects/GCN_program/largescale_GCN/HPC_version_GCN/21_GraphSage_custom_hpc_version/Post_utils.py:178: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  plt.figure()\n",
      "/home/xiangli/anaconda3/envs/pytorch_1_4_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:324: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/projects/GCN_program/largescale_GCN/HPC_version_GCN/21_GraphSage_custom_hpc_version/Post_utils.py:178: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  plt.figure()\n",
      "/home/xiangli/anaconda3/envs/pytorch_1_4_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:324: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start running training for dataset: Flickr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangli/projects/GCN_program/largescale_GCN/HPC_version_GCN/21_GraphSage_custom_hpc_version/Post_utils.py:178: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  plt.figure()\n",
      "/home/xiangli/anaconda3/envs/pytorch_1_4_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:324: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/projects/GCN_program/largescale_GCN/HPC_version_GCN/21_GraphSage_custom_hpc_version/Post_utils.py:178: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  plt.figure()\n",
      "/home/xiangli/anaconda3/envs/pytorch_1_4_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:324: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/projects/GCN_program/largescale_GCN/HPC_version_GCN/21_GraphSage_custom_hpc_version/Post_utils.py:178: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  plt.figure()\n",
      "/home/xiangli/anaconda3/envs/pytorch_1_4_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:324: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/projects/GCN_program/largescale_GCN/HPC_version_GCN/21_GraphSage_custom_hpc_version/Post_utils.py:178: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  plt.figure()\n",
      "/home/xiangli/anaconda3/envs/pytorch_1_4_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:324: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start running training for dataset: Flickr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangli/projects/GCN_program/largescale_GCN/HPC_version_GCN/21_GraphSage_custom_hpc_version/Post_utils.py:178: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  plt.figure()\n",
      "/home/xiangli/anaconda3/envs/pytorch_1_4_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:324: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/projects/GCN_program/largescale_GCN/HPC_version_GCN/21_GraphSage_custom_hpc_version/Post_utils.py:178: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  plt.figure()\n",
      "/home/xiangli/anaconda3/envs/pytorch_1_4_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:324: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/projects/GCN_program/largescale_GCN/HPC_version_GCN/21_GraphSage_custom_hpc_version/Post_utils.py:178: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  plt.figure()\n",
      "/home/xiangli/anaconda3/envs/pytorch_1_4_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:324: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for tune_val_label, tune_val in zip(tune_val_label_list, tune_val_list):\n",
    "    for trainer_id in trainer_list:\n",
    "        step51_run_investigation_summarize_whole(data_name, img_path,\n",
    "                                         tune_param_name, tune_val_label, tune_val,\n",
    "                                            trainer_list, model_epoch_list)\n",
    "    \n",
    "for tune_val_label, tune_val in zip(tune_val_label_list, tune_val_list):\n",
    "    for trainer_id in trainer_list:\n",
    "        step50_run_tune_summarize_whole(data_name, img_path, \n",
    "                                    tune_param_name, tune_val_label_list, tune_val_list,\n",
    "                                    trainer_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_1_4_geometric]",
   "language": "python",
   "name": "conda-env-pytorch_1_4_geometric-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
