{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Post_utils import *\n",
    "# from multi_exec import *\n",
    "import torch\n",
    "import time\n",
    "\n",
    "import pickle\n",
    "import dill\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "from metric import *\n",
    "from model_graphsaint import GraphSAINT\n",
    "\n",
    "from utils import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import scipy.sparse as sp\n",
    "\n",
    "\n",
    "F_ACT = {'relu': nn.ReLU(),\n",
    "         'I': lambda x:x}\n",
    "\n",
    "\n",
    "class HighOrderAggregator(nn.Module):\n",
    "    \"\"\"\n",
    "        Graph convolution layer.\n",
    "        Here each layer concatenate the embedding alongside the inputs\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_in, dim_out, dropout=0., act='relu', order=1, aggr='mean', bias='norm', **kwargs):\n",
    "        super(HighOrderAggregator, self).__init__()\n",
    "        \n",
    "        self.order, self.aggr = order, aggr\n",
    "        self.act, self.bias = F_ACT[act], bias\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        \n",
    "        self.f_lin = list()\n",
    "        self.f_bias = list()\n",
    "        self.offset=list()\n",
    "        self.scale=list()\n",
    "        for o in range(self.order+1):\n",
    "            self.f_lin.append(nn.Linear(dim_in,dim_out,bias=False))  # applies a linear transformation layer\n",
    "            \n",
    "            # mannuly initilaize the liearn layer initial values: \n",
    "            # Fills the input Tensor with values according to the method described in \n",
    "            # Understanding the difficulty of training deep feedforward neural networks - Glorot\n",
    "            nn.init.xavier_uniform_(self.f_lin[-1].weight)   \n",
    "            \n",
    "            # Parameters are Tensor subclasses, that have a very special property when used with Module s - \n",
    "            # when they’re assigned as Module attributes they are automatically added to the list of its parameters,\n",
    "            self.f_bias.append(nn.Parameter(torch.zeros(dim_out)))\n",
    "            self.offset.append(nn.Parameter(torch.zeros(dim_out)))\n",
    "            self.scale.append(nn.Parameter(torch.ones(dim_out)))\n",
    "        \n",
    "        # ModuleList can be indexed like a regular Python list, but modules it contains are properly registered, and will be visible by all Module methods.\n",
    "        self.f_lin = nn.ModuleList(self.f_lin)  \n",
    "        self.f_dropout = nn.Dropout(p=self.dropout)  # \n",
    "        \n",
    "        # ParameterList can be indexed like a regular Python list, but parameters it contains are properly registered, and will be visible by all Module methods.\n",
    "        self.params=nn.ParameterList(self.f_bias + self.offset + self.scale)\n",
    "        \n",
    "        # re-indexing each part of the ParameterList, just to make sure each list is now a nn.ParameterList\n",
    "        self.f_bias=self.params[:self.order+1]\n",
    "        self.offset=self.params[self.order+1:2*self.order+2]\n",
    "        self.scale=self.params[2*self.order+2:]\n",
    "\n",
    "    def _spmm(self, adj_norm, _feat):\n",
    "        # alternative ways: use geometric.propagate or torch.mm\n",
    "        \n",
    "#         Performs a matrix multiplication of the sparse matrix mat1 and dense matrix mat2. Similar to torch.mm(), \n",
    "#         If mat1 is a (n \\times m)(n×m) tensor, mat2 is a (m \\times p)(m×p) tensor, out will be a (n \\times p)(n×p) dense tensor. \n",
    "#         mat1 need to have sparse_dim = 2. This function also supports backward for both matrices. Note that the gradients of mat1 is a coalesced sparse tensor.\n",
    "        return torch.sparse.mm(adj_norm, _feat)\n",
    "\n",
    "    def _f_feat_trans(self, _feat, _id):\n",
    "        # _id : the selected feature id\n",
    "        feat=self.act(self.f_lin[_id](_feat) + self.f_bias[_id])\n",
    "        \n",
    "        # whether to apply the feature normalization after each layer forward func\n",
    "        if self.bias=='norm':\n",
    "            mean=feat.mean(dim=1).view(feat.shape[0], 1)   # mean values along axis 1 : viewed as N by 1 vector \n",
    "            var=feat.var(dim=1,unbiased=False).view(feat.shape[0],1) + 1e-9  # If unbiased is False, then the variance will be calculated via the biased estimator\n",
    "            feat_out=(feat-mean)*self.scale[_id] * torch.rsqrt(var) + self.offset[_id]   # Returns a new tensor with the reciprocal of the square-root of each of the elements of input\n",
    "        else:\n",
    "            feat_out=feat   # for the last layer: i.e. classifier, no need for normalization\n",
    "        return feat_out\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Inputs:.\n",
    "            adj_norm        edge-list represented adj matrix\n",
    "        \"\"\"\n",
    "        adj_norm, feat_in = inputs\n",
    "        # During training, randomly zeroes some of the elements of the input tensor with probability p using samples from a Bernoulli distribution.\n",
    "        feat_in = self.f_dropout(feat_in)\n",
    "        \n",
    "        feat_hop = [feat_in]\n",
    "        # generate A^i X\n",
    "        for o in range(self.order):\n",
    "            # propagate(edge_index, x=x, norm=norm)\n",
    "            feat_hop.append(self._spmm(adj_norm, feat_hop[-1]))\n",
    "            \n",
    "        feat_partial = [self._f_feat_trans(ft, idf) for idf, ft in enumerate(feat_hop)]\n",
    "        \n",
    "        if self.aggr == 'mean':\n",
    "            feat_out = feat_partial[0]\n",
    "            for o in range(len(feat_partial)-1):\n",
    "                feat_out += feat_partial[o+1]\n",
    "        elif self.aggr == 'concat':    # this is to concatenate the featuer tensors alongside with each other\n",
    "            feat_out = torch.cat(feat_partial, 1)  # concatenate tensors alongside the columns\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        return adj_norm, feat_out       # return adj_norm to support Sequential\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphSAINT(nn.Module):\n",
    "    \"\"\"\n",
    "        Trainer model\n",
    "        Transfer data to GPU:   A  init:  1) feat_full   2) label_full   3) label_full_cat\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, arch_gcn, train_params, feat_full, label_full, cpu_eval=False):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            arch_gcn            parsed arch of GCN\n",
    "            train_params        parameters for training\n",
    "            cpu_eval (bool)  :   whether use CPU side for evalution\n",
    "        \"\"\"\n",
    "        super(GraphSAINT,self).__init__()\n",
    "        \n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "        if cpu_eval:\n",
    "            self.use_cuda=False\n",
    "        \n",
    "        self.aggregator_cls=HighOrderAggregator\n",
    "        self.mulhead=1\n",
    "        \n",
    "        # each layer in arch_gcn['arch']:  is a string separated by '-'\n",
    "        self.num_layers = len(arch_gcn['arch'].split('-'))\n",
    "        self.weight_decay = train_params['weight_decay']\n",
    "        self.dropout = train_params['dropout']\n",
    "        self.lr = train_params['lr']\n",
    "        self.arch_gcn = arch_gcn\n",
    "        \n",
    "        # check if the task is a multi-label task\n",
    "        # sigmoid: means this is a multi-label task\n",
    "        self.sigmoid_loss = (arch_gcn['loss']=='sigmoid')   # use sigmoid for multi-label loss function\n",
    "        \n",
    "        self.feat_full = torch.from_numpy(feat_full.astype(np.float32))\n",
    "        self.label_full = torch.from_numpy(label_full.astype(np.float32))\n",
    "        \n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        _dims, self.order_layer, self.act_layer, self.bias_layer, self.aggr_layer \\\n",
    "                        = parse_layer_yml(arch_gcn, self.feat_full.shape[1])\n",
    "        \n",
    "        # get layer index for each conv layer, useful for jk net last layer aggregation\n",
    "        self.set_dims(_dims)\n",
    "\n",
    "        self.loss = 0\n",
    "\n",
    "        # build the model below        \n",
    "        self.aggregators = self.get_aggregators()\n",
    "        self.conv_layers = nn.Sequential(*self.aggregators)  # make sure the previous layer's output is the next one's output\n",
    "        # calculate the final embeddings:\n",
    "        self.classifier = HighOrderAggregator(self.dims_feat[-1], self.num_classes,\\\n",
    "                            act='I', order=0, dropout=self.dropout, bias='bias')\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.parameters(),lr=self.lr)\n",
    "\n",
    "    def set_dims(self,dims):\n",
    "        \"\"\"\n",
    "            dims_feat: Obtain the dimension of each embedding layer\n",
    "            dims_weight:  Obtain the dimension of each weight\n",
    "        \"\"\"\n",
    "        # dims[0] will the number of features\n",
    "        # if using the concatenation as the feature aggration pattern, then need to count the dimension expansion\n",
    "        self.dims_feat = [dims[0]] + [( (self.aggr_layer[l]=='concat') * self.order_layer[l] + 1) * dims[l+1] for l in range(len(dims)-1)]\n",
    "        \n",
    "        # set the current model weights dimensions, generated based on the embedding dimension:\n",
    "        self.dims_weight = [(self.dims_feat[l], dims[l+1]) for l in range(len(dims)-1)]\n",
    "\n",
    "\n",
    "    def forward(self, adj_subgraph, feat_subg):\n",
    "        \n",
    "        _, emb_subg = self.conv_layers((adj_subgraph, feat_subg))\n",
    "        emb_subg_norm = F.normalize(emb_subg, p=2, dim=1)\n",
    "        \n",
    "        # obtain the prediction\n",
    "        pred_subg = self.classifier((None, emb_subg_norm))[1]\n",
    "        return pred_subg\n",
    "\n",
    "\n",
    "    def _loss(self, preds, labels, norm_loss):\n",
    "        \"\"\"\n",
    "            use the norm_loss as the weight factor\n",
    "        \"\"\"\n",
    "        if self.sigmoid_loss:\n",
    "            norm_loss = norm_loss.unsqueeze(1)\n",
    "            return torch.nn.BCEWithLogitsLoss(weight = norm_loss,reduction='sum')(preds, labels)\n",
    "        else:\n",
    "            _ls = torch.nn.CrossEntropyLoss(reduction='none')(preds, labels)\n",
    "            return (norm_loss * _ls).sum()\n",
    "\n",
    "\n",
    "    def get_aggregators(self):\n",
    "        \"\"\"\n",
    "        Return a list of aggregator instances. to be used in self.build()\n",
    "        \"\"\"\n",
    "        aggregators = []\n",
    "        for l in range(self.num_layers):\n",
    "            aggrr = self.aggregator_cls(*self.dims_weight[l], dropout=self.dropout,\\\n",
    "                    act=self.act_layer[l], order=self.order_layer[l], \\\n",
    "                    aggr=self.aggr_layer[l], bias=self.bias_layer[l], mulhead=self.mulhead)\n",
    "            aggregators.append(aggrr)\n",
    "        return aggregators\n",
    "\n",
    "    def predict(self, preds):\n",
    "        return nn.Sigmoid()(preds) if self.sigmoid_loss else F.softmax(preds, dim=1)\n",
    "        \n",
    "        \n",
    "    def train_step(self, node_subgraph, adj_subgraph, norm_loss_subgraph, feat_subg, label_subg_converted):\n",
    "        \"\"\"\n",
    "        Purpose:  only count the time for the training process, including forward and backward propogation\n",
    "        Forward and backward propagation\n",
    "        norm_loss_subgraph : is the key to rescale the current batch/subgraph\n",
    "        \"\"\"\n",
    "        self.train()\n",
    "        # =============== start of the training process for one step: =================\n",
    "        self.optimizer.zero_grad()\n",
    "        # here call the forward propagation\n",
    "        preds = self(adj_subgraph, feat_subg)    # will call the forward function\n",
    "        loss = self._loss(preds, label_subg_converted, norm_loss_subgraph) # labels.squeeze()?\n",
    "        \n",
    "        # call the back propagation\n",
    "        loss.backward()\n",
    "        # any clipin ggradient optimization?\n",
    "        torch.nn.utils.clip_grad_norm_(self.parameters(), 5)  #\n",
    "#         Clips gradient norm of an iterable of parameters.\n",
    "#         The norm is computed over all gradients together, as if they were concatenated into a single vector. Gradients are modified in-place.\n",
    "        \n",
    "        self.optimizer.step()\n",
    "        # ending of the training process\n",
    "        \n",
    "        # also return the total training time and uploading time in seconds\n",
    "        return loss, self.predict(preds)\n",
    "\n",
    "    def eval_step(self, node_subgraph, adj_subgraph, norm_loss_subgraph):\n",
    "        \"\"\"\n",
    "        Purpose: evaluation only on the CPU side\n",
    "        Forward propagation only\n",
    "        No backpropagation and thus no need for gradients\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        feat_subg = self.feat_full[node_subgraph]\n",
    "        label_subg = self.label_full[node_subgraph]\n",
    "        \n",
    "        if not self.sigmoid_loss:\n",
    "            self.label_full_cat = torch.from_numpy(self.label_full.numpy().argmax(axis=1).astype(np.int64))\n",
    "                \n",
    "        label_subg_converted = label_subg if self.sigmoid_loss else self.label_full_cat[node_subgraph]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # only call the forward propagation\n",
    "            print(\"during the evaluation step, report the input matrices size: \")\n",
    "            print(\"adj_subgraph size :  {}; \\t feat_subg size : {}\".format(adj_subgraph.size(), feat_subg.size()) )\n",
    "\n",
    "            preds = self(adj_subgraph, feat_subg)\n",
    "            loss = self._loss(preds, label_subg_converted, norm_loss_subgraph)\n",
    "            \n",
    "        return loss, self.predict(preds), label_subg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partition_Graph func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Collections of partitioning functions.\"\"\"\n",
    "\n",
    "import time\n",
    "import metis\n",
    "import scipy.sparse as sp\n",
    "\n",
    "\n",
    "def partition_graph(adj_full, target_nodes, num_clusters):\n",
    "    \"\"\"partition a graph by METIS into smaller mini-clusters\n",
    "        Later these mini-clusters will be re-orginaized/combined into larger batches\n",
    "    Input:\n",
    "        adj_full (sp.csr_matrix): full adjacent matrix of the whole graph \n",
    "        target_nodes (np.array): to-be partitioned target nodes, usually the train_nodes\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    num_nodes = len(target_nodes)     # just the to-be partitioned target nodes\n",
    "    num_all_nodes = adj_full.shape[0]   # all nodes in the graph\n",
    "    \n",
    "    neighbor_intervals = []\n",
    "    neighbors = []\n",
    "    edge_cnt = 0\n",
    "    neighbor_intervals.append(0)\n",
    "    train_adj_lil = adj_full[target_nodes, :][:, target_nodes].tolil()\n",
    "    train_ord_map = dict()\n",
    "    train_adj_lists = [[] for _ in range(num_nodes)]\n",
    "    for i in range(num_nodes):\n",
    "        rows = train_adj_lil[i].rows[0]\n",
    "        # self-edge needs to be removed for valid format of METIS\n",
    "        if i in rows:\n",
    "            rows.remove(i)\n",
    "        train_adj_lists[i] = rows\n",
    "        neighbors += rows\n",
    "        edge_cnt += len(rows)\n",
    "        neighbor_intervals.append(edge_cnt)\n",
    "        train_ord_map[target_nodes[i]] = i\n",
    "        \n",
    "    if num_clusters > 1:\n",
    "        _, groups = metis.part_graph(train_adj_lists, num_clusters, seed=1)\n",
    "    else:\n",
    "        groups = [0] * num_nodes\n",
    "        \n",
    "    part_row = []\n",
    "    part_col = []\n",
    "    part_data = []\n",
    "    parts = [[] for _ in range(num_clusters)]\n",
    "    for nd_idx in range(num_nodes):\n",
    "        gp_idx = groups[nd_idx]\n",
    "        nd_orig_idx = target_nodes[nd_idx]\n",
    "        # add nodes to each group inside the parts\n",
    "        parts[gp_idx].append(nd_orig_idx)\n",
    "    \n",
    "        for nb_orig_idx in adj_full[nd_orig_idx].indices:\n",
    "            nb_idx = train_ord_map[nb_orig_idx]\n",
    "            if groups[nb_idx] == gp_idx:\n",
    "                part_data.append(1)\n",
    "                part_row.append(nd_orig_idx)\n",
    "                part_col.append(nb_orig_idx)\n",
    "    part_data.append(0)\n",
    "    part_row.append(num_all_nodes - 1)\n",
    "    part_col.append(num_all_nodes - 1)\n",
    "    part_adj = sp.coo_matrix((part_data, (part_row, part_col))).tocsr()\n",
    "    \n",
    "    # parts: is the divided groups of nodes\n",
    "    # part_adj: is the csr_matrix for the adjacency matrix of the whole graph\n",
    "    # here this part_adj is different since some of the inter-cluster edges are lost\n",
    "    return part_adj, parts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minibatch machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import scipy.sparse as sp\n",
    "import scipy\n",
    "import numpy as np\n",
    "\n",
    "def _coo_scipy2torch(adj):\n",
    "    \"\"\"\n",
    "    convert a scipy sparse COO matrix to torch\n",
    "    \n",
    "    Torch supports sparse tensors in COO(rdinate) format, which can efficiently store and process tensors \n",
    "    for which the majority of elements are zeros.\n",
    "    A sparse tensor is represented as a pair of dense tensors: a tensor of values and a 2D tensor of indices. \n",
    "    A sparse tensor can be constructed by providing these two tensors, as well as the size of the sparse tensor \n",
    "    \"\"\"\n",
    "    values = adj.data\n",
    "    indices = np.vstack((adj.row, adj.col))\n",
    "    i = torch.LongTensor(indices)\n",
    "    v = torch.FloatTensor(values)\n",
    "    \n",
    "    return torch.sparse.FloatTensor(i,v, torch.Size(adj.shape))\n",
    "\n",
    "\n",
    "class Minibatch:\n",
    "    \"\"\"\n",
    "        This minibatch iterator iterates over nodes for supervised learning.\n",
    "        Data transferred to GPU:     A  init: 1) self.adj_full_norm;  2) self.norm_loss_test;\n",
    "                                     B  set_sampler:  1) self.norm_loss_train\n",
    "                                     C  one_batch : 1) subgraph adjacency matrix (adj)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, adj_full, adj_train, role, train_params, cpu_eval = False, mode = \"train\", \n",
    "                 num_clusters = 128, batch_num = 32):\n",
    "        \"\"\"\n",
    "        role:       array of string (length |V|)\n",
    "                    storing role of the node ('tr'/'va'/'te')\n",
    "        \"\"\"\n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "        if cpu_eval:\n",
    "            self.use_cuda = False\n",
    "        \n",
    "        # store all the node roles as the numpy array:\n",
    "        self.node_train = np.array(role['tr'])\n",
    "        self.node_val = np.array(role['va'])\n",
    "        self.node_test = np.array(role['te'])\n",
    "\n",
    "        \n",
    "        self.adj_train = adj_train\n",
    "        print(\"adj train type is: {}; and shape is {}\".format(type(adj_train), adj_train.shape))\n",
    "\n",
    "        # norm_loss_test is used in full batch evaluation (without sampling). so neighbor features are simply averaged.\n",
    "        self.norm_loss_test = np.zeros(adj_full.shape[0])\n",
    "        \n",
    "        _denom = len(self.node_train) + len(self.node_val) +  len(self.node_test)\n",
    "        \n",
    "        # instead of assign all elements of self.norm_loss_test to the same averaged denominator, separately assingment instead. \n",
    "        # does this mean there are other meaningless roles beyond: test, train and validation?\n",
    "        self.norm_loss_test[self.node_train] = 1./_denom     \n",
    "        self.norm_loss_test[self.node_val] = 1./_denom\n",
    "        self.norm_loss_test[self.node_test] = 1./_denom\n",
    "        self.norm_loss_test = torch.from_numpy(self.norm_loss_test.astype(np.float32))\n",
    "            \n",
    "        self.deg_train = np.array(self.adj_train.sum(1)).flatten()   # sum the degree of each train node, here sum along column for adjacency matrix\n",
    "        \n",
    "        # for train part: use the modified adjacency matrix: with inter-cluster edges broken\n",
    "        if mode == \"train\": \n",
    "            self.adj_full, self.parts = partition_graph(adj_train, self.node_train, num_clusters)\n",
    "            self.generate_norm_loss_train(num_clusters, batch_num)\n",
    "            self.num_training_batches = batch_num\n",
    "            self.num_mini_clusters = num_clusters\n",
    "        else:\n",
    "            self.adj_full = adj_full\n",
    "\n",
    "\n",
    "    def generate_norm_loss_train(self, num_clusters, batch_num):\n",
    "        \"\"\"\n",
    "            Train_phases (a dict defined in the .yml file) : usually including : end, smapler, size_subg_edge\n",
    "            end:  number of total epochs to stop\n",
    "            sampler: category for sampler (e.g. edge)\n",
    "            size_subg_edge:  size of the subgraph in number of edges\n",
    "        \"\"\"\n",
    "        self.norm_loss_train = np.zeros(self.adj_train.shape[0])\n",
    "\n",
    "        self.norm_loss_train[self.node_train] += 1\n",
    "        assert self.norm_loss_train[self.node_val].sum() + self.norm_loss_train[self.node_test].sum() == 0\n",
    "        \n",
    "        # normalize the self.norm_loss_train:\n",
    "        self.norm_loss_train[np.where(self.norm_loss_train==0)[0]] = 0.1\n",
    "        self.norm_loss_train[self.node_val] = 0\n",
    "        self.norm_loss_train[self.node_test] = 0\n",
    "        self.norm_loss_train[self.node_train] = batch_num/self.norm_loss_train[self.node_train]/self.node_train.size\n",
    "        self.norm_loss_train = torch.from_numpy(self.norm_loss_train.astype(np.float32))\n",
    "\n",
    "        \n",
    "    def generate_train_batch(self, diag_lambda=-1):\n",
    "        \"\"\"\n",
    "        Train batch Generator: Generate the batch for multiple clusters.\n",
    "        \"\"\"\n",
    "\n",
    "        block_size = self.num_mini_clusters // self.num_training_batches\n",
    "        np.random.shuffle(self.parts)  # each time shuffle different mini-clusters so that the combined batches are shuffled correspondingly\n",
    "        \n",
    "        for _, st in enumerate(range(0, self.num_mini_clusters, block_size)):\n",
    "            # recombine mini-clusters into a single batch: pt\n",
    "            node_subgraph = self.parts[st]\n",
    "            for pt_idx in range(st + 1, min(st + block_size, self.num_mini_clusters)):\n",
    "                node_subgraph = np.concatenate((node_subgraph, self.parts[pt_idx]), axis=0)\n",
    "            \n",
    "            norm_loss = self.norm_loss_train[node_subgraph]\n",
    "            subgraph_adj = self.adj_full[node_subgraph, :][:, node_subgraph]\n",
    "\n",
    "            # normlize subgraph_adj locally for each isolate subgraph\n",
    "            if diag_lambda == -1:\n",
    "                subgraph_adj = adj_norm(subgraph_adj, deg = self.deg_train[node_subgraph])\n",
    "            else:\n",
    "                subgraph_adj = adj_norm_diag_enhance(subgraph_adj, deg = self.deg_train[node_subgraph], diag_lambda = diag_lambda)\n",
    "            subgraph_adj = _coo_scipy2torch(subgraph_adj.tocoo())\n",
    "            \n",
    "            yield (node_subgraph, subgraph_adj, norm_loss)    \n",
    "    \n",
    "    def generate_eval_batch(self):\n",
    "        \"\"\"\n",
    "            Generate evaluation batch for validation/test procedures, whole graph \n",
    "        \"\"\"\n",
    "        node_subgraph = np.arange(self.adj_full.shape[0])  # include all the nodes inside the graph\n",
    "        adj_full_norm = adj_norm(self.adj_full)  # return the normalized whole graph adj matrix; optional: diag_enhanced normalization: adj_norm_diag_enhance(...)\n",
    "#             adj = adj_norm_diag_enhance(self.adj_full, diag_lambda = -1)\n",
    "        adj_full_norm = _coo_scipy2torch(adj_full_norm.tocoo())\n",
    "\n",
    "        return node_subgraph, adj_full_norm, self.norm_loss_test\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic execution components "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a lambda func\n",
    "f_mean = lambda l: sum(l)/len(l)\n",
    "\n",
    "def evaluate_full_batch(model, minibatch, mode='val'):\n",
    "    \"\"\"\n",
    "        Full batch evaluation: for validation and test sets only.\n",
    "        When calculating the F1 score, we will mask the relevant root nodes.\n",
    "        mode: can be val or test\n",
    "    \"\"\"\n",
    "    loss, preds, labels = model.eval_step(*minibatch.generate_eval_batch())\n",
    "    node_val_test = minibatch.node_val if mode=='val' else minibatch.node_test\n",
    "    # may not be necessary \n",
    "    f1_scores = calc_f1(to_numpy(labels[node_val_test]), to_numpy(preds[node_val_test]), model.sigmoid_loss)\n",
    "    return loss, f1_scores[0], f1_scores[1]\n",
    "\n",
    "\n",
    "def train_setting(dataname, datapath, train_config_file):\n",
    "    \"\"\"\n",
    "        YAML (a recursive acronym for \"YAML Ain't Markup Language\") is a human-readable data-serialization language. \n",
    "        It is commonly used for configuration files and in applications where data is being stored or transmitted.\n",
    "    \n",
    "        yaml.load is as powerful as pickle.load and so may call any Python function. Check the yaml.safe_load function though.\n",
    "        The function yaml.load converts a YAML document to a Python object.\n",
    "    \"\"\"\n",
    "    with open(train_config_file) as f_train_config:\n",
    "        train_config = yaml.load(f_train_config)\n",
    "        \n",
    "    arch_gcn = {'dim':-1,\n",
    "                'aggr':'concat',\n",
    "                'loss':'softmax',\n",
    "                'arch':'1',\n",
    "                'act':'I',\n",
    "                'bias':'norm'}\n",
    "    # check the loss:  default to be softmax, multi-class problem, each node can only belong to just one class at last\n",
    "    arch_gcn.update(train_config['network'][0])   # train_config['network'] is a list of dict\n",
    "    \n",
    "    \n",
    "    train_params = {'lr' : 0.01, 'weight_decay' : 0., 'norm_loss':True, 'norm_aggr':True, 'q_threshold' : 50, 'q_offset':0}\n",
    "    train_params.update(train_config['params'][0])\n",
    "    train_phases = train_config['phase']\n",
    "    for ph in train_phases:\n",
    "        assert 'end' in ph\n",
    "        assert 'sampler' in ph\n",
    "    print(\"Loading training data..\")\n",
    "    temp_data = load_data(dataname, datapath = datapath)\n",
    "    train_data = process_graph_data(*temp_data)\n",
    "    print(\"Done loading training data..\")\n",
    "    \n",
    "    # train_data is a tuple: adj_full, adj_train, feats, class_arr, role\n",
    "    return train_params, train_phases, train_data, arch_gcn\n",
    "\n",
    "def prepare(working_dir, train_data, train_params, arch_gcn):\n",
    "    \"\"\"\n",
    "        working_dir: main working dir for experiments\n",
    "        train_params: contain settings for the mini-batch setting\n",
    "        arch_gcn: contain all the settings \n",
    "    \"\"\"\n",
    "    adj_full, adj_train, feat_full, class_arr, role = train_data\n",
    "    adj_full = adj_full.astype(np.int32)  # change the original np.bool into 0-1 ints\n",
    "    adj_train = adj_train.astype(np.int32)\n",
    "    \n",
    "    num_classes = class_arr.shape[1]\n",
    "    \n",
    "    # key switch :  cpu_eval (bool)\n",
    "    # establish two models, one for train, one for evaluation, because later the model_eval will load the trained model parameters\n",
    "    \n",
    "#     # for training process: on GPU\n",
    "    minibatch = Minibatch(adj_full, adj_train, role, train_params, cpu_eval = False, mode = \"train\", num_clusters = 16, batch_num = 8)\n",
    "    model = GraphSAINT(num_classes, arch_gcn, train_params, feat_full, class_arr)\n",
    "    \n",
    "    \n",
    "    # for evaluation: validaiton/test  : on CPU\n",
    "    minibatch_eval = Minibatch(adj_full, adj_train,role, train_params, cpu_eval=True, mode = \"eval\")\n",
    "    model_eval = GraphSAINT(num_classes, arch_gcn, train_params, feat_full, class_arr, cpu_eval=True)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "        \n",
    "    # model, model_eval, mini_batch_eval can be saved as pickle file for use later\n",
    "\n",
    "    ### but cannot pickle lambda func for now\n",
    "\n",
    "    prepare_data_folder = working_dir + 'prepare_data/'\n",
    "    os.makedirs(os.path.dirname(prepare_data_folder), exist_ok=True)\n",
    "\n",
    "    train_input_file_name = prepare_data_folder + 'model_train_input'\n",
    "    with open(train_input_file_name, \"wb\") as fp:\n",
    "        dill.dump((minibatch, model), fp)\n",
    "        \n",
    "\n",
    "    evaluation_input_file_name = prepare_data_folder + 'model_eval_input'\n",
    "    with open(evaluation_input_file_name, \"wb\") as fp:\n",
    "        dill.dump((minibatch_eval, model_eval), fp)\n",
    "    \n",
    "    # return model, minibatch, minibatch_eval, model_eval\n",
    "\n",
    "\n",
    "def train_investigate(snap_model_folder, train_phases, model, minibatch, eval_train_every, snapshot_every = 10,\n",
    "          mini_epoch_num = 5, multilabel = True, core_par_sampler = 1, samples_per_processor = 200):\n",
    "    \"\"\"\n",
    "    PURPOSE:  to go through each training phase and take a snapshot of current mode and saved as pickle files\n",
    "        snap_model_folder : folder to save the model snapshots during training\n",
    "        train_phases:  use defined train fases defined in the .yml file\n",
    "        model :  graphsaint model for training\n",
    "        minibatch:   minibatch for training, usually with batches pool\n",
    "        eval_train_every :  periodically store the train loss during the training process\n",
    "        snapshot_every :  periodically store the states of the trained model for later evaluation\n",
    "        mini_epoch_num :  how long the training will focus on one single batch\n",
    "        multilabel : True if a multi-label task, otherwise a multi-class case\n",
    "        core_par_sampler : how many CPU cores  will be used on each CPU\n",
    "        samples_per_processor : how many samples will be generated from each CPU\n",
    "    return: 1) total training time;  2) data uploading time\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    epoch_ph_start = 0\n",
    "    time_train, time_upload, pure_time_train = 0, 0, 0\n",
    "    \n",
    "    # establish a new folder if it does not exist\n",
    "    \n",
    "#     os.makedirs(snap_model_folder, exist_ok = True)\n",
    "    num_batches = minibatch.num_training_batches\n",
    "    for ip, phase in enumerate(train_phases):\n",
    "        printf('START PHASE {:4d}'.format(ip),style='underline')\n",
    "        \n",
    "#         print('calculated batch number is: ', num_batches)\n",
    "        \n",
    "        \n",
    "        epoch_ph_end = int(phase['end'])\n",
    "        macro_epoch_part_num = (epoch_ph_end - epoch_ph_start) // mini_epoch_num\n",
    "        for macro_epoch_idx in range(macro_epoch_part_num):\n",
    "            \n",
    "            l_loss_tr, l_f1mic_tr, l_f1mac_tr = [], [], []\n",
    "            \n",
    "            actual_batch_num = 0\n",
    "            \n",
    "#             while not minibatch.end():\n",
    "            # set the generator for all the one_batch\n",
    "            train_batch_generator = minibatch.generate_train_batch(diag_lambda=-1)\n",
    "            batch_idx = 0\n",
    "            # iterate through the train batch generator:\n",
    "            for node_subgraph, adj_subgraph, norm_loss_subgraph in train_batch_generator:\n",
    "                \n",
    "                ### =========== prepare for all the data to be used ==============\n",
    "                feat_subg = model.feat_full[node_subgraph]\n",
    "                label_subg = model.label_full[node_subgraph]\n",
    "                \n",
    "                if not multilabel:\n",
    "                    # for the multi-class, need type conversion\n",
    "                    label_full_cat = torch.from_numpy(model.label_full.numpy().argmax(axis=1).astype(np.int64))\n",
    "\n",
    "                label_subg_converted = label_subg if multilabel else label_full_cat[node_subgraph]\n",
    "                \n",
    "                t0 = time.time()\n",
    "                # transfer data to the GPU\n",
    "                feat_subg = feat_subg.cuda()\n",
    "                label_subg = label_subg.cuda()\n",
    "                adj_subgraph = adj_subgraph.cuda()\n",
    "                norm_loss_subgraph = norm_loss_subgraph.cuda()\n",
    "                label_subg_converted = label_subg_converted.cuda()\n",
    "                time_upload += time.time() - t0\n",
    "                \n",
    "                \n",
    "                for micro_epoch_idx in range(mini_epoch_num):\n",
    "                    real_epoch_idx = 1 + micro_epoch_idx + macro_epoch_idx * mini_epoch_num + epoch_ph_start\n",
    "                    printf('Epoch {:4d}, Batch ID {}'.format(real_epoch_idx, batch_idx),style='bold')\n",
    "                    # pure training process:\n",
    "                    t1 = time.time()\n",
    "                    loss_train, preds_train = \\\n",
    "                            model.train_step(node_subgraph, adj_subgraph, norm_loss_subgraph, feat_subg, label_subg_converted)\n",
    "                    \n",
    "                    pure_time_train += time.time() - t1\n",
    "                    labels_train = label_subg\n",
    "                    \n",
    "                    # take a snapshot of current model for later validation\n",
    "                    if batch_idx == num_batches - 1 and real_epoch_idx % snapshot_every == 0:\n",
    "                        snap_model_file = snap_model_folder + 'snapshot_epoch_' + str(real_epoch_idx) + '.pkl'\n",
    "                        torch.save(model.state_dict(), snap_model_file)  # store the current state_dict() into the file: 'tmp.pkl'\n",
    "                    \n",
    "                    \n",
    "                        # periodically calculate all the statistics and store them\n",
    "#                             if not minibatch.batch_num % eval_train_every:\n",
    "                        # to_numpy already convert tensor onto CPU\n",
    "                        f1_mic, f1_mac = calc_f1(to_numpy(labels_train), to_numpy(preds_train), model.sigmoid_loss)\n",
    "                        l_loss_tr.append(loss_train)\n",
    "                        l_f1mic_tr.append(f1_mic)\n",
    "                        l_f1mac_tr.append(f1_mac)\n",
    "                \n",
    "                # increase the iteration index of the train batch generator \n",
    "                batch_idx += 1\n",
    "            \n",
    "            \n",
    "        # for different training phase, train it continuously \n",
    "        epoch_ph_start = int(phase['end'])\n",
    "        printf(\"Optimization Finished!\", style=\"yellow\")\n",
    "    \n",
    "    time_train = pure_time_train + time_upload\n",
    "    # after going through all the training phases, print out the total time\n",
    "    printf(\"Total training time: {:6.2f} ms\".format(time_train * 1000), style='red')\n",
    "    printf(\"Total train data uploading time: {:6.2f} ms\".format(time_upload * 1000), style='red')\n",
    "    \n",
    "    return time_train * 1000, time_upload * 1000\n",
    "            \n",
    "\n",
    "def evaluate(snap_model_folder, minibatch_eval, model_eval, epoch_idx, mode='val'):\n",
    "    \"\"\"\n",
    "        Perform the evaluation: either validaiton or test offline from saved snapshot of the models\n",
    "        generate evaluation results from a single timepoint snapshot of the trained model\n",
    "        return : micro_f1 score, macro_f1 score\n",
    "        \n",
    "    \"\"\"\n",
    "    ### location to output the evaluation result\n",
    "    \n",
    "    snap_model_file = snap_model_folder + 'snapshot_epoch_' + str(epoch_idx) + '.pkl'\n",
    "\n",
    "    model_eval.load_state_dict(torch.load(snap_model_file, map_location=lambda storage, loc: storage))\n",
    "\n",
    "    loss_val, f1mic_val, f1mac_val = evaluate_full_batch(model_eval, minibatch_eval, mode = mode)\n",
    "\n",
    "    return f1mic_val, f1mac_val\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### multi-execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiexec module:\n",
    "def execute_train_investigate(image_path, work_dir, train_phases, model, minibatch, eval_train_every, \n",
    "                              tune_param_name, tune_val_label, tune_val, trainer_id = 0,\n",
    "                         snapshot_every = 5, mini_epoch_num = 5, multilabel = True, core_par_sampler = 1, samples_per_processor = 200):\n",
    "    \"\"\"\n",
    "        return all validation-F1 for all four models\n",
    "    \"\"\"\n",
    "    # run the training process for the model\n",
    "    tune_model_folder = work_dir + 'model_snapshot/tune_' + tune_param_name + '_' + str(tune_val_label) + \\\n",
    "                        '/model_trainer_' + str(trainer_id) + '/'\n",
    "    \n",
    "    os.makedirs(os.path.dirname(tune_model_folder), exist_ok=True)\n",
    "    \n",
    "    # to apply any tuning values\n",
    "    total_time_train, time_upload = train_investigate(tune_model_folder, train_phases, model, minibatch, eval_train_every, \n",
    "                                    snapshot_every = snapshot_every, mini_epoch_num = tune_val, multilabel = multilabel, \n",
    "                              core_par_sampler = core_par_sampler, samples_per_processor = samples_per_processor)\n",
    "    \n",
    "    time_info_folder = image_path + 'train_res/tune_' + tune_param_name + '_' + str(tune_val_label) + \\\n",
    "                                    '/model_trainer_' + str(trainer_id) + '/'\n",
    "    \n",
    "    os.makedirs(os.path.dirname(time_info_folder), exist_ok=True)\n",
    "    \n",
    "    time_info_file_name = time_info_folder + 'train_time'\n",
    "    with open(time_info_file_name, \"wb\") as fp:\n",
    "        pickle.dump((total_time_train, time_upload), fp)\n",
    "            \n",
    "    \n",
    "def execute_validation_investigate(image_path, work_dir, minibatch_eval, model_eval, snapshot_epoch_list, \n",
    "                                 tune_param_name, tune_val_label, tune_val, trainer_id = 0):\n",
    "    \"\"\"\n",
    "        Perform the validaiton offline from saved snapshot of the models\n",
    "        snapshot_epoch_list :  a list of the saved models to perform evaluation\n",
    "    \"\"\"\n",
    "    \n",
    "    tune_model_folder = work_dir + 'model_snapshot/tune_' + tune_param_name + '_' + str(tune_val_label) + \\\n",
    "                                    '/model_trainer_' + str(trainer_id) + '/'\n",
    "    \n",
    "    validation_res_folder = image_path + 'validation_res/tune_' + tune_param_name + '_' + str(tune_val_label) + \\\n",
    "                                    '/validation_trainer_' + str(trainer_id) + '/'\n",
    "    \n",
    "    os.makedirs(os.path.dirname(validation_res_folder), exist_ok=True)\n",
    "    \n",
    "    # start evaluation:\n",
    "    for validation_epoch in snapshot_epoch_list:\n",
    "        \n",
    "        res = evaluate(tune_model_folder, minibatch_eval, model_eval, validation_epoch)\n",
    "        \n",
    "        validation_res_file_name = validation_res_folder + 'model_epoch_' + str(validation_epoch)\n",
    "        with open(validation_res_file_name, \"wb\") as fp:\n",
    "            pickle.dump(res, fp)\n",
    "\n",
    "            \n",
    "    \n",
    "def execute_test_tuning(image_path, work_dir, minibatch_eval, model_eval, snapshot_epoch_list, \n",
    "                                 tune_param_name, tune_val_label, tune_val, trainer_id = 0):\n",
    "    \"\"\"\n",
    "        1) After the validation, select the epoch with the best validation score\n",
    "        2) use the trained model at the selected optimal epoch of validation\n",
    "        3) perform the evaluate func for the test data\n",
    "    \"\"\"\n",
    "    # start to search for the trained model epoch with the best validation f1 socre\n",
    "    f1mic_best, ep_best = 0, -1\n",
    "    validation_res_folder = image_path + 'validation_res/tune_' + tune_param_name + '_' + str(tune_val_label) + \\\n",
    "                                    '/validation_trainer_' + str(trainer_id) + '/'\n",
    "    \n",
    "    for validation_epoch in snapshot_epoch_list:\n",
    "        validation_res_file_name = validation_res_folder + 'model_epoch_' + str(validation_epoch)\n",
    "        with open(validation_res_file_name, \"rb\") as fp:\n",
    "            f1mic_val, f1mac_val = pickle.load(fp)\n",
    "        \n",
    "        if f1mic_val > f1mic_best:\n",
    "            f1mic_best, ep_best = f1mic_val, validation_epoch\n",
    "        \n",
    "    # use the selected model to perform on the test\n",
    "    tune_model_folder = work_dir + 'model_snapshot/tune_' + tune_param_name + '_' + str(tune_val_label) + \\\n",
    "                                    '/model_trainer_' + str(trainer_id) + '/'\n",
    "    \n",
    "    # return 1) micro-f1 ;  2) macro-f1\n",
    "    res = evaluate(tune_model_folder, minibatch_eval, model_eval, ep_best, mode = 'test')\n",
    "    \n",
    "    # save the selected best saved snapshot\n",
    "    best_model_file = tune_model_folder + 'snapshot_epoch_' + str(ep_best) + '.pkl'\n",
    "    shutil.copy2(best_model_file, tune_model_folder + 'best_saved_snapshot.pkl')\n",
    "    \n",
    "    # store the resulting data on the disk\n",
    "    test_res_folder = image_path + 'test_res/tune_' + tune_param_name + '_' + str(tune_val_label) + '/'\n",
    "    os.makedirs(os.path.dirname(test_res_folder), exist_ok=True)\n",
    "    test_res_file = test_res_folder + 'res_trainer_' + str(trainer_id)\n",
    "    \n",
    "    with open(test_res_file, \"wb\") as fp:\n",
    "        pickle.dump(res, fp)\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  load data settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = '/home/xiangli/projects/tmpdata/GCN/GraphSaint/'\n",
    "\n",
    "working_dir = './res_step0_all_in_one/'\n",
    "prepare_data_folder = working_dir + 'prepare_data/'\n",
    "img_path = working_dir + 'result/'\n",
    "\n",
    "core_par_sampler = 1\n",
    "samples_per_processor = -(-200 // core_par_sampler) # round up division\n",
    "eval_train_every = 5  # period to record the train loss\n",
    "\n",
    "### ================ Start to do flexible settings according to different dataset: \n",
    "# read the total epoch number from the yml file to determine the mini_epoch_num and eval_train_every\n",
    "data_name = 'Flickr'\n",
    "train_config_yml = './table2/flickr2_e.yml'\n",
    "multilabel_tag = False\n",
    "\n",
    "# data_name = 'PPI_small'\n",
    "# train_config_yml = './table2/ppi2_e.yml'\n",
    "\n",
    "\n",
    "tune_param_name = 'mini_epoch_num'\n",
    "tune_val_label_list = [5] \n",
    "tune_val_list = [val for val in tune_val_label_list]\n",
    "\n",
    "snapshot_period = 5   # period when to take a snapshot of the model for validation later\n",
    "\n",
    "# refer to the yml file to decide the training period:\n",
    "model_epoch_list = list(range(snapshot_period, 16, snapshot_period))    # snapshot epoch list for validation\n",
    "\n",
    "trainer_list = list(range(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangli/anaconda3/envs/pytorch_1_4_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:26: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data..\n",
      "Done loading training data..\n"
     ]
    }
   ],
   "source": [
    "# =============== Step1 *** prepare for the batches, models, model_evaluation\n",
    "train_params, train_phases, train_data, arch_gcn = train_setting(data_name, datapath, train_config_yml)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adj train type is: <class 'scipy.sparse.csr.csr_matrix'>; and shape is (89250, 89250)\n",
      "adj train type is: <class 'scipy.sparse.csr.csr_matrix'>; and shape is (89250, 89250)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prepare(working_dir, train_data, train_params, arch_gcn)\n",
    "train_phase_file_name = prepare_data_folder + 'model_train_phase'\n",
    "with open(train_phase_file_name, \"wb\") as fp:\n",
    "    dill.dump(train_phases, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[4mSTART PHASE    0\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangli/projects/GCN_program/largescale_GCN/HPC_version_GCN/22_clusterGCN_custom_hpc/gold_all_in_one/utils.py:184: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  norm_diag = sp.dia_matrix((1/D,0),shape=diag_shape)  # offset is 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mEpoch    1, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    2, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    3, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    4, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    5, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    1, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    2, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    3, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    4, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    5, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    1, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    2, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    3, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    4, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    5, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    1, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    2, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    3, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    4, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    5, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    1, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch    2, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch    3, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch    4, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch    5, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch    1, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch    2, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch    3, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch    4, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch    5, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch    1, Batch ID 6\u001b[0m\n",
      "\u001b[1mEpoch    2, Batch ID 6\u001b[0m\n",
      "\u001b[1mEpoch    3, Batch ID 6\u001b[0m\n",
      "\u001b[1mEpoch    4, Batch ID 6\u001b[0m\n",
      "\u001b[1mEpoch    5, Batch ID 6\u001b[0m\n",
      "\u001b[1mEpoch    1, Batch ID 7\u001b[0m\n",
      "\u001b[1mEpoch    2, Batch ID 7\u001b[0m\n",
      "\u001b[1mEpoch    3, Batch ID 7\u001b[0m\n",
      "\u001b[1mEpoch    4, Batch ID 7\u001b[0m\n",
      "\u001b[1mEpoch    5, Batch ID 7\u001b[0m\n",
      "\u001b[1mEpoch    6, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    7, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    8, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    9, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   10, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    6, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    7, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    8, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    9, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   10, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    6, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    7, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    8, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    9, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   10, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    6, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    7, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    8, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    9, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   10, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    6, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch    7, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch    8, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch    9, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   10, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch    6, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch    7, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch    8, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch    9, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   10, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch    6, Batch ID 6\u001b[0m\n",
      "\u001b[1mEpoch    7, Batch ID 6\u001b[0m\n",
      "\u001b[1mEpoch    8, Batch ID 6\u001b[0m\n",
      "\u001b[1mEpoch    9, Batch ID 6\u001b[0m\n",
      "\u001b[1mEpoch   10, Batch ID 6\u001b[0m\n",
      "\u001b[1mEpoch    6, Batch ID 7\u001b[0m\n",
      "\u001b[1mEpoch    7, Batch ID 7\u001b[0m\n",
      "\u001b[1mEpoch    8, Batch ID 7\u001b[0m\n",
      "\u001b[1mEpoch    9, Batch ID 7\u001b[0m\n",
      "\u001b[1mEpoch   10, Batch ID 7\u001b[0m\n",
      "\u001b[1mEpoch   11, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   12, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   13, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   14, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   15, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   11, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   12, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   13, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   14, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   15, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   11, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   12, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   13, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   14, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   15, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   11, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   12, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   13, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   14, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   15, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   11, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   12, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   13, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   14, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   15, Batch ID 4\u001b[0m\n",
      "\u001b[1mEpoch   11, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   12, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   13, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   14, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   15, Batch ID 5\u001b[0m\n",
      "\u001b[1mEpoch   11, Batch ID 6\u001b[0m\n",
      "\u001b[1mEpoch   12, Batch ID 6\u001b[0m\n",
      "\u001b[1mEpoch   13, Batch ID 6\u001b[0m\n",
      "\u001b[1mEpoch   14, Batch ID 6\u001b[0m\n",
      "\u001b[1mEpoch   15, Batch ID 6\u001b[0m\n",
      "\u001b[1mEpoch   11, Batch ID 7\u001b[0m\n",
      "\u001b[1mEpoch   12, Batch ID 7\u001b[0m\n",
      "\u001b[1mEpoch   13, Batch ID 7\u001b[0m\n",
      "\u001b[1mEpoch   14, Batch ID 7\u001b[0m\n",
      "\u001b[1mEpoch   15, Batch ID 7\u001b[0m\n",
      "\u001b[93mOptimization Finished!\u001b[0m\n",
      "\u001b[91mTotal training time: 6444.43 ms\u001b[0m\n",
      "\u001b[91mTotal train data uploading time:  44.18 ms\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# ============== Step2 *** conduct the training process\n",
    "train_input_file_name = prepare_data_folder + 'model_train_input'\n",
    "with open(train_input_file_name, \"rb\") as fp:\n",
    "    minibatch, model = dill.load(fp)\n",
    "\n",
    "\n",
    "train_phase_file_name = prepare_data_folder + 'model_train_phase'\n",
    "with open(train_phase_file_name, \"rb\") as fp:\n",
    "    train_phases = dill.load(fp)\n",
    "\n",
    "for tune_val_label, tune_val in zip(tune_val_label_list, tune_val_list):\n",
    "    for trainer_id in trainer_list:\n",
    "        execute_train_investigate(img_path, working_dir, train_phases, model, minibatch, eval_train_every, \n",
    "                                  tune_param_name, tune_val_label, tune_val, trainer_id = trainer_id,\n",
    "                                  snapshot_every = snapshot_period, mini_epoch_num = 5, multilabel = multilabel_tag, \n",
    "                                  core_par_sampler = core_par_sampler, samples_per_processor = samples_per_processor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "during the evaluation step, report the input matrices size: \n",
      "adj_subgraph size :  torch.Size([89250, 89250]); \t feat_subg size : torch.Size([89250, 500])\n",
      "during the evaluation step, report the input matrices size: \n",
      "adj_subgraph size :  torch.Size([89250, 89250]); \t feat_subg size : torch.Size([89250, 500])\n",
      "during the evaluation step, report the input matrices size: \n",
      "adj_subgraph size :  torch.Size([89250, 89250]); \t feat_subg size : torch.Size([89250, 500])\n"
     ]
    }
   ],
   "source": [
    "# ================ Step3*** investigate validation:\n",
    "evaluation_input_file_name = prepare_data_folder + 'model_eval_input'\n",
    "with open(evaluation_input_file_name, \"rb\") as fp:\n",
    "    minibatch_eval, model_eval = dill.load(fp)\n",
    "\n",
    "for tune_val_label, tune_val in zip(tune_val_label_list, tune_val_list):\n",
    "    for trainer_id in trainer_list:\n",
    "        execute_validation_investigate(img_path, working_dir, minibatch_eval, model_eval, model_epoch_list, \n",
    "                                tune_param_name, tune_val_label, tune_val, trainer_id = trainer_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "during the evaluation step, report the input matrices size: \n",
      "adj_subgraph size :  torch.Size([89250, 89250]); \t feat_subg size : torch.Size([89250, 500])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ================= Step4*** investigate test:\n",
    "evaluation_input_file_name = prepare_data_folder + 'model_eval_input'\n",
    "with open(evaluation_input_file_name, \"rb\") as fp:\n",
    "    minibatch_eval, model_eval = dill.load(fp)\n",
    "\n",
    "for tune_val_label, tune_val in zip(tune_val_label_list, tune_val_list):\n",
    "    for trainer_id in trainer_list:\n",
    "        execute_test_tuning(img_path, working_dir, minibatch_eval, model_eval, model_epoch_list, \n",
    "                                tune_param_name, tune_val_label, tune_val, trainer_id = trainer_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start summarizing for dataset : Flickr\n",
      "Start running training for dataset: Flickr\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for tune_val_label, tune_val in zip(tune_val_label_list, tune_val_list):\n",
    "    for trainer_id in trainer_list:\n",
    "        step51_run_investigation_summarize_whole(data_name, img_path,\n",
    "                                         tune_param_name, tune_val_label, tune_val,\n",
    "                                            trainer_list, model_epoch_list)\n",
    "    \n",
    "for tune_val_label, tune_val in zip(tune_val_label_list, tune_val_list):\n",
    "    for trainer_id in trainer_list:\n",
    "        step50_run_tune_summarize_whole(data_name, img_path, \n",
    "                                    tune_param_name, tune_val_label_list, tune_val_list,\n",
    "                                    trainer_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_1_4_geometric]",
   "language": "python",
   "name": "conda-env-pytorch_1_4_geometric-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
