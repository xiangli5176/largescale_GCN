{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## isolated clustering including recombination of mini-cluster for hpc run \n",
    "\n",
    "Comments:\n",
    "\n",
    "ClusterGCN baseline, KDD 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import csv\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pickle\n",
    "import shutil\n",
    "\n",
    "from utils import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import torch\n",
    "from torch_geometric.utils import scatter_\n",
    "\n",
    "import math\n",
    "import random\n",
    "\n",
    "from torch.nn import Parameter\n",
    "from torch_scatter import scatter_add\n",
    "# from torch_geometric.nn import MessagePassing\n",
    "import torch.nn.functional as F\n",
    "# from torch_geometric.nn import SAGEConv\n",
    "from sagegcn_conv_enhance import SAGEConv\n",
    "from torch_geometric.utils import add_remaining_self_loops\n",
    "\n",
    "### ====================== Establish a GCN based model ========================\n",
    "class ListModule(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Abstract list layer class.\n",
    "    \"\"\"\n",
    "    def __init__(self, *args):\n",
    "        \"\"\"\n",
    "        Module initializing.\n",
    "        \"\"\"\n",
    "        super(ListModule, self).__init__()\n",
    "        idx = 0\n",
    "        for module in args:\n",
    "            self.add_module(str(idx), module)\n",
    "            idx += 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Getting the indexed layer.\n",
    "        \"\"\"\n",
    "        if idx < 0 or idx >= len(self._modules):\n",
    "            raise IndexError('index {} is out of range'.format(idx))\n",
    "        it = iter(self._modules.values())\n",
    "        for i in range(idx):\n",
    "            next(it)\n",
    "        return next(it)\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Iterating on the layers.\n",
    "        \"\"\"\n",
    "        return iter(self._modules.values())\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Number of layers.\n",
    "        \"\"\"\n",
    "        return len(self._modules)\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, input_layers = [16, 16], dropout=0.3, improved = False, diag_lambda = -1):\n",
    "        \"\"\"\n",
    "        input layers: list of integers\n",
    "        dropout: probability of droping out \n",
    "        \"\"\"\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.input_layers = input_layers\n",
    "        self.dropout = dropout\n",
    "        self.improved = improved \n",
    "        self.diag_lambda = diag_lambda\n",
    "        self.setup_layers()\n",
    "\n",
    "    def setup_layers(self):\n",
    "        \"\"\"\n",
    "        Creating the layes based on the args.\n",
    "        \"\"\"\n",
    "        self.layers = []\n",
    "        self.input_layers = [self.in_channels] + self.input_layers + [self.out_channels]\n",
    "        \n",
    "        for i in range(len(self.input_layers)-1):\n",
    "            self.layers.append(SAGEConv(self.input_layers[i], self.input_layers[i+1], \n",
    "                                       improved = self.improved, diag_lambda = self.diag_lambda, dropout = self.dropout))\n",
    "        self.layers = ListModule(*self.layers)\n",
    "        \n",
    "        self.bn_layers = []\n",
    "        for i in range(1, len(self.input_layers)):\n",
    "            self.bn_layers.append(torch.nn.BatchNorm1d(self.input_layers[i]))\n",
    "        self.bn_layers = ListModule(*self.bn_layers)\n",
    "\n",
    "    # change the dropout positions: \n",
    "    def forward(self, edge_index, features):\n",
    "        if len(self.layers) > 1:\n",
    "            for i in range(len(self.layers)-1):\n",
    "                features = self.layers[i](features, edge_index, dropout_training=self.training)\n",
    "#                 features = F.dropout(features, p = self.dropout, training = self.training)\n",
    "                features = self.bn_layers[i](features)\n",
    "                features = F.relu(features)\n",
    "            \n",
    "            features = self.layers[len(self.layers)-1](features, edge_index, dropout_training=self.training)\n",
    "        else:\n",
    "            features = self.layers[0](features, edge_index, dropout_training=self.training)    # for a single layer case\n",
    "        \n",
    "        predictions = features\n",
    "        # just use the linear layer output, since we are using the cross-entropy loss\n",
    "        # just pay attention to the test part, change the predictions\n",
    "        \n",
    "#         predictions = F.log_softmax(features, dim=1)\n",
    "        # if using the nll loss, then we need this log_softmax layer\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partition GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import metis\n",
    "import random\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import chain\n",
    "\n",
    "class ClusteringMachine(object):\n",
    "    \"\"\"\n",
    "    Clustering the graph, feature set and label. Performed on the CPU side\n",
    "    \"\"\"\n",
    "    def __init__(self, edge_index, features, label, tmp_folder = './tmp/', info_folder = './info/'):\n",
    "        \"\"\"\n",
    "        :param edge_index: COO format of the edge indices.\n",
    "        :param features: Feature matrix (ndarray).\n",
    "        :param label: label vector (ndarray).\n",
    "        :tmp_folder(string): the path of the folder to contain all the clustering information files\n",
    "        \"\"\"\n",
    "        self.features = features\n",
    "        self.label = label\n",
    "        self._set_sizes()\n",
    "        self.edge_index = edge_index\n",
    "        # store the information folder for memory tracing\n",
    "        self.tmp_folder = tmp_folder\n",
    "        self.info_folder = info_folder\n",
    "        \n",
    "        tmp = edge_index.t().numpy().tolist()\n",
    "        self.graph = nx.from_edgelist(tmp)\n",
    "        \n",
    "    def _set_sizes(self):\n",
    "        \"\"\"\n",
    "        Setting the feature and class count.\n",
    "        \"\"\"\n",
    "        self.node_count = self.features.shape[0]\n",
    "        self.feature_count = self.features.shape[1]    # features all always in the columns\n",
    "        self.label_count = len(np.unique(self.label.numpy()) )\n",
    "        \n",
    "    def split_whole_nodes_edges_then_cluster(self, validation_ratio, test_ratio, train_batch_num = 2, validation_batch_num = 2, test_batch_num = 2):\n",
    "        \"\"\"\n",
    "            Only split nodes\n",
    "            First create train-test splits, then split train and validation into different batch seeds\n",
    "            Input:  \n",
    "                1) ratio of validation (for convergnece), test\n",
    "                2) partition number of train nodes, validation nodes (test nodes will be uses as a whole graph)\n",
    "            Output:\n",
    "                1) self.sg_validation_nodes_global, self.sg_train_nodes_global, self.sg_test_nodes_global\n",
    "        \"\"\"\n",
    "        self.train_batch_num = train_batch_num\n",
    "        relative_validation_ratio = (validation_ratio) / (1 - test_ratio)\n",
    "        \n",
    "        # first divide the nodes for the whole graph, result will always be a list of lists \n",
    "        model_nodes_global, self.test_nodes_global = train_test_split(list(self.graph.nodes()), test_size = test_ratio)\n",
    "        train_nodes_global, self.validation_nodes_global = train_test_split(model_nodes_global, test_size = relative_validation_ratio)\n",
    "        \n",
    "        self.sg_train_nodes_global = self.random_clustering(train_nodes_global, train_batch_num)\n",
    "        self.sg_validation_nodes_global = self.random_clustering(self.validation_nodes_global, validation_batch_num)\n",
    "        self.sg_test_nodes_global = self.random_clustering(self.test_nodes_global, test_batch_num)\n",
    "\n",
    "#         self.sg_train_nodes_global = self.metis_clustering(train_nodes_global, train_batch_num)\n",
    "#         self.sg_validation_nodes_global = self.metis_clustering(validation_nodes_global, validation_batch_num)\n",
    "        \n",
    "    \n",
    "    # just allocate each node to arandom cluster, store the membership inside each dict\n",
    "    def random_clustering(self, target_nodes, partition_num):\n",
    "        \"\"\"\n",
    "            Random clustering the nodes.\n",
    "            Input: \n",
    "                1) target_nodes: list of node \n",
    "                2) partition_num: number of partition to be generated\n",
    "            Output: \n",
    "                1) membership of each node\n",
    "        \"\"\"\n",
    "        # randomly divide into two clusters\n",
    "        nodes_order = [node for node in target_nodes]\n",
    "        random.shuffle(nodes_order)\n",
    "        n = (len(nodes_order) + partition_num - 1) // partition_num\n",
    "        partition_list = [nodes_order[i * n:(i + 1) * n] for i in range(partition_num)]\n",
    "#         cluster_membership = {node : i for i, node_list in enumerate(partition_list) for node in node_list}\n",
    "        cluster_nodes_global = {i : node_list for i, node_list in enumerate(partition_list)}\n",
    "        \n",
    "        return cluster_nodes_global\n",
    "\n",
    "    def metis_clustering(self, target_nodes, partition_num):\n",
    "        \"\"\"\n",
    "            Random clustering the nodes.\n",
    "            Input: \n",
    "                1) target_nodes: list of node \n",
    "                2) partition_num: number of partition to be generated\n",
    "            Output: \n",
    "                1) membership of each node\n",
    "        \"\"\"\n",
    "        target_graph = self.graph.subgraph(target_nodes)\n",
    "        (st, parts) = metis.part_graph(target_graph, partition_num)\n",
    "        clusters = list(set(parts))\n",
    "        cluster_nodes_global = defaultdict(list)\n",
    "        for node, cluster_id in enumerate(parts):\n",
    "            cluster_nodes_global[cluster_id].append(node)\n",
    "        return cluster_nodes_global\n",
    "\n",
    "    # for use of test on the whole graph as a whole in CPU-side memory\n",
    "    def whole_batch_generate(self, batch_file_folder, test_nodes, batch_name = 'test_batch_whole'):\n",
    "        \"\"\"\n",
    "            For use of testing the model: generate the needed tensors for testing in CPU-memory side\n",
    "        \"\"\"\n",
    "        # store the global edges\n",
    "        whole_nodes_global = sorted(self.graph.nodes())\n",
    "        whole_edges_global = {edge for edge in self.graph.edges()}\n",
    "        \n",
    "        whole_edges_local = \\\n",
    "                       [ [ left, right ] for left, right in whole_edges_global ] + \\\n",
    "                       [ [ right, left ] for left, right in whole_edges_global ] + \\\n",
    "                       [ [i, i] for i in whole_nodes_global ]  \n",
    "        \n",
    "        # store local features and lables\n",
    "        whole_features_local = self.features\n",
    "        whole_labels_local = self.label\n",
    "\n",
    "        # transform all the data to the tensor form\n",
    "        whole_edges_local = torch.LongTensor(whole_edges_local).t()\n",
    "        whole_features_local = torch.FloatTensor(whole_features_local)\n",
    "        whole_labels_local = torch.LongTensor(whole_labels_local)\n",
    "        whole_test_nodes_local = torch.LongTensor( sorted(test_nodes) )\n",
    "\n",
    "        whole_batch_data = [whole_test_nodes_local, whole_edges_local, whole_features_local, whole_labels_local]\n",
    "\n",
    "        batch_file_name = batch_file_folder + batch_name\n",
    "\n",
    "        # store the batch files\n",
    "        t0 = time.time()\n",
    "        with open(batch_file_name, \"wb\") as fp:\n",
    "            pickle.dump(whole_batch_data, fp)\n",
    "        store_time = ((time.time() - t0) * 1000)\n",
    "        print('*** Generate batch file for # {0} batch, writing the batch file costed {1:.2f} ms ***'.format(\"whole graph\", store_time) )\n",
    "    \n",
    "    \n",
    "    def mini_batch_neighbor_sample(self, target_seed, sample_num = 10, layer_num = 2):\n",
    "        \"\"\"\n",
    "            This function is to generate the neighbors of the seed (either train nodes or validation nodes)\n",
    "            params: cluster index, number of layer k, fraction of sampling from each neighbor layer\n",
    "            input: \n",
    "                1) target_seed: this is the 0 layer inside self.neighbor (can be either train or validation data)\n",
    "                2) sample_num : number of sampling neighbor nodes around each seed; \n",
    "                    if negative, then include all the neighbors within layer_num of layers\n",
    "            output:\n",
    "                1) accum_neighbor: accumulating neighbors , i.e. the final batch nodes\n",
    "        \"\"\"\n",
    "        accum_neighbor = defaultdict(set)\n",
    "        \n",
    "        if sample_num < 0:\n",
    "            for cluster in target_seed.keys():\n",
    "                pre_layer = set(target_seed[cluster])  # first layer of the neighbor nodes of each cluster\n",
    "                accum_neighbor[cluster] |= pre_layer  # use the union instead of assignment here, avoid shallow copy issue\n",
    "                for _ in range(layer_num):\n",
    "                    curr_layer = set()\n",
    "                    for node in pre_layer:\n",
    "                        node_neigh = set(self.graph.neighbors(node))\n",
    "                        curr_layer |= node_neigh\n",
    "                    pre_layer = curr_layer - accum_neighbor[cluster]\n",
    "                    accum_neighbor[cluster] |= pre_layer\n",
    "        else:\n",
    "            for cluster in target_seed.keys():\n",
    "                neighbor = set(target_seed[cluster])  # first layer of the neighbor nodes of each cluster\n",
    "                tmp_level = set()\n",
    "                for node in neighbor:\n",
    "                    node_neigh = set(self.graph.neighbors(node))\n",
    "                    tmp_level |= set(random.sample(node_neigh, sample_num)) if len(node_neigh) > sample_num else node_neigh\n",
    "                    # the key here we are using self.graph, extract neighbor from the whole graph\n",
    "                accum_neighbor[cluster] |= neighbor | tmp_level\n",
    "        \n",
    "        return accum_neighbor\n",
    "    \n",
    "    def mini_batch_generate_tensor(self, target_batch_nodes, target_seed_nodes):\n",
    "        \"\"\"\n",
    "            sampled_neighbor_nodes : selected neighbor nodes, to be combined with train seeds to form a train batch\n",
    "            input: \n",
    "                1) target_batch_nodes: batch nodes including sampling neighbors around seed nodes \n",
    "                2) target_seed_nodes : seed nodes for forming neighbors (can be either train or validation data)\n",
    "        \"\"\"\n",
    "        batch_subgraph = self.graph.subgraph(target_batch_nodes)\n",
    "            \n",
    "         # first select all the overlapping nodes of the train nodes\n",
    "        mini_nodes_global = sorted(node for node in batch_subgraph.nodes())\n",
    "\n",
    "        # store the global edges\n",
    "        mini_edges_global = {edge for edge in batch_subgraph.edges()}\n",
    "\n",
    "        # map nodes from global index to local index\n",
    "        mini_mapper = {node: i for i, node in enumerate(mini_nodes_global)}\n",
    "\n",
    "        # store local index of batch nodes\n",
    "        mini_nodes_local = [ mini_mapper[global_idx] for global_idx in target_seed_nodes ]\n",
    "\n",
    "        # store local index of batch edges\n",
    "        mini_edges_local = \\\n",
    "                       [ [ mini_mapper[edge[0]], mini_mapper[edge[1]] ] for edge in mini_edges_global ] + \\\n",
    "                       [ [ mini_mapper[edge[1]], mini_mapper[edge[0]] ] for edge in mini_edges_global ] \n",
    "\n",
    "        # store local features and lables\n",
    "        mini_features = self.features[mini_nodes_global,:]\n",
    "        mini_labels = self.label[mini_nodes_global]\n",
    "        \n",
    "        # record information \n",
    "        info_batch_node_size = len(mini_nodes_global)\n",
    "        info_batch_edge_size = len(mini_edges_local) + info_batch_node_size  # add the self-edges\n",
    "\n",
    "        mini_nodes_local = torch.LongTensor(mini_nodes_local)\n",
    "        mini_edges_local = torch.LongTensor(mini_edges_local).t()\n",
    "        mini_features = torch.FloatTensor(mini_features)\n",
    "        mini_labels = torch.LongTensor(mini_labels)\n",
    "\n",
    "        minibatch_data = [mini_nodes_local, mini_edges_local, mini_features, mini_labels]\n",
    "        \n",
    "        return minibatch_data, info_batch_node_size, info_batch_edge_size\n",
    "    \n",
    "    def mini_batch_generate(self, batch_file_folder, target_seed, sample_num = 10, layer_num = 2, batch_range = (0, 1)):\n",
    "        \"\"\"\n",
    "            create the mini-batch focused on the train nodes only, include a total of k layers of neighbors of the original training nodes\n",
    "            k: number of layers of neighbors for each training node\n",
    "            fraction: fraction of neighbor nodes in each layer to be considered\n",
    "            Input:\n",
    "                1) target_seed: global ids of the nodes for seed to generate the batch (can be train or validation)\n",
    "                2) sample_num : number of sampling nodes to be selected from each seed node's neighbors\n",
    "            Output: all tensors which are gonna be used in the train, forward procedure\n",
    "                local:\n",
    "                    1) sg_mini_edges_local\n",
    "                    2) self.sg_mini_train_nodes_local\n",
    "                    3) self.sg_mini_train_features\n",
    "                    4) self.sg_mini_train_labels\n",
    "            \n",
    "        \"\"\"\n",
    "        accum_neighbor = self.mini_batch_neighbor_sample(target_seed, sample_num = sample_num, layer_num = layer_num)\n",
    "        batch_start, batch_end = batch_range\n",
    "        \n",
    "        info_batch_node_size = {}\n",
    "        info_batch_edge_size = {}\n",
    "        batch_start, batch_end = batch_range\n",
    "        for cluster in range(batch_start, batch_end):\n",
    "            # main purpose is to avoid too large size of this batch_subgraph with too many intra-edges inside\n",
    "            \n",
    "            minibatch_data, info_batch_node_size[cluster], info_batch_edge_size[cluster] = self.mini_batch_generate_tensor(accum_neighbor[cluster], target_seed[cluster])\n",
    "            \n",
    "            # store the batch files\n",
    "            t0 = time.time()\n",
    "            batch_file_name = batch_file_folder + 'batch_' + str(cluster)\n",
    "            \n",
    "            with open(batch_file_name, \"wb\") as fp:\n",
    "                pickle.dump(minibatch_data, fp)\n",
    "            store_time = ((time.time() - t0) * 1000)\n",
    "            print('*** Generate batch file for # {0:3d} batch, writing the batch file costed {1:.2f} ms ***'.format(cluster, store_time) )\n",
    "#             print('writing to the path: ', batch_file_name)\n",
    "            \n",
    "        return info_batch_node_size, info_batch_edge_size\n",
    "    \n",
    "    \n",
    "    def save_info_dict(self, data, file_name, target_folder, header = 'key, value'):\n",
    "        # output the batch size information as the csv file\n",
    "#         os.makedirs(os.path.dirname(target_folder), exist_ok=True)\n",
    "        target_file = target_folder + file_name\n",
    "        with open(target_file, 'a', newline='\\n') as fp:\n",
    "            wr = csv.writer(fp, delimiter = ',')\n",
    "            fp.write('\\n')\n",
    "            wr.writerow(header.split(','))\n",
    "            for key, val in data.items():\n",
    "                if isinstance(val, list):\n",
    "                    wr.writerow([key+1] + val)\n",
    "                else:\n",
    "                    wr.writerow([key+1, val])\n",
    "                    \n",
    "    def mini_batch_train_clustering(self, batch_folder, sample_num = 10, layer_num = 2, batch_range = (0, 1), info_folder = './info/', info_file = 'train_batch_size_info.csv'):\n",
    "        data_type = 'train'\n",
    "        batch_file_folder = batch_folder + '{}_batch/'.format(data_type)\n",
    "#         check_folder_exist(batch_file_folder)\n",
    "        os.makedirs(os.path.dirname(batch_file_folder), exist_ok=True)\n",
    "        \n",
    "        self.info_train_batch_node_size, self.info_train_batch_edge_size  = self.mini_batch_generate(batch_file_folder, self.sg_train_nodes_global, sample_num = sample_num, layer_num = layer_num, batch_range = batch_range)\n",
    "        self.info_train_seed_size = {key : len(val) for key, val in self.sg_train_nodes_global.items()}\n",
    "        \n",
    "        self.save_info_dict(self.info_train_batch_node_size, info_file, info_folder, header = 'train_batch_node_id, train_batch_node_size')\n",
    "        self.save_info_dict(self.info_train_batch_edge_size, info_file, info_folder, header = 'train_batch_edge_id, train_batch_edge_size')\n",
    "        self.save_info_dict(self.info_train_seed_size, info_file, info_folder, header = 'train_seed_node_id, train_seed_node_size')\n",
    "        \n",
    "        \n",
    "    def mini_batch_validation_clustering(self, batch_folder, sample_num = -1, layer_num = 2, batch_range = (0, 1), info_folder = './info/', info_file = 'validation_batch_size_info.csv'):\n",
    "        data_type = 'validation'\n",
    "        batch_file_folder = batch_folder + '{}_batch/'.format(data_type)\n",
    "#         check_folder_exist(batch_file_folder)\n",
    "        os.makedirs(os.path.dirname(batch_file_folder), exist_ok=True)\n",
    "        \n",
    "        self.info_validation_batch_node_size, self.info_validation_batch_edge_size  = self.mini_batch_generate(batch_file_folder, self.sg_validation_nodes_global, sample_num = sample_num, layer_num = layer_num, batch_range = batch_range)\n",
    "        self.info_validation_seed_size = {key : len(val) for key, val in self.sg_validation_nodes_global.items()}\n",
    "        \n",
    "        self.save_info_dict(self.info_validation_batch_node_size, info_file, info_folder, header = 'validation_batch_node_id, validation_batch_node_size')\n",
    "        self.save_info_dict(self.info_validation_batch_edge_size, info_file, info_folder, header = 'validation_batch_edge_id, validation_batch_edge_size')\n",
    "        self.save_info_dict(self.info_validation_seed_size, info_file, info_folder, header = 'validation_seed_node_id, validation_seed_node_size')\n",
    "        \n",
    "    def mini_batch_test_clustering(self, batch_folder, sample_num = -1, layer_num = 2, batch_range = (0, 1), info_folder = './info/', info_file = 'test_batch_size_info.csv'):\n",
    "        data_type = 'test'\n",
    "        batch_file_folder = batch_folder + '{}_batch/'.format(data_type)\n",
    "#         check_folder_exist(batch_file_folder)\n",
    "        os.makedirs(os.path.dirname(batch_file_folder), exist_ok=True)\n",
    "        \n",
    "        self.info_test_batch_node_size, self.info_test_batch_edge_size  = self.mini_batch_generate(batch_file_folder, self.sg_test_nodes_global, sample_num = sample_num, layer_num = layer_num, batch_range = batch_range)\n",
    "        self.info_test_seed_size = {key : len(val) for key, val in self.sg_test_nodes_global.items()}\n",
    "        \n",
    "        self.save_info_dict(self.info_test_batch_node_size, info_file, info_folder, header = 'test_batch_node_id, test_batch_node_size')\n",
    "        self.save_info_dict(self.info_test_batch_edge_size, info_file, info_folder, header = 'test_batch_edge_id, test_batch_edge_size')\n",
    "        self.save_info_dict(self.info_test_seed_size, info_file, info_folder, header = 'test_seed_node_id, test_seed_node_size')\n",
    "\n",
    "    \n",
    "    def whole_test_clustering(self, batch_folder, info_folder = './info/'):\n",
    "        data_type = 'test'\n",
    "        batch_file_folder = batch_folder + '{}_batch/'.format(data_type)\n",
    "#         check_folder_exist(batch_file_folder)\n",
    "        os.makedirs(os.path.dirname(batch_file_folder), exist_ok=True)\n",
    "        \n",
    "        self.whole_batch_generate(batch_file_folder, self.test_nodes_global, batch_name = data_type + '_batch_whole')        \n",
    "        \n",
    "        \n",
    "    def whole_validation_clustering(self, batch_folder, info_folder = './info/'):\n",
    "        data_type = 'validation'\n",
    "        batch_file_folder = batch_folder + '{}_batch/'.format(data_type)\n",
    "#         check_folder_exist(batch_file_folder)\n",
    "        os.makedirs(os.path.dirname(batch_file_folder), exist_ok=True)\n",
    "        \n",
    "        self.whole_batch_generate(batch_file_folder, self.validation_nodes_global, batch_name = data_type + '_batch_whole')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Partition Graph with trainiing and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Custom_GCN_layer import Net\n",
    "import time\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class ClusterGCNTrainer_mini_Train(object):\n",
    "    \"\"\"\n",
    "    Training a ClusterGCN.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_folder, in_channels, out_channels, input_layers = [32, 16], dropout=0.3, improved = False, diag_lambda = -1):\n",
    "        \"\"\"\n",
    "        :param in_channels, out_channels: input and output feature dimension\n",
    "        :param clustering_machine:\n",
    "        \"\"\"  \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.test_device = torch.device(\"cpu\")\n",
    "        \n",
    "        self.data_folder = data_folder\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.input_layers = input_layers\n",
    "        self.dropout = dropout\n",
    "        self.improved = improved\n",
    "        self.diag_lambda = diag_lambda\n",
    "        \n",
    "        self.create_model()\n",
    "\n",
    "    def create_model(self):\n",
    "        \"\"\"\n",
    "        Creating a StackedGCN and transferring to CPU/GPU.\n",
    "        \"\"\"\n",
    "#         print('used layers are: ', str(self.input_layers))\n",
    "        self.model = Net(self.in_channels, self.out_channels, input_layers = self.input_layers, \n",
    "                         dropout = self.dropout, improved = self.improved, diag_lambda = self.diag_lambda)\n",
    "        self.model = self.model.to(self.device)\n",
    "    \n",
    "    # call the forward function batch by batch\n",
    "    def do_forward_pass(self, tr_train_nodes, tr_edges, tr_features, tr_target):\n",
    "        \"\"\"\n",
    "        Making a forward pass with data from a given partition.\n",
    "        :param cluster: Cluster index.\n",
    "        :return average_loss: Average loss on the cluster.\n",
    "        :return node_count: Number of nodes.\n",
    "        \"\"\"\n",
    "        \n",
    "        '''Target and features are one-one mapping'''\n",
    "        # calculate the probabilites from log_sofmax\n",
    "        predictions = self.model(tr_edges, tr_features)\n",
    "        \n",
    "        ave_loss = torch.nn.functional.cross_entropy(predictions[tr_train_nodes], tr_target[tr_train_nodes])\n",
    "#         ave_loss = torch.nn.functional.nll_loss(predictions[tr_train_nodes], tr_target[tr_train_nodes])\n",
    "        node_count = tr_train_nodes.shape[0]\n",
    "\n",
    "        # for each cluster keep track of the counts of the nodes\n",
    "        return ave_loss, node_count\n",
    "\n",
    "    def update_average_loss(self, batch_average_loss, node_count, isolate = True):\n",
    "        \"\"\"\n",
    "        Updating the average loss in the epoch.\n",
    "        :param batch_average_loss: Loss of the cluster. \n",
    "        :param node_count: Number of nodes in currently processed cluster.\n",
    "        :return average_loss: Average loss in the epoch.\n",
    "        \"\"\"\n",
    "        self.accumulated_training_loss = self.accumulated_training_loss + batch_average_loss.item() * node_count\n",
    "        if isolate:\n",
    "            self.node_count_seen = self.node_count_seen + node_count\n",
    "        average_loss = self.accumulated_training_loss / self.node_count_seen\n",
    "        return average_loss\n",
    "    \n",
    "    def train_investigate_F1(self, tune_model_folder, epoch_num=10, learning_rate=0.01, weight_decay = 0.01, mini_epoch_num = 1, output_period = 10, train_batch_num = 2):\n",
    "        \"\"\"\n",
    "            *** Periodically output the F1 score during training. After certain number of epochs ***\n",
    "            epoch_num:  number of total training epoch number\n",
    "            learning rate: learning rate during training\n",
    "            weight_decay:  decay coefficients for the regularization\n",
    "            mini_epoch_num:  number of epochs of repeating training after loading data on the GPU\n",
    "            output_period:  number of epochs after which output the F1 and accuray to investigate the model refining process\n",
    "        \"\"\"\n",
    "        os.makedirs(os.path.dirname(tune_model_folder), exist_ok=True)\n",
    "        model_snapshot_list = []\n",
    "        \n",
    "        # start the training investigation\n",
    "        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        self.model.train()   #   set into train mode, only effective for certain modules such as dropout and batchNorm\n",
    "        self.record_ave_training_loss = []\n",
    "        \n",
    "        self.time_train_load_data = 0\n",
    "        total_train_data_IO_time = 0\n",
    "        total_validation_processing_time = 0\n",
    "        \n",
    "        epoch_partition = epoch_num // mini_epoch_num\n",
    "        t0 = time.time()\n",
    "        train_clusters = list(range(train_batch_num))\n",
    "        for epoch_part in range(epoch_partition):\n",
    "#             For test purpose, we let the clusters to follow specific order\n",
    "            random.shuffle(train_clusters)\n",
    "            \n",
    "            for cluster in train_clusters:\n",
    "                # read in the train data from the pickle files\n",
    "                \n",
    "                # 1) readin the train node batch data\n",
    "                train_batch_file_name = self.data_folder + 'train_batch/batch_' + str(cluster)\n",
    "                \n",
    "                t2 = time.time()\n",
    "                with open(train_batch_file_name, \"rb\") as fp:\n",
    "                    minibatch_data_train = pickle.load(fp)\n",
    "                total_train_data_IO_time += (time.time() - t2) * 1000\n",
    "                \n",
    "                tr_train_nodes, tr_edges, tr_features, tr_target = minibatch_data_train\n",
    "                \n",
    "                # for each cluster, we load once and train it for multiple epochs:\n",
    "                t1 = time.time()\n",
    "                tr_train_nodes = tr_train_nodes.to(self.device)\n",
    "                tr_edges = tr_edges.to(self.device)\n",
    "                tr_features = tr_features.to(self.device)\n",
    "                tr_target = tr_target.to(self.device)\n",
    "                \n",
    "                self.time_train_load_data += (time.time() - t1) * 1000\n",
    "                \n",
    "                                \n",
    "                # train each batch for multiple epochs\n",
    "                for mini_epoch in range(mini_epoch_num):\n",
    "                    self.node_count_seen = 0\n",
    "                    self.accumulated_training_loss = 0\n",
    "\n",
    "                    # record the current overall epoch index:\n",
    "                    real_epoch_num = 1 + mini_epoch + mini_epoch_num * epoch_part # real_epoch_num starts from 0, therefore we add 1\n",
    "\n",
    "                    self.optimizer.zero_grad()\n",
    "                    batch_ave_loss, node_count = self.do_forward_pass(tr_train_nodes, tr_edges, tr_features, tr_target)\n",
    "                    batch_ave_loss.backward()\n",
    "                    self.optimizer.step()\n",
    "                    ave_loss = self.update_average_loss(batch_ave_loss, node_count)\n",
    "                    self.record_ave_training_loss.append(ave_loss)\n",
    "\n",
    "                    # at this point finish a single train duration: update the parameter and calcualte the loss function\n",
    "                    # periodically output the F1-score in the middle of the training process\n",
    "                    if real_epoch_num % output_period == 0:\n",
    "                        test_model = self.model.to(self.test_device)\n",
    "                        test_model.eval()   # set into test mode, only effective for certain modules such as dropout and batchNorm\n",
    "                        \n",
    "                        model_snapshot_list.append(real_epoch_num)\n",
    "                        tune_model_file_name = tune_model_folder + 'model_epoch_' + str(real_epoch_num)\n",
    "                        with open(tune_model_file_name, \"wb\") as fp:\n",
    "                            pickle.dump(test_model, fp)\n",
    "                        \n",
    "                        # this to func is change in-place: need to restore the state:\n",
    "                        self.model = self.model.to(self.device)\n",
    "                        self.model.train()\n",
    "            \n",
    "        # convert to ms\n",
    "        print('*** During training, reading all batch file I/O costed {0:.2f} ms ***'.format(total_train_data_IO_time) )\n",
    "        self.time_train_total = ((time.time() - t0) * 1000) - total_train_data_IO_time - total_validation_processing_time\n",
    "        \n",
    "        tune_model_list_name = tune_model_folder + 'model_index'\n",
    "        with open(tune_model_list_name, \"wb\") as fp:\n",
    "                            pickle.dump(model_snapshot_list, fp)\n",
    "        \n",
    "    \n",
    "    # iterate through epoch and also the clusters\n",
    "    def train(self, epoch_num=10, learning_rate=0.01, weight_decay = 0.01, mini_epoch_num = 1, train_batch_num = 2):\n",
    "        \"\"\"\n",
    "            *** Training a model. ***\n",
    "            epoch_num:  number of total training epoch number\n",
    "            learning rate: learning rate during training\n",
    "            weight_decay:  decay coefficients for the regularization\n",
    "            mini_epoch_num:  number of epochs of repeating training after loading data on the GPU\n",
    "        \"\"\"\n",
    "        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        self.model.train()\n",
    "        self.record_ave_training_loss = []\n",
    "        # record the data uploading to GPU time, and the data IO time for each train batch\n",
    "        self.time_train_load_data = 0\n",
    "        total_data_IO_time = 0\n",
    "        \n",
    "        epoch_partition = epoch_num // mini_epoch_num\n",
    "        t0 = time.time()\n",
    "        train_clusters = list(range(train_batch_num))\n",
    "        for epoch in range(epoch_partition):\n",
    "#             For test purpose, we let the clusters to follow specific order\n",
    "            random.shuffle(train_clusters)\n",
    "            \n",
    "            for cluster in train_clusters:\n",
    "                # read in the train data from the pickle files\n",
    "                batch_file_name = self.data_folder + 'train_batch/batch_' + str(cluster)\n",
    "                \n",
    "                t2 = time.time()\n",
    "                with open(batch_file_name, \"rb\") as fp:\n",
    "                    minibatch_data_train = pickle.load(fp)\n",
    "                total_data_IO_time += (time.time() - t2) * 1000\n",
    "                \n",
    "                tr_train_nodes, tr_edges, tr_features, tr_target = minibatch_data_train\n",
    "                \n",
    "                # for each cluster, we load once and train it for multiple epochs:\n",
    "                t1 = time.time()\n",
    "                tr_train_nodes = tr_train_nodes.to(self.device)\n",
    "                tr_edges = tr_edges.to(self.device)\n",
    "                tr_features = tr_features.to(self.device)\n",
    "                tr_target = tr_target.to(self.device)\n",
    "                \n",
    "                self.time_train_load_data += (time.time() - t1) * 1000\n",
    "                # train each batch for multiple epochs\n",
    "                for mini_epoch in range(mini_epoch_num):\n",
    "                    self.node_count_seen = 0\n",
    "                    self.accumulated_training_loss = 0\n",
    "                    \n",
    "                    self.optimizer.zero_grad()\n",
    "                    batch_ave_loss, node_count = self.do_forward_pass(tr_train_nodes, tr_edges, tr_features, tr_target)\n",
    "                    batch_ave_loss.backward()\n",
    "                    self.optimizer.step()\n",
    "                    ave_loss = self.update_average_loss(batch_ave_loss, node_count)\n",
    "                    # record training loss per epoch\n",
    "                    self.record_ave_training_loss.append(ave_loss)\n",
    "            \n",
    "        # convert to ms\n",
    "        print('*** During training, total IO data reading time for all batches costed {0:.2f} ms ***'.format(total_data_IO_time) )\n",
    "        self.time_train_total = ((time.time() - t0) * 1000) - total_data_IO_time\n",
    "    \n",
    "    \n",
    "    def distr_cpu_eval(self, batch_ids = [0]):\n",
    "        \"\"\"\n",
    "            Scoring the test and printing the F-1 score.\n",
    "        \"\"\"\n",
    "        self.test_device = torch.device(\"cpu\")\n",
    "        test_model = self.model.to(self.test_device)\n",
    "        test_model.eval()   # set into test mode, only effective for certain modules such as dropout and batchNorm\n",
    "        \n",
    "        eval_folder = self.data_folder + 'test_batch/'\n",
    "        \n",
    "        targets_list, predictions_list = [], []\n",
    "        \n",
    "        for batch_id in batch_ids:\n",
    "            batch_file_name = eval_folder + \"batch_\" + str(batch_id)\n",
    "\n",
    "            t2 = time.time()\n",
    "            with open(batch_file_name, \"rb\") as fp:\n",
    "                minibatch_data_test = pickle.load(fp)\n",
    "            read_time = (time.time() - t2) * 1000\n",
    "            print('*** During test for # {0} batch, reading batch file costed {1:.2f} ms ***'.format(batch_id, read_time) )\n",
    "\n",
    "            test_nodes, test_edges, test_features, test_target = minibatch_data_test\n",
    "\n",
    "            prediction = test_model(test_edges, test_features)\n",
    "            # select the testing nodes predictions and real labels\n",
    "            prediction = F.log_softmax(prediction, dim=1)\n",
    "            predictions = prediction[test_nodes].cpu().detach().numpy()\n",
    "\n",
    "            targets = test_target[test_nodes].cpu().detach().numpy()\n",
    "            # along axis:    axis == 1\n",
    "            predictions = predictions.argmax(1)  # return the indices of maximum probability \n",
    "            targets_list.append(targets)\n",
    "            predictions_list.append(predictions)\n",
    "        \n",
    "        all_targets = np.concatenate(targets_list, axis = 0)\n",
    "        all_predictions = np.concatenate(predictions_list, axis = 0)\n",
    "        \n",
    "        f1 = f1_score(all_targets, all_predictions, average=\"micro\")\n",
    "        accuracy = accuracy_score(all_targets, all_predictions)\n",
    "#         print(\"\\nTest F-1 score: {:.4f}\".format(score))\n",
    "        return (f1, accuracy)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Trivial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 2.],\n",
      "        [0., 3.],\n",
      "        [0., 4.],\n",
      "        [0., 5.],\n",
      "        [0., 6.],\n",
      "        [0., 7.],\n",
      "        [0., 8.],\n",
      "        [0., 9.]]) torch.Size([10, 2])\n"
     ]
    }
   ],
   "source": [
    "'''Trivial data'''\n",
    "edge_index = torch.tensor([[0, 1, 1, 3, 1, 2, 4, 2, 4, 6, 6, 7, 7, 9, 2, 5, 9, 8], \n",
    "                           [1, 0, 3, 1, 2, 1, 2, 4, 6, 4, 7, 6, 9, 7, 5, 2, 8, 9]])\n",
    "# features = torch.rand(10, 3)\n",
    "features = torch.tensor([[0, 0], [0, 1], [0, 2], [0, 3], [0, 4],  \n",
    "                           [0, 5], [0, 6], [0, 7], [0, 8], [0, 9]], dtype = torch.float)\n",
    "# label = torch.tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "label = torch.tensor([0, 1, 1, 0, 1, 1, 1, 0, 0, 0])\n",
    "print(features, features.shape)\n",
    "\n",
    "# set the tmp folder\n",
    "# tmp_folder = './tmp/'\n",
    "# check_folder_exist(tmp_folder)\n",
    "# os.makedirs(os.path.dirname(tmp_folder), exist_ok=True)\n",
    "# set the store clustering path\n",
    "tmp_folder = './res_save_batch/tmp/'\n",
    "check_folder_exist(tmp_folder)\n",
    "clustering_file_name = tmp_folder + 'check_clustering_machine.txt'\n",
    "os.makedirs(os.path.dirname(tmp_folder), exist_ok=True)\n",
    "info_folder = './res_save_batch/info/'\n",
    "check_folder_exist(info_folder)\n",
    "os.makedirs(os.path.dirname(info_folder), exist_ok=True)\n",
    "\n",
    "\n",
    "node_count = features.shape[0]\n",
    "clustering_machine = ClusteringMachine(edge_index, features, label)\n",
    "clustering_machine.split_whole_nodes_edges_then_cluster(0.4, 0.4, train_batch_num = 2, validation_batch_num = 2, test_batch_num = 2)\n",
    "output_GPU_memory_usage('Memory_use.txt', info_folder, comment ='after split: ')\n",
    "\n",
    "with open(clustering_file_name, \"wb\") as fp:\n",
    "    pickle.dump(clustering_machine, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### minibatch train nodes and batch validatioin "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Generate batch file for #   0 batch, writing the batch file costed 0.78 ms ***\n",
      "*** Generate batch file for #   1 batch, writing the batch file costed 0.67 ms ***\n",
      "*** Generate batch file for #   0 batch, writing the batch file costed 0.56 ms ***\n",
      "*** Generate batch file for #   1 batch, writing the batch file costed 0.63 ms ***\n",
      "*** Generate batch file for #   0 batch, writing the batch file costed 0.66 ms ***\n",
      "*** Generate batch file for #   1 batch, writing the batch file costed 0.45 ms ***\n",
      "*** During training, total IO data reading time for all batches costed 0.80 ms ***\n",
      "*** During test for # 0 batch, reading batch file costed 0.24 ms ***\n",
      "*** During test for # 1 batch, reading batch file costed 0.27 ms ***\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mini_batch_folder = './res_save_batch/mini_batch_files/'\n",
    "check_folder_exist(mini_batch_folder)\n",
    "\n",
    "with open(clustering_file_name, \"rb\") as fp:\n",
    "    clustering_machine = pickle.load(fp)\n",
    "\n",
    "# generate the batches for train and validation\n",
    "clustering_machine.mini_batch_train_clustering(mini_batch_folder, sample_num = 10, layer_num = 2, batch_range = (0, 2), info_folder = info_folder) # include number of layers\n",
    "output_GPU_memory_usage('Memory_use.txt', info_folder, comment ='after train clustering: ')\n",
    "\n",
    "clustering_machine.mini_batch_validation_clustering(mini_batch_folder, sample_num = -1, layer_num = 2, batch_range = (0, 2), info_folder = info_folder) # include number of layers\n",
    "clustering_machine.mini_batch_test_clustering(mini_batch_folder, sample_num = -1, layer_num = 2, batch_range = (0, 2), info_folder = info_folder) # include number of layers\n",
    "\n",
    "# clustering_machine.whole_test_clustering(mini_batch_folder, info_folder = info_folder)\n",
    "output_GPU_memory_usage('Memory_use.txt', info_folder, comment ='after test clustering:  ')\n",
    "\n",
    "# construct the batch trainer\n",
    "gcn_trainer_batch = ClusterGCNTrainer_mini_Train(mini_batch_folder, 2, 2, input_layers = [16], dropout=0.3)\n",
    "\n",
    "output_GPU_memory_usage('Memory_use.txt', info_folder, comment = 'after generating trainer:  ')\n",
    "\n",
    "gcn_trainer_batch.train(1, 0.0001, 0.1, train_batch_num = 2)\n",
    "\n",
    "output_GPU_memory_usage('Memory_use.txt', info_folder, comment = 'after training the batch:  ')\n",
    "\n",
    "# gcn_trainer_batch.batch_validate(valid_batch_num = 2)\n",
    "\n",
    "# gcn_trainer_batch.whole_cpu_test()\n",
    "gcn_trainer_batch.distr_cpu_eval(batch_ids = [0, 1])\n",
    "output_GPU_memory_usage('Memory_use.txt', info_folder, comment = 'after validating the batch:  ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### basic execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Execute the testing program '''\n",
    "def set_clustering_machine(data, dataset, intermediate_data_folder, validation_ratio = 0.05, test_ratio = 0.85, train_batch_num = 2, validation_batch_num = 2):\n",
    "    \"\"\"\n",
    "        Set the batch machine plus generate the training batches\n",
    "            1) data: the target dataset data\n",
    "            2) intermediate_data_folder: path to store the intermediate generated data\n",
    "            3) test_ratio, validation_ratio: data split ratio\n",
    "            4) train_batch_num :  batch number for train\n",
    "    \"\"\"\n",
    "    # set the tmp file for garbage tmp files, just collect the info:\n",
    "    tmp_folder = intermediate_data_folder + 'tmp/'\n",
    "    check_folder_exist(tmp_folder)\n",
    "    os.makedirs(os.path.dirname(tmp_folder), exist_ok=True)\n",
    "    \n",
    "    # Set the clustering information storing path\n",
    "    clustering_file_folder = intermediate_data_folder + 'clustering/'\n",
    "    data_info_file_folder = intermediate_data_folder + 'data_info/'\n",
    "    check_folder_exist(clustering_file_folder)  # if exist then delete\n",
    "    check_folder_exist(data_info_file_folder)  # if exist then delete\n",
    "    \n",
    "    clustering_file_name = clustering_file_folder + 'clustering_machine.txt'\n",
    "    data_info_file_name = data_info_file_folder + 'data_info_file.txt'\n",
    "    os.makedirs(os.path.dirname(clustering_file_folder), exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(data_info_file_folder), exist_ok=True)\n",
    "    \n",
    "\n",
    "    print('\\n' + '=' * 100)\n",
    "    print('Start to generate the clustering machine:')\n",
    "    t0 = time.time()\n",
    "    # if we use the random assignment of the code, then filtering out the isolated data may not be necessary\n",
    "    connect_edge_index, connect_features, connect_label = filter_out_isolate_normalize_feature(data.edge_index, data.x, data.y)\n",
    "    clustering_machine = ClusteringMachine(connect_edge_index, connect_features, connect_label, tmp_folder)\n",
    "    \n",
    "#     clustering_machine = ClusteringMachine(data.edge_index, data.x, data.y, tmp_folder)\n",
    "    batch_machine_create = time.time() - t0\n",
    "    print('Batch machine creation costs a total of {0:.4f} seconds!'.format(batch_machine_create))\n",
    "    \n",
    "    node_count = connect_features.shape[0]\n",
    "    feature_count = connect_features.shape[1]    # features all always in the columns\n",
    "    edge_count = connect_edge_index.shape[1]\n",
    "    print('\\nEdge number: ', edge_count, '\\nNode number: ', node_count, '\\nFeature number: ', feature_count) \n",
    "    \n",
    "    # at last output the information inside the folder:\n",
    "    print_dir_content_info(tmp_folder)\n",
    "    \n",
    "#     clustering_machine.split_cluster_nodes_edges(test_ratio, validation_ratio, partition_num = train_batch_num)\n",
    "    # mini-batch only: split to train test valid before clustering\n",
    "    print('Start to split data into train, test, validation:')\n",
    "    t1 = time.time()\n",
    "    clustering_machine.split_whole_nodes_edges_then_cluster(validation_ratio, test_ratio, train_batch_num = train_batch_num, validation_batch_num = validation_batch_num)\n",
    "    data_split_time = time.time() - t1\n",
    "    print('Data splitting costs a total of {0:.4f} seconds!'.format(data_split_time))\n",
    "    \n",
    "    print('Start to store the batch machine file:')\n",
    "    t3 = time.time()\n",
    "    with open(clustering_file_name, \"wb\") as fp:\n",
    "        pickle.dump(clustering_machine, fp)\n",
    "    \n",
    "    # data number we requred are the feature number and the classes, note after the filtering, the node number may be smaller by removing isolated nodes\n",
    "    data_info = (dataset.num_node_features, dataset.num_classes )\n",
    "    with open(data_info_file_name, \"wb\") as fp:\n",
    "        pickle.dump(data_info, fp)\n",
    "\n",
    "    batch_machine_store_time = time.time() - t3\n",
    "    print('Storing batch machine after training batches generation costs a total of {0:.4f} seconds!'.format(batch_machine_store_time))\n",
    "    print('\\n' + '=' * 100)\n",
    "    # output the memory usage information\n",
    "    info_GPU_memory_folder = intermediate_data_folder + 'info_GPU_memory/'\n",
    "    output_GPU_memory_usage('Memory_use_setting_cluster.txt', info_GPU_memory_folder, comment ='after setting clustering machine: ')\n",
    "    \n",
    "def set_clustering_machine_train_batch(intermediate_data_folder, sample_num = 10, \\\n",
    "                                      batch_range = (0, 1), info_folder = 'info_train_batch/', info_file = 'train_batch_size_info.csv'):\n",
    "    \"\"\"\n",
    "        Generate the train batches\n",
    "    \"\"\"\n",
    "    clustering_file_folder = intermediate_data_folder + 'clustering/'\n",
    "    clustering_file_name = clustering_file_folder + 'clustering_machine.txt'\n",
    "    print('\\n' + '=' * 100)\n",
    "    \n",
    "    t0 = time.time()\n",
    "    with open(clustering_file_name, \"rb\") as fp:\n",
    "        clustering_machine = pickle.load(fp)\n",
    "    batch_machine_read = time.time() - t0\n",
    "    print('Batch machine reading costs a total of {0:.4f} seconds!'.format(batch_machine_read))\n",
    "    \n",
    "#     check_folder_exist(intermediate_data_folder)  # if exist then delete\n",
    "    print('Start to generate the training batches:')\n",
    "    info_folder = intermediate_data_folder + info_folder\n",
    "    os.makedirs(os.path.dirname(info_folder), exist_ok=True)\n",
    "    t2 = time.time()\n",
    "    clustering_machine.mini_batch_train_clustering(intermediate_data_folder, sample_num = sample_num,\n",
    "                                                   batch_range = batch_range, info_folder = info_folder, info_file = info_file)\n",
    "    train_batch_production_time = time.time() - t2\n",
    "    print('Train batches production costs a total of {0:.4f} seconds!'.format(train_batch_production_time))\n",
    "    print_dir_content_info(intermediate_data_folder + 'train_batch/')\n",
    "    print('=' * 100)\n",
    "    # output the memory usage information\n",
    "    info_GPU_memory_folder = intermediate_data_folder + 'info_GPU_memory/'\n",
    "    output_GPU_memory_usage('GPU_cost_setting_train_batch.txt', info_GPU_memory_folder, comment ='after generating train batches: ')\n",
    "\n",
    "    \n",
    "def set_clustering_machine_eval_batch(intermediate_data_folder, sample_num = 10, layer_num = 2, eval_type = \"validation\",\n",
    "                                      batch_range = (0, 1), info_folder = 'info_eval_batch/'):\n",
    "    \"\"\"\n",
    "        Generate the test batches\n",
    "    \"\"\"\n",
    "    clustering_file_folder = intermediate_data_folder + 'clustering/'\n",
    "    clustering_file_name = clustering_file_folder + 'clustering_machine.txt'\n",
    "    print('\\n' + '=' * 100)\n",
    "    \n",
    "    t0 = time.time()\n",
    "    with open(clustering_file_name, \"rb\") as fp:\n",
    "        clustering_machine = pickle.load(fp)\n",
    "    batch_machine_read = time.time() - t0\n",
    "    print('Batch machine reading costs a total of {0:.4f} seconds!'.format(batch_machine_read))\n",
    "    \n",
    "    info_folder = intermediate_data_folder + info_folder\n",
    "    os.makedirs(os.path.dirname(info_folder), exist_ok=True)\n",
    "    \n",
    "    ### ============= eval batches =================\n",
    "    print('Start to generate the test batches:')\n",
    "    t2 = time.time()\n",
    "    if eval_type == \"validation\":\n",
    "        clustering_machine.mini_batch_validation_clustering(intermediate_data_folder, sample_num = sample_num, layer_num = layer_num,\n",
    "                                                   batch_range = batch_range, info_folder = info_folder, info_file = \"{}_batch_size_info.csv\".format(eval_type))\n",
    "    elif eval_type == \"test\":\n",
    "        clustering_machine.mini_batch_test_clustering(intermediate_data_folder, sample_num = sample_num, layer_num = layer_num,\n",
    "                                                   batch_range = batch_range, info_folder = info_folder, info_file = \"{}_batch_size_info.csv\".format(eval_type))\n",
    "    else:\n",
    "        assert(\"eval type can only be either validation or test\")\n",
    "    \n",
    "    eval_batch_production_time = time.time() - t2\n",
    "    print('{0} batches production costs a total of {1:.4f} seconds!'.format(eval_type, eval_batch_production_time))\n",
    "    print_dir_content_info(intermediate_data_folder + \"{}_batch/\".format(eval_type))\n",
    "    print('=' * 100)\n",
    "    # output the memory usage information\n",
    "    info_GPU_memory_folder = intermediate_data_folder + 'info_GPU_memory/'\n",
    "    output_GPU_memory_usage('GPU_cost_setting_{}_batch.txt'.format(eval_type), info_GPU_memory_folder, comment ='after generating {} batches: '.format(eval_type))\n",
    "    \n",
    "def set_clustering_machine_batch_whole_graph(intermediate_data_folder, info_folder = 'info_batch_whole_graph/'):\n",
    "    \"\"\"\n",
    "        Generate the test and validation for the whole graph\n",
    "    \"\"\"\n",
    "    clustering_file_folder = intermediate_data_folder + 'clustering/'\n",
    "    clustering_file_name = clustering_file_folder + 'clustering_machine.txt'\n",
    "    print('\\n' + '=' * 100)\n",
    "    \n",
    "    t0 = time.time()\n",
    "    with open(clustering_file_name, \"rb\") as fp:\n",
    "        clustering_machine = pickle.load(fp)\n",
    "    batch_machine_read = time.time() - t0\n",
    "    print('Batch machine reading costs a total of {0:.4f} seconds!'.format(batch_machine_read))\n",
    "    \n",
    "    print('Start to generate the test whole graph:')\n",
    "    info_folder = intermediate_data_folder + info_folder\n",
    "    os.makedirs(os.path.dirname(info_folder), exist_ok=True)\n",
    "    t1 = time.time()\n",
    "    \n",
    "    # create the test batch for the whole graph\n",
    "    clustering_machine.whole_test_clustering(intermediate_data_folder, info_folder = info_folder + 'test_batch/')\n",
    "    # create the validation batch for the whole graph\n",
    "    clustering_machine.whole_validation_clustering(intermediate_data_folder, info_folder = info_folder + 'validation_batch/')\n",
    "    \n",
    "    test_batch_production_time = time.time() - t1\n",
    "    print('Test batches production costs a total of {0:.4f} seconds!'.format(test_batch_production_time))\n",
    "    print_dir_content_info(intermediate_data_folder + 'test/')\n",
    "    print('=' * 100)\n",
    "    # output the memory usage information\n",
    "    info_GPU_memory_folder = intermediate_data_folder + 'info_GPU_memory/'\n",
    "    output_GPU_memory_usage('GPU_memory_cost_generate_validatoin_data.txt', info_GPU_memory_folder, comment ='after generating test for whole graph: ')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cluster_investigate_train(tune_model_folder, intermediate_data_folder, input_layer = [16, 16], epochs=300,\n",
    "                           dropout = 0.3, lr = 0.01, weight_decay = 0.01, mini_epoch_num = 5, improved = False, diag_lambda = -1,\n",
    "                              output_period = 10, train_part_num = 2):\n",
    "    \"\"\"\n",
    "        *** dynamically investigate the F1 score in the middle of the training after certain period ***\n",
    "        output: time during of training and time duration of time transferring onto GPU\n",
    "    \"\"\"\n",
    "    data_info_file_folder = intermediate_data_folder + 'data_info/'\n",
    "    data_info_file = data_info_file_folder + 'data_info_file.txt'\n",
    "    with open(data_info_file, \"rb\") as fp:\n",
    "        num_node_features, num_classes = pickle.load(fp)\n",
    "    \n",
    "    print('\\n' + '=' * 100)\n",
    "    print('Start generate the trainer:')\n",
    "    t0 = time.time()\n",
    "    gcn_trainer = ClusterGCNTrainer_mini_Train(intermediate_data_folder, num_node_features, num_classes, input_layers = input_layer, \n",
    "                                               dropout = dropout, improved = improved, diag_lambda = diag_lambda)\n",
    "    train_create = time.time() - t0\n",
    "    print('Trainer creation costs a total of {0:.4f} seconds!'.format(train_create))\n",
    "    \n",
    "    print('Start train the model:')\n",
    "    t1 = time.time()\n",
    "    \n",
    "    gcn_trainer.train_investigate_F1(tune_model_folder, epoch_num=epochs, learning_rate=lr, weight_decay=weight_decay, mini_epoch_num = mini_epoch_num, \\\n",
    "                                                            output_period = output_period, train_batch_num = train_part_num)\n",
    "    train_period = time.time() - t1\n",
    "    print('Training costs a total of {0:.4f} seconds!'.format(train_period))\n",
    "    print('-' * 80)\n",
    "    info_GPU_memory_folder = intermediate_data_folder + 'info_GPU_memory/'\n",
    "    output_GPU_memory_usage('Memory_use_train_batch_investigate.txt', info_GPU_memory_folder, comment ='after train batch investigate: ')\n",
    "    \n",
    "    return gcn_trainer.time_train_total, gcn_trainer.time_train_load_data\n",
    "\n",
    "def Cluster_investigate_evaluation(validation_model, mini_batch_folder, eval_target = 'validation_batch/validation_batch_whole'):\n",
    "    \"\"\"\n",
    "        Evaluation func: using whole dataset\n",
    "            Use the validation whole batch for validation\n",
    "            Use the test whole batch for test\n",
    "    \"\"\"\n",
    "    batch_file_name = mini_batch_folder + eval_target\n",
    "\n",
    "    with open(batch_file_name, \"rb\") as fp:\n",
    "        minibatch_data_test = pickle.load(fp)\n",
    "\n",
    "    eval_nodes, eval_edges, eval_features, eval_target = minibatch_data_test\n",
    "\n",
    "    prediction = validation_model(eval_edges, eval_features)\n",
    "    # select the testing nodes predictions and real labels\n",
    "    prediction = F.log_softmax(prediction, dim=1)\n",
    "    predictions = prediction[eval_nodes].cpu().detach().numpy()\n",
    "\n",
    "    targets = eval_target[eval_nodes].cpu().detach().numpy()\n",
    "\n",
    "    # along axis:    axis == 1\n",
    "    predictions = predictions.argmax(1)  # return the indices of maximum probability \n",
    "\n",
    "    f1 = f1_score(targets, predictions, average=\"micro\")\n",
    "    accuracy = accuracy_score(targets, predictions)\n",
    "    \n",
    "    return f1, accuracy\n",
    "\n",
    "def Cluster_investigate_evaluation_scatter_distr(eval_model, working_folder, distr_eval_folder, eval_type = \"validation\", batch_ids = [0]):\n",
    "    \"\"\"\n",
    "        Distributed version of evaluation func:\n",
    "            Use the validation batches inside validation_batch_distr for validation\n",
    "            Use the test batches inside test_batch_distr for testing\n",
    "    \"\"\"\n",
    "    \n",
    "    eval_batch_folder = working_folder + '{}_batch/'.format(eval_type)\n",
    "    os.makedirs(os.path.dirname(distr_eval_folder), exist_ok=True)\n",
    "    \n",
    "    for batch_id in batch_ids:\n",
    "        batch_file_name = eval_batch_folder + \"batch_\" + str(batch_id)\n",
    "\n",
    "        with open(batch_file_name, \"rb\") as fp:\n",
    "            minibatch_data_eval = pickle.load(fp)\n",
    "\n",
    "        eval_nodes, eval_edges, eval_features, eval_target = minibatch_data_eval\n",
    "\n",
    "        prediction = eval_model(eval_edges, eval_features)\n",
    "        # select the testing nodes predictions and real labels\n",
    "        prediction = F.log_softmax(prediction, dim=1)\n",
    "        predictions = prediction[eval_nodes].cpu().detach().numpy()\n",
    "\n",
    "        targets = eval_target[eval_nodes].cpu().detach().numpy()\n",
    "\n",
    "        # along axis:    axis == 1\n",
    "        predictions = predictions.argmax(1)  # return the indices of maximum probability \n",
    "    \n",
    "        eval_distr_val_file = \"{}eval_batch_{}\".format(distr_eval_folder, batch_id)\n",
    "        with open(eval_distr_val_file, \"wb\") as fp:\n",
    "            pickle.dump((targets, predictions), fp)\n",
    "\n",
    "\n",
    "def Cluster_investigate_evaluation_aggr_distr(working_folder, distr_eval_folder, eval_type = \"validation\", batch_ids = [0]):\n",
    "    \n",
    "    targets_list, predictions_list = [], []\n",
    "    \n",
    "    for batch_id in batch_ids:\n",
    "        eval_distr_val_file = \"{}eval_batch_{}\".format(distr_eval_folder, batch_id)\n",
    "        \n",
    "        with open(eval_distr_val_file, \"rb\") as fp:\n",
    "            targets, predictions = pickle.load(fp)\n",
    "        \n",
    "        targets_list.append(targets)\n",
    "        predictions_list.append(predictions)\n",
    "    \n",
    "    all_targets = np.concatenate(targets_list, axis = 0)\n",
    "    all_predictions = np.concatenate(predictions_list, axis = 0)\n",
    "    \n",
    "    f1 = f1_score(all_targets, all_predictions, average=\"micro\")\n",
    "    accuracy = accuracy_score(all_targets, all_predictions)\n",
    "    \n",
    "    return f1, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Investigate the F1 score during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### ======== Investigate F1-score offline\n",
    "def execute_investigate_train(mini_batch_folder, tune_param_name, tune_val_label, tune_val, trainer_id = 0, input_layer = [32], epoch_num = 300,\n",
    "                        dropout = 0.3, lr = 0.0001, weight_decay = 0.01, mini_epoch_num = 20, improved = False, diag_lambda = -1,\n",
    "                        output_period = 10, train_part_num = 2):\n",
    "    \"\"\"\n",
    "        return all validation-F1 for all four models\n",
    "    \"\"\"\n",
    "    # store the resulting data on the disk\n",
    "    tune_model_folder = mini_batch_folder + 'model_snapshot/tune_' + tune_param_name + '_' + str(tune_val_label) + '/model_trainer_' + str(trainer_id) + '/'\n",
    "    os.makedirs(os.path.dirname(tune_model_folder), exist_ok=True)\n",
    "    \n",
    "    time_res = Cluster_investigate_train(tune_model_folder, mini_batch_folder, input_layer = input_layer, epochs=epoch_num,\n",
    "                                          dropout = dropout, lr = tune_val, weight_decay = weight_decay, mini_epoch_num = mini_epoch_num, improved = improved, diag_lambda = diag_lambda,\n",
    "                                          output_period = output_period, train_part_num = train_part_num)\n",
    "    \n",
    "    time_res_folder = mini_batch_folder + 'validation_res/tune_' + tune_param_name + '_' + str(tune_val_label) + '/validation_trainer_' + str(trainer_id) + '/'\n",
    "    os.makedirs(os.path.dirname(time_res_folder), exist_ok=True)\n",
    "    \n",
    "    time_res_file_name = time_res_folder + 'time_total_and_load'\n",
    "    with open(time_res_file_name, \"wb\") as fp:\n",
    "        pickle.dump(time_res, fp)\n",
    "\n",
    "\n",
    "def execute_investigate_validation(mini_batch_folder, tune_param_name, tune_val_label, tune_val, trainer_id = 0, model_epoch = [1]):\n",
    "    \"\"\"\n",
    "        return all validation-F1 for all four models\n",
    "    \"\"\"\n",
    "    # store the resulting data on the disk\n",
    "    tune_model_folder = mini_batch_folder + 'model_snapshot/tune_' + tune_param_name + '_' + str(tune_val_label) + '/model_trainer_' + str(trainer_id) + '/'\n",
    "    \n",
    "    res_model_folder = mini_batch_folder + 'validation_res/tune_' + tune_param_name + '_' + str(tune_val_label) + '/validation_trainer_' + str(trainer_id) + '/'\n",
    "    os.makedirs(os.path.dirname(res_model_folder), exist_ok=True)\n",
    "    \n",
    "    for validation_epoch in model_epoch:\n",
    "        tune_model_file_name = tune_model_folder + 'model_epoch_' + str(validation_epoch)\n",
    "        with open(tune_model_file_name, \"rb\") as fp:\n",
    "            validation_model = pickle.load(fp)\n",
    "        \n",
    "        res = Cluster_investigate_evaluation(validation_model, mini_batch_folder, eval_target = 'validation_batch/validation_batch_whole')\n",
    "        # saved the validation results, later can be used in the testing step to choose the trained_model_epoch best validation-F1 score to generate the Test-F1 score\n",
    "        res_model_file_name = res_model_folder + 'model_epoch_' + str(validation_epoch)\n",
    "        with open(res_model_file_name, \"wb\") as fp:\n",
    "            pickle.dump(res, fp)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [hello 5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = \"    [hello {}]\\n\".format(5)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distributed version of the validation F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_investigate_validation_scatter_distr(working_folder, tune_param_name, tune_val_label, tune_val, trainer_id = 0, \n",
    "                                                 model_epoch = [1], batch_ids = [0]):\n",
    "    \"\"\"\n",
    "        generate the partial evaluation results from batches of subgraphs\n",
    "    \"\"\"\n",
    "    tune_model_folder = working_folder + 'model_snapshot/tune_' + tune_param_name + '_' + str(tune_val_label) + '/model_trainer_' + str(trainer_id) + '/'\n",
    "    record_eval_folder = \"{}validation_distr_eval_res/tune_{}_{}/trainer_{}/\".format(working_folder, tune_param_name, tune_val_label, trainer_id)\n",
    "    for validation_epoch in model_epoch:\n",
    "        tune_model_file_name = tune_model_folder + 'model_epoch_' + str(validation_epoch)\n",
    "        with open(tune_model_file_name, \"rb\") as fp:\n",
    "            validation_model = pickle.load(fp)\n",
    "        \n",
    "        distr_eval_folder = \"{}model_epoch_{}/\".format(record_eval_folder, validation_epoch)\n",
    "        \n",
    "        os.makedirs(os.path.dirname(distr_eval_folder), exist_ok=True)\n",
    "        # generate the evaluation for each validation batch cut from the whole graph\n",
    "        Cluster_investigate_evaluation_scatter_distr(validation_model, working_folder, distr_eval_folder, eval_type = \"validation\", batch_ids = batch_ids)\n",
    "        \n",
    "def execute_investigate_validation_aggr_distr(working_folder, tune_param_name, tune_val_label, tune_val, trainer_id = 0, \n",
    "                                              model_epoch = [1], batch_ids = [0]):\n",
    "    \"\"\"\n",
    "        Aggregate the evaluation results from the validation of graph batches\n",
    "    \"\"\"\n",
    "    res_model_folder = \"{}validation_res/tune_{}_{}/validation_trainer_{}/\".format(working_folder, tune_param_name, tune_val_label, trainer_id)\n",
    "    os.makedirs(os.path.dirname(res_model_folder), exist_ok=True)\n",
    "    \n",
    "    record_eval_folder = \"{}validation_distr_eval_res/tune_{}_{}/trainer_{}/\".format(working_folder, tune_param_name, tune_val_label, trainer_id)\n",
    "    \n",
    "    for validation_epoch in model_epoch:\n",
    "        distr_eval_folder = \"{}model_epoch_{}/\".format(record_eval_folder, validation_epoch)\n",
    "        res = Cluster_investigate_evaluation_aggr_distr(working_folder, distr_eval_folder, eval_type = \"validation\", batch_ids = batch_ids)\n",
    "        # saved the validation results, later can be used in the testing step to choose the trained_model_epoch best validation-F1 score to generate the Test-F1 score\n",
    "        res_model_file_name = res_model_folder + 'model_epoch_' + str(validation_epoch)\n",
    "        with open(res_model_file_name, \"wb\") as fp:\n",
    "            pickle.dump(res, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate the test-F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ======== Investigate test-score offline\n",
    "def execute_test_tuning(res_folder, mini_batch_folder, snapshot_epoch_list,\n",
    "                        tune_param_name, tune_val_label, tune_val, trainer_id = 0):\n",
    "    \"\"\"\n",
    "        1) After the validation, select the epoch with the best validation score\n",
    "        2) use the trained model at the selected optimal epoch of validation\n",
    "        3) perform the evaluate func for the test data\n",
    "    \"\"\"\n",
    "    # start to search for the trained model epoch with the best validation f1 socre\n",
    "    f1mic_best, ep_best = 0, -1\n",
    "    \n",
    "    validation_res_folder = mini_batch_folder + 'validation_res/tune_' + tune_param_name + '_' + str(tune_val_label) + '/validation_trainer_' + str(trainer_id) + '/'\n",
    "    \n",
    "    for validation_epoch in snapshot_epoch_list:\n",
    "        validation_res_file_name = validation_res_folder + 'model_epoch_' + str(validation_epoch)\n",
    "        with open(validation_res_file_name, \"rb\") as fp:\n",
    "            f1mic_val, accuracy_val = pickle.load(fp)\n",
    "            \n",
    "        if f1mic_val > f1mic_best:\n",
    "            f1mic_best, ep_best = f1mic_val, validation_epoch\n",
    "        \n",
    "    # use the selected model to perform on the test\n",
    "    tune_model_folder = mini_batch_folder + 'model_snapshot/tune_' + tune_param_name + '_' + str(tune_val_label) + \\\n",
    "                                    '/model_trainer_' + str(trainer_id) + '/'\n",
    "    \n",
    "    # save the selected best saved snapshot\n",
    "    best_model_file = tune_model_folder + 'model_epoch_' + str(ep_best)\n",
    "    shutil.copy2(best_model_file, tune_model_folder + 'best_saved_snapshot.pkl')\n",
    "    \n",
    "    with open(best_model_file, \"rb\") as fp:\n",
    "        best_model = pickle.load(fp)\n",
    "    \n",
    "    f1, accuracy = Cluster_investigate_evaluation(best_model, mini_batch_folder, eval_target = 'test/test_batch_whole')\n",
    "    \n",
    "    # store the resulting data on the disk\n",
    "    test_res_folder = mini_batch_folder + 'test_res/tune_' + tune_param_name + '_' + str(tune_val_label) + '/'\n",
    "    os.makedirs(os.path.dirname(test_res_folder), exist_ok=True)\n",
    "    test_res_file = test_res_folder + 'res_trainer_' + str(trainer_id)\n",
    "    \n",
    "    with open(test_res_file, \"wb\") as fp:\n",
    "        pickle.dump((f1, accuracy), fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distributed version of test F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_test_tuning_scatter_distr(working_folder, snapshot_epoch_list, tune_param_name, tune_val_label, tune_val, \n",
    "                                      trainer_id = 0, batch_ids = [0]):\n",
    "    \"\"\"\n",
    "        1) After the validation, select the epoch with the best validation score\n",
    "        2) use the trained model at the selected optimal epoch of validation\n",
    "        3) perform the evaluate func for the test data\n",
    "    \"\"\"\n",
    "    # start to search for the trained model epoch with the best validation f1 socre\n",
    "    f1mic_best, ep_best = 0, -1\n",
    "    \n",
    "    validation_res_folder = working_folder + 'validation_res/tune_' + tune_param_name + '_' + str(tune_val_label) + '/validation_trainer_' + str(trainer_id) + '/'\n",
    "    \n",
    "    for validation_epoch in snapshot_epoch_list:\n",
    "        validation_res_file_name = validation_res_folder + 'model_epoch_' + str(validation_epoch)\n",
    "        with open(validation_res_file_name, \"rb\") as fp:\n",
    "            f1mic_val, accuracy_val = pickle.load(fp)\n",
    "            \n",
    "        if f1mic_val > f1mic_best:\n",
    "            f1mic_best, ep_best = f1mic_val, validation_epoch\n",
    "        \n",
    "    # use the selected model to perform on the test\n",
    "    tune_model_folder = working_folder + 'model_snapshot/tune_' + tune_param_name + '_' + str(tune_val_label) + \\\n",
    "                                    '/model_trainer_' + str(trainer_id) + '/'\n",
    "    \n",
    "    # save the selected best saved snapshot\n",
    "    best_model_file = tune_model_folder + 'model_epoch_' + str(ep_best)\n",
    "    shutil.copy2(best_model_file, tune_model_folder + 'best_saved_snapshot.pkl')\n",
    "    \n",
    "    with open(best_model_file, \"rb\") as fp:\n",
    "        best_model = pickle.load(fp)\n",
    "    \n",
    "    ### ============== After choosing the best model from validation\n",
    "    distr_test_folder = \"{}test_distr_eval_res/tune_{}_{}/trainer_{}/\".format(working_folder, tune_param_name, tune_val_label, trainer_id)\n",
    "    os.makedirs(os.path.dirname(distr_test_folder), exist_ok=True)\n",
    "    \n",
    "    Cluster_investigate_evaluation_scatter_distr(best_model, working_folder, distr_test_folder, eval_type = \"test\", batch_ids = batch_ids)\n",
    "\n",
    "\n",
    "def execute_test_tuning_aggr_distr(working_folder, tune_param_name, tune_val_label, tune_val, \n",
    "                                   trainer_id = 0, batch_ids = [0]):\n",
    "    distr_test_folder = \"{}test_distr_eval_res/tune_{}_{}/trainer_{}/\".format(working_folder, tune_param_name, tune_val_label, trainer_id)\n",
    "    \n",
    "    f1, accuracy = Cluster_investigate_evaluation_aggr_distr(working_folder, distr_test_folder, eval_type = \"test\", batch_ids = batch_ids)\n",
    "    \n",
    "    # store the resulting data on the disk\n",
    "    test_res_folder = working_folder + 'test_res/tune_' + tune_param_name + '_' + str(tune_val_label) + '/'\n",
    "    os.makedirs(os.path.dirname(test_res_folder), exist_ok=True)\n",
    "    test_res_file = test_res_folder + 'res_trainer_' + str(trainer_id)\n",
    "    \n",
    "    with open(test_res_file, \"wb\") as fp:\n",
    "        pickle.dump((f1, accuracy), fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate steps of HPC execution for one single task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step0_generate_clustering_machine(data, dataset, intermediate_data_path, train_batch_num = 2, validation_batch_num = 2, \\\n",
    "                                      validation_ratio = 0.05, test_ratio = 0.85):            \n",
    "    print('Start running for train batch num: ' + str(train_batch_num) )\n",
    "\n",
    "    # set the basic settings for the future batches generation\n",
    "    set_clustering_machine(data, dataset, intermediate_data_path, validation_ratio = validation_ratio, test_ratio = test_ratio, \\\n",
    "                           train_batch_num = train_batch_num, validation_batch_num = validation_batch_num)\n",
    "\n",
    "def step1_generate_train_batch(intermediate_data_path, sample_num = 10, \\\n",
    "                               batch_range = (0, 1), info_folder = 'info/'):\n",
    "    \"\"\"\n",
    "        generate all the batches for train process: train\n",
    "    \"\"\"\n",
    "    # set the save path\n",
    "    print('Start running for train batch num: ' + str(batch_range) )\n",
    "    info_file = 'train_batch_size_info_{}.csv'.format(str(batch_range))\n",
    "\n",
    "    # generate the train batches\n",
    "    set_clustering_machine_train_batch(intermediate_data_path, sample_num = sample_num, \\\n",
    "                                       batch_range = batch_range, info_folder = info_folder, info_file = info_file)\n",
    "\n",
    "    \n",
    "def step2_generate_batch_whole_graph(intermediate_data_path, info_folder = 'info/'): \n",
    "    \"\"\"\n",
    "        generate all the tensors from the  whole graph for validattion and test\n",
    "    \"\"\"\n",
    "    set_clustering_machine_batch_whole_graph(intermediate_data_path, info_folder = info_folder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate steps for investigation training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step31_run_investigation_train_batch(intermediate_data_path, tune_param_name, tune_val_label, tune_val, train_batch_num, net_layer_num, GCN_layer,\n",
    "                    trainer_id = 0, dropout = 0.1, lr = 0.0001, weight_decay = 0.1, mini_epoch_num = 20, improved = False, diag_lambda = -1, \n",
    "                                         epoch_num = 400, output_period = 40):            \n",
    "    # start to tune the model, and investigate the performance in the middle\n",
    "    print('Start running investigation traing for train batch num: ' + str(train_batch_num) + ' for_trainer_id_' + str(trainer_id))\n",
    "    execute_investigate_train(intermediate_data_path, tune_param_name, tune_val_label, tune_val, trainer_id = trainer_id, input_layer = GCN_layer, epoch_num = epoch_num,\n",
    "                    dropout = dropout, lr = lr, weight_decay = weight_decay, mini_epoch_num = mini_epoch_num, improved = improved, diag_lambda = diag_lambda,\n",
    "                              output_period = output_period, train_part_num = train_batch_num)\n",
    "\n",
    "def step41_run_investigation_validation_whole(intermediate_data_path, tune_param_name, tune_val_label, tune_val, \n",
    "                                              trainer_id = 0, model_epoch = [1]):\n",
    "    \n",
    "    print('Start running investigation validation for_trainer_id_{}'.format(trainer_id))\n",
    "    execute_investigate_validation(intermediate_data_path, tune_param_name, tune_val_label, tune_val, trainer_id = trainer_id, model_epoch = model_epoch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step50_run_tune_summarize_whole(data_name, image_data_path, intermediate_data_path, tune_param_name, tune_val_label_list, tune_val_list, \\\n",
    "                                    train_batch_num, net_layer_num, trainer_list): \n",
    "    \n",
    "    print('Start running training for partition num: ' + str(train_batch_num))\n",
    "    # set the batch for test and train\n",
    "    img_path = image_data_path + 'test_res/'  # further subfolder for different task\n",
    "\n",
    "    # start to summarize the results into images for output\n",
    "\n",
    "    test_f1, test_accuracy, time_total_train, time_data_load = summarize_tuning_res(img_path, intermediate_data_path, tune_param_name, tune_val_label_list, tune_val_list, trainer_list)\n",
    "\n",
    "    generate_tuning_raw_data_table(test_accuracy, img_path, 'test_acc.csv', tune_param_name)\n",
    "    test_accuracy_file = store_data_multi_tuning(tune_val_list, test_accuracy, data_name, img_path, 'accuracy_cluster_num_' + str(train_batch_num) + 'net_layer_num_' + str(net_layer_num))\n",
    "    draw_data_multi_tests(test_accuracy_file, data_name, 'test_cluster_num_' + str(train_batch_num) + 'net_layer_num_' + str(net_layer_num), 'epochs_per_batch', 'Accuracy')\n",
    "\n",
    "    generate_tuning_raw_data_table(test_f1, img_path, 'test_f1.csv', tune_param_name)\n",
    "    test_f1_file = store_data_multi_tuning(tune_val_list, test_f1, data_name, img_path, 'test_cluster_num_' + str(train_batch_num) + 'net_layer_num_' + str(net_layer_num))\n",
    "    draw_data_multi_tests(test_f1_file, data_name, 'vali_cluster_num_' + str(train_batch_num) + 'net_layer_num_' + str(net_layer_num), 'epochs_per_batch', 'F1 score')\n",
    "\n",
    "    generate_tuning_raw_data_table(time_total_train, img_path, 'time_train_total.csv', tune_param_name)\n",
    "    time_train_file = store_data_multi_tuning(tune_val_list, time_total_train, data_name, img_path, 'train_time_cluster_num_' + str(train_batch_num) + 'net_layer_num_' + str(net_layer_num))\n",
    "    draw_data_multi_tests(time_train_file, data_name, 'train_time_cluster_num_' + str(train_batch_num) + 'net_layer_num_' + str(net_layer_num), 'epochs_per_batch', 'Train Time (ms)')\n",
    "\n",
    "    generate_tuning_raw_data_table(time_data_load, img_path, 'time_load_data.csv', tune_param_name)\n",
    "    time_load_file = store_data_multi_tuning(tune_val_list, time_data_load, data_name, img_path, 'load_time_cluster_num_' + str(train_batch_num) + 'net_layer_num_' + str(net_layer_num))\n",
    "    draw_data_multi_tests(time_load_file, data_name, 'load_time_cluster_num_' + str(train_batch_num) + 'net_layer_num_' + str(net_layer_num), 'epochs_per_batch', 'Load Time (ms)')\n",
    " \n",
    "def step51_run_investigation_summarize_whole(data_name, image_data_path, intermediate_data_path, tune_param_name, tune_val_label, tune_val, \\\n",
    "                                    train_batch_num, net_layer_num, trainer_list, model_epoch_list): \n",
    "    \"\"\"\n",
    "        Train investigation post-processing\n",
    "        Train-validation at the same time\n",
    "    \"\"\"\n",
    "    print('Start summarizing for train batch num: ' + str(train_batch_num) )\n",
    "    # set the batch for validation and train\n",
    "    img_path = image_data_path + 'validation_res/tune_' + tune_param_name + '_' + str(tune_val_label) + '/'\n",
    "    # start to summarize the results into images for output\n",
    "    \n",
    "    validation_res_folder = intermediate_data_path + 'validation_res/tune_' + tune_param_name + '_' + str(tune_val_label) + '/'\n",
    "    Train_peroid_f1, Train_peroid_accuracy = summarize_investigation_distr_res(validation_res_folder, trainer_list, model_epoch_list)\n",
    "\n",
    "    Train_peroid_f1_file = store_data_each_trainer_investigate(Train_peroid_f1, data_name, 'F1_score', img_path, 'validation')\n",
    "    draw_data_validation_F1_trainer(Train_peroid_f1_file, data_name, 'validation', 'epoch number', 'F1 score')\n",
    "    \n",
    "    Train_peroid_accuracy_file = store_data_each_trainer_investigate(Train_peroid_accuracy, data_name, 'Accuracy', img_path, 'validation')\n",
    "    draw_data_validation_F1_trainer(Train_peroid_accuracy_file, data_name, 'validation', 'epoch number', 'Accuracy')\n",
    "    \n",
    "def check_train_loss_converge(image_path, mini_batch_folder, subfolder, data_name, trainer_id = 0):\n",
    "    # mini-batch, but valid also in batches\n",
    "    Trainer_folder = mini_batch_folder + subfolder\n",
    "    trainer_file_name = Trainer_folder + 'GCN_trainer_' + str(trainer_id)\n",
    "    \n",
    "    print('Start to read the Sage trainer model (parameters: weights, bias):')\n",
    "    t1 = time.time()\n",
    "    with open(trainer_file_name, \"rb\") as fp:\n",
    "        gcn_trainer = pickle.load(fp)\n",
    "    read_trainer_time = (time.time() - t1) * 1000\n",
    "    print('Reading the trainer costs a total of {0:.4f} seconds!'.format(read_trainer_time))\n",
    "    \n",
    "    # store the resulting data on the disk\n",
    "    train_loss_path = image_path + 'trainer_loss/' + subfolder\n",
    "    os.makedirs(os.path.dirname(train_loss_path), exist_ok=True)\n",
    "    loss_res_file = train_loss_path + 'loss_trainer_' + str(trainer_id)\n",
    "    \n",
    "    draw_Cluster_train_valid_batch = draw_trainer_info(data_name, gcn_trainer)\n",
    "    draw_Cluster_train_valid_batch.draw_ave_loss_per_node(loss_res_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use data from pytorch geometric datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_data_root = '/home/xiangli/projects/tmpdata/GCN/Geometric/'\n",
    "test_folder_name = 'GraphSage/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cora dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "data_name = 'Cora'\n",
    "dataset = Planetoid(root = local_data_root + 'Planetoid/Cora', name=data_name)\n",
    "data = dataset[0]\n",
    "\n",
    "intermediate_data_folder = './Step41_Sage_multiclass_distr_validation/'\n",
    "image_data_path = intermediate_data_folder + 'results/' + data_name + '/' + test_folder_name\n",
    "\n",
    "# this is the parts we divide the graph\n",
    "train_batch_num = 8\n",
    "validation_batch_num = 2\n",
    "test_batch_num = 2\n",
    "\n",
    "GCN_layer = [32]\n",
    "net_layer_num = len(GCN_layer) + 1\n",
    "# for non-optimization: hop_layer_num == net_layer_num\n",
    "hop_layer_num = net_layer_num\n",
    "\n",
    "tune_param_name = 'learning_rate'\n",
    "tune_val_label_list = [3, 4]\n",
    "tune_val_list = [10**(-label) for label in tune_val_label_list]\n",
    "\n",
    "trainer_list = list(range(3))\n",
    "res_model_epoch_list = list(range(10, 101, 10))\n",
    "validation_batch_ids = list(range(validation_batch_num))\n",
    "test_batch_ids = list(range(test_batch_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start running for train batch num: 8\n",
      "\n",
      "====================================================================================================\n",
      "Start to generate the clustering machine:\n",
      "No isolated nodes number is found \n",
      "Label shape is: torch.Size([2708])\n",
      "Batch machine creation costs a total of 0.1481 seconds!\n",
      "\n",
      "Edge number:  10556 \n",
      "Node number:  2708 \n",
      "Feature number:  1433\n",
      "\n",
      " Information about the content of ./Step41_Sage_distr_validation/tmp/\n",
      "\n",
      "Start to split data into train, test, validation:\n",
      "Data splitting costs a total of 0.0022 seconds!\n",
      "Start to store the batch machine file:\n",
      "Storing batch machine after training batches generation costs a total of 0.0094 seconds!\n",
      "\n",
      "====================================================================================================\n",
      "Start running for train batch num: (0, 8)\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0072 seconds!\n",
      "Start to generate the training batches:\n",
      "*** Generate batch file for #   0 batch, writing the batch file costed 2.62 ms ***\n",
      "*** Generate batch file for #   1 batch, writing the batch file costed 2.94 ms ***\n",
      "*** Generate batch file for #   2 batch, writing the batch file costed 2.56 ms ***\n",
      "*** Generate batch file for #   3 batch, writing the batch file costed 2.78 ms ***\n",
      "*** Generate batch file for #   4 batch, writing the batch file costed 2.62 ms ***\n",
      "*** Generate batch file for #   5 batch, writing the batch file costed 3.56 ms ***\n",
      "*** Generate batch file for #   6 batch, writing the batch file costed 2.71 ms ***\n",
      "*** Generate batch file for #   7 batch, writing the batch file costed 2.58 ms ***\n",
      "Train batches production costs a total of 0.0775 seconds!\n",
      "\n",
      " Information about the content of ./Step41_Sage_distr_validation/train_batch/\n",
      "File name: [ batch_3 ]; with size: 4324.0869140625 KB\n",
      "File name: [ batch_2 ]; with size: 4722.0009765625 KB\n",
      "File name: [ batch_4 ]; with size: 4529.6962890625 KB\n",
      "File name: [ batch_7 ]; with size: 4408.6923828125 KB\n",
      "File name: [ batch_1 ]; with size: 4682.8876953125 KB\n",
      "File name: [ batch_6 ]; with size: 4500.5751953125 KB\n",
      "File name: [ batch_0 ]; with size: 4631.0947265625 KB\n",
      "File name: [ batch_5 ]; with size: 4579.8955078125 KB\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0079 seconds!\n",
      "Start to generate the test batches:\n",
      "*** Generate batch file for #   0 batch, writing the batch file costed 0.78 ms ***\n",
      "*** Generate batch file for #   1 batch, writing the batch file costed 0.70 ms ***\n",
      "validation batches production costs a total of 0.0060 seconds!\n",
      "\n",
      " Information about the content of ./Step41_Sage_distr_validation/validation_batch/\n",
      "File name: [ batch_1 ]; with size: 933.103515625 KB\n",
      "File name: [ batch_0 ]; with size: 916.037109375 KB\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0074 seconds!\n",
      "Start to generate the test batches:\n",
      "*** Generate batch file for #   0 batch, writing the batch file costed 3.76 ms ***\n",
      "*** Generate batch file for #   1 batch, writing the batch file costed 3.87 ms ***\n",
      "test batches production costs a total of 0.0299 seconds!\n",
      "\n",
      " Information about the content of ./Step41_Sage_distr_validation/test_batch/\n",
      "File name: [ batch_1 ]; with size: 5728.5419921875 KB\n",
      "File name: [ batch_0 ]; with size: 5629.4248046875 KB\n",
      "\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# need to make sure the train and validation has the same number of batches\n",
    "step0_generate_clustering_machine(data, dataset, intermediate_data_folder, train_batch_num = train_batch_num, validation_batch_num = train_batch_num,\n",
    "                                  validation_ratio = 0.12, test_ratio = 0.22)\n",
    "\n",
    "step1_generate_train_batch(intermediate_data_folder, sample_num = 10,\n",
    "                           batch_range = (0, train_batch_num), \n",
    "                           info_folder = 'info_train_batch/' )\n",
    "\n",
    "# step2_generate_batch_whole_graph(intermediate_data_folder, info_folder = 'info_whole/')\n",
    "\n",
    "set_clustering_machine_eval_batch(intermediate_data_folder, sample_num = 10, layer_num = hop_layer_num, eval_type = \"validation\",\n",
    "                                      batch_range = (0, validation_batch_num), info_folder = 'info_eval_batch/')\n",
    "\n",
    "set_clustering_machine_eval_batch(intermediate_data_folder, sample_num = 10, layer_num = hop_layer_num, eval_type = \"test\",\n",
    "                                      batch_range = (0, test_batch_num), info_folder = 'info_eval_batch/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start running investigation traing for train batch num: 8 for_trainer_id_0\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0186 seconds!\n",
      "Start train the model:\n",
      "*** During training, reading all batch file I/O costed 411.16 ms ***\n",
      "Training costs a total of 15.5530 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start running investigation traing for train batch num: 8 for_trainer_id_1\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0040 seconds!\n",
      "Start train the model:\n",
      "*** During training, reading all batch file I/O costed 419.88 ms ***\n",
      "Training costs a total of 16.9628 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start running investigation traing for train batch num: 8 for_trainer_id_2\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0019 seconds!\n",
      "Start train the model:\n",
      "*** During training, reading all batch file I/O costed 395.36 ms ***\n",
      "Training costs a total of 15.6540 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start running investigation traing for train batch num: 8 for_trainer_id_0\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0034 seconds!\n",
      "Start train the model:\n",
      "*** During training, reading all batch file I/O costed 406.87 ms ***\n",
      "Training costs a total of 16.4600 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start running investigation traing for train batch num: 8 for_trainer_id_1\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0043 seconds!\n",
      "Start train the model:\n",
      "*** During training, reading all batch file I/O costed 423.96 ms ***\n",
      "Training costs a total of 15.5810 seconds!\n",
      "--------------------------------------------------------------------------------\n",
      "Start running investigation traing for train batch num: 8 for_trainer_id_2\n",
      "\n",
      "====================================================================================================\n",
      "Start generate the trainer:\n",
      "Trainer creation costs a total of 0.0048 seconds!\n",
      "Start train the model:\n",
      "*** During training, reading all batch file I/O costed 424.58 ms ***\n",
      "Training costs a total of 16.7390 seconds!\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# investigate train\n",
    "for tune_val_label, tune_val in zip(tune_val_label_list, tune_val_list):\n",
    "    for tainer_id in trainer_list:\n",
    "        step31_run_investigation_train_batch(intermediate_data_folder, tune_param_name, tune_val_label, tune_val, train_batch_num, net_layer_num, GCN_layer,\n",
    "                                            trainer_id = tainer_id, dropout = 0.1, lr = 0.0001, weight_decay = 0.01, mini_epoch_num = 10, improved = True, diag_lambda = -1,\n",
    "                                             epoch_num = 200, output_period = 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start summarizing for train batch num: 8\n",
      "Start summarizing for train batch num: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangli/anaconda3/envs/pytorch_1_4_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:324: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/projects/GCN_program/largescale_GCN/HPC_version_GCN/9_GraphSage_geometric_hpc_version/utils.py:272: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  plt.figure()\n",
      "/home/xiangli/anaconda3/envs/pytorch_1_4_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:324: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/projects/GCN_program/largescale_GCN/HPC_version_GCN/9_GraphSage_geometric_hpc_version/utils.py:272: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  plt.figure()\n",
      "/home/xiangli/anaconda3/envs/pytorch_1_4_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:324: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# investigate validation, scatter the tasks among validation batches\n",
    "for tune_val_label, tune_val in zip(tune_val_label_list, tune_val_list):\n",
    "    for trainer_id in trainer_list:\n",
    "        execute_investigate_validation_scatter_distr(intermediate_data_folder, tune_param_name, tune_val_label, tune_val, \n",
    "                                                 trainer_id = trainer_id, model_epoch = res_model_epoch_list, batch_ids = validation_batch_ids)\n",
    "        \n",
    "# investigate validation, aggregate results from distributed validation results\n",
    "for tune_val_label, tune_val in zip(tune_val_label_list, tune_val_list):\n",
    "    for trainer_id in trainer_list:\n",
    "        execute_investigate_validation_aggr_distr(intermediate_data_folder, tune_param_name, tune_val_label, tune_val,\n",
    "                                              trainer_id = trainer_id, model_epoch = res_model_epoch_list, batch_ids = validation_batch_ids)\n",
    "\n",
    "for tune_val_label, tune_val in zip(tune_val_label_list, tune_val_list):\n",
    "    step51_run_investigation_summarize_whole(data_name, image_data_path, intermediate_data_folder, tune_param_name, tune_val_label, tune_val,\n",
    "                                    train_batch_num, net_layer_num, trainer_list, res_model_epoch_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test investigate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# investigate test, scatter the tasks among test batches\n",
    "for tune_val_label, tune_val in zip(tune_val_label_list, tune_val_list):\n",
    "    for trainer_id in trainer_list:\n",
    "        execute_test_tuning_scatter_distr(intermediate_data_folder, res_model_epoch_list, tune_param_name, tune_val_label, tune_val, \n",
    "                                      trainer_id = trainer_id, batch_ids = test_batch_ids)\n",
    "        \n",
    "# investigate test, aggregate results from distributed test results\n",
    "for tune_val_label, tune_val in zip(tune_val_label_list, tune_val_list):\n",
    "    for trainer_id in trainer_list:\n",
    "        execute_test_tuning_aggr_distr(intermediate_data_folder, tune_param_name, tune_val_label, tune_val, \n",
    "                                   trainer_id = trainer_id, batch_ids = test_batch_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start running training for partition num: 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "step50_run_tune_summarize_whole(data_name, image_data_path, intermediate_data_folder, tune_param_name, tune_val_label_list, tune_val_list, \\\n",
    "                                train_batch_num, net_layer_num, trainer_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Training loss convergence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output for the train loss\n",
    "subfolder = 'GCN_tuning/tune_batch_epoch_num_5/'\n",
    "check_train_loss_converge(image_data_path, intermediate_data_folder, subfolder, data_name, trainer_id = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GraphSaint_dataset import print_data_info, Flickr, Yelp, PPI_large, Amazon, Reddit\n",
    "# suppose this is on the OSC cluster\n",
    "remote_data_root = '/home/xiangli/projects/tmpdata/GCN/GraphSaint/'\n",
    "\n",
    "data_name = 'Flickr'\n",
    "dataset = Flickr(root = remote_data_root + data_name)\n",
    "print('number of data', len(dataset))\n",
    "data = dataset[0]\n",
    "print_data_info(data, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another way to create class object from a string class name using eval():\n",
    "data_name = 'Flickr'\n",
    "data_class = eval(data_name)\n",
    "print(data_class)\n",
    "dataset = data_class(root = remote_data_root + data_name)\n",
    "print('number of data', len(dataset))\n",
    "data = dataset[0]\n",
    "print_data_info(data, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free GPU memory\n",
    "# !(nvidia-smi | grep 'python' | awk '{ print $3 }' | xargs -n1 kill -9)\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_1_4_geometric]",
   "language": "python",
   "name": "conda-env-pytorch_1_4_geometric-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
