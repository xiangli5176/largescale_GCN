{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# package from the pytorch side\n",
    "import copy\n",
    "import csv\n",
    "import sys\n",
    "import torch\n",
    "from pytorch_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "\n",
    "import networkx as nx\n",
    "import metis\n",
    "import time\n",
    "# import models\n",
    "import numpy as np\n",
    "import partition_utils\n",
    "import tensorflow as tf\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.compat.v1.logging.ERROR\n",
    "# verbose information: tf.logging.INFO  \n",
    "tf.logging.set_verbosity(tf.compat.v1.logging.ERROR)  # Sets the threshold for what messages will be logged.\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Num CPUs Available: \", len(tf.config.experimental.list_physical_devices('CPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCN model used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inits\n",
    "import tensorflow as tf\n",
    "import metrics\n",
    "\n",
    "def sparse_dropout(x, keep_prob, noise_shape):\n",
    "    \"\"\"Dropout for sparse tensors.\"\"\"\n",
    "    random_tensor = keep_prob\n",
    "    random_tensor += tf.compat.v1.random_uniform(noise_shape)\n",
    "    dropout_mask = tf.compat.v1.cast(tf.compat.v1.floor(random_tensor), dtype=tf.compat.v1.bool)\n",
    "    pre_out = tf.compat.v1.sparse_retain(x, dropout_mask)\n",
    "    return pre_out * (1. / keep_prob)\n",
    "\n",
    "\n",
    "def dot(x, y, sparse=False):\n",
    "    \"\"\"Wrapper for tf.compat.v1.matmul (sparse vs dense).\"\"\"\n",
    "    if sparse:\n",
    "        res = tf.compat.v1.sparse_tensor_dense_matmul(x, y)\n",
    "    else:\n",
    "        res = tf.compat.v1.matmul(x, y)\n",
    "    return res\n",
    "\n",
    "\n",
    "def layernorm(x, offset, scale):\n",
    "    mean, variance = tf.compat.v1.nn.moments(x, axes=[1], keep_dims=True)\n",
    "    return tf.compat.v1.nn.batch_normalization(x, mean, variance, offset, scale, 1e-9)\n",
    "\n",
    "\n",
    "class Layer(object):\n",
    "    \"\"\"Base layer class.\n",
    "\n",
    "    Defines basic API for all layer objects.\n",
    "    Implementation inspired by keras (http://keras.io).\n",
    "\n",
    "    # Properties\n",
    "        name: String, defines the variable scope of the layer.\n",
    "        logging: Boolean, switches Tensorflow histogram logging on/off\n",
    "\n",
    "    # Methods\n",
    "        _call(inputs): Defines computation graph of layer\n",
    "            (i.e. takes input, returns output)\n",
    "        __call__(inputs): Wrapper for _call()\n",
    "        _log_vars(): Log all variables\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name = None, logging = False):\n",
    "        \n",
    "        if not name:\n",
    "            layer = self.__class__.__name__.lower()\n",
    "            name = layer + '_' \n",
    "\n",
    "        self.name = name\n",
    "        \n",
    "        self.vars = {}\n",
    "        self.logging = logging\n",
    "        \n",
    "        self.sparse_inputs = False\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        return inputs\n",
    "\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        with tf.compat.v1.name_scope(self.name):\n",
    "          if self.logging and not self.sparse_inputs:\n",
    "            tf.compat.v1.summary.histogram(self.name + '/inputs', inputs)\n",
    "\n",
    "        outputs = self._call(inputs)\n",
    "\n",
    "        if self.logging:\n",
    "            tf.compat.v1.summary.histogram(self.name + '/outputs', outputs)\n",
    "        return outputs\n",
    "\n",
    "    def _log_vars(self):\n",
    "        for var in self.vars:\n",
    "            tf.compat.v1.summary.histogram(self.name + '/vars/' + var, self.vars[var])\n",
    "\n",
    "            \n",
    "class GraphConvolution(Layer):\n",
    "    \"\"\"Graph convolution layer.\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, placeholders,\n",
    "               dropout=0., sparse_inputs=False, act=tf.nn.relu, bias=False, featureless=False, norm=False, precalc=False,\n",
    "               name = None, logging = False):\n",
    "        super(GraphConvolution, self).__init__(name = name, logging = logging)\n",
    "#         print('During the constructor of GCN layer, input dim: {} ; output dim: {}'.format(input_dim, output_dim))\n",
    "\n",
    "        if dropout:\n",
    "            self.dropout = placeholders['dropout']\n",
    "        else:\n",
    "            self.dropout = 0.\n",
    "\n",
    "        self.act = act\n",
    "        self.support = placeholders['support']\n",
    "\n",
    "        self.sparse_inputs = sparse_inputs\n",
    "        self.featureless = featureless\n",
    "        self.bias = bias\n",
    "        self.norm = norm\n",
    "        self.precalc = precalc\n",
    "\n",
    "        # helper variable for sparse dropout\n",
    "        # self.num_features_nonzero = placeholders['num_features_nonzero']\n",
    "\n",
    "        with tf.variable_scope(self.name + '_vars'):\n",
    "            self.vars['weights'] = inits.glorot([input_dim, output_dim], name='weights')\n",
    "            if self.bias:\n",
    "                self.vars['bias'] = inits.zeros([output_dim], name='bias')\n",
    "\n",
    "            if self.norm:\n",
    "                self.vars['offset'] = inits.zeros([1, output_dim], name='offset')\n",
    "                self.vars['scale'] = inits.ones([1, output_dim], name='scale')\n",
    "\n",
    "        if self.logging:\n",
    "            self._log_vars()\n",
    "    \n",
    "\n",
    "    def _call(self, inputs):\n",
    "        x = inputs\n",
    "\n",
    "        # convolve\n",
    "        if self.precalc:\n",
    "            support = x\n",
    "        else:\n",
    "            support = dot(self.support, x, sparse=True)\n",
    "            support = tf.concat((support, x), axis=1)\n",
    "\n",
    "        # dropout\n",
    "        support = tf.nn.dropout(support, rate = self.dropout)\n",
    "\n",
    "        tf.Print(support, [support], \"During the call of GCN layer, final input to be multiplied with weights: \")\n",
    "\n",
    "#         print('\\n inside the call of convolutiongraph layer: ')\n",
    "#         print('support vecotr dimension is : {} ;'.format(support.shape), 'weight matrix dimension is : {} ;'.format(self.vars['weights'].shape))\n",
    "\n",
    "        output = dot(support, self.vars['weights'], sparse=self.sparse_inputs)\n",
    "\n",
    "        # bias\n",
    "        if self.bias:\n",
    "            output += self.vars['bias']\n",
    "\n",
    "        with tf.variable_scope(self.name + '_vars'):\n",
    "            if self.norm:\n",
    "                output = layernorm(output, self.vars['offset'], self.vars['scale'])\n",
    "\n",
    "        return self.act(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct GCN layer nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Collections of different Models.\"\"\"\n",
    "\n",
    "\n",
    "class Model(object):\n",
    "    \"\"\"Model class to be inherited.\"\"\"\n",
    "\n",
    "    def __init__(self, weight_decay = 0, num_layers = 2, name = None, logging = False, multilabel = False, norm = False, precalc = False):\n",
    "        self.weight_decay = weight_decay\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        if not name:\n",
    "            name = self.__class__.__name__.lower()\n",
    "        self.name = name\n",
    "\n",
    "        self.logging = logging\n",
    "        self.multilabel = multilabel\n",
    "        self.norm = norm\n",
    "        self.precalc = precalc\n",
    "\n",
    "        self.vars = {}\n",
    "        self.placeholders = {}\n",
    "\n",
    "        self.layers = []\n",
    "        self.activations = []\n",
    "\n",
    "        self.inputs = None\n",
    "        self.outputs = None\n",
    "\n",
    "        self.loss = 0\n",
    "        self.accuracy = 0\n",
    "        self.pred = 0\n",
    "        self.optimizer = None\n",
    "        self.opt_op = None\n",
    "\n",
    "    def _build(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def build(self):\n",
    "        \"\"\"Wrapper for _build().\"\"\"\n",
    "        with tf.variable_scope(self.name):\n",
    "            self._build()\n",
    "\n",
    "        # Build sequential layer model\n",
    "        self.activations.append(self.inputs)\n",
    "        \n",
    "        # debug to output the embedding:\n",
    "#         self.hidden1 = layer(self.activations[-1])\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            hidden = layer(self.activations[-1])\n",
    "        \n",
    "            if isinstance(hidden, tuple):\n",
    "                tf.logging.info('{} shape = {}'.format(layer.name, hidden[0].get_shape()))\n",
    "            else:\n",
    "                tf.logging.info('{} shape = {}'.format(layer.name, hidden.get_shape()))\n",
    "\n",
    "            self.activations.append(hidden)\n",
    "            \n",
    "        self.outputs = self.activations[-1]\n",
    "\n",
    "        # Store model variables for easy access\n",
    "        variables = tf.get_collection( tf.GraphKeys.GLOBAL_VARIABLES, scope=self.name)\n",
    "        self.vars = variables\n",
    "        for k in self.vars:\n",
    "            tf.logging.info((k.name, k.get_shape()))\n",
    "\n",
    "        # Build metrics\n",
    "        self._loss()\n",
    "        self._accuracy()\n",
    "        self._predict()\n",
    "\n",
    "        self.opt_op = self.optimizer.minimize(self.loss)\n",
    "\n",
    "    def _loss(self):\n",
    "        \"\"\"Construct the loss function.\"\"\"\n",
    "        # Weight decay loss\n",
    "        if self.weight_decay > 0.0:\n",
    "            for var in self.layers[0].vars.values():\n",
    "                self.loss += self.weight_decay * tf.nn.l2_loss(var)\n",
    "\n",
    "        # Cross entropy error\n",
    "        if self.multilabel:\n",
    "            self.loss += metrics.masked_sigmoid_cross_entropy(self.outputs, self.placeholders['labels'], self.placeholders['labels_mask'])\n",
    "        else:\n",
    "            self.loss += metrics.masked_softmax_cross_entropy(self.outputs, self.placeholders['labels'], self.placeholders['labels_mask'])\n",
    "\n",
    "    def _accuracy(self):\n",
    "        if self.multilabel:\n",
    "            self.accuracy = metrics.masked_accuracy_multilabel(self.outputs, self.placeholders['labels'], self.placeholders['labels_mask'])\n",
    "        else:\n",
    "            self.accuracy = metrics.masked_accuracy(self.outputs, self.placeholders['labels'], self.placeholders['labels_mask'])\n",
    "\n",
    "    def _predict(self):\n",
    "        if self.multilabel:\n",
    "            self.pred = tf.nn.sigmoid(self.outputs)\n",
    "        else:\n",
    "            self.pred = tf.nn.softmax(self.outputs)\n",
    "\n",
    "    def save(self, sess=None):\n",
    "        if not sess:\n",
    "            raise AttributeError('TensorFlow session not provided.')\n",
    "        saver = tf.train.Saver(self.vars)\n",
    "        save_path = saver.save(sess, './tmp/%s.ckpt' % self.name)\n",
    "        tf.logging.info('Model saved in file:', save_path)\n",
    "\n",
    "    def load(self, sess=None):\n",
    "        if not sess:\n",
    "            raise AttributeError('TensorFlow session not provided.')\n",
    "        saver = tf.train.Saver(self.vars)\n",
    "        save_path = './tmp/%s.ckpt' % self.name\n",
    "        saver.restore(sess, save_path)\n",
    "        tf.logging.info('Model restored from file:', save_path)\n",
    "\n",
    "\n",
    "class GCN(Model):\n",
    "    \"\"\"Implementation of GCN model.\"\"\"\n",
    "\n",
    "    def __init__(self, placeholders, input_dim, output_dim, hidden_neuron_num, learning_rate = 0.01, \n",
    "                 num_layers = 3, name = None, logging = False, multilabel = False, norm = False, precalc = False):\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.hidden1 = hidden_neuron_num\n",
    "        \n",
    "        super(GCN, self).__init__(weight_decay = 0, num_layers = num_layers, name = name, logging = logging, \\\n",
    "                                  multilabel = multilabel, norm = norm, precalc = precalc)\n",
    "\n",
    "        self.inputs = placeholders['features']\n",
    "        self.input_dim = input_dim\n",
    "#         self.output_dim = placeholders['labels'].get_shape().as_list()[1]\n",
    "        self.output_dim = output_dim\n",
    "    \n",
    "        self.placeholders = placeholders\n",
    "\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate = self.learning_rate)\n",
    "\n",
    "        self.build()\n",
    "\n",
    "    def _build(self):\n",
    "        self.layers.append(\n",
    "            GraphConvolution(\n",
    "                input_dim = self.input_dim if self.precalc else self.input_dim * 2,\n",
    "                output_dim = self.hidden1,\n",
    "                placeholders = self.placeholders,\n",
    "                act=tf.nn.relu,\n",
    "                dropout = True,\n",
    "                sparse_inputs = False,\n",
    "                logging = self.logging,\n",
    "                norm = self.norm,\n",
    "                precalc = self.precalc))\n",
    "        \n",
    "        for _ in range(self.num_layers - 2):\n",
    "            self.layers.append(\n",
    "              GraphConvolution(\n",
    "                  input_dim = self.hidden1 * 2,\n",
    "                  output_dim = self.hidden1,\n",
    "                  placeholders = self.placeholders,\n",
    "                  act=tf.nn.relu,\n",
    "                  dropout = True,\n",
    "                  sparse_inputs = False,\n",
    "                  logging = self.logging,\n",
    "                  norm = self.norm,\n",
    "                  precalc = False))\n",
    "            \n",
    "        self.layers.append(\n",
    "            GraphConvolution(\n",
    "                input_dim = self.hidden1 * 2,\n",
    "                output_dim = self.output_dim,\n",
    "                placeholders = self.placeholders,\n",
    "                act = lambda x: x,\n",
    "                dropout = True,\n",
    "                logging = self.logging,\n",
    "                norm = False,\n",
    "                precalc = False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_scatter import scatter_add\n",
    "from torch_geometric.utils import add_remaining_self_loops\n",
    "\n",
    "def get_edge_weight(edge_index, num_nodes, edge_weight = None, improved = False, dtype=None, store_path='./tmp/'):\n",
    "    \"\"\"\n",
    "        edge_index(ndarray): undirected edge index (two-directions both included)\n",
    "        num_nodes(int):  number of nodes inside the graph\n",
    "        edge_weight(ndarray): if any weights already assigned, otherwise will be generated \n",
    "        improved(boolean):   may assign 2 to the self loop weight if true\n",
    "        store_path(string): the path of the folder to contain all the clustering information files\n",
    "    \"\"\"\n",
    "    # calculate the global graph properties, global edge weights\n",
    "    if edge_weight is None:\n",
    "        edge_weight = torch.ones((edge_index.size(1), ), dtype=dtype, device=edge_index.device)\n",
    "\n",
    "    fill_value = 1 if not improved else 2\n",
    "    # there are num_nodes self-loop edges added after the edge_index\n",
    "    edge_index, edge_weight = add_remaining_self_loops(edge_index, edge_weight, fill_value, num_nodes)\n",
    "\n",
    "    row, col = edge_index   \n",
    "    # row includes the starting points of the edges  (first row of edge_index)\n",
    "    # col includes the ending points of the edges   (second row of edge_index)\n",
    "\n",
    "    deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n",
    "    # row records the source nodes, which is the index we are trying to add\n",
    "    # deg will record the out-degree of each node of x_i in all edges (x_i, x_j) including self_loops\n",
    "\n",
    "    deg_inv_sqrt = deg.pow(-0.5)\n",
    "    deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "    # normalize the edge weight\n",
    "    normalized_edge_weight = deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n",
    "\n",
    "    # transfer from tensor to the numpy to construct the dict for the edge_weights\n",
    "    edge_index = edge_index.t().numpy()\n",
    "    normalized_edge_weight = normalized_edge_weight.numpy()\n",
    "\n",
    "    num_edge = edge_index.shape[0]\n",
    "\n",
    "    output = ([edge_index[i][0], edge_index[i][1], normalized_edge_weight[i]] for i in range(num_edge))\n",
    "\n",
    "    # output the edge weights as the csv file\n",
    "    input_edge_weight_txt_file = store_path + 'input_edge_weight_list.csv'\n",
    "    os.makedirs(os.path.dirname(input_edge_weight_txt_file), exist_ok=True)\n",
    "    with open(input_edge_weight_txt_file, 'w', newline='\\n') as fp:\n",
    "        wr = csv.writer(fp, delimiter = ' ')\n",
    "        for line in output:\n",
    "            wr.writerow(line)\n",
    "    return input_edge_weight_txt_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import metis\n",
    "import random\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import chain\n",
    "import scipy.sparse as sp\n",
    "\n",
    "def construct_adj(edges, nodes_count):\n",
    "    # Compressed Sparse Row matrix\n",
    "    # csr_matrix((data, ij), [shape=(M, N)])\n",
    "    # where data and ij satisfy the relationship a[ij[0, k], ij[1, k]] = data[k]\n",
    "    adj = sp.csr_matrix( ( np.ones((edges.shape[0]), dtype=np.float32), (edges[:, 0], edges[:, 1]) ), shape=(nodes_count, nodes_count) )\n",
    "    adj += adj.transpose()   # double the weight of each edge if it is two direction\n",
    "    return adj\n",
    "\n",
    "class ClusteringMachine(object):\n",
    "    \"\"\"\n",
    "    Clustering the graph, feature set and label. Performed on the CPU side\n",
    "    \"\"\"\n",
    "    def __init__(self, edge_index, features, label, tmp_folder = './tmp/', info_folder = './info/'):\n",
    "        \"\"\"\n",
    "        :param edge_index: COO format of the edge indices.\n",
    "        :param features: Feature matrix (ndarray).\n",
    "        :param label: label vector (ndarray).\n",
    "        :tmp_folder(string): the path of the folder to contain all the clustering information files\n",
    "        \"\"\"\n",
    "        self.features = features\n",
    "        self.label = label\n",
    "        self._set_sizes()\n",
    "        self.edge_index = edge_index\n",
    "        # store the information folder for memory tracing\n",
    "        self.tmp_folder = tmp_folder\n",
    "        self.info_folder = info_folder\n",
    "        \n",
    "        # first, use the get edge weights func to construct the two-sided self-loop added graph\n",
    "        edge_weight_file = self.tmp_folder + 'input_edge_weight_list.csv'\n",
    "        self.graph = nx.read_weighted_edgelist(edge_weight_file, create_using = nx.Graph, nodetype = int)\n",
    "        \n",
    "#         # second construct teh graph directly from the edge_list, here we are lacking the self-loop age\n",
    "#         tmp = edge_index.t().numpy().tolist()\n",
    "#         self.graph = nx.from_edgelist(tmp)\n",
    "        \n",
    "    def _set_sizes(self):\n",
    "        \"\"\"\n",
    "        Setting the feature and class count.\n",
    "        \"\"\"\n",
    "        self.node_count = self.features.shape[0]\n",
    "        self.feature_count = self.features.shape[1]    # features all always in the columns\n",
    "        self.label_count = len(np.unique(self.label.numpy()) )\n",
    "    \n",
    "    # 2) first assign train, test, validation nodes, split edges; this is based on the assumption that the clustering is no longer that important\n",
    "    def split_whole_nodes_edges_then_cluster(self, validation_ratio, test_ratio, normalize = False, precalc = True):\n",
    "        \"\"\"\n",
    "            Only split nodes\n",
    "            First create train-test splits, then split train and validation into different batch seeds\n",
    "            Input:  \n",
    "                1) ratio of test, validation\n",
    "                2) partition number of train nodes, test nodes, validation nodes\n",
    "            Output:\n",
    "                1) sg_validation_nodes_global, sg_train_nodes_global, sg_test_nodes_global\n",
    "        \"\"\"\n",
    "        relative_validation_ratio = (validation_ratio) / (1 - test_ratio)\n",
    "        \n",
    "        # first divide the nodes for the whole graph, result will always be a list of lists \n",
    "        model_nodes_global, self.test_nodes_global = train_test_split(list(self.graph.nodes()), test_size = test_ratio)\n",
    "        self.train_nodes_global, self.valid_nodes_global = train_test_split(model_nodes_global, test_size = relative_validation_ratio)\n",
    "        \n",
    "        edges = np.array(self.graph.edges(), dtype=np.int32)\n",
    "        self.feats = np.array(self.features.numpy(), dtype=np.float32)\n",
    "        \n",
    "        \n",
    "#         # normalize all the features\n",
    "#         if normalize:\n",
    "#             scaler = sklearn.preprocessing.StandardScaler()\n",
    "#             scaler.fit(feats)\n",
    "#             feats = scaler.transform(feats)\n",
    "        \n",
    "        classes = np.array(self.label.numpy(), dtype=np.int32)\n",
    "\n",
    "        # construct the 1-hot labels:\n",
    "        self.labels = np.zeros((self.node_count, self.label_count), dtype=np.float32)\n",
    "        for i, label in enumerate(classes):\n",
    "            self.labels[i, label] = 1\n",
    "            \n",
    "        self.full_adj = construct_adj(edges, self.node_count)\n",
    "    \n",
    "    def train_batch_generation(self, train_batch_num = 2, diag_lambda = -1, precalc = True, mini_cluster_num = 1):\n",
    "        # all the properties needed for the training\n",
    "        train_subgraph = self.graph.subgraph(self.train_nodes_global)\n",
    "        train_edges = np.array(train_subgraph.edges(), dtype=np.int32)\n",
    "        # check the double direction inside this adjacent matrix\n",
    "        train_adj = construct_adj(train_edges, self.node_count)\n",
    "        train_feats = self.feats[self.train_nodes_global]\n",
    "        y_train = np.zeros(self.labels.shape)\n",
    "        y_train[self.train_nodes_global, :] = self.labels[self.train_nodes_global, :]\n",
    "        train_mask = utils.sample_mask(self.train_nodes_global, self.labels.shape[0])\n",
    "        \n",
    "        if precalc:\n",
    "            train_feats = train_adj.dot(self.feats)   # calculate the feature matrix weighted by the edge weights\n",
    "            # numpy.hstack: Stack arrays in sequence horizontally (column wise).\n",
    "            train_feats = np.hstack((train_feats, self.feats))\n",
    "        visible_data = self.train_nodes_global\n",
    "        \n",
    "        # Partition graph and do preprocessing\n",
    "        if mini_cluster_num > 1:\n",
    "            _, parts = partition_utils.partition_graph(train_adj, visible_data, train_batch_num)\n",
    "            parts = [np.array(pt) for pt in parts]\n",
    "            return train_adj, parts, train_feats, y_train, train_mask\n",
    "        else:\n",
    "            (parts, features_batches, support_batches, y_train_batches, train_mask_batches) = \\\n",
    "            utils.preprocess(train_adj, train_feats, y_train, train_mask, visible_data, train_batch_num, diag_lambda)\n",
    "            return parts, features_batches, support_batches, y_train_batches, train_mask_batches\n",
    "    \n",
    "    \n",
    "    \n",
    "    def test_batch_generation(self, test_batch_num = 2, diag_lambda = -1, precalc = True):\n",
    "        # all the properties needed for the test\n",
    "        self.test_feats = self.feats   # why test gives the full feature of the whole graph\n",
    "        y_test = np.zeros(self.labels.shape)\n",
    "        y_test[self.test_nodes_global, :] = self.labels[self.test_nodes_global, :]\n",
    "        test_mask = utils.sample_mask(self.test_nodes_global, self.labels.shape[0])\n",
    "        if precalc:\n",
    "            self.test_feats = self.full_adj.dot(self.feats)\n",
    "            self.test_feats = np.hstack((self.test_feats, self.feats))\n",
    "            \n",
    "        (_, test_features_batches, test_support_batches, y_test_batches, test_mask_batches) = \\\n",
    "            utils.preprocess(self.full_adj, self.test_feats, y_test, test_mask, np.arange(self.node_count), test_batch_num, diag_lambda)\n",
    "        return test_features_batches, test_support_batches, y_test_batches, test_mask_batches, self.test_nodes_global\n",
    "        \n",
    "    def validation_batch_generation(self, valid_batch_num = 2, diag_lambda = -1):\n",
    "        y_val = np.zeros(self.labels.shape)\n",
    "        y_val[self.valid_nodes_global, :] = self.labels[self.valid_nodes_global, :]\n",
    "        val_mask = utils.sample_mask(self.valid_nodes_global, self.labels.shape[0])\n",
    "\n",
    "        (_, val_features_batches, val_support_batches, y_val_batches, val_mask_batches) = \\\n",
    "            utils.preprocess(self.full_adj, self.test_feats, y_val, val_mask, np.arange(self.node_count), valid_batch_num, diag_lambda)\n",
    "        return val_features_batches, val_support_batches, y_val_batches, val_mask_batches, self.valid_nodes_global\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the Graph_sage format data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model evaluation function\n",
    "def evaluate(sess, model, val_features_batches, val_support_batches,\n",
    "             y_val_batches, val_mask_batches, val_data, placeholders, multilabel = True):\n",
    "    \"\"\"evaluate GCN model.\"\"\"\n",
    "    total_pred = []\n",
    "    total_lab = []\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "\n",
    "    num_batches = len(val_features_batches)\n",
    "    for i in range(num_batches):\n",
    "        features_b = val_features_batches[i]\n",
    "        support_b = val_support_batches[i]\n",
    "        y_val_b = y_val_batches[i]\n",
    "        val_mask_b = val_mask_batches[i]\n",
    "        num_data_b = np.sum(val_mask_b)\n",
    "        if num_data_b == 0:\n",
    "            continue\n",
    "        else:\n",
    "            feed_dict = utils.construct_feed_dict(features_b, support_b, y_val_b, val_mask_b, placeholders)\n",
    "            outs = sess.run([model.loss, model.accuracy, model.outputs], feed_dict = feed_dict)\n",
    "\n",
    "        total_pred.append(outs[2][val_mask_b])\n",
    "        total_lab.append(y_val_b[val_mask_b])\n",
    "        total_loss += outs[0] * num_data_b\n",
    "        total_acc += outs[1] * num_data_b\n",
    "\n",
    "    total_pred = np.vstack(total_pred)\n",
    "    total_lab = np.vstack(total_lab)\n",
    "    loss = total_loss / len(val_data)\n",
    "    # this acc1 is calculated by hand\n",
    "    acc1 = total_acc / len(val_data)\n",
    "    # this acc is calculated from the sklearn\n",
    "    micro, macro, acc = utils.calc_f1(total_pred, total_lab, multilabel)\n",
    "    return loss, acc, micro, macro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "class ClusterGCNTrainer(object):\n",
    "    \"\"\"\n",
    "    Training a ClusterGCN.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim, num_layers = 3, hidden_neuron_num = 16, learning_rate = 0.001, \\\n",
    "                 logging = False, multilabel = True, norm = True, precalc = True):\n",
    "        \n",
    "        # Define self.placeholders\n",
    "        self.placeholders = {\n",
    "          'support':\n",
    "              tf.sparse_placeholder(tf.float32),\n",
    "          'features':\n",
    "              tf.placeholder(tf.float32),\n",
    "          'labels':\n",
    "              tf.placeholder(tf.float32),\n",
    "          'labels_mask':\n",
    "              tf.placeholder(tf.int32),\n",
    "          'dropout':\n",
    "              tf.placeholder_with_default(0., shape=()),\n",
    "    #       'num_features_nonzero':\n",
    "    #           tf.placeholder(tf.int32)  # helper variable for sparse dropout\n",
    "        }\n",
    "        \n",
    "        now_input_dim = 2 * input_dim if precalc else input_dim\n",
    "        self.model = GCN(self.placeholders,\n",
    "              input_dim = now_input_dim,\n",
    "              output_dim = output_dim,\n",
    "              hidden_neuron_num = hidden_neuron_num,\n",
    "              learning_rate = learning_rate,\n",
    "              logging = logging,\n",
    "              multilabel = multilabel,\n",
    "              norm = norm,\n",
    "              precalc = precalc,\n",
    "              num_layers = num_layers)\n",
    "    \n",
    "    def train_test_investigate(self, clustering_machine, seed = 6, diag_lambda = 1, precalc = True, mini_cluster_num = 1, \\\n",
    "                               train_batch_num = 2, test_batch_num = 2, valid_batch_num = 2, \\\n",
    "                               epoch_num = 400, mini_epoch_num = 20, dropout = 0.3, train_save_name = './tmp/saver.txt'):\n",
    "        # load the needed data\n",
    "        if mini_cluster_num > 1:\n",
    "            train_adj, parts, train_feats, y_train, train_mask = \\\n",
    "                    clustering_machine.train_batch_generation(train_batch_num = train_batch_num,\n",
    "                                                diag_lambda = diag_lambda, precalc = precalc, mini_cluster_num = mini_cluster_num)\n",
    "        else:\n",
    "            parts, features_batches, support_batches, y_train_batches, train_mask_batches = \\\n",
    "                    clustering_machine.train_batch_generation(train_batch_num = train_batch_num, \\\n",
    "                                                diag_lambda = diag_lambda, precalc = precalc, mini_cluster_num = mini_cluster_num)\n",
    "        \n",
    "        test_features_batches, test_support_batches, y_test_batches, test_mask_batches, test_data = \\\n",
    "                    clustering_machine.test_batch_generation(test_batch_num = test_batch_num, diag_lambda = diag_lambda, precalc = precalc)\n",
    "        \n",
    "        val_features_batches, val_support_batches, y_val_batches, val_mask_batches, val_data = \\\n",
    "                    clustering_machine.validation_batch_generation(valid_batch_num = valid_batch_num, diag_lambda = diag_lambda)\n",
    "        \n",
    "        idx_parts = list(range(len(parts)))\n",
    "        \n",
    "        # Initialize session\n",
    "        sess = tf.Session()\n",
    "        tf.set_random_seed(seed)\n",
    "\n",
    "        # Init variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "        cost_val = []\n",
    "        self.time_train_total = 0.0\n",
    "        # Train model:  epoch_num\n",
    "        epoch_partition = epoch_num // mini_epoch_num\n",
    "        for epoch in range(epoch_partition):\n",
    "            t0 = time.time()\n",
    "            np.random.shuffle(idx_parts)\n",
    "\n",
    "            # recombine mini-clusters to form larger batches\n",
    "            if mini_cluster_num > 1:\n",
    "                (features_batches, support_batches, y_train_batches,\n",
    "                train_mask_batches) = utils.preprocess_multicluster(\n",
    "                   train_adj, parts, train_feats, y_train, train_mask,\n",
    "                   train_batch_num, mini_cluster_num, diag_lambda)\n",
    "\n",
    "                for pid in range(len(features_batches)):\n",
    "                    # Use preprocessed batch data\n",
    "                    features_b = features_batches[pid]\n",
    "                    support_b = support_batches[pid]\n",
    "                    y_train_b = y_train_batches[pid]\n",
    "                    train_mask_b = train_mask_batches[pid]\n",
    "                    # Construct feed dictionary\n",
    "                    feed_dict = utils.construct_feed_dict(features_b, support_b, y_train_b, train_mask_b, self.placeholders)\n",
    "\n",
    "                    feed_dict.update({self.placeholders['dropout']: dropout})\n",
    "                    for _ in range(mini_epoch_num):\n",
    "                        # Training step\n",
    "                        outs = sess.run([self.model.opt_op, self.model.loss, self.model.accuracy], feed_dict=feed_dict)\n",
    "            else:\n",
    "                np.random.shuffle(idx_parts)\n",
    "\n",
    "                for pid in idx_parts:\n",
    "                    # Use preprocessed batch data\n",
    "                    features_b = features_batches[pid]\n",
    "                    support_b = support_batches[pid]\n",
    "                    y_train_b = y_train_batches[pid]\n",
    "                    train_mask_b = train_mask_batches[pid]\n",
    "\n",
    "                    # Construct feed dictionary\n",
    "                    feed_dict = utils.construct_feed_dict(features_b, support_b, y_train_b, train_mask_b, self.placeholders)\n",
    "\n",
    "                    # investigate the constructed matrix for inputs:\n",
    "                    feed_dict.update({self.placeholders['dropout'] : dropout})\n",
    "                    for _ in range(mini_epoch_num):\n",
    "                        # Training step:\n",
    "                        outs = sess.run([self.model.opt_op, self.model.loss, self.model.accuracy], feed_dict=feed_dict)\n",
    "\n",
    "            self.time_train_total += time.time() - t0\n",
    "            \n",
    "            print_str = 'Epoch: %04d ' % (epoch + 1) + 'training time: {:.5f} '.format(\n",
    "                self.time_train_total) + 'train_acc= {:.5f} '.format(outs[2])\n",
    "\n",
    "#             # Validation\n",
    "#             cost, acc, micro, macro = evaluate(sess, self.model, val_features_batches, val_support_batches, y_val_batches,\n",
    "#                                              val_mask_batches, val_data, self.placeholders)\n",
    "#             cost_val.append(cost)\n",
    "#             print_str += 'val_acc= {:.5f} '.format(acc) + 'mi F1= {:.5f} ma F1= {:.5f} '.format(micro, macro)\n",
    "\n",
    "#             tf.logging.info(print_str)\n",
    "\n",
    "#             if epoch > early_stopping_epoch_thresh and cost_val[-1] > np.mean(\n",
    "#                 cost_val[-(early_stopping_epoch_thresh + 1):-1]):\n",
    "#                 tf.logging.info('Early stopping...')\n",
    "#                 break\n",
    "\n",
    "        tf.logging.info('Optimization Finished!')\n",
    "\n",
    "        # Save model\n",
    "        saver.save(sess, train_save_name)\n",
    "\n",
    "        with tf.device('/cpu:0'):\n",
    "            sess_cpu = tf.Session(config=tf.ConfigProto(device_count={'GPU': 0}))\n",
    "            sess_cpu.run(tf.global_variables_initializer())\n",
    "            saver = tf.train.Saver()\n",
    "            saver.restore(sess_cpu, train_save_name)\n",
    "\n",
    "            # Testing\n",
    "            test_cost, test_acc, micro, macro = evaluate(\n",
    "                sess_cpu, self.model, test_features_batches, test_support_batches, y_test_batches, test_mask_batches, test_data, self.placeholders)\n",
    "\n",
    "            print_str = 'Test set results: ' + 'cost= {:.5f} '.format(test_cost) + 'accuracy= {:.5f} '.format(test_acc) + 'mi F1= {:.5f} ma F1= {:.5f}'.format(micro, macro)\n",
    "            tf.logging.info(print_str)\n",
    "#             print(print_str)\n",
    "        \n",
    "        return micro, test_acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use trivial dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index = torch.tensor([[0, 1, 1, 3, 1, 2, 4, 2, 4, 6, 6, 7, 7, 9, 2, 5, 9, 8], \n",
    "                           [1, 0, 3, 1, 2, 1, 2, 4, 6, 4, 7, 6, 9, 7, 5, 2, 8, 9]])\n",
    "# features = torch.rand(10, 3)\n",
    "features = torch.tensor([[0, 0], [0, 1], [0, 2], [0, 3], [0, 4],  \n",
    "                           [0, 5], [0, 6], [0, 7], [0, 8], [0, 9]], dtype = torch.float)\n",
    "# label = torch.tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "label = torch.tensor([0, 1, 1, 0, 1, 1, 1, 0, 0, 0])\n",
    "print(features, features.shape)\n",
    "\n",
    "clustering_folder = './res_save_batch/clustering/'\n",
    "check_folder_exist(clustering_folder)\n",
    "clustering_file_name = clustering_folder + 'check_clustering_machine.txt'\n",
    "os.makedirs(os.path.dirname(clustering_folder), exist_ok=True)\n",
    "info_folder = './res_save_batch/info/'\n",
    "check_folder_exist(info_folder)\n",
    "os.makedirs(os.path.dirname(info_folder), exist_ok=True)\n",
    "\n",
    "node_count = features.shape[0]\n",
    "get_edge_weight(edge_index, node_count, store_path = clustering_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_machine = ClusteringMachine(edge_index, features, label, clustering_folder, info_folder = clustering_folder)\n",
    "clustering_machine.split_whole_nodes_edges_then_cluster(0.25, 0.25, normalize = False, precalc = True)\n",
    "\n",
    "gcn_trainer = ClusterGCNTrainer(clustering_machine.feature_count, clustering_machine.label_count, num_layers = 3, hidden_neuron_num = 16, learning_rate = 0.001, \\\n",
    "                 logging = False, multilabel = True, norm = True, precalc = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn_trainer.train_test_investigate(clustering_machine, seed = 6, diag_lambda = -1, precalc = True, mini_cluster_num = 4, \\\n",
    "                               train_batch_num = 2, test_batch_num = 2, epoch_num = 400, dropout = 0.3, train_save_name = './tmp/saver.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cluster_train_valid_batch_run(clustering_machine, num_layers = 3, hidden_neuron_num = 32, learning_rate = 0.001, \\\n",
    "                 logging = False, multilabel = True, norm = True, precalc = True, diag_lambda = -1, \\\n",
    "                 mini_cluster_num = 1, train_batch_num = 2, test_batch_num = 2, \\\n",
    "                 epoch_num = 400, mini_epoch_num = 20, dropout = 0.3, train_save_name = './tmp/saver.txt'):\n",
    "    \"\"\"\n",
    "        # Run the mini-batch model (train and validate both in batches)\n",
    "        Tuning parameters:  dropout, lr (learning rate), weight_decay: l2 regularization\n",
    "        return: validation accuracy value, validation F-1 value, time_training (ms), time_data_load (ms)\n",
    "    \"\"\"\n",
    "    \n",
    "    gcn_trainer = ClusterGCNTrainer(clustering_machine.feature_count, clustering_machine.label_count, \\\n",
    "                num_layers = num_layers, hidden_neuron_num = hidden_neuron_num, learning_rate = learning_rate, \\\n",
    "                 logging = logging, multilabel = multilabel, norm = norm, precalc = precalc)\n",
    "    \n",
    "    validation_F1, validation_accuracy = gcn_trainer.train_test_investigate(clustering_machine, seed = 6, \\\n",
    "                diag_lambda = diag_lambda, precalc = precalc, mini_cluster_num = mini_cluster_num, \\\n",
    "                train_batch_num = train_batch_num, test_batch_num = test_batch_num, \\\n",
    "                epoch_num = epoch_num, mini_epoch_num = mini_epoch_num, dropout = dropout, train_save_name = train_save_name)\n",
    "    \n",
    "    time_train_total = gcn_trainer.time_train_total\n",
    "#     time_data_load = gcn_trainer.time_train_load_data\n",
    "    \n",
    "    return validation_accuracy, validation_F1, time_train_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"To test one single model for different parameter values\"\"\"\n",
    "def execute_tuning(tune_params, clustering_machine, repeate_time = 7, num_layers = 3, hidden_neuron_num = 32, learning_rate = 0.001, \\\n",
    "                 logging = False, multilabel = True, norm = True, precalc = True, diag_lambda = -1, \\\n",
    "                 mini_cluster_num = 1, train_batch_num = 2, test_batch_num = 2, \\\n",
    "                 epoch_num = 400, mini_epoch_num = 20, dropout = 0.3, train_save_name = './tmp/saver.txt'):\n",
    "    \"\"\"\n",
    "        Tune all the hyperparameters\n",
    "        1) learning rate\n",
    "        2) dropout\n",
    "        3) layer unit number\n",
    "        4) weight decay\n",
    "    \"\"\"\n",
    "    validation_accuracy = {}\n",
    "    validation_f1 = {}\n",
    "    time_total_train = {}\n",
    "#     time_data_load = {}\n",
    "    \n",
    "    res = [{tune_val : Cluster_train_valid_batch_run(clustering_machine, num_layers = num_layers, hidden_neuron_num = hidden_neuron_num, learning_rate = learning_rate, \\\n",
    "                 logging = logging, multilabel = multilabel, norm = norm, precalc = precalc, diag_lambda = diag_lambda, \\\n",
    "                 mini_cluster_num = mini_cluster_num, train_batch_num = train_batch_num, test_batch_num = test_batch_num, \\\n",
    "                 epoch_num = epoch_num, mini_epoch_num = tune_val, dropout = dropout, train_save_name = train_save_name) for tune_val in tune_params} for i in range(repeate_time)]\n",
    "    \n",
    "    for i, ref in enumerate(res):\n",
    "        validation_accuracy[i] = {tune_val : res_lst[0] for tune_val, res_lst in ref.items()}\n",
    "        validation_f1[i] = {tune_val : res_lst[1] for tune_val, res_lst in ref.items()}\n",
    "        time_total_train[i] = {tune_val : res_lst[2] for tune_val, res_lst in ref.items()}\n",
    "#         time_data_load[i] = {tune_val : res_lst[3] for tune_val, res_lst in ref.items()}\n",
    "        \n",
    "    return validation_accuracy, validation_f1, time_total_train\n",
    "\n",
    "def store_data_multi_tuning(tune_params, target, data_name, img_path, comments):\n",
    "    \"\"\"\n",
    "        tune_params: is the tuning parameter list\n",
    "        target: is the result, here should be F1-score, accuraycy, load time, train time\n",
    "    \"\"\"\n",
    "    run_ids = sorted(target.keys())   # key is the run_id\n",
    "    run_data = {'run_id': run_ids}\n",
    "    # the key can be converted to string or not: i.e. str(tune_val)\n",
    "    # here we keep it as integer such that we want it to follow order\n",
    "    tmp = {tune_val : [target[run_id][tune_val] for run_id in run_ids] for tune_val in tune_params}  # the value is list\n",
    "    run_data.update(tmp)\n",
    "    \n",
    "    pickle_filename = img_path + data_name + '_' + comments + '.pkl'\n",
    "    os.makedirs(os.path.dirname(pickle_filename), exist_ok=True)\n",
    "    df = pd.DataFrame(data=run_data, dtype=np.int32)\n",
    "    df.to_pickle(pickle_filename)\n",
    "    return pickle_filename\n",
    "\n",
    "def draw_data_multi_tests(pickle_filename, data_name, comments, xlabel, ylabel):\n",
    "    df = pd.read_pickle(pickle_filename)\n",
    "    df_reshape = df.melt('run_id', var_name = 'model', value_name = ylabel)\n",
    "\n",
    "    plt.clf()\n",
    "    plt.figure()\n",
    "    sns.set(style='whitegrid')\n",
    "    g = sns.catplot(x=\"model\", y=ylabel, kind='box', data=df_reshape)\n",
    "    g.despine(left=True)\n",
    "    g.fig.suptitle(data_name + ' ' + ylabel + ' ' + comments)\n",
    "    g.set_xlabels(xlabel)\n",
    "    g.set_ylabels(ylabel)\n",
    "\n",
    "    img_name = pickle_filename[:-4] + '_img'\n",
    "    os.makedirs(os.path.dirname(img_name), exist_ok=True)\n",
    "    plt.savefig(img_name, bbox_inches='tight')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_tune_param(data, data_name, image_data_path, tmp_folder, partition_nums, layers, hidden_neuron_num = 32, test_batch_num = 2):\n",
    "    node_count = data.x.shape[0]\n",
    "    get_edge_weight(data.edge_index, node_count, store_path = tmp_folder)\n",
    "    \n",
    "    for partn in partition_nums:\n",
    "        for layer_num in layers:\n",
    "            net_layer = layer_num - 1\n",
    "            # Set the tune parameters and name\n",
    "            tune_name = 'batch_epoch_num'\n",
    "            tune_params = [400, 200, 100, 50, 20, 10, 5, 1]\n",
    "#             tune_name = 'weight_decay'\n",
    "#             tune_params = [0.0001, 0.001, 0.01, 0.1]\n",
    "\n",
    "            img_path = image_data_path + 'cluster_num_' + str(partn) + '/' + 'net_layer_' + str(net_layer) + '/'\n",
    "            img_path += 'tune_' + tune_name + '/'  # further subfolder for different task\n",
    "            print('Start tuning for tuning param: ' + tune_name + ' partition num: ' + str(partn) + 'net_layer_' + str(net_layer) )  \n",
    "            \n",
    "            clustering_machine = ClusteringMachine(data.edge_index, data.x, data.y, tmp_folder, info_folder = image_data_path)\n",
    "            clustering_machine.split_whole_nodes_edges_then_cluster(0.05, 0.85, normalize = False, precalc = True)\n",
    "            \n",
    "            validation_accuracy, validation_f1, time_total_train = \\\n",
    "                execute_tuning(tune_params, clustering_machine, repeate_time = 7, num_layers = layer_num, hidden_neuron_num = hidden_neuron_num, learning_rate = 0.001, \\\n",
    "                 logging = False, multilabel = True, norm = True, precalc = True, diag_lambda = 1, \\\n",
    "                 mini_cluster_num = 32, train_batch_num = partn, test_batch_num = test_batch_num, \\\n",
    "                 epoch_num = 400, mini_epoch_num = 20, dropout = 0.3, train_save_name = tmp_folder + 'saver.txt')\n",
    "            \n",
    "\n",
    "            validation_accuracy = store_data_multi_tuning(tune_params,validation_accuracy, data_name, img_path, 'accuracy_cluster_num_' + str(partn))\n",
    "            draw_data_multi_tests(validation_accuracy, data_name, 'Train_cluster_num_' + str(partn) + '_net_layer_' + str(net_layer), tune_name, 'Accuracy')\n",
    "\n",
    "            validation_f1 = store_data_multi_tuning(tune_params, validation_f1, data_name, img_path, 'validation_cluster_num_' + str(partn) )\n",
    "            draw_data_multi_tests(validation_f1, data_name, 'Train_cluster_num_' + str(partn) + '_net_layer_' + str(net_layer), tune_name, 'F1 score')\n",
    "\n",
    "            time_train = store_data_multi_tuning(tune_params, time_total_train, data_name, img_path, 'train_time_cluster_num_' + str(partn) )\n",
    "            draw_data_multi_tests(time_train, data_name, 'train_time_cluster_num_' + str(partn) + '_net_layer_' + str(net_layer), tune_name, 'Train Time (ms)')\n",
    "\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Pytorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_data_root = '/media/xiangli/storage1/projects/tmpdata/'\n",
    "test_folder_name = 'test_gcn/mini_cluster/'\n",
    "\n",
    "from torch_geometric.datasets import Planetoid\n",
    "data_name = 'Cora'\n",
    "dataset = Planetoid(root = local_data_root + 'Planetoid/Cora', name=data_name)\n",
    "data = dataset[0]\n",
    "image_data_path = './results/' + data_name + '/' + test_folder_name\n",
    "# set the current folder as the intermediate data folder so that we can easily copy either clustering \n",
    "intermediate_data_folder = './'\n",
    "partition_nums = [2]\n",
    "layers = [3, 4]\n",
    "\n",
    "tmp_folder = './tmp/'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_machine = ClusteringMachine(data.edge_index, data.x, data.y, tmp_folder, info_folder = image_data_path)\n",
    "\n",
    "clustering_machine.split_whole_nodes_edges_then_cluster(0.05, 0.85, normalize = False, precalc = True)\n",
    "gcn_trainer = ClusterGCNTrainer(clustering_machine.feature_count, clustering_machine.label_count, num_layers = 3, hidden_neuron_num = 16, learning_rate = 0.001, \\\n",
    "                 logging = False, multilabel = True, norm = True, precalc = True)\n",
    "\n",
    "gcn_trainer.train_test_investigate(clustering_machine, seed = 6, diag_lambda = 1, precalc = True, mini_cluster_num = 32, \\\n",
    "                               train_batch_num = 2, test_batch_num = 2, epoch_num = 400, mini_epoch_num = 20, dropout = 0.3, train_save_name = './tmp/saver.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_tune_param(data, data_name, image_data_path, tmp_folder, partition_nums, layers, hidden_neuron_num = 32, test_batch_num = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free GPU memory\n",
    "# !(nvidia-smi | grep 'python' | awk '{ print $3 }' | xargs -n1 kill -9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_1_pytorch_geometric]",
   "language": "python",
   "name": "conda-env-tensorflow_1_pytorch_geometric-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
