{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from utils import filter_out_isolate, draw_cluster_info, draw_isolate_cluster_info, draw_trainer_info, print_data_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "def print_data_info(data):\n",
    "    \n",
    "    print('Info (attributes) of a single data instance')\n",
    "    print(data, '\\n number of nodes: ', data.num_nodes, '\\n number of edges: ', data.num_edges, \\\n",
    "      '\\n number of features per ndoe: ', data.num_node_features, '\\n number of edge features: ', data.num_edge_features, \\\n",
    "      '\\n number of classifying labels of dataset: ', dataset.num_classes, \\\n",
    "      '\\n all the attributes of data: ', data.keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_data_root = '/media/xiangli/storage1/projects/tmpdata/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cora dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info (attributes) of a single data instance\n",
      "Data(edge_index=[2, 10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708]) \n",
      " number of nodes:  2708 \n",
      " number of edges:  10556 \n",
      " number of features per ndoe:  1433 \n",
      " number of edge features:  0 \n",
      " number of classifying labels of dataset:  7 \n",
      " all the attributes of data:  ['x', 'edge_index', 'y', 'train_mask', 'val_mask', 'test_mask']\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "data_name = 'Cora'\n",
    "dataset = Planetoid(root = local_data_root + 'Planetoid/Cora', name=data_name)\n",
    "data = dataset[0]\n",
    "print_data_info(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Citeseer Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info (attributes) of a single data instance\n",
      "Data(edge_index=[2, 9104], test_mask=[3327], train_mask=[3327], val_mask=[3327], x=[3327, 3703], y=[3327]) \n",
      " number of nodes:  3327 \n",
      " number of edges:  9104 \n",
      " number of features per ndoe:  3703 \n",
      " number of edge features:  0 \n",
      " number of classifying labels of dataset:  6 \n",
      " all the attributes of data:  ['x', 'edge_index', 'y', 'train_mask', 'val_mask', 'test_mask']\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "data_name = 'CiteSeer'\n",
    "dataset = Planetoid(root = local_data_root + 'Planetoid/CiteSeer', name=data_name)\n",
    "data = dataset[0]\n",
    "print_data_info(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PubMed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.x\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.tx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.allx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.y\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.ty\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.ally\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.graph\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.test.index\n",
      "Processing...\n",
      "Done!\n",
      "Info (attributes) of a single data instance\n",
      "Data(edge_index=[2, 88648], test_mask=[19717], train_mask=[19717], val_mask=[19717], x=[19717, 500], y=[19717]) \n",
      " number of nodes:  19717 \n",
      " number of edges:  88648 \n",
      " number of features per ndoe:  500 \n",
      " number of edge features:  0 \n",
      " number of classifying labels of dataset:  3 \n",
      " all the attributes of data:  ['x', 'edge_index', 'y', 'train_mask', 'val_mask', 'test_mask']\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "data_name = 'PubMed'\n",
    "dataset = Planetoid(root = local_data_root + 'Planetoid/PubMed', name=data_name)\n",
    "data = dataset[0]\n",
    "print_data_info(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CoraFull dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of data:  1\n",
      "Info (attributes) of a single data instance\n",
      "Data(edge_index=[2, 126842], x=[19793, 8710], y=[19793]) \n",
      " number of nodes:  19793 \n",
      " number of edges:  126842 \n",
      " number of features per ndoe:  8710 \n",
      " number of edge features:  0 \n",
      " number of classifying labels of dataset:  70 \n",
      " all the attributes of data:  ['x', 'edge_index', 'y']\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import CoraFull\n",
    "data_name = 'CoraFull'\n",
    "dataset = CoraFull(root = local_data_root + 'CoralFull')\n",
    "print('number of data: ', len(dataset))\n",
    "data = dataset[0]\n",
    "print_data_info(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coauthor dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/shchur/gnn-benchmark/raw/master/data/npz/ms_academic_cs.npz\n",
      "Processing...\n",
      "Done!\n",
      "number of data:  1\n",
      "Info (attributes) of a single data instance\n",
      "Data(edge_index=[2, 163788], x=[18333, 6805], y=[18333]) \n",
      " number of nodes:  18333 \n",
      " number of edges:  163788 \n",
      " number of features per ndoe:  6805 \n",
      " number of edge features:  0 \n",
      " number of classifying labels of dataset:  15 \n",
      " all the attributes of data:  ['x', 'edge_index', 'y']\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import Coauthor\n",
    "data_name = 'cs'\n",
    "dataset = Coauthor(root = local_data_root + 'Coauthor/' + data_name, name=data_name)\n",
    "print('number of data: ', len(dataset))\n",
    "data = dataset[0]\n",
    "print_data_info(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazon dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of data:  1\n",
      "Info (attributes) of a single data instance\n",
      "Data(edge_index=[2, 491722], x=[13752, 767], y=[13752]) \n",
      " number of nodes:  13752 \n",
      " number of edges:  491722 \n",
      " number of features per ndoe:  767 \n",
      " number of edge features:  0 \n",
      " number of classifying labels of dataset:  10 \n",
      " all the attributes of data:  ['x', 'edge_index', 'y']\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import Amazon\n",
    "data_name = 'computers'    # can also be 'computers'\n",
    "dataset = Amazon(root = local_data_root + 'Amazon/' + data_name, name=data_name)\n",
    "print('number of data: ', len(dataset))\n",
    "data = dataset[0]\n",
    "print_data_info(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://s3.us-east-2.amazonaws.com/dgl.ai/dataset/ppi.zip\n",
      "Extracting /media/xiangli/storage/projects/tmpdata/PPI/PPI/ppi.zip\n",
      "Processing...\n",
      "Done!\n",
      "number of data:  20\n",
      "Info (attributes) of a single data instance\n",
      "Data(edge_index=[2, 32318], x=[1767, 50], y=[1767, 121]) \n",
      " number of nodes:  1767 \n",
      " number of edges:  32318 \n",
      " number of features per ndoe:  50 \n",
      " number of edge features:  0 \n",
      " number of classifying labels of dataset:  121 \n",
      " all the attributes of data:  ['x', 'edge_index', 'y']\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import PPI\n",
    "data_name = 'PPI'    # can also be 'computers'\n",
    "dataset = PPI(root = local_data_root + 'PPI/' + data_name)\n",
    "print('number of data: ', len(dataset))\n",
    "data = dataset[0]\n",
    "print_data_info(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reddit dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://s3.us-east-2.amazonaws.com/dgl.ai/dataset/reddit.zip\n",
      "Extracting /media/xiangli/storage1/projects/tmpdata/Reddit/raw/reddit.zip\n",
      "Processing...\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import Reddit\n",
    "data_name = 'Reddit'    # can also be 'computers'\n",
    "dataset = Reddit(root = local_data_root + '/' + data_name)\n",
    "print('number of data: ', len(dataset))\n",
    "data = dataset[0]\n",
    "print_data_info(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QM7b dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://deepchem.io.s3-website-us-west-1.amazonaws.com/datasets/qm7b.mat\n",
      "Processing...\n",
      "Done!\n",
      "number of data:  7211\n",
      "Info (attributes) of a single data instance\n",
      "Data(edge_attr=[25], edge_index=[2, 25], y=[1, 14]) \n",
      " number of nodes:  5 \n",
      " number of edges:  25 \n",
      " number of features per ndoe:  0 \n",
      " number of edge features:  1 \n",
      " number of classifying labels of dataset:  14 \n",
      " all the attributes of data:  ['edge_index', 'edge_attr', 'y']\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import QM7b\n",
    "data_name = 'QM7b'    # can also be 'computers'\n",
    "dataset = QM7b(root = local_data_root + '/' + data_name)\n",
    "print('number of data: ', len(dataset))\n",
    "data = dataset[0]\n",
    "print_data_info(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QM9 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://www.roemisch-drei.de/qm9.tar.gz\n",
      "Extracting /media/xiangli/storage/projects/tmpdata/QM9/raw/qm9.tar.gz\n",
      "Processing...\n",
      "Done!\n",
      "number of data:  133246\n",
      "Info (attributes) of a single data instance\n",
      "Data(edge_attr=[8, 4], edge_index=[2, 8], pos=[5, 3], x=[5, 13], y=[1, 12]) \n",
      " number of nodes:  5 \n",
      " number of edges:  8 \n",
      " number of features per ndoe:  13 \n",
      " number of edge features:  4 \n",
      " number of classifying labels of dataset:  12 \n",
      " all the attributes of data:  ['x', 'edge_index', 'edge_attr', 'y', 'pos']\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import QM9\n",
    "data_name = 'QM9'    # can also be 'computers'\n",
    "dataset = QM9(root = local_data_root + '/' + data_name)\n",
    "print('number of data: ', len(dataset))\n",
    "data = dataset[0]\n",
    "print_data_info(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://s3.us-east-2.amazonaws.com/dgl.ai/dataset/aifb.tgz\n",
      "Extracting /media/xiangli/storage/projects/tmpdata/Entities/AIFB/aifb.tgz\n",
      "Processing...\n",
      "Done!\n",
      "number of data:  1\n",
      "Info (attributes) of a single data instance\n",
      "Data(edge_index=[2, 58086], edge_norm=[58086], edge_type=[58086], test_idx=[36], test_y=[36], train_idx=[140], train_y=[140]) \n",
      " number of nodes:  8285 \n",
      " number of edges:  58086 \n",
      " number of features per ndoe:  0 \n",
      " number of edge features:  0 \n",
      " number of classifying labels of dataset:  4 \n",
      " all the attributes of data:  ['edge_index', 'edge_type', 'edge_norm', 'train_idx', 'train_y', 'test_idx', 'test_y']\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import Entities\n",
    "data_name = 'AIFB'    # can also be 'computers'\n",
    "dataset = Entities(root = local_data_root + 'Entities/' + data_name, name=data_name)\n",
    "print('number of data: ', len(dataset))\n",
    "data = dataset[0]\n",
    "print_data_info(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Large data scale :  Entities  AM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://s3.us-east-2.amazonaws.com/dgl.ai/dataset/am.tgz\n",
      "Extracting /media/xiangli/storage/projects/tmpdata/Entities/AM/am.tgz\n",
      "Processing...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at CPUAllocator.cpp:64] . DefaultCPUAllocator: can't allocate memory: you tried to allocate 25486294176 bytes. Error code 12 (Cannot allocate memory)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-bc60b553bc30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'AM'\u001b[0m    \u001b[0;31m# can also be 'computers'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEntities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocal_data_root\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'Entities/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'number of data: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint_data_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/torch_geometric/datasets/entities.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, name, transform, pre_transform)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'AIFB'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'AM'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'MUTAG'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'BGS'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEntities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessed_paths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/torch_geometric/data/in_memory_dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, pre_transform, pre_filter)\u001b[0m\n\u001b[1;32m     51\u001b[0m                  pre_filter=None):\n\u001b[1;32m     52\u001b[0m         super(InMemoryDataset, self).__init__(root, transform, pre_transform,\n\u001b[0;32m---> 53\u001b[0;31m                                               pre_filter)\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/torch_geometric/data/dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, pre_transform, pre_filter)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/torch_geometric/data/dataset.py\u001b[0m in \u001b[0;36m_process\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessed_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Done!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/torch_geometric/datasets/entities.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         oh = F.one_hot(\n\u001b[0;32m--> 105\u001b[0;31m             edge_type, num_classes=2 * len(relations)).to(torch.float)\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0mdeg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscatter_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0medge_type\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at CPUAllocator.cpp:64] . DefaultCPUAllocator: can't allocate memory: you tried to allocate 25486294176 bytes. Error code 12 (Cannot allocate memory)\n"
     ]
    }
   ],
   "source": [
    "data_name = 'AM'    # can also be 'computers'\n",
    "dataset = Entities(root = local_data_root + 'Entities/' + data_name, name=data_name)\n",
    "print('number of data: ', len(dataset))\n",
    "data = dataset[0]\n",
    "print_data_info(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://s3.us-east-2.amazonaws.com/dgl.ai/dataset/mutag.tgz\n",
      "Extracting /media/xiangli/storage/projects/tmpdata/Entities/MUTAG/mutag.tgz\n",
      "Processing...\n",
      "Done!\n",
      "number of data:  1\n",
      "Info (attributes) of a single data instance\n",
      "Data(edge_index=[2, 148454], edge_norm=[148454], edge_type=[148454], test_idx=[68], test_y=[68], train_idx=[272], train_y=[272]) \n",
      " number of nodes:  23644 \n",
      " number of edges:  148454 \n",
      " number of features per ndoe:  0 \n",
      " number of edge features:  0 \n",
      " number of classifying labels of dataset:  2 \n",
      " all the attributes of data:  ['edge_index', 'edge_type', 'edge_norm', 'train_idx', 'train_y', 'test_idx', 'test_y']\n"
     ]
    }
   ],
   "source": [
    "data_name = 'MUTAG'    # can also be 'computers'\n",
    "dataset = Entities(root = local_data_root + 'Entities/' + data_name, name=data_name)\n",
    "print('number of data: ', len(dataset))\n",
    "data = dataset[0]\n",
    "print_data_info(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://s3.us-east-2.amazonaws.com/dgl.ai/dataset/bgs.tgz\n",
      "Extracting /media/xiangli/storage/projects/tmpdata/Entities/BGS/bgs.tgz\n",
      "Processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "http://data.bgs.ac.uk/id/EarthMaterialClass/RockName/+^PYSD does not look like a valid URI, trying to serialize this will break.\n",
      "http://data.bgs.ac.uk/id/EarthMaterialClass/RockName/+^PYSD does not look like a valid URI, trying to serialize this will break.\n",
      "http://data.bgs.ac.uk/id/EarthMaterialClass/RockName/+#^PRS does not look like a valid URI, trying to serialize this will break.\n",
      "http://data.bgs.ac.uk/id/EarthMaterialClass/RockName/+#^PRS does not look like a valid URI, trying to serialize this will break.\n",
      "http://data.bgs.ac.uk/id/EarthMaterialClass/RockName/+#^PRS does not look like a valid URI, trying to serialize this will break.\n",
      "http://data.bgs.ac.uk/id/EarthMaterialClass/RockName/+#^RSR does not look like a valid URI, trying to serialize this will break.\n",
      "http://data.bgs.ac.uk/id/EarthMaterialClass/RockName/+#^PRS does not look like a valid URI, trying to serialize this will break.\n",
      "http://data.bgs.ac.uk/id/EarthMaterialClass/RockName/+#^RSR does not look like a valid URI, trying to serialize this will break.\n",
      "http://data.bgs.ac.uk/id/EarthMaterialClass/RockName/+#^RSR does not look like a valid URI, trying to serialize this will break.\n",
      "http://data.bgs.ac.uk/id/EarthMaterialClass/RockName/+#^RSR does not look like a valid URI, trying to serialize this will break.\n",
      "http://data.bgs.ac.uk/id/EarthMaterialClass/RockName/+#^SSR does not look like a valid URI, trying to serialize this will break.\n",
      "http://data.bgs.ac.uk/id/EarthMaterialClass/RockName/+#^SSR does not look like a valid URI, trying to serialize this will break.\n",
      "http://data.bgs.ac.uk/id/EarthMaterialClass/RockName/+^~VIS does not look like a valid URI, trying to serialize this will break.\n",
      "http://data.bgs.ac.uk/id/EarthMaterialClass/RockName/+^PYSD does not look like a valid URI, trying to serialize this will break.\n",
      "http://data.bgs.ac.uk/id/EarthMaterialClass/RockName/+#^RSR does not look like a valid URI, trying to serialize this will break.\n",
      "http://data.bgs.ac.uk/id/EarthMaterialClass/RockName/+#^PRS does not look like a valid URI, trying to serialize this will break.\n",
      "http://data.bgs.ac.uk/id/EarthMaterialClass/RockName/+#^SSR does not look like a valid URI, trying to serialize this will break.\n",
      "http://data.bgs.ac.uk/id/EarthMaterialClass/RockName/+#^RSR does not look like a valid URI, trying to serialize this will break.\n",
      "http://data.bgs.ac.uk/id/EarthMaterialClass/RockName/+#^RSR does not look like a valid URI, trying to serialize this will break.\n",
      "http://data.bgs.ac.uk/id/EarthMaterialClass/RockName/+^PYSD does not look like a valid URI, trying to serialize this will break.\n",
      "http://data.bgs.ac.uk/id/EarthMaterialClass/RockName/+^~VIS does not look like a valid URI, trying to serialize this will break.\n",
      "http://data.bgs.ac.uk/id/EarthMaterialClass/RockName/+^*SSD does not look like a valid URI, trying to serialize this will break.\n",
      "http://data.bgs.ac.uk/id/EarthMaterialClass/RockName/+^PYSD does not look like a valid URI, trying to serialize this will break.\n",
      "http://data.bgs.ac.uk/id/EarthMaterialClass/RockName/+^PYSD does not look like a valid URI, trying to serialize this will break.\n",
      "http://data.bgs.ac.uk/id/EarthMaterialClass/RockName/+^PYSD does not look like a valid URI, trying to serialize this will break.\n",
      "http://data.bgs.ac.uk/id/EarthMaterialClass/RockName/+^PYSD does not look like a valid URI, trying to serialize this will break.\n",
      "http://data.bgs.ac.uk/id/EarthMaterialClass/RockName/+^PYSD does not look like a valid URI, trying to serialize this will break.\n",
      "http://data.bgs.ac.uk/id/EarthMaterialClass/RockName/+^~VIS does not look like a valid URI, trying to serialize this will break.\n",
      "http://data.bgs.ac.uk/id/EarthMaterialClass/RockName/+^~VIS does not look like a valid URI, trying to serialize this will break.\n",
      "http://data.bgs.ac.uk/id/EarthMaterialClass/RockName/+^~VIS does not look like a valid URI, trying to serialize this will break.\n",
      "http://data.bgs.ac.uk/id/EarthMaterialClass/RockName/+^*SSD does not look like a valid URI, trying to serialize this will break.\n",
      "http://data.bgs.ac.uk/id/EarthMaterialClass/RockName/+^*SSD does not look like a valid URI, trying to serialize this will break.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "number of data:  1\n",
      "Info (attributes) of a single data instance\n",
      "Data(edge_index=[2, 1832398], edge_norm=[1832398], edge_type=[1832398], test_idx=[29], test_y=[29], train_idx=[117], train_y=[117]) \n",
      " number of nodes:  333845 \n",
      " number of edges:  1832398 \n",
      " number of features per ndoe:  0 \n",
      " number of edge features:  0 \n",
      " number of classifying labels of dataset:  2 \n",
      " all the attributes of data:  ['edge_index', 'edge_type', 'edge_norm', 'train_idx', 'train_y', 'test_idx', 'test_y']\n"
     ]
    }
   ],
   "source": [
    "data_name = 'BGS'    # can also be 'computers'\n",
    "dataset = Entities(root = local_data_root + 'Entities/' + data_name, name=data_name)\n",
    "print('number of data: ', len(dataset))\n",
    "data = dataset[0]\n",
    "print_data_info(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GEDDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of data:  800\n",
      "Info (attributes) of a single data instance\n",
      "Data(edge_index=[2, 18], i=[1]) \n",
      " number of nodes:  8 \n",
      " number of edges:  18 \n",
      " number of features per ndoe:  0 \n",
      " number of edge features:  0 \n",
      " all the attributes of data:  ['edge_index', 'i']\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import GEDDataset\n",
    "data_name = 'LINUX'    # can also be 'computers'\n",
    "dataset = GEDDataset(root = local_data_root + 'GEDDataset/' + data_name, name=data_name)\n",
    "print('number of data: ', len(dataset))\n",
    "data = dataset[0]\n",
    "print('Info (attributes) of a single data instance')\n",
    "print(data, '\\n number of nodes: ', data.num_nodes, '\\n number of edges: ', data.num_edges, \\\n",
    "  '\\n number of features per ndoe: ', data.num_node_features, '\\n number of edge features: ', data.num_edge_features, \\\n",
    "  '\\n all the attributes of data: ', data.keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNISTSuperpixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://ls7-www.cs.uni-dortmund.de/cvpr_geometric_dl/mnist_superpixels.tar.gz\n",
      "Extracting /media/xiangli/storage/projects/tmpdata/MNISTSuperpixels/raw/mnist_superpixels.tar.gz\n",
      "Processing...\n",
      "Done!\n",
      "number of data:  60000\n",
      "Info (attributes) of a single data instance\n",
      "Data(edge_index=[2, 1399], pos=[75, 2], x=[75, 1], y=[1]) \n",
      " number of nodes:  75 \n",
      " number of edges:  1399 \n",
      " number of features per ndoe:  1 \n",
      " number of edge features:  0 \n",
      " number of classifying labels of dataset:  10 \n",
      " all the attributes of data:  ['x', 'edge_index', 'y', 'pos']\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import MNISTSuperpixels\n",
    "data_name = 'MNISTSuperpixels'    # can also be 'computers'\n",
    "dataset = MNISTSuperpixels(root = local_data_root + '/' + data_name)\n",
    "print('number of data: ', len(dataset))\n",
    "data = dataset[0]\n",
    "print_data_info(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ShapeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://shapenet.cs.stanford.edu/iccv17/partseg/train_data.zip\n",
      "Extracting /media/xiangli/storage/projects/tmpdata/ShapeNet/raw/train_data.zip\n",
      "Downloading https://shapenet.cs.stanford.edu/iccv17/partseg/train_label.zip\n",
      "Extracting /media/xiangli/storage/projects/tmpdata/ShapeNet/raw/train_label.zip\n",
      "Downloading https://shapenet.cs.stanford.edu/iccv17/partseg/val_data.zip\n",
      "Extracting /media/xiangli/storage/projects/tmpdata/ShapeNet/raw/val_data.zip\n",
      "Downloading https://shapenet.cs.stanford.edu/iccv17/partseg/val_label.zip\n",
      "Extracting /media/xiangli/storage/projects/tmpdata/ShapeNet/raw/val_label.zip\n",
      "Downloading https://shapenet.cs.stanford.edu/iccv17/partseg/test_data.zip\n",
      "Extracting /media/xiangli/storage/projects/tmpdata/ShapeNet/raw/test_data.zip\n",
      "Downloading https://shapenet.cs.stanford.edu/iccv17/partseg/test_label.zip\n",
      "Extracting /media/xiangli/storage/projects/tmpdata/ShapeNet/raw/test_label.zip\n",
      "Processing...\n",
      "Done!\n",
      "number of data:  14007\n",
      "Info (attributes) of a single data instance\n",
      "Data(category=[1], pos=[2518, 3], y=[2518]) \n",
      " number of nodes:  2518 \n",
      " number of edges:  None \n",
      " number of features per ndoe:  0 \n",
      " number of edge features:  0 \n",
      " number of classifying labels of dataset:  50 \n",
      " all the attributes of data:  ['y', 'pos', 'category']\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import ShapeNet\n",
    "data_name = 'ShapeNet'    # can also be 'computers'\n",
    "dataset = ShapeNet(root = local_data_root + '/' + data_name)\n",
    "print('number of data: ', len(dataset))\n",
    "data = dataset[0]\n",
    "print_data_info(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCPNetDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of data:  32\n",
      "Info (attributes) of a single data instance\n",
      "Data(pos=[100000, 3], test_idx=[5000], x=[100000, 5]) \n",
      " number of nodes:  100000 \n",
      " number of edges:  None \n",
      " number of features per ndoe:  5 \n",
      " number of edge features:  0 \n",
      " all the attributes of data:  ['x', 'pos', 'test_idx']\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import PCPNetDataset\n",
    "data_name = 'Noisy'    # can also be 'computers'\n",
    "dataset = PCPNetDataset(root = local_data_root + 'PCPNetDataset/' + data_name, category = 'Noisy')\n",
    "print('number of data: ', len(dataset))\n",
    "data = dataset[0]\n",
    "print('Info (attributes) of a single data instance')\n",
    "print(data, '\\n number of nodes: ', data.num_nodes, '\\n number of edges: ', data.num_edges, \\\n",
    "  '\\n number of features per ndoe: ', data.num_node_features, '\\n number of edge features: ', data.num_edge_features, \\\n",
    "  '\\n all the attributes of data: ', data.keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S3DIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the data set here contains 4096 nodes, with a total number of 20291 data\n",
    "\n",
    "This dataset can be used as the mini-batch directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://shapenet.cs.stanford.edu/media/indoor3d_sem_seg_hdf5_data.zip\n",
      "Extracting /media/xiangli/storage/projects/tmpdata/S3DIS/indoor3d_sem_seg_hdf5_data.zip\n",
      "Processing...\n",
      "Done!\n",
      "number of data:  20291\n",
      "Info (attributes) of a single data instance\n",
      "Data(pos=[4096, 3], x=[4096, 6], y=[4096]) \n",
      " number of nodes:  4096 \n",
      " number of edges:  None \n",
      " number of features per ndoe:  6 \n",
      " number of edge features:  0 \n",
      " number of classifying labels of dataset:  13 \n",
      " all the attributes of data:  ['x', 'y', 'pos']\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import S3DIS\n",
    "data_name = 'S3DIS'    # can also be 'computers'\n",
    "dataset = S3DIS(root = local_data_root + '/' + data_name)\n",
    "print('number of data: ', len(dataset))\n",
    "data = dataset[0]\n",
    "print_data_info(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "select = [(idx, data.num_nodes) for idx, data in enumerate(dataset) if data.num_nodes > 5000]\n",
    "print(select)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_geometric]",
   "language": "python",
   "name": "conda-env-pytorch_geometric-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
