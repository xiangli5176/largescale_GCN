{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import metis\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dig into the networkx for graph partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orginal graph G : \n",
      "<class 'networkx.classes.graph.Graph'> [0, 1, 2, 3] [(0, 1), (1, 2), (2, 3)]\n",
      "subgraph H from G: \n",
      "<class 'networkx.classes.graph.Graph'> <class 'networkx.classes.reportviews.NodeView'> [0, 1, 2] [(0, 1), (1, 2)]\n"
     ]
    }
   ],
   "source": [
    "G = nx.path_graph(4)  # or DiGraph, MultiGraph, MultiDiGraph, etc\n",
    "print('orginal graph G : ')\n",
    "print(type(G), list(G.nodes()), list(G.edges))\n",
    "H = G.subgraph([0, 1, 2])\n",
    "print('subgraph H from G: ')\n",
    "print(type(H), type(H.nodes()), list(H.nodes()), list(H.edges))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partition GCN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1) First use this customized GCNConv single layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Parameter\n",
    "from torch_scatter import scatter_add\n",
    "from torch_geometric.nn import MessagePassing\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import add_remaining_self_loops\n",
    "import math\n",
    "\n",
    "def glorot(tensor):\n",
    "    if tensor is not None:\n",
    "        stdv = math.sqrt(6.0 / (tensor.size(-2) + tensor.size(-1)))\n",
    "        tensor.data.uniform_(-stdv, stdv)\n",
    "#         tensor.data.fill_(1.0)   # trivial example\n",
    "        \n",
    "def zeros(tensor):\n",
    "    if tensor is not None:\n",
    "        tensor.data.fill_(0)\n",
    "\n",
    "class custom_GCNConv(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels, improved=False, cached=False,\n",
    "                 bias=True, **kwargs):\n",
    "        super().__init__(aggr='add', **kwargs)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.improved = improved\n",
    "        self.cached = cached\n",
    "\n",
    "        self.weight = Parameter(torch.Tensor(in_channels, out_channels))\n",
    "\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        glorot(self.weight)\n",
    "        zeros(self.bias)\n",
    "        self.cached_result = None\n",
    "        self.cached_num_edges = None\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def norm(edge_index, num_nodes, edge_weight=None, improved=False, dtype=None):\n",
    "        \n",
    "        if edge_weight is None:\n",
    "            edge_weight = torch.ones((edge_index.size(1), ), dtype=dtype, device=edge_index.device)\n",
    "        \n",
    "        fill_value = 1 if not improved else 2\n",
    "        \n",
    "        edge_index, edge_weight = add_remaining_self_loops(\n",
    "            edge_index, edge_weight, fill_value, num_nodes)\n",
    "        \n",
    "        row, col = edge_index\n",
    "        deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "        \n",
    "        return edge_index, deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        \"\"\"\"\"\"\n",
    "        x = torch.matmul(x, self.weight)\n",
    "        \n",
    "        \n",
    "        if self.cached and self.cached_result is not None:\n",
    "            if edge_index.size(1) != self.cached_num_edges:\n",
    "                raise RuntimeError(\n",
    "                    'Cached {} number of edges, but found {}. Please '\n",
    "                    'disable the caching behavior of this layer by removing '\n",
    "                    'the `cached=True` argument in its constructor.'.format(\n",
    "                        self.cached_num_edges, edge_index.size(1)))\n",
    "        \n",
    "        if not self.cached or self.cached_result is None:\n",
    "            self.cached_num_edges = edge_index.size(1)\n",
    "            edge_index, norm = self.norm(edge_index, x.size(0), edge_weight,\n",
    "                                         self.improved, x.dtype)\n",
    "            self.cached_result = edge_index, norm\n",
    "\n",
    "        edge_index, norm = self.cached_result\n",
    "        \n",
    "        return self.propagate(edge_index, x=x, norm=norm)\n",
    "\n",
    "    def message(self, x_j, norm):\n",
    "        return norm.view(-1, 1) * x_j\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        if self.bias is not None:\n",
    "            aggr_out = aggr_out + self.bias\n",
    "        return aggr_out\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}({}, {})'.format(self.__class__.__name__, self.in_channels,\n",
    "                                   self.out_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2) Establish a simple model based on a customized single GCNConv layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Net, self).__init__()\n",
    "        # one trivial example\n",
    "        self.conv1 = custom_GCNConv(in_channels, out_channels)\n",
    "#         self.conv2 = GCNConv(16, dataset.num_classes)\n",
    "\n",
    "    def forward(self, edge_index, features):\n",
    "        \n",
    "        features = self.conv1(features, edge_index)\n",
    "        predictions = F.log_softmax(features, dim=1)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Partition method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import metis\n",
    "import random\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class ClusteringMachine(object):\n",
    "    \"\"\"\n",
    "    Clustering the graph, feature set and label. Performed on the CPU side\n",
    "    \"\"\"\n",
    "    def __init__(self, edge_index, features, label, partition_num = 2):\n",
    "        \"\"\"\n",
    "        :param edge_index: COO format of the edge indices.\n",
    "        :param features: Feature matrix (ndarray).\n",
    "        :param label: label vector (ndarray).\n",
    "        \"\"\"\n",
    "        tmp = edge_index.t().numpy().tolist()\n",
    "#         tmp = edge_index.t().cpu().numpy().tolist()\n",
    "        self.graph = nx.from_edgelist(tmp)\n",
    "        self.edge_index = edge_index\n",
    "        self.features = features\n",
    "        self.label = label\n",
    "        self.partition_num = partition_num\n",
    "        self._set_sizes()\n",
    "\n",
    "    def _set_sizes(self):\n",
    "        \"\"\"\n",
    "        Setting the feature and class count.\n",
    "        \"\"\"\n",
    "        self.node_count = self.features.shape[0]\n",
    "        self.feature_count = self.features.shape[1]    # features all always in the columns\n",
    "        self.label_count = torch.max(self.label)+1\n",
    "\n",
    "    def decompose(self, test_ratio):\n",
    "        \"\"\"\n",
    "        Decomposing the graph, partitioning the features and label, creating Torch arrays.\n",
    "        \"\"\"\n",
    "#         self.metis_clustering()\n",
    "        print(\"\\nRandom graph clustering started.\\n\")\n",
    "        self.random_clustering()\n",
    "        \n",
    "        self.general_data_partitioning(test_ratio)\n",
    "        self.transfer_edges_and_nodes()\n",
    "\n",
    "    # just allocate each node to arandom cluster, store the membership inside each dict\n",
    "    def random_clustering(self):\n",
    "        \"\"\"\n",
    "        Random clustering the nodes.\n",
    "        \"\"\"\n",
    "        self.clusters = [cluster for cluster in range(self.partition_num)]\n",
    "        # randomly divide into two clusters\n",
    "        self.cluster_membership = {node: random.choice(self.clusters) for node in self.graph.nodes()}\n",
    "\n",
    "    def metis_clustering(self):\n",
    "        \"\"\"\n",
    "        Clustering the graph with Metis. For details see:\n",
    "        \"\"\"\n",
    "        (st, parts) = metis.part_graph(self.graph, self.partition_num)\n",
    "        self.clusters = list(set(parts))\n",
    "        self.cluster_membership = {node: membership for node, membership in enumerate(parts)}\n",
    "\n",
    "\n",
    "    def general_data_partitioning(self, test_ratio):\n",
    "        \"\"\"\n",
    "        Creating data partitions and train-test splits.\n",
    "        \"\"\"\n",
    "        self.sg_nodes = {}\n",
    "        self.sg_edges = {}\n",
    "        \n",
    "        self.sg_train_nodes = {}\n",
    "        self.sg_test_nodes = {}\n",
    "        \n",
    "        self.sg_features = {}\n",
    "        self.sg_labels = {}\n",
    "        # for each cluster we have six dicts to separate overall attributes into different clusters\n",
    "        for cluster in self.clusters:\n",
    "            # Returns a SubGraph view of the subgraph induced on nodes.\n",
    "            # The induced subgraph of the graph contains the nodes in nodes and the edges between those nodes.\n",
    "\n",
    "            subgraph = self.graph.subgraph([node for node in sorted(self.graph.nodes()) if self.cluster_membership[node] == cluster])\n",
    "            self.sg_nodes[cluster] = [node for node in sorted(subgraph.nodes())]\n",
    "            # what about the edges outside? Currently ignore those in-between clusters edges\n",
    "\n",
    "            # map each node into it's index inside its own cluster\n",
    "            # create own indices of each node inside its cluster, can we still use the global index ? yes\n",
    "            mapper = {node: i for i, node in enumerate(sorted(self.sg_nodes[cluster]))}\n",
    "\n",
    "            # the edges inside its own partition, from two directions since it is an undirected graph\n",
    "            self.sg_edges[cluster] = [[mapper[edge[0]], mapper[edge[1]]] for edge in subgraph.edges()] +  \\\n",
    "                                       [[mapper[edge[1]], mapper[edge[0]]] for edge in subgraph.edges()]\n",
    "            \n",
    "            # for each cluster divide into train/test groups:\n",
    "            self.sg_train_nodes[cluster], self.sg_test_nodes[cluster] = train_test_split(list(mapper.values()), test_size = test_ratio)\n",
    "            \n",
    "            self.sg_test_nodes[cluster] = sorted(self.sg_test_nodes[cluster])\n",
    "            self.sg_train_nodes[cluster] = sorted(self.sg_train_nodes[cluster])\n",
    "\n",
    "            # extract specific rows from the feature matrix\n",
    "            self.sg_features[cluster] = self.features[self.sg_nodes[cluster],:]\n",
    "            # labels are 1-D tensor with class labels\n",
    "            self.sg_labels[cluster] = self.label[self.sg_nodes[cluster]]\n",
    "\n",
    "    def transfer_edges_and_nodes(self):\n",
    "        \"\"\"\n",
    "        Transfering the data to PyTorch format.\n",
    "        \"\"\"\n",
    "        for cluster in self.clusters:\n",
    "            self.sg_nodes[cluster] = torch.LongTensor(self.sg_nodes[cluster])\n",
    "\n",
    "            self.sg_edges[cluster] = torch.LongTensor(self.sg_edges[cluster]).t()\n",
    "            # Expects input to be <= 2-D tensor and transposes dimensions 0 and 1.\n",
    "\n",
    "            self.sg_train_nodes[cluster] = torch.LongTensor(self.sg_train_nodes[cluster])\n",
    "            self.sg_test_nodes[cluster] = torch.LongTensor(self.sg_test_nodes[cluster])\n",
    "            self.sg_features[cluster] = torch.FloatTensor(self.sg_features[cluster])\n",
    "            self.sg_labels[cluster] = torch.LongTensor(self.sg_labels[cluster])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Partition Graph with trainiing and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class ClusterGCNTrainer(object):\n",
    "    \"\"\"\n",
    "    Training a ClusterGCN.\n",
    "    \"\"\"\n",
    "    def __init__(self, clustering_machine, in_channels, out_channels):\n",
    "        \"\"\"\n",
    "        :param ags: Arguments object.\n",
    "        :param clustering_machine:\n",
    "        \"\"\"  \n",
    "        self.clustering_machine = clustering_machine\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.create_model(in_channels, out_channels)\n",
    "\n",
    "    def create_model(self, in_channels, out_channels):\n",
    "        \"\"\"\n",
    "        Creating a StackedGCN and transferring to CPU/GPU.\n",
    "        \"\"\"\n",
    "        self.model = Net(in_channels, out_channels)\n",
    "#         self.model = StackedGCN(self.args, self.clustering_machine.feature_count, self.clustering_machine.class_count)\n",
    "        self.model = self.model.to(self.device)\n",
    "    \n",
    "    # call the forward function batch by batch\n",
    "    def do_forward_pass(self, cluster):\n",
    "        \"\"\"\n",
    "        Making a forward pass with data from a given partition.\n",
    "        :param cluster: Cluster index.\n",
    "        :return average_loss: Average loss on the cluster.\n",
    "        :return node_count: Number of nodes.\n",
    "        \"\"\"\n",
    "        # the edges inside each clustered have been re-mark with local indices inside the cluster\n",
    "        edges = self.clustering_machine.sg_edges[cluster].to(self.device)\n",
    "        macro_nodes = self.clustering_machine.sg_nodes[cluster].to(self.device)\n",
    "        \n",
    "        # already re-index each node inside its local cluster\n",
    "        train_nodes = self.clustering_machine.sg_train_nodes[cluster].to(self.device)\n",
    "        # features has the implicit index, for N1 by K matrix, it always implies node indices: [0, N1-1]\n",
    "        features = self.clustering_machine.sg_features[cluster].to(self.device)\n",
    "        # torch.squeeze()  removes all the dimension with value 1, change the target from 2-D  (N by 1) into 1-D N tensor\n",
    "        \n",
    "        target = self.clustering_machine.sg_labels[cluster].to(self.device)\n",
    "        \n",
    "        # calculate the probabilites from log_sofmax\n",
    "        predictions = self.model(edges, features)\n",
    "        # calculate the loss scalar from a cluster, only use the train nodes\n",
    "#         average_loss = torch.nn.functional.nll_loss(predictions[train_nodes], target[train_nodes])\n",
    "        average_loss = torch.nn.functional.nll_loss(predictions, target)\n",
    "#         print(average_loss)\n",
    "#         print('after the GCN based model forward, loss type and shape: ', type(average_loss), average_loss.shape, average_loss.item())\n",
    "        \n",
    "        node_count = train_nodes.shape[0]\n",
    "\n",
    "        # for each cluster keep track of the counts of the nodes\n",
    "        return average_loss, node_count\n",
    "\n",
    "    def update_average_loss(self, batch_average_loss, node_count):\n",
    "        \"\"\"\n",
    "        Updating the average loss in the epoch.\n",
    "        :param batch_average_loss: Loss of the cluster. \n",
    "        :param node_count: Number of nodes in currently processed cluster.\n",
    "        :return average_loss: Average loss in the epoch.\n",
    "        \"\"\"\n",
    "        self.accumulated_training_loss = self.accumulated_training_loss + batch_average_loss.item()*node_count\n",
    "        self.node_count_seen = self.node_count_seen + node_count\n",
    "        average_loss = self.accumulated_training_loss/self.node_count_seen\n",
    "        return average_loss\n",
    "\n",
    "    def do_prediction(self, cluster):\n",
    "        \"\"\"\n",
    "        Scoring a cluster.\n",
    "        :param cluster: Cluster index.\n",
    "        :return prediction: Prediction matrix with probabilities.\n",
    "        :return target: Target vector.\n",
    "        \"\"\"\n",
    "        edges = self.clustering_machine.sg_edges[cluster].to(self.device)\n",
    "        macro_nodes = self.clustering_machine.sg_nodes[cluster].to(self.device)\n",
    "        test_nodes = self.clustering_machine.sg_test_nodes[cluster].to(self.device)\n",
    "        features = self.clustering_machine.sg_features[cluster].to(self.device)\n",
    "\n",
    "        target = self.clustering_machine.sg_labels[cluster].to(self.device).squeeze()\n",
    "        target = target[test_nodes]\n",
    "        # when we do the model forward, we still use all the samples in one cluster, indludng both test and train\n",
    "        prediction = self.model(edges, features)\n",
    "        prediction = prediction[test_nodes,:]\n",
    "        return prediction, target\n",
    "\n",
    "    # iterate through epoch and also the clusters\n",
    "    def train(self, epoch_num=10, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Training a model.\n",
    "        \"\"\"\n",
    "        print(\"Training started.\\n\")\n",
    "\n",
    "#         epochs = trange(epoch_num, desc = \"Train Loss\")\n",
    "        epochs = tqdm(range(epoch_num), desc = \"Train Loss\")\n",
    "        # A shortcut for tqdm(xrange(args), *kwargs). On Python3+ range is used instead of xrange.\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        self.model.train()\n",
    "\n",
    "        for epoch in epochs:\n",
    "            random.shuffle(self.clustering_machine.clusters)\n",
    "            self.node_count_seen = 0\n",
    "            self.accumulated_training_loss = 0\n",
    "            for cluster in self.clustering_machine.clusters:\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                batch_average_loss, node_count = self.do_forward_pass(cluster)\n",
    "                batch_average_loss.requres_grad = True\n",
    "                batch_average_loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                average_loss = self.update_average_loss(batch_average_loss, node_count)\n",
    "            epochs.set_description(\"Train Loss: %g\" % round(average_loss,4))\n",
    "\n",
    "    def test(self):\n",
    "        \"\"\"\n",
    "        Scoring the test and printing the F-1 score.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        self.predictions = []\n",
    "        self.targets = []\n",
    "        for cluster in self.clustering_machine.clusters:\n",
    "            prediction, target = self.do_prediction(cluster)\n",
    "\n",
    "            self.predictions.append(prediction.cpu().detach().numpy())\n",
    "            self.targets.append(target.cpu().detach().numpy())\n",
    "        \n",
    "        self.targets = np.concatenate(self.targets)\n",
    "\n",
    "        self.predictions = np.concatenate(self.predictions).argmax(1)  # return the indices of maximum probability \n",
    "\n",
    "        \n",
    "        score = f1_score(self.targets, self.predictions, average=\"micro\")\n",
    "        print(\"\\nF-1 score: {:.4f}\".format(score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4994, 0.5789, 0.8399],\n",
      "        [0.4115, 0.2692, 0.5330],\n",
      "        [0.1837, 0.5959, 0.2002],\n",
      "        [0.3653, 0.0972, 0.6054],\n",
      "        [0.0648, 0.5534, 0.1197],\n",
      "        [0.5768, 0.7887, 0.6853],\n",
      "        [0.3990, 0.5463, 0.5564],\n",
      "        [0.5065, 0.7685, 0.1517],\n",
      "        [0.4716, 0.3974, 0.3875],\n",
      "        [0.1711, 0.1506, 0.2569]])\n"
     ]
    }
   ],
   "source": [
    "edge_index = torch.tensor([[0, 1, 0, 8, 1, 3, 1, 2, 4, 2, 4, 6, 6, 7, 7, 9, 2, 5, 5, 9, 9, 8], \n",
    "                           [1, 0, 8, 0, 3, 1, 2, 1, 2, 4, 6, 4, 7, 6, 9, 7, 5, 2, 9, 5, 8, 9]])\n",
    "features = torch.rand(10, 3)\n",
    "label = torch.tensor([0, 0, 1, 1, 1, 0, 1, 0, 1, 1])\n",
    "\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random graph clustering started.\n",
      "\n",
      "{0: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])}\n",
      "{0: tensor([[0, 0, 1, 1, 8, 2, 2, 4, 6, 7, 9, 1, 8, 3, 2, 9, 4, 5, 6, 7, 9, 5],\n",
      "        [1, 8, 3, 2, 9, 4, 5, 6, 7, 9, 5, 0, 0, 1, 1, 8, 2, 2, 4, 6, 7, 9]])}\n"
     ]
    }
   ],
   "source": [
    "clustering_machine = ClusteringMachine(edge_index, features, label, partition_num = 2)\n",
    "clustering_machine.decompose(0.1)\n",
    "\n",
    "print(clustering_machine.sg_nodes)\n",
    "print(clustering_machine.sg_edges)\n",
    "\n",
    "gcn_trainer = ClusterGCNTrainer(clustering_machine, 3, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1005, 0.7285, 0.1923],\n",
      "        [0.6998, 0.1699, 0.6603],\n",
      "        [0.7601, 0.4222, 0.8662],\n",
      "        [0.5915, 0.7944, 0.2057],\n",
      "        [0.5081, 0.7098, 0.7875],\n",
      "        [0.3620, 0.6342, 0.7338],\n",
      "        [0.7700, 0.4322, 0.9169],\n",
      "        [0.4816, 0.7911, 0.9840],\n",
      "        [0.9101, 0.4922, 0.1306],\n",
      "        [0.2412, 0.8277, 0.6060]])\n"
     ]
    }
   ],
   "source": [
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b24e7c2d5b4b4b6bbd8fc74ff5e62b11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Train Loss', max=200, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gcn_trainer.train(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use data from pytorch geometric datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 3 500\n",
      "Info (attributes) of a single data instance\n",
      "Data(edge_index=[2, 88648], test_mask=[19717], train_mask=[19717], val_mask=[19717], x=[19717, 500], y=[19717]) \n",
      " number of nodes:  19717 \n",
      " number of edges:  88648 \n",
      " number of features per ndoe:  500 \n",
      " number of edge features:  0 \n",
      " all the attributes of data:  ['x', 'edge_index', 'y', 'train_mask', 'val_mask', 'test_mask']\n"
     ]
    }
   ],
   "source": [
    "# this data is also used the in the trivial example of the cluster-GCN paper\n",
    "from torch_geometric.datasets import Planetoid\n",
    "dataset = Planetoid(root='~/tmp/Planetoid/PubMed', name='PubMed')\n",
    "print(len(dataset), dataset.num_classes, dataset.num_node_features)\n",
    "data = dataset[0]\n",
    "print('Info (attributes) of a single data instance')\n",
    "print(data, '\\n number of nodes: ', data.num_nodes, '\\n number of edges: ', data.num_edges, \\\n",
    "      '\\n number of features per ndoe: ', data.num_node_features, '\\n number of edge features: ', data.num_edge_features, \\\n",
    "      '\\n all the attributes of data: ', data.keys)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# data = data.to(device)\n",
    "\n",
    "# x, edge_index, y = data.x.cuda(), data.edge_index.cuda(), data.y.cuda()\n",
    "# print(edge_index[:,0])\n",
    "# tmp = edge_index.t().cpu().numpy().tolist()\n",
    "# print(tmp[1])\n",
    "\n",
    "# conv1 = custom_GCNConv(dataset.num_node_features, 2).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute the GCN partitioning graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random graph clustering started.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clustering_machine = ClusteringMachine(data.edge_index, data.x, data.y, partition_num = 2)\n",
    "clustering_machine.decompose(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: tensor([    0,     3,     4,  ..., 19712, 19714, 19715]), 1: tensor([    1,     2,     5,  ..., 19710, 19713, 19716])}\n",
      "{0: tensor([[   0,    0,    0,  ..., 9846, 6933, 9685],\n",
      "        [3049, 3809, 7242,  ..., 6582, 6748, 9044]]), 1: tensor([[   0,    0,    1,  ..., 9786, 9508, 9584],\n",
      "        [1495, 4174, 5252,  ..., 9470, 9507, 9515]])}\n"
     ]
    }
   ],
   "source": [
    "print(clustering_machine.sg_nodes)\n",
    "print(clustering_machine.sg_edges)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn_trainer = ClusterGCNTrainer(clustering_machine, dataset.num_node_features, dataset.num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8993ab7f32a415a82a816567f585f2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Train Loss', max=200, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gcn_trainer.train(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 22])\n",
      "<class 'networkx.classes.graph.Graph'>\n",
      "<class 'networkx.classes.reportviews.NodeView'> [0, 1, 8, 3, 2, 4, 6, 7, 9, 5]\n",
      "[0, 1]\n",
      "{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1}\n",
      "<class 'networkx.classes.graph.Graph'>\n",
      "[(8, 9), (6, 7), (7, 9), (9, 5)]\n",
      "11\n",
      "[5, 6, 7, 8, 9]\n",
      "{5: 0, 6: 1, 7: 2, 8: 3, 9: 4}\n",
      "[[3, 4], [1, 2], [2, 4], [4, 0], [4, 3], [2, 1], [4, 2], [0, 4]]\n",
      "[[8, 9], [6, 7], [7, 9], [9, 5], [9, 8], [7, 6], [9, 7], [5, 9]]\n",
      "\n",
      "investigate the point of mapper:\n",
      "[4, 2, 3, 1] [0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# trial on the subgraph\n",
    "\n",
    "edge_index = torch.tensor([[0, 1, 0, 8, 1, 3, 1, 2, 4, 2, 4, 6, 6, 7, 7, 9, 2, 5, 5, 9, 9, 8], \n",
    "                           [1, 0, 8, 0, 3, 1, 2, 1, 2, 4, 6, 4, 7, 6, 9, 7, 5, 2, 9, 5, 8, 9]])\n",
    "\n",
    "print(edge_index.shape)\n",
    "tmp = edge_index.t().numpy().tolist()\n",
    "\n",
    "graph = nx.from_edgelist(tmp)\n",
    "# or  G=nx.Graph(edgelist) # use Graph constructor\n",
    "# here Graph class is for undirected graph, for directed: use DiGraph class instead\n",
    "print(type(graph))\n",
    "# begin to partition\n",
    "partition_num = 2\n",
    "clusters = [cluster for cluster in range(partition_num)]\n",
    "print(type(graph.nodes()), graph.nodes())\n",
    "# randomdized \n",
    "# cluster_membership = {node: random.choice(clusters) for node in graph.nodes()}\n",
    "\n",
    "# trivial for test case\n",
    "node_count = 10\n",
    "cluster_membership = {node: 0 for node in range(node_count//partition_num)}\n",
    "cluster_membership.update({node: 1 for node in range(node_count//partition_num, node_count)})\n",
    "\n",
    "print(clusters)\n",
    "print(cluster_membership)\n",
    "\n",
    "# for subfgraph\n",
    "cluster = 1\n",
    "# print(graph.nodes())\n",
    "\n",
    "subgraph = graph.subgraph([node for node in sorted(graph.nodes()) if cluster_membership[node] == cluster])\n",
    "print(type(subgraph))\n",
    "# for undirected graph, it is only stored once in networkx class\n",
    "print(subgraph.edges())\n",
    "print(len(graph.edges()))\n",
    "\n",
    "sg_nodes = {}\n",
    "sg_edges = {}\n",
    "\n",
    "sg_nodes[cluster] = [node for node in sorted(subgraph.nodes())]\n",
    "print(sg_nodes[cluster])  # just convert it into a list of nodes and store inside the dict\n",
    "mapper = {node: i for i, node in enumerate(sorted(sg_nodes[cluster]))}\n",
    "print(mapper)\n",
    "mapper2 = {node: node for i, node in enumerate(sorted(sg_nodes[cluster]))}\n",
    "\n",
    "# the edges inside its own partition, from two directions since it is an undirected graph\n",
    "sg_edges[cluster] = [[mapper[edge[0]], mapper[edge[1]]] for edge in subgraph.edges()] +  \\\n",
    "                           [[mapper[edge[1]], mapper[edge[0]]] for edge in subgraph.edges()]\n",
    "# there will be no key errors since all is inside subgraph\n",
    "original = [[mapper2[edge[0]], mapper2[edge[1]]] for edge in subgraph.edges()] +  \\\n",
    "                           [[mapper2[edge[1]], mapper2[edge[0]]] for edge in subgraph.edges()]\n",
    "\n",
    "print(sg_edges[cluster])\n",
    "print(original)\n",
    "# investigate the point of mapper:\n",
    "print()\n",
    "print('investigate the point of mapper:')\n",
    "sg_train_nodes = {}\n",
    "sg_test_nodes = {}\n",
    "sg_train_nodes[cluster], sg_test_nodes[cluster] = train_test_split(list(mapper.values()), test_size = 0.1)\n",
    "\n",
    "print(sg_train_nodes[cluster], sg_test_nodes[cluster])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "14",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-254-2a524e8bfa11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m17\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m19\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-254-2a524e8bfa11>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m17\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m19\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 14"
     ]
    }
   ],
   "source": [
    "ref = {15:3, 16:2, 17:1}\n",
    "a = list(range(14, 19))\n",
    "b = [ref[node] for node in a]\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_geometric]",
   "language": "python",
   "name": "conda-env-pytorch_geometric-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
