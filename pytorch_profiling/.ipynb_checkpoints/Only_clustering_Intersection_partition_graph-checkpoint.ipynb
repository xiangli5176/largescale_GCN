{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import metis\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dig into the networkx for graph partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orginal graph G : \n",
      "<class 'networkx.classes.graph.Graph'> [0, 1, 2, 3] [(0, 1), (1, 2), (2, 3)]\n",
      "subgraph H from G: \n",
      "<class 'networkx.classes.graph.Graph'> <class 'networkx.classes.reportviews.NodeView'> [0, 1, 2] [(0, 1), (1, 2)]\n"
     ]
    }
   ],
   "source": [
    "G = nx.path_graph(4)  # or DiGraph, MultiGraph, MultiDiGraph, etc\n",
    "print('orginal graph G : ')\n",
    "print(type(G), list(G.nodes()), list(G.edges))\n",
    "H = G.subgraph([0, 1, 2])\n",
    "print('subgraph H from G: ')\n",
    "print(type(H), type(H.nodes()), list(H.nodes()), list(H.edges))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partition GCN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1) First use this customized GCNConv single layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Parameter\n",
    "from torch_scatter import scatter_add\n",
    "from torch_geometric.nn import MessagePassing\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import add_remaining_self_loops\n",
    "import math\n",
    "\n",
    "def glorot(tensor):\n",
    "    if tensor is not None:\n",
    "        stdv = math.sqrt(6.0 / (tensor.size(-2) + tensor.size(-1)))\n",
    "        tensor.data.uniform_(-stdv, stdv)\n",
    "#         tensor.data.fill_(1.0)   # trivial example\n",
    "        \n",
    "def zeros(tensor):\n",
    "    if tensor is not None:\n",
    "        tensor.data.fill_(0)\n",
    "\n",
    "class custom_GCNConv(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels, improved=False, cached=False,\n",
    "                 bias=True, **kwargs):\n",
    "        super().__init__(aggr='add', **kwargs)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.improved = improved\n",
    "        self.cached = cached\n",
    "\n",
    "        self.weight = Parameter(torch.Tensor(in_channels, out_channels))\n",
    "\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        glorot(self.weight)\n",
    "        zeros(self.bias)\n",
    "        self.cached_result = None\n",
    "        self.cached_num_edges = None\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def norm(edge_index, num_nodes, edge_weight=None, improved=False, dtype=None):\n",
    "        \n",
    "        if edge_weight is None:\n",
    "            edge_weight = torch.ones((edge_index.size(1), ), dtype=dtype, device=edge_index.device)\n",
    "        \n",
    "        fill_value = 1 if not improved else 2\n",
    "        \n",
    "        edge_index, edge_weight = add_remaining_self_loops(\n",
    "            edge_index, edge_weight, fill_value, num_nodes)\n",
    "        \n",
    "        row, col = edge_index\n",
    "        deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "        \n",
    "        return edge_index, deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        \"\"\"\"\"\"\n",
    "        x = torch.matmul(x, self.weight)\n",
    "        \n",
    "        \n",
    "        if self.cached and self.cached_result is not None:\n",
    "            if edge_index.size(1) != self.cached_num_edges:\n",
    "                raise RuntimeError(\n",
    "                    'Cached {} number of edges, but found {}. Please '\n",
    "                    'disable the caching behavior of this layer by removing '\n",
    "                    'the `cached=True` argument in its constructor.'.format(\n",
    "                        self.cached_num_edges, edge_index.size(1)))\n",
    "        \n",
    "        if not self.cached or self.cached_result is None:\n",
    "            self.cached_num_edges = edge_index.size(1)\n",
    "            edge_index, norm = self.norm(edge_index, x.size(0), edge_weight,\n",
    "                                         self.improved, x.dtype)\n",
    "            self.cached_result = edge_index, norm\n",
    "\n",
    "        edge_index, norm = self.cached_result\n",
    "        \n",
    "        return self.propagate(edge_index, x=x, norm=norm)\n",
    "\n",
    "    def message(self, x_j, norm):\n",
    "        return norm.view(-1, 1) * x_j\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        if self.bias is not None:\n",
    "            aggr_out = aggr_out + self.bias\n",
    "        return aggr_out\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}({}, {})'.format(self.__class__.__name__, self.in_channels,\n",
    "                                   self.out_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2) Establish a simple model based on a customized single GCNConv layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Net, self).__init__()\n",
    "        # one trivial example\n",
    "        self.conv1 = custom_GCNConv(in_channels, out_channels)\n",
    "#         self.conv2 = GCNConv(16, dataset.num_classes)\n",
    "\n",
    "    def forward(self, edge_index, features):\n",
    "        \n",
    "        features = self.conv1(features, edge_index)\n",
    "        predictions = F.log_softmax(features, dim=1)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Partition method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import metis\n",
    "import random\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class ClusteringMachine(object):\n",
    "    \"\"\"\n",
    "    Clustering the graph, feature set and label. Performed on the CPU side\n",
    "    \"\"\"\n",
    "    def __init__(self, edge_index, features, label, partition_num = 2):\n",
    "        \"\"\"\n",
    "        :param edge_index: COO format of the edge indices.\n",
    "        :param features: Feature matrix (ndarray).\n",
    "        :param label: label vector (ndarray).\n",
    "        \"\"\"\n",
    "        tmp = edge_index.t().numpy().tolist()\n",
    "#         tmp = edge_index.t().cpu().numpy().tolist()\n",
    "        self.graph = nx.from_edgelist(tmp)\n",
    "        self.edge_index = edge_index\n",
    "        self.features = features\n",
    "        self.label = label\n",
    "        self.partition_num = partition_num\n",
    "        self._set_sizes()\n",
    "\n",
    "    def _set_sizes(self):\n",
    "        \"\"\"\n",
    "        Setting the feature and class count.\n",
    "        \"\"\"\n",
    "        self.node_count = self.features.shape[0]\n",
    "        self.feature_count = self.features.shape[1]    # features all always in the columns\n",
    "        self.label_count = torch.max(self.label)+1\n",
    "\n",
    "    def decompose(self, test_ratio):\n",
    "        \"\"\"\n",
    "        Decomposing the graph, partitioning the features and label, creating Torch arrays.\n",
    "        \"\"\"\n",
    "#         self.metis_clustering()\n",
    "        print(\"\\nRandom graph clustering started.\\n\")\n",
    "        self.random_clustering()\n",
    "        \n",
    "        self.general_data_partitioning(test_ratio)\n",
    "        self.general_data_intersect(test_ratio)\n",
    "        \n",
    "        self.transfer_edges_and_nodes()\n",
    "\n",
    "    # just allocate each node to arandom cluster, store the membership inside each dict\n",
    "    def random_clustering(self):\n",
    "        \"\"\"\n",
    "        Random clustering the nodes.\n",
    "        \"\"\"\n",
    "        self.clusters = [cluster for cluster in range(self.partition_num)]\n",
    "        # randomly divide into two clusters\n",
    "#         self.cluster_membership = {node: random.choice(self.clusters) for node in self.graph.nodes()}\n",
    "        # for trial case : half-half devision, single part one or two partitions\n",
    "        self.cluster_membership = {node: 0 for node in range(self.node_count // self.partition_num)}\n",
    "        if self.partition_num > 1:\n",
    "            self.cluster_membership.update({node: 1 for node in range(self.node_count // self.partition_num, self.node_count)})\n",
    "        \n",
    "        # independent of the clustering method:\n",
    "        self.intersect_cluster = []\n",
    "        for i in range(1, self.partition_num):\n",
    "            tmp = [(m, n) for m, n in zip(self.clusters, self.clusters[i:])]\n",
    "            self.intersect_cluster.extend(tmp)\n",
    "            \n",
    "        self.macro_inter_edges = set(self.graph.edges())\n",
    "\n",
    "    def metis_clustering(self):\n",
    "        \"\"\"\n",
    "        Clustering the graph with Metis. For details see:\n",
    "        \"\"\"\n",
    "        (st, parts) = metis.part_graph(self.graph, self.partition_num)\n",
    "        self.clusters = list(set(parts))\n",
    "        self.cluster_membership = {node: membership for node, membership in enumerate(parts)}\n",
    "\n",
    "\n",
    "    def general_data_partitioning(self, test_ratio):\n",
    "        \"\"\"\n",
    "        Creating data partitions and train-test splits.\n",
    "        \"\"\"\n",
    "        self.sg_nodes = {}\n",
    "        self.sg_edges = {}\n",
    "        \n",
    "        self.sg_train_nodes = {}\n",
    "        self.sg_test_nodes = {}\n",
    "        \n",
    "        self.sg_features = {}\n",
    "        self.sg_labels = {}\n",
    "        \n",
    "        \n",
    "        # for each cluster we have six dicts to separate overall attributes into different clusters\n",
    "        for cluster in self.clusters:\n",
    "            # Returns a SubGraph view of the subgraph induced on nodes.\n",
    "            # The induced subgraph of the graph contains the nodes in nodes and the edges between those nodes.\n",
    "\n",
    "            subgraph = self.graph.subgraph([node for node in sorted(self.graph.nodes()) if self.cluster_membership[node] == cluster])\n",
    "            self.sg_nodes[cluster] = [node for node in sorted(subgraph.nodes())]\n",
    "            \n",
    "            # what about the edges outside? Currently ignore those in-between clusters edges\n",
    "\n",
    "            # map each node into it's index inside its own cluster\n",
    "            # create own indices of each node inside its cluster, can we still use the global index ? yes\n",
    "            # we sort the nodes is to make sure it is consistent with the features and labels, nothing to do with edges\n",
    "            mapper = {node: i for i, node in enumerate(sorted(self.sg_nodes[cluster]))}\n",
    "\n",
    "            # the edges inside its own partition, from two directions since it is an undirected graph\n",
    "            # the edges order does not matter\n",
    "            self.sg_edges[cluster] = [[mapper[edge[0]], mapper[edge[1]]] for edge in subgraph.edges()] +  \\\n",
    "                                       [[mapper[edge[1]], mapper[edge[0]]] for edge in subgraph.edges()]\n",
    "            \n",
    "            # update the macro inter-sect edges:\n",
    "            # because graph.subgraph's edge index may not follow the same order as graph.edges()\n",
    "            self.macro_inter_edges -= set([(edge[0], edge[1]) for edge in subgraph.edges()] +  \\\n",
    "                                       [(edge[1], edge[0]) for edge in subgraph.edges()])\n",
    "            print('cluster: ', cluster, 'mcro_inter_edges: ', self.macro_inter_edges)\n",
    "            \n",
    "            # for each cluster divide into train/test groups:\n",
    "            self.sg_train_nodes[cluster], self.sg_test_nodes[cluster] = train_test_split(list(mapper.values()), test_size = test_ratio)\n",
    "            \n",
    "            self.sg_test_nodes[cluster] = sorted(self.sg_test_nodes[cluster])\n",
    "            self.sg_train_nodes[cluster] = sorted(self.sg_train_nodes[cluster])\n",
    "\n",
    "            # extract specific rows from the feature matrix\n",
    "            self.sg_features[cluster] = self.features[self.sg_nodes[cluster],:]\n",
    "            # labels are 1-D tensor with class labels\n",
    "            self.sg_labels[cluster] = self.label[self.sg_nodes[cluster]]\n",
    "\n",
    "    def general_data_intersect(self, test_ratio):\n",
    "        \"\"\"\n",
    "            create data intersection between different isolate partitions of data\n",
    "        \"\"\"\n",
    "        self.inter_nodes = defaultdict(set)\n",
    "        self.inter_edges = defaultdict(list)\n",
    "        \n",
    "        self.inter_train_nodes = {}\n",
    "        self.inter_test_nodes = {}\n",
    "        \n",
    "        self.inter_features = {}\n",
    "        self.inter_labels = {}\n",
    "        \n",
    "        # first we have to divide the remaining inter-cluster nodes and edges, in isolate cluster, we know that be graph.nodes() or graph.edges()\n",
    "        # here we only know the edges as stored inside the macro_inter_edges\n",
    "        for start, end in self.macro_inter_edges:\n",
    "            start_cluster_id = self.cluster_membership[start]\n",
    "            end_cluster_id = self.cluster_membership[end]\n",
    "            \n",
    "            # make sure the smaller is before the larger, to be consistent with the rule as indicated by the self.intersect_cluster\n",
    "            if start_cluster_id > end_cluster_id:\n",
    "                start_cluster_id, end_cluster_id = end_cluster_id, start_cluster_id\n",
    "            \n",
    "            self.inter_nodes[(start_cluster_id, end_cluster_id)] |= {start, end}\n",
    "            \n",
    "            # the edges inside its own partition, from two directions since it is an undirected graph\n",
    "            self.inter_edges[(start_cluster_id, end_cluster_id)].append([start, end])\n",
    "        \n",
    "        for inter_cluster in self.intersect_cluster:\n",
    "            self.inter_nodes[inter_cluster] = sorted(list(self.inter_nodes[inter_cluster]))\n",
    "            \n",
    "            # need to use the mapper re-index the edges\n",
    "            mapper = {node: i for i, node in enumerate(self.inter_nodes[inter_cluster])}\n",
    "            self.inter_edges[inter_cluster] = [[mapper[edge[0]], mapper[edge[1]]] for edge in self.inter_edges[inter_cluster]] +  \\\n",
    "                                       [[mapper[edge[1]], mapper[edge[0]]] for edge in self.inter_edges[inter_cluster]]\n",
    "            \n",
    "            # for each cluster divide into train/test groups:\n",
    "            self.inter_train_nodes[inter_cluster], self.inter_test_nodes[inter_cluster] = train_test_split(list(mapper.values()), test_size = test_ratio)\n",
    "            \n",
    "            self.inter_test_nodes[inter_cluster] = sorted(self.inter_test_nodes[inter_cluster])\n",
    "            self.inter_train_nodes[inter_cluster] = sorted(self.inter_train_nodes[inter_cluster])\n",
    "\n",
    "            # extract specific rows from the feature matrix\n",
    "            self.inter_features[inter_cluster] = self.features[self.inter_nodes[inter_cluster],:]\n",
    "            # labels are 1-D tensor with class labels\n",
    "            self.inter_labels[inter_cluster] = self.label[self.inter_nodes[inter_cluster]]\n",
    "        \n",
    "            \n",
    "    def transfer_edges_and_nodes(self):\n",
    "        \"\"\"\n",
    "        Transfering the data to PyTorch format.\n",
    "        \"\"\"\n",
    "        for cluster in self.clusters:\n",
    "            # for the isolated-cluster data\n",
    "            self.sg_nodes[cluster] = torch.LongTensor(self.sg_nodes[cluster])\n",
    "\n",
    "            self.sg_edges[cluster] = torch.LongTensor(self.sg_edges[cluster]).t()\n",
    "            # Expects input to be <= 2-D tensor and transposes dimensions 0 and 1.\n",
    "            self.sg_train_nodes[cluster] = torch.LongTensor(self.sg_train_nodes[cluster])\n",
    "            self.sg_test_nodes[cluster] = torch.LongTensor(self.sg_test_nodes[cluster])\n",
    "            self.sg_features[cluster] = torch.FloatTensor(self.sg_features[cluster])\n",
    "            self.sg_labels[cluster] = torch.LongTensor(self.sg_labels[cluster])\n",
    "        \n",
    "        for inter_cluster in self.intersect_cluster:\n",
    "            # for the inter-cluster data\n",
    "            self.inter_nodes[inter_cluster] = torch.LongTensor(self.inter_nodes[inter_cluster])\n",
    "\n",
    "            self.inter_edges[inter_cluster] = torch.LongTensor(self.inter_edges[inter_cluster]).t()\n",
    "            # Expects input to be <= 2-D tensor and transposes dimensions 0 and 1.\n",
    "            self.inter_train_nodes[inter_cluster] = torch.LongTensor(self.inter_train_nodes[inter_cluster])\n",
    "            self.inter_test_nodes[inter_cluster] = torch.LongTensor(self.inter_test_nodes[inter_cluster])\n",
    "            self.inter_features[inter_cluster] = torch.FloatTensor(self.inter_features[inter_cluster])\n",
    "            self.inter_labels[inter_cluster] = torch.LongTensor(self.inter_labels[inter_cluster])\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Partition Graph with trainiing and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class ClusterGCNTrainer(object):\n",
    "    \"\"\"\n",
    "    Training a ClusterGCN.\n",
    "    \"\"\"\n",
    "    def __init__(self, clustering_machine, in_channels, out_channels):\n",
    "        \"\"\"\n",
    "        :param ags: Arguments object.\n",
    "        :param clustering_machine:\n",
    "        \"\"\"  \n",
    "        self.clustering_machine = clustering_machine\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.create_model(in_channels, out_channels)\n",
    "\n",
    "    def create_model(self, in_channels, out_channels):\n",
    "        \"\"\"\n",
    "        Creating a StackedGCN and transferring to CPU/GPU.\n",
    "        \"\"\"\n",
    "        self.model = Net(in_channels, out_channels)\n",
    "#         self.model = StackedGCN(self.args, self.clustering_machine.feature_count, self.clustering_machine.class_count)\n",
    "        self.model = self.model.to(self.device)\n",
    "    \n",
    "    # call the forward function batch by batch\n",
    "    def do_forward_pass(self, cluster):\n",
    "        \"\"\"\n",
    "        Making a forward pass with data from a given partition.\n",
    "        :param cluster: Cluster index.\n",
    "        :return average_loss: Average loss on the cluster.\n",
    "        :return node_count: Number of nodes.\n",
    "        \"\"\"\n",
    "        # the edges inside each clustered have been re-mark with local indices inside the cluster\n",
    "        edges = self.clustering_machine.sg_edges[cluster].to(self.device)\n",
    "        macro_nodes = self.clustering_machine.sg_nodes[cluster].to(self.device)\n",
    "        \n",
    "        # already re-index each node inside its local cluster\n",
    "        train_nodes = self.clustering_machine.sg_train_nodes[cluster].to(self.device)\n",
    "        # features has the implicit index, for N1 by K matrix, it always implies node indices: [0, N1-1]\n",
    "        features = self.clustering_machine.sg_features[cluster].to(self.device)\n",
    "        # torch.squeeze()  removes all the dimension with value 1, change the target from 2-D  (N by 1) into 1-D N tensor\n",
    "        \n",
    "        target = self.clustering_machine.sg_labels[cluster].to(self.device)\n",
    "        \n",
    "        # calculate the probabilites from log_sofmax\n",
    "        predictions = self.model(edges, features)\n",
    "        print('info about predictions of each batch before combination: ')\n",
    "        print(predictions)\n",
    "        print('after the GCN based model forward, predictions type and shape: ', type(predictions), predictions.shape)\n",
    "        # calculate the loss scalar from a cluster, only use the train nodes\n",
    "#         average_loss = torch.nn.functional.nll_loss(predictions[train_nodes], target[train_nodes])\n",
    "        average_loss = torch.nn.functional.nll_loss(predictions, target)\n",
    "        # trial sum loss\n",
    "#         sum_loss = torch.nn.functional.nll_loss(predictions, target)\n",
    "        print('info about average loss of each batch before combination: ')\n",
    "        print(average_loss)\n",
    "        print('after the GCN based model forward, loss type and shape: ', type(average_loss), average_loss.shape, average_loss.item())\n",
    "        \n",
    "        node_count = train_nodes.shape[0]\n",
    "\n",
    "        # for each cluster keep track of the counts of the nodes\n",
    "        return average_loss, node_count\n",
    "\n",
    "    def update_average_loss(self, batch_average_loss, node_count):\n",
    "        \"\"\"\n",
    "        Updating the average loss in the epoch.\n",
    "        :param batch_average_loss: Loss of the cluster. \n",
    "        :param node_count: Number of nodes in currently processed cluster.\n",
    "        :return average_loss: Average loss in the epoch.\n",
    "        \"\"\"\n",
    "        self.accumulated_training_loss = self.accumulated_training_loss + batch_average_loss.item()*node_count\n",
    "        self.node_count_seen = self.node_count_seen + node_count\n",
    "        average_loss = self.accumulated_training_loss / self.node_count_seen\n",
    "        return average_loss\n",
    "\n",
    "    def do_prediction(self, cluster):\n",
    "        \"\"\"\n",
    "        Scoring a cluster.\n",
    "        :param cluster: Cluster index.\n",
    "        :return prediction: Prediction matrix with probabilities.\n",
    "        :return target: Target vector.\n",
    "        \"\"\"\n",
    "        edges = self.clustering_machine.sg_edges[cluster].to(self.device)\n",
    "        macro_nodes = self.clustering_machine.sg_nodes[cluster].to(self.device)\n",
    "        test_nodes = self.clustering_machine.sg_test_nodes[cluster].to(self.device)\n",
    "        features = self.clustering_machine.sg_features[cluster].to(self.device)\n",
    "\n",
    "        target = self.clustering_machine.sg_labels[cluster].to(self.device).squeeze()\n",
    "        target = target[test_nodes]\n",
    "        # when we do the model forward, we still use all the samples in one cluster, indludng both test and train\n",
    "        prediction = self.model(edges, features)\n",
    "        prediction = prediction[test_nodes,:]\n",
    "        return prediction, target\n",
    "\n",
    "    # iterate through epoch and also the clusters\n",
    "    def train(self, epoch_num=10, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Training a model.\n",
    "        \"\"\"\n",
    "        print(\"Training started.\\n\")\n",
    "\n",
    "#         epochs = trange(epoch_num, desc = \"Train Loss\")\n",
    "        epochs = tqdm(range(epoch_num), desc = \"Train Loss\")\n",
    "        # A shortcut for tqdm(xrange(args), *kwargs). On Python3+ range is used instead of xrange.\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        self.model.train()\n",
    "\n",
    "        for epoch in epochs:\n",
    "            random.shuffle(self.clustering_machine.clusters)\n",
    "            self.node_count_seen = 0\n",
    "            self.accumulated_training_loss = 0\n",
    "            for cluster in self.clustering_machine.clusters:\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                batch_average_loss, node_count = self.do_forward_pass(cluster)\n",
    "#                 batch_average_loss.requres_grad = True\n",
    "                batch_average_loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                average_loss = self.update_average_loss(batch_average_loss, node_count)\n",
    "            epochs.set_description(\"Train Loss: %g\" % round(average_loss,4))\n",
    "\n",
    "    def test(self):\n",
    "        \"\"\"\n",
    "        Scoring the test and printing the F-1 score.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        self.predictions = []\n",
    "        self.targets = []\n",
    "        for cluster in self.clustering_machine.clusters:\n",
    "            prediction, target = self.do_prediction(cluster)\n",
    "\n",
    "            self.predictions.append(prediction.cpu().detach().numpy())\n",
    "            self.targets.append(target.cpu().detach().numpy())\n",
    "        \n",
    "        self.targets = np.concatenate(self.targets)\n",
    "\n",
    "        self.predictions = np.concatenate(self.predictions).argmax(1)  # return the indices of maximum probability \n",
    "\n",
    "        \n",
    "        score = f1_score(self.targets, self.predictions, average=\"micro\")\n",
    "        print(\"\\nF-1 score: {:.4f}\".format(score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6465, 0.0656, 0.2485],\n",
      "        [0.8424, 0.6868, 0.3109],\n",
      "        [0.3925, 0.2171, 0.0060],\n",
      "        [0.9772, 0.5544, 0.8128],\n",
      "        [0.8922, 0.9545, 0.8406],\n",
      "        [0.7992, 0.3143, 0.1241],\n",
      "        [0.1133, 0.2646, 0.3426],\n",
      "        [0.4261, 0.0237, 0.5118],\n",
      "        [0.1877, 0.1330, 0.3163],\n",
      "        [0.4127, 0.8186, 0.6520]])\n"
     ]
    }
   ],
   "source": [
    "edge_index = torch.tensor([[0, 1, 0, 8, 1, 3, 1, 2, 4, 2, 4, 6, 6, 7, 7, 9, 2, 5, 5, 9, 9, 8], \n",
    "                           [1, 0, 8, 0, 3, 1, 2, 1, 2, 4, 6, 4, 7, 6, 9, 7, 5, 2, 9, 5, 8, 9]])\n",
    "features = torch.rand(10, 3)\n",
    "label = torch.tensor([0, 0, 1, 1, 1, 0, 1, 0, 1, 1])\n",
    "\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random graph clustering started.\n",
      "\n",
      "cluster:  0 mcro_inter_edges:  {(6, 7), (4, 6), (8, 9), (9, 5), (2, 5), (0, 8), (7, 9)}\n",
      "cluster:  1 mcro_inter_edges:  {(4, 6), (2, 5), (0, 8)}\n"
     ]
    }
   ],
   "source": [
    "clustering_machine = ClusteringMachine(edge_index, features, label, partition_num = 2)\n",
    "clustering_machine.decompose(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1}\n",
      "\n",
      "{0: tensor([0, 1, 2, 3, 4]), 1: tensor([5, 6, 7, 8, 9])}\n",
      "{0: tensor([[0, 1, 1, 2, 1, 3, 2, 4],\n",
      "        [1, 3, 2, 4, 0, 1, 1, 2]]), 1: tensor([[3, 1, 2, 4, 4, 2, 4, 0],\n",
      "        [4, 2, 4, 0, 3, 1, 2, 4]])}\n",
      "[(0, 1)]\n",
      "{(4, 6), (2, 5), (0, 8)}\n",
      "[(0, 1), (0, 8), (1, 3), (1, 2), (8, 9), (2, 4), (2, 5), (4, 6), (6, 7), (7, 9), (9, 5)]\n",
      "defaultdict(<class 'set'>, {(0, 1): tensor([0, 2, 4, 5, 6, 8])})\n",
      "defaultdict(<class 'list'>, {(0, 1): tensor([[2, 1, 0, 4, 3, 5],\n",
      "        [4, 3, 5, 2, 1, 0]])})\n"
     ]
    }
   ],
   "source": [
    "print(clustering_machine.node_count)\n",
    "print(clustering_machine.cluster_membership)\n",
    "print()\n",
    "print(clustering_machine.sg_nodes)\n",
    "print(clustering_machine.sg_edges)\n",
    "\n",
    "\n",
    "print(clustering_machine.intersect_cluster)\n",
    "\n",
    "print(clustering_machine.macro_inter_edges)\n",
    "\n",
    "print(clustering_machine.graph.edges())\n",
    "\n",
    "print(clustering_machine.inter_nodes)\n",
    "print(clustering_machine.inter_edges)\n",
    "\n",
    "gcn_trainer = ClusterGCNTrainer(clustering_machine, 3, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72edbd7b5c524b20b85d488c86e90162",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Train Loss', max=2, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "info about predictions of each batch before combination: \n",
      "tensor([[-0.7838, -0.6100],\n",
      "        [-0.8442, -0.5620],\n",
      "        [-0.8000, -0.5966],\n",
      "        [-0.8356, -0.5685],\n",
      "        [-0.7903, -0.6046]], device='cuda:0', grad_fn=<LogSoftmaxBackward>)\n",
      "after the GCN based model forward, predictions type and shape:  <class 'torch.Tensor'> torch.Size([5, 2])\n",
      "info about average loss of each batch before combination: \n",
      "tensor(0.6795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "after the GCN based model forward, loss type and shape:  <class 'torch.Tensor'> torch.Size([]) 0.6795441508293152\n",
      "info about predictions of each batch before combination: \n",
      "tensor([[-0.7943, -0.6013],\n",
      "        [-0.7955, -0.6003],\n",
      "        [-0.8121, -0.5868],\n",
      "        [-0.7804, -0.6129],\n",
      "        [-0.8409, -0.5645]], device='cuda:0', grad_fn=<LogSoftmaxBackward>)\n",
      "after the GCN based model forward, predictions type and shape:  <class 'torch.Tensor'> torch.Size([5, 2])\n",
      "info about average loss of each batch before combination: \n",
      "tensor(0.6768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "after the GCN based model forward, loss type and shape:  <class 'torch.Tensor'> torch.Size([]) 0.6768307685852051\n",
      "info about predictions of each batch before combination: \n",
      "tensor([[-0.8267, -0.5753],\n",
      "        [-0.9028, -0.5199],\n",
      "        [-0.8569, -0.5525],\n",
      "        [-0.8943, -0.5258],\n",
      "        [-0.8425, -0.5632]], device='cuda:0', grad_fn=<LogSoftmaxBackward>)\n",
      "after the GCN based model forward, predictions type and shape:  <class 'torch.Tensor'> torch.Size([5, 2])\n",
      "info about average loss of each batch before combination: \n",
      "tensor(0.6742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "after the GCN based model forward, loss type and shape:  <class 'torch.Tensor'> torch.Size([]) 0.6741956472396851\n",
      "info about predictions of each batch before combination: \n",
      "tensor([[-0.8355, -0.5685],\n",
      "        [-0.8300, -0.5728],\n",
      "        [-0.8538, -0.5547],\n",
      "        [-0.8179, -0.5822],\n",
      "        [-0.8867, -0.5311]], device='cuda:0', grad_fn=<LogSoftmaxBackward>)\n",
      "after the GCN based model forward, predictions type and shape:  <class 'torch.Tensor'> torch.Size([5, 2])\n",
      "info about average loss of each batch before combination: \n",
      "tensor(0.6751, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "after the GCN based model forward, loss type and shape:  <class 'torch.Tensor'> torch.Size([]) 0.675090491771698\n"
     ]
    }
   ],
   "source": [
    "gcn_trainer.train(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 4.561684727668762\n"
     ]
    }
   ],
   "source": [
    "print(gcn_trainer.node_count_seen, gcn_trainer.accumulated_training_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use data from pytorch geometric datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 3 500\n",
      "Info (attributes) of a single data instance\n",
      "Data(edge_index=[2, 88648], test_mask=[19717], train_mask=[19717], val_mask=[19717], x=[19717, 500], y=[19717]) \n",
      " number of nodes:  19717 \n",
      " number of edges:  88648 \n",
      " number of features per ndoe:  500 \n",
      " number of edge features:  0 \n",
      " all the attributes of data:  ['x', 'edge_index', 'y', 'train_mask', 'val_mask', 'test_mask']\n"
     ]
    }
   ],
   "source": [
    "# this data is also used the in the trivial example of the cluster-GCN paper\n",
    "from torch_geometric.datasets import Planetoid\n",
    "dataset = Planetoid(root='~/tmp/Planetoid/PubMed', name='PubMed')\n",
    "print(len(dataset), dataset.num_classes, dataset.num_node_features)\n",
    "data = dataset[0]\n",
    "print('Info (attributes) of a single data instance')\n",
    "print(data, '\\n number of nodes: ', data.num_nodes, '\\n number of edges: ', data.num_edges, \\\n",
    "      '\\n number of features per ndoe: ', data.num_node_features, '\\n number of edge features: ', data.num_edge_features, \\\n",
    "      '\\n all the attributes of data: ', data.keys)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# data = data.to(device)\n",
    "\n",
    "# x, edge_index, y = data.x.cuda(), data.edge_index.cuda(), data.y.cuda()\n",
    "# print(edge_index[:,0])\n",
    "# tmp = edge_index.t().cpu().numpy().tolist()\n",
    "# print(tmp[1])\n",
    "\n",
    "# conv1 = custom_GCNConv(dataset.num_node_features, 2).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute the GCN partitioning graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random graph clustering started.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clustering_machine = ClusteringMachine(data.edge_index, data.x, data.y, partition_num = 2)\n",
    "clustering_machine.decompose(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: tensor([    0,     3,     4,  ..., 19712, 19714, 19715]), 1: tensor([    1,     2,     5,  ..., 19710, 19713, 19716])}\n",
      "{0: tensor([[   0,    0,    0,  ..., 9846, 6933, 9685],\n",
      "        [3049, 3809, 7242,  ..., 6582, 6748, 9044]]), 1: tensor([[   0,    0,    1,  ..., 9786, 9508, 9584],\n",
      "        [1495, 4174, 5252,  ..., 9470, 9507, 9515]])}\n"
     ]
    }
   ],
   "source": [
    "print(clustering_machine.sg_nodes)\n",
    "print(clustering_machine.sg_edges)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn_trainer = ClusterGCNTrainer(clustering_machine, dataset.num_node_features, dataset.num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25d4ba9319694916a77536cb114a6834",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Train Loss', max=2, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [5 x 3], m2: [500 x 3] at /opt/conda/conda-bld/pytorch_1565272271120/work/aten/src/THC/generic/THCTensorMathBlas.cu:273",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-342-f1c6f9d16e9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgcn_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-340-9a9c5303dc7c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epoch_num, learning_rate)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m                 \u001b[0mbatch_average_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_forward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;31m#                 batch_average_loss.requres_grad = True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0mbatch_average_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-340-9a9c5303dc7c>\u001b[0m in \u001b[0;36mdo_forward_pass\u001b[0;34m(self, cluster)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m# calculate the probabilites from log_sofmax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medges\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0;31m# calculate the loss scalar from a cluster, only use the train nodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m#         average_loss = torch.nn.functional.nll_loss(predictions[train_nodes], target[train_nodes])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-284-af05aa1d007a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, edge_index, features)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-283-1ddad23a1378>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;34m\"\"\"\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [5 x 3], m2: [500 x 3] at /opt/conda/conda-bld/pytorch_1565272271120/work/aten/src/THC/generic/THCTensorMathBlas.cu:273"
     ]
    }
   ],
   "source": [
    "gcn_trainer.train(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 22])\n",
      "<class 'networkx.classes.graph.Graph'>\n",
      "<class 'networkx.classes.reportviews.NodeView'> [0, 1, 8, 3, 2, 4, 6, 7, 9, 5]\n",
      "[0, 1]\n",
      "{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1}\n",
      "<class 'networkx.classes.graph.Graph'>\n",
      "[(8, 9), (6, 7), (7, 9), (9, 5)]\n",
      "11\n",
      "[5, 6, 7, 8, 9]\n",
      "{5: 0, 6: 1, 7: 2, 8: 3, 9: 4}\n",
      "[[3, 4], [1, 2], [2, 4], [4, 0], [4, 3], [2, 1], [4, 2], [0, 4]]\n",
      "[[8, 9], [6, 7], [7, 9], [9, 5], [9, 8], [7, 6], [9, 7], [5, 9]]\n",
      "\n",
      "investigate the point of mapper:\n",
      "[1, 0, 3, 4] [2]\n",
      "valid the set use of the graph/subgraph edges:\n",
      "{(0, 1), (1, 2), (1, 3), (6, 7), (4, 6), (8, 9), (9, 5), (2, 5), (0, 8), (2, 4), (7, 9)}\n",
      "{(8, 9), (9, 5), (6, 7), (7, 9)}\n",
      "print the graph difference: \n",
      "{(0, 1), (1, 2), (1, 3), (4, 6), (2, 5), (0, 8), (2, 4)}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# trial on the subgraph\n",
    "\n",
    "edge_index = torch.tensor([[0, 1, 0, 8, 1, 3, 1, 2, 4, 2, 4, 6, 6, 7, 7, 9, 2, 5, 5, 9, 9, 8], \n",
    "                           [1, 0, 8, 0, 3, 1, 2, 1, 2, 4, 6, 4, 7, 6, 9, 7, 5, 2, 9, 5, 8, 9]])\n",
    "\n",
    "print(edge_index.shape)\n",
    "tmp = edge_index.t().numpy().tolist()\n",
    "\n",
    "graph = nx.from_edgelist(tmp)\n",
    "# or  G=nx.Graph(edgelist) # use Graph constructor\n",
    "# here Graph class is for undirected graph, for directed: use DiGraph class instead\n",
    "print(type(graph))\n",
    "# begin to partition\n",
    "partition_num = 2\n",
    "clusters = [cluster for cluster in range(partition_num)]\n",
    "print(type(graph.nodes()), graph.nodes())\n",
    "# randomdized \n",
    "# cluster_membership = {node: random.choice(clusters) for node in graph.nodes()}\n",
    "\n",
    "# trivial for test case\n",
    "node_count = 10\n",
    "cluster_membership = {node: 0 for node in range(node_count//partition_num)}\n",
    "cluster_membership.update({node: 1 for node in range(node_count//partition_num, node_count)})\n",
    "\n",
    "print(clusters)\n",
    "print(cluster_membership)\n",
    "\n",
    "# for subfgraph\n",
    "cluster = 1\n",
    "# print(graph.nodes())\n",
    "\n",
    "subgraph = graph.subgraph([node for node in sorted(graph.nodes()) if cluster_membership[node] == cluster])\n",
    "print(type(subgraph))\n",
    "# for undirected graph, it is only stored once in networkx class\n",
    "print(subgraph.edges())\n",
    "print(len(graph.edges()))\n",
    "\n",
    "sg_nodes = {}\n",
    "sg_edges = {}\n",
    "\n",
    "sg_nodes[cluster] = [node for node in sorted(subgraph.nodes())]\n",
    "print(sg_nodes[cluster])  # just convert it into a list of nodes and store inside the dict\n",
    "mapper = {node: i for i, node in enumerate(sorted(sg_nodes[cluster]))}\n",
    "print(mapper)\n",
    "mapper2 = {node: node for i, node in enumerate(sorted(sg_nodes[cluster]))}\n",
    "\n",
    "# the edges inside its own partition, from two directions since it is an undirected graph\n",
    "sg_edges[cluster] = [[mapper[edge[0]], mapper[edge[1]]] for edge in subgraph.edges()] +  \\\n",
    "                           [[mapper[edge[1]], mapper[edge[0]]] for edge in subgraph.edges()]\n",
    "# there will be no key errors since all is inside subgraph\n",
    "original = [[mapper2[edge[0]], mapper2[edge[1]]] for edge in subgraph.edges()] +  \\\n",
    "                           [[mapper2[edge[1]], mapper2[edge[0]]] for edge in subgraph.edges()]\n",
    "\n",
    "print(sg_edges[cluster])\n",
    "print(original)\n",
    "# investigate the point of mapper:\n",
    "print()\n",
    "print('investigate the point of mapper:')\n",
    "sg_train_nodes = {}\n",
    "sg_test_nodes = {}\n",
    "sg_train_nodes[cluster], sg_test_nodes[cluster] = train_test_split(list(mapper.values()), test_size = 0.1)\n",
    "\n",
    "print(sg_train_nodes[cluster], sg_test_nodes[cluster])\n",
    "\n",
    "\n",
    "# validiate the graph set\n",
    "print('valid the set use of the graph/subgraph edges:')\n",
    "print(set(graph.edges()) )\n",
    "print(set(subgraph.edges()))\n",
    "print('print the graph difference: ')\n",
    "print(set(graph.edges()) - set(subgraph.edges()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'set'>, {1: {'a'}, 2: [5]})\n"
     ]
    }
   ],
   "source": [
    "a = defaultdict(set)\n",
    "a[1].add('a')\n",
    "\n",
    "a[2] = [5]\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_geometric]",
   "language": "python",
   "name": "conda-env-pytorch_geometric-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
