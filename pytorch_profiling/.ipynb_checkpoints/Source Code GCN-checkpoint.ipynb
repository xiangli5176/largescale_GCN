{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base class: messange passing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import inspect\n",
    "#  provides several useful functions to help get information about live objects such as modules, classes, methods,\n",
    "#     functions, tracebacks, frame objects, and code objects.\n",
    "\n",
    "import torch\n",
    "from torch_geometric.utils import scatter_\n",
    "\n",
    "special_args = [\n",
    "    'edge_index', 'edge_index_i', 'edge_index_j', 'size', 'size_i', 'size_j'\n",
    "]\n",
    "__size_error_msg__ = ('All tensors which should get mapped to the same source '\n",
    "                      'or target nodes must be of same size in dimension 0.')\n",
    "\n",
    "is_python2 = sys.version_info[0] < 3\n",
    "getargspec = inspect.getargspec if is_python2 else inspect.getfullargspec\n",
    "\n",
    "class MessagePassing(torch.nn.Module):\n",
    "    r\"\"\"Base class for creating message passing layers\n",
    "\n",
    "    .. math::\n",
    "        \\mathbf{x}_i^{\\prime} = \\gamma_{\\mathbf{\\Theta}} \\left( \\mathbf{x}_i,\n",
    "        \\square_{j \\in \\mathcal{N}(i)} \\, \\phi_{\\mathbf{\\Theta}}\n",
    "        \\left(\\mathbf{x}_i, \\mathbf{x}_j,\\mathbf{e}_{i,j}\\right) \\right),\n",
    "\n",
    "    where :math:`\\square` denotes a differentiable, permutation invariant\n",
    "    function, *e.g.*, sum, mean or max, and :math:`\\gamma_{\\mathbf{\\Theta}}`\n",
    "    and :math:`\\phi_{\\mathbf{\\Theta}}` denote differentiable functions such as\n",
    "    MLPs.\n",
    "    See `here <https://pytorch-geometric.readthedocs.io/en/latest/notes/\n",
    "    create_gnn.html>`__ for the accompanying tutorial.\n",
    "\n",
    "    Args:\n",
    "        aggr (string, optional): The aggregation scheme to use\n",
    "            (:obj:`\"add\"`, :obj:`\"mean\"` or :obj:`\"max\"`).\n",
    "            (default: :obj:`\"add\"`)\n",
    "        flow (string, optional): The flow direction of message passing\n",
    "            (:obj:`\"source_to_target\"` or :obj:`\"target_to_source\"`).\n",
    "            (default: :obj:`\"source_to_target\"`)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, aggr='add', flow='source_to_target'):\n",
    "        super(MessagePassing, self).__init__()\n",
    "\n",
    "        self.aggr = aggr\n",
    "        assert self.aggr in ['add', 'mean', 'max']\n",
    "\n",
    "        self.flow = flow\n",
    "        assert self.flow in ['source_to_target', 'target_to_source']\n",
    "        \n",
    "        # the self.message is just hte member function\n",
    "#         Get the names and default values of a Python function’s parameters. A named tuple is returned:\n",
    "# FullArgSpec(args, varargs, varkw, defaults, kwonlyargs, kwonlydefaults, annotations)\n",
    "        self.__message_args__ = getargspec(self.message)[0][1:]  # get the arguments of the self.message except: x_j\n",
    "        self.__special_args__ = [(i, arg)\n",
    "                                 for i, arg in enumerate(self.__message_args__)\n",
    "                                 if arg in special_args]\n",
    "        # !!! exclude the special arguments\n",
    "        self.__message_args__ = [\n",
    "            arg for arg in self.__message_args__ if arg not in special_args\n",
    "        ]\n",
    "        self.__update_args__ = getargspec(self.update)[0][2:]    # we skip the edge_index and size args in propogate\n",
    "\n",
    "    def propagate(self, edge_index, size=None, **kwargs):\n",
    "        r\"\"\"The initial call to start propagating messages.\n",
    "\n",
    "        Args:\n",
    "            edge_index (Tensor): The indices of a general (sparse) assignment\n",
    "                matrix with shape :obj:`[N, M]` (can be directed or\n",
    "                undirected).\n",
    "            size (list or tuple, optional): The size :obj:`[N, M]` of the\n",
    "                assignment matrix. If set to :obj:`None`, the size is tried to\n",
    "                get automatically inferred. (default: :obj:`None`)\n",
    "            **kwargs: Any additional data which is needed to construct messages\n",
    "                and to update node embeddings.\n",
    "        \"\"\"\n",
    "        # some candiates for the kwargs: may be test_mask\n",
    "        dim = 0\n",
    "        size = [None, None] if size is None else list(size)\n",
    "        assert len(size) == 2\n",
    "\n",
    "        i, j = (0, 1) if self.flow == 'target_to_source' else (1, 0)\n",
    "        ij = {\"_i\": i, \"_j\": j}\n",
    "        \n",
    "        # collect the message arguments\n",
    "        message_args = []\n",
    "        for arg in self.__message_args__:\n",
    "            if arg[-2:] in ij.keys():   # check the suffix\n",
    "                tmp = kwargs.get(arg[:-2], None)\n",
    "                if tmp is None:  # pragma: no cover\n",
    "                    message_args.append(tmp)\n",
    "                else:\n",
    "                    idx = ij[arg[-2:]]\n",
    "                    if isinstance(tmp, tuple) or isinstance(tmp, list):\n",
    "                        assert len(tmp) == 2\n",
    "                        if tmp[1 - idx] is not None:\n",
    "                            if size[1 - idx] is None:\n",
    "                                size[1 - idx] = tmp[1 - idx].size(dim)\n",
    "                            if size[1 - idx] != tmp[1 - idx].size(dim):\n",
    "                                raise ValueError(__size_error_msg__)\n",
    "                        tmp = tmp[idx]\n",
    "\n",
    "                    if tmp is None:\n",
    "                        message_args.append(tmp)\n",
    "                    else:\n",
    "                        if size[idx] is None:\n",
    "                            size[idx] = tmp.size(dim)\n",
    "                        if size[idx] != tmp.size(dim):\n",
    "                            raise ValueError(__size_error_msg__)\n",
    "\n",
    "                        tmp = torch.index_select(tmp, dim, edge_index[idx])\n",
    "                        message_args.append(tmp)\n",
    "            else:\n",
    "                message_args.append(kwargs.get(arg, None))\n",
    "        # either one of the size dimension is None, we make it the (N, N)\n",
    "        size[0] = size[1] if size[0] is None else size[0]\n",
    "        size[1] = size[0] if size[1] is None else size[1]\n",
    "\n",
    "        kwargs['edge_index'] = edge_index\n",
    "        kwargs['size'] = size\n",
    "\n",
    "        for (idx, arg) in self.__special_args__:\n",
    "            if arg[-2:] in ij.keys():\n",
    "                message_args.insert(idx, kwargs[arg[:-2]][ij[arg[-2:]]])\n",
    "            else:\n",
    "                message_args.insert(idx, kwargs[arg])\n",
    "\n",
    "        update_args = [kwargs[arg] for arg in self.__update_args__]\n",
    "\n",
    "        out = self.message(*message_args)\n",
    "        # aggreates all values from the src\n",
    "        '''\n",
    "         scatter_(name, src, index, dim_size=None)[source]\n",
    "\n",
    "            Aggregates all values from the src tensor at the indices specified in the index tensor along the first dimension. If multiple \n",
    "            indices reference the same location, their contributions are aggregated according to name (either \"add\", \"mean\" or \"max\").\n",
    "                Parameters:\t\n",
    "\n",
    "                name (string) – The aggregation to use (\"add\", \"mean\", \"max\").\n",
    "                src (Tensor) – The source tensor.\n",
    "                index (LongTensor) – The indices of elements to scatter.\n",
    "                dim_size (int, optional) – Automatically create output tensor with size dim_size in the first dimension. \n",
    "                If set to None, a minimal sized output tensor is returned. (default: None)\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        from source to the target: i == 1, therefore, the output tensor is in batches of column size\n",
    "         scatter_(name, src, index, dim_size=None)[source]\n",
    "            Aggregates all values from the src tensor at the indices specified in the index tensor along the first dimension. \n",
    "            If multiple indices reference the same location, their contributions are aggregated according to name (either \"add\", \"mean\" or \"max\").\n",
    "        here we take the dest of the edge as the index in the scatter\n",
    "         out = op(src, index, 0, None, dim_size, fill_value)\n",
    "        the default dim will be 0\n",
    "        '''\n",
    "        out = scatter_(self.aggr, out, edge_index[i], dim_size=size[i])\n",
    "        \n",
    "        out = self.update(out, *update_args)\n",
    "        # at last return the updated arguments\n",
    "        return out\n",
    "\n",
    "#     when the derivative class which inherits from this base class can override this function by providing more arguments\n",
    "    def message(self, x_j):  # pragma: no cover\n",
    "        r\"\"\"Constructs messages in analogy to :math:`\\phi_{\\mathbf{\\Theta}}`\n",
    "        for each edge in :math:`(i,j) \\in \\mathcal{E}`.\n",
    "        Can take any argument which was initially passed to :meth:`propagate`.\n",
    "        In addition, features can be lifted to the source node :math:`i` and\n",
    "        target node :math:`j` by appending :obj:`_i` or :obj:`_j` to the\n",
    "        variable name, *.e.g.* :obj:`x_i` and :obj:`x_j`.\"\"\"\n",
    "\n",
    "        return x_j\n",
    "\n",
    "    def update(self, aggr_out):  # pragma: no cover\n",
    "        r\"\"\"Updates node embeddings in analogy to\n",
    "        :math:`\\gamma_{\\mathbf{\\Theta}}` for each node\n",
    "        :math:`i \\in \\mathcal{V}`.\n",
    "        Takes in the output of aggregation as first argument and any argument\n",
    "        which was initially passed to :meth:`propagate`.\"\"\"\n",
    "\n",
    "        return aggr_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullArgSpec(args=['a', 'b'], varargs=None, varkw=None, defaults=(3, 4), kwonlyargs=[], kwonlydefaults=None, annotations={})\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:7: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "\n",
    "def message(a=3, b=4):\n",
    "    return a+b\n",
    "\n",
    "print(inspect.getfullargspec(message))\n",
    "print(inspect.getargspec(message)[0][2:])\n",
    "a = [2, 3]\n",
    "# b = *a\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use messagepassing to contruct the GCNConv (informal of GCNConv definition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops, degree\n",
    "\n",
    "class GCNConv(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        # this calling of the super function is just old fasioned\n",
    "        # in python3: we can use the super().__init__(args)\n",
    "        super(GCNConv, self).__init__(aggr='add')  # \"Add\" aggregation.\n",
    "        # weight is gienerated by  self.weight = Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.lin = torch.nn.Linear(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # x has shape [N, in_channels], which is the matrix\n",
    "        # edge_index has shape [2, E]\n",
    "\n",
    "        # Step 1: Add self-loops to the adjacency matrix.\n",
    "        # add extra columns of node inces and igore the edge_weights as _\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
    "        \n",
    "        # Step 2: Linearly transform node feature matrix.\n",
    "        # this should be where we apply the weight\n",
    "        x = self.lin(x)\n",
    "\n",
    "        # Step 3-5: Start propagating messages.\n",
    "        return self.propagate(edge_index, size=(x.size(0), x.size(0)), x=x)\n",
    "\n",
    "    # msg here are all the special messages\n",
    "    def message(self, x_j, edge_index, size):\n",
    "        # x_j has shape [E, out_channels], still a tensor, this E should contain the self loops\n",
    "\n",
    "        # Step 3: Normalize node features.\n",
    "        # row is the tensor of the source nodes while col is the dest nodes in an edge\n",
    "        row, col = edge_index\n",
    "        # size(0) should be the number of row in the assignment matrix\n",
    "        # since row is the source node, the degree calculates the out degree of each node\n",
    "        deg = degree(row, size[0], dtype=x_j.dtype)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
    "        # view the columns as one columns. (E by 1)\n",
    "        # this is not GEMM, this is the element wise, each feature values is normalized by the square root of both out-degree and in-degree\n",
    "        return norm.view(-1, 1) * x_j\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        # aggr_out has shape [N, out_channels]\n",
    "\n",
    "        # Step 5: Return new node embeddings.\n",
    "        return aggr_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[8.5305e+02, 1.8515e+28, 3.0866e+29],\n",
      "        [1.1675e-10, 8.9683e-44, 0.0000e+00]])\n",
      "tensor([[-6.9565e+33,  3.0880e-41, -3.4440e+35],\n",
      "        [ 3.0880e-41,  7.1426e+05,  6.2706e+22]])\n",
      "tensor([[3., 4.],\n",
      "        [5., 6.]]) torch.Size([2, 2]) \n",
      " tensor([3., 4.]) torch.Size([2]) tensor([5., 6.]) torch.Size([2])\n",
      "tensor([[16.],\n",
      "        [16.],\n",
      "        [16.],\n",
      "        [16.],\n",
      "        [ 4.],\n",
      "        [ 4.],\n",
      "        [ 4.],\n",
      "        [ 4.]])\n",
      "tensor([[ 16.,  32.],\n",
      "        [ 48.,  64.],\n",
      "        [ 80.,  96.],\n",
      "        [112., 128.],\n",
      "        [ 32.,  28.],\n",
      "        [ 24.,  20.],\n",
      "        [ 16.,  12.],\n",
      "        [  8.,   4.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Test the random initialization of the Tensor\n",
    "a = torch.Tensor(2, 3)\n",
    "print(a)\n",
    "a = torch.Tensor(2, 3)\n",
    "print(a)\n",
    "# for a 2-D tensor, we can assign them by the first dimension\n",
    "b = torch.Tensor([[3, 4], [5, 6]])\n",
    "m, n = b\n",
    "print(b, b.shape, '\\n', m, m.shape, n, n.shape)\n",
    "print(norm.view(-1, 1) )\n",
    "trial_x = torch.tensor([[1, 2], [3, 4], [5, 6], [7, 8], [8, 7], [6, 5], [4, 3], [2, 1]], dtype=torch.float)\n",
    "print(norm.view(-1, 1) * trial_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 2., 2., 2., 1., 1.]) <class 'torch.Tensor'> torch.Size([6]) tensor([0., 4., 4., 4., 1., 1.])\n",
      "tensor([4., 4., 4., 4., 4., 1., 4., 1.]) torch.Size([8]) \n",
      " tensor([4., 4., 4., 4., 1., 4., 1., 4.]) torch.Size([8])\n",
      "tensor([16., 16., 16., 16.,  4.,  4.,  4.,  4.]) torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.utils import degree\n",
    "a = torch.tensor([2, 1, 3, 2, 1, 5, 3, 4])\n",
    "b = torch.tensor([1, 2, 2, 3, 5, 1, 4, 3])\n",
    "deg = degree(a, 6, torch.float)\n",
    "deg_square = deg.pow(2)\n",
    "print( deg, type(deg), deg.shape, deg_square)\n",
    "\n",
    "print(deg_square[a], deg_square[a].shape, '\\n', deg_square[b], deg_square[b].shape)\n",
    "norm = deg_square[a] * deg_square[b]\n",
    "print(norm, norm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torch_geometric.utils :  add_self_loops, degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_self_loops(edge_index, edge_weight=None, fill_value=1, num_nodes=None):\n",
    "    r\"\"\"Adds a self-loop :math:`(i,i) \\in \\mathcal{E}` to every node\n",
    "    :math:`i \\in \\mathcal{V}` in the graph given by :attr:`edge_index`.\n",
    "    In case the graph is weighted, self-loops will be added with edge weights\n",
    "    denoted by :obj:`fill_value`.\n",
    "\n",
    "    Args:\n",
    "        edge_index (LongTensor): The edge indices.\n",
    "        edge_weight (Tensor, optional): One-dimensional edge weights.\n",
    "            (default: :obj:`None`)\n",
    "        fill_value (int, optional): If :obj:`edge_weight` is not :obj:`None`,\n",
    "            will add self-loops with edge weights of :obj:`fill_value` to the\n",
    "            graph. (default: :obj:`1`)\n",
    "        num_nodes (int, optional): The number of nodes, *i.e.*\n",
    "            :obj:`max_val + 1` of :attr:`edge_index`. (default: :obj:`None`)\n",
    "\n",
    "    :rtype: (:class:`LongTensor`, :class:`Tensor`)\n",
    "    \"\"\"\n",
    "    num_nodes = maybe_num_nodes(edge_index, num_nodes)\n",
    "    \n",
    "    '''\n",
    "    Returns a 1-D tensor of size (end-start)/step with values from the interval [start, end) taken with common difference step beginning from start.\n",
    "    '''\n",
    "    loop_index = torch.arange(0, num_nodes, dtype=torch.long, device=edge_index.device)\n",
    "    \n",
    "    '''\n",
    "    unsqueeze() :\n",
    "        Returns a new tensor with a dimension of size one inserted at the specified position.\n",
    "        The returned tensor shares the same underlying data with this tensor.\n",
    "        \n",
    "    tensor.repeat(shape):\n",
    "    repeat the tnesor with shape (m, n) matrix\n",
    "    Here, we have two rows of the same node ([[i], [i]])\n",
    "    '''\n",
    "    loop_index = loop_index.unsqueeze(0).repeat(2, 1)\n",
    "\n",
    "    if edge_weight is not None:\n",
    "        # torch.numel(input):  return the total number of elements in the input tnesor\n",
    "        assert edge_weight.numel() == edge_index.size(1)   # check whether the number of of weight and # of edges match\n",
    "#         new_full(size, fill_value, dtype=None, device=None, requires_grad=False) → Tensor\n",
    "#         Returns a Tensor of size size filled with fill_value. By default, the returned Tensor has the same torch.dtype and torch.device as this tensor.\n",
    "        loop_weight = edge_weight.new_full((num_nodes, ), fill_value)\n",
    "        # cancatenate tensors in the 0 (row) dimension\n",
    "        edge_weight = torch.cat([edge_weight, loop_weight], dim=0)\n",
    "\n",
    "    edge_index = torch.cat([edge_index, loop_index], dim=1)\n",
    "    # at last add the num_nodes columns to the end of edge_index\n",
    "    return edge_index, edge_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3]) tensor([[2, 3, 4]]) torch.Size([1, 3])\n",
      "torch.Size([2, 3]) \n",
      " torch.Size([1, 2, 3]) \n",
      " torch.Size([2, 1, 3])\n",
      "tensor([[2, 3, 4],\n",
      "        [2, 3, 4]]) \n",
      " tensor([[2, 3, 4],\n",
      "        [2, 3, 4]])\n",
      "tensor([[2, 3, 4, 2, 3, 4],\n",
      "        [5, 6, 7, 2, 3, 4]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([2, 3, 4])\n",
    "print(a.shape, a.unsqueeze(0), a.unsqueeze(0).shape)\n",
    "b = torch.tensor([[2, 3, 4], [5, 6, 7]])\n",
    "print(b.shape,'\\n', b.unsqueeze(0).shape,'\\n', b.unsqueeze(1).shape)\n",
    "\n",
    "aa = a.unsqueeze(0)\n",
    "print(a.repeat(2, 1),'\\n', aa.repeat(2,1))\n",
    "c =  aa.repeat(2,1)\n",
    "# self loop to modify the edge_index\n",
    "print(torch.cat([b, c], dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formal definition of GCNConv layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Parameter\n",
    "from torch_scatter import scatter_add\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.utils import add_remaining_self_loops\n",
    "\n",
    "# from ..inits import glorot, zeros  # This two imports are replaced by the source functions:\n",
    "import math\n",
    "def glorot(tensor):\n",
    "    if tensor is not None:\n",
    "        stdv = math.sqrt(6.0 / (tensor.size(-2) + tensor.size(-1)))\n",
    "        '''\n",
    "         uniform_(from=0, to=1) → Tensor\n",
    "            Fills self tensor with numbers sampled from the continuous uniform distribution:\n",
    "            P(x)=1/(to - from)\n",
    "        '''\n",
    "        # very similar to how we initialize the weight of the neural networks\n",
    "        tensor.data.uniform_(-stdv, stdv)\n",
    "        \n",
    "def zeros(tensor):\n",
    "    if tensor is not None:\n",
    "        tensor.data.fill_(0)\n",
    "\n",
    "class GCNConv(MessagePassing):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, improved=False, cached=False,\n",
    "                 bias=True, **kwargs):\n",
    "        super(GCNConv, self).__init__(aggr='add', **kwargs)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.improved = improved  # Can be A+2I for improvements\n",
    "        self.cached = cached   \n",
    "        \n",
    "        '''\n",
    "        torch.nn.Parameter\n",
    "            A kind of Tensor that is to be considered a module parameter.\n",
    "            Parameters are Tensor subclasses, that have a very special property when used with Module s - when they’re assigned as Module attributes\n",
    "            they are automatically added to the list of its parameters, and will appear e.g. in parameters() iterator.\n",
    "        '''\n",
    "        self.weight = Parameter(torch.Tensor(in_channels, out_channels))\n",
    "\n",
    "        if bias:\n",
    "            # random bias for the output\n",
    "            self.bias = Parameter(torch.Tensor(out_channels))  # this Parameter() func will implicitly call the register function                \n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        glorot(self.weight)   # at first self.weight is a random generated tensor, here we use a specific continous uniform distribution for weights\n",
    "        zeros(self.bias)      \n",
    "        self.cached_result = None\n",
    "        self.cached_num_edges = None\n",
    "\n",
    "    # use the python decorator \n",
    "    @staticmethod\n",
    "    def norm(edge_index, num_nodes, edge_weight=None, improved=False, dtype=None):\n",
    "        if edge_weight is None:\n",
    "#             1-D tensor, eah weight is for each edge\n",
    "            edge_weight = torch.ones((edge_index.size(1), ), dtype=dtype, device=edge_index.device)\n",
    "        \n",
    "        # fill value will only define the missing self-loop-edges weights\n",
    "        fill_value = 1 if not improved else 2  # diagonal elements\n",
    "        '''\n",
    "        Adds remaining self-loop (i,i)∈E to every node i∈V in the graph given by edge_index. In case the graph is weighted and already contains a few self-loops, \n",
    "        only non-existent self-loops will be added with edge weights denoted by fill_value.\n",
    "        '''\n",
    "        edge_index, edge_weight = add_remaining_self_loops(edge_index, edge_weight, fill_value, num_nodes)\n",
    "    \n",
    "        row, col = edge_index   # row stores the source nodes while col stores the destination nodes\n",
    "        deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "        # return the added self-loops edges and the normlaized edge weights\n",
    "        return edge_index, deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        \"\"\"\"\"\"\n",
    "        # these weight is for the node features\n",
    "        x = torch.matmul(x, self.weight)   # self.weight shape: (in_channels, out_channels)\n",
    "        # after this, x already became E by out_channels matrix\n",
    "        if self.cached and self.cached_result is not None:\n",
    "            if edge_index.size(1) != self.cached_num_edges:\n",
    "                raise RuntimeError(\n",
    "                    'Cached {} number of edges, but found {}. Please '\n",
    "                    'disable the caching behavior of this layer by removing '\n",
    "                    'the `cached=True` argument in its constructor.'.format(\n",
    "                        self.cached_num_edges, edge_index.size(1)))\n",
    "\n",
    "        if not self.cached or self.cached_result is None:\n",
    "            self.cached_num_edges = edge_index.size(1)\n",
    "            edge_index, norm = self.norm(edge_index, x.size(0), edge_weight, self.improved, x.dtype)\n",
    "            # store the temp results as the tuple\n",
    "            self.cached_result = edge_index, norm\n",
    "\n",
    "        edge_index, norm = self.cached_result\n",
    "        # function calling\n",
    "        # inside the propagate function, the final used message_args as the list for out:\n",
    "        '''\n",
    "     1) first out returns the message_args list:\n",
    "        a) for kwargs in propagate function:\n",
    "        1. add  x_j:\n",
    "             tmp = x not None  (x is a tensor)\n",
    "             idx = j which is 0\n",
    "             \n",
    "             inferred: size[0] = x.size(0)\n",
    "             tmp = torch.index_select(tmp, 0, edge_index[0])\n",
    "                The finla tmp will be E by in_channel matrix, E is the number of edges including the self_loops\n",
    "             message_args.append(tmp)\n",
    "             \n",
    "        2. message_args.append(kwargs.get(norm) )\n",
    "        3. add kwargs['edge_index'] = edge_index; kwargs['size'] = size\n",
    "        \n",
    "        b) out = self.message(*message_args):\n",
    "        Normalize all the features, return a E by out_channel matrix\n",
    "        '''\n",
    "        # inside: torch.index_select(input, dim, index, out=None) → Tensor\n",
    "#         Returns a new tensor which indexes the input tensor along dimension dim using the entries in index which is a LongTensor.\n",
    "#         The returned tensor has the same number of dimensions as the original tensor (input). \n",
    "#         The dimth dimension has the same size as the length of index; other dimensions have the same size as in the original tensor.\n",
    "        \n",
    "        '''\n",
    "    2) second out use the scatter function to call the aggregation\n",
    "        \n",
    "        self.aggr = add ; i = 1\n",
    "        out = scatter_(self.aggr, out, edge_index[i], dim_size=size[i])\n",
    "            Still E by out_channels, redistribute the weighted feature values of each node to the target nodes\n",
    "            \n",
    "        '''\n",
    "    '''\n",
    "    3) out = self.update(out, *update_args)\n",
    "        \n",
    "        out = self.update(out, *update_args)\n",
    "            may add bias in the self.update function\n",
    "    \n",
    "    '''\n",
    "    # aggreates all values from the src\n",
    "        '''\n",
    "         scatter_(name, src, index, dim_size=None)[source]\n",
    "\n",
    "            Aggregates all values from the src tensor at the indices specified in the index tensor along the first dimension. If multiple \n",
    "            indices reference the same location, their contributions are aggregated according to name (either \"add\", \"mean\" or \"max\").\n",
    "                Parameters:\t\n",
    "\n",
    "                name (string) – The aggregation to use (\"add\", \"mean\", \"max\").\n",
    "                src (Tensor) – The source tensor.\n",
    "                index (LongTensor) – The indices of elements to scatter.\n",
    "                dim_size (int, optional) – Automatically create output tensor with size dim_size in the first dimension. \n",
    "                If set to None, a minimal sized output tensor is returned. (default: None)\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        from source to the target: i == 1, therefore, the output tensor is in batches of column size\n",
    "         scatter_(name, src, index, dim_size=None)[source]\n",
    "            Aggregates all values from the src tensor at the indices specified in the index tensor along the first dimension. \n",
    "            If multiple indices reference the same location, their contributions are aggregated according to name (either \"add\", \"mean\" or \"max\").\n",
    "        here we take the dest of the edge as the index in the scatter\n",
    "         out = op(src, index, 0, None, dim_size, fill_value)\n",
    "        the default dim will be 0\n",
    "        '''\n",
    "    \n",
    "        # progate three msg: edge_index (including self-loops), weighted node-feature matrix, normlized weights of edges\n",
    "        return self.propagate(edge_index, x=x, norm=norm)\n",
    "    \n",
    "    # according to the base class: messagepassing\n",
    "    ''' x_j and norm are both the message_args, they will be append back in the same order in the propagate func '''\n",
    "    def message(self, x_j, norm):\n",
    "        # x_j is of the shape: (E, out_channel), E is the number of the edges with self-loops\n",
    "        return norm.view(-1, 1) * x_j    # E by 1 (single column) multiply (E by out_channel), element-wise normalization\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        if self.bias is not None:\n",
    "            aggr_out = aggr_out + self.bias   # bet this also the element-wise tensor op\n",
    "        return aggr_out\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}({}, {})'.format(self.__class__.__name__, self.in_channels,\n",
    "                                   self.out_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3]) 2 3\n",
      "tensor([[3.2241, 2.3083, 9.0218],\n",
      "        [9.8002, 8.3985, 2.1255]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[2, 3, 4], [4, 5, 6]], dtype=torch.float)\n",
    "print(a.size(), a.size(-2), a.size(-1))\n",
    "# uniform distriubtion for a tensor\n",
    "# a = a.cuda()\n",
    "a.data.uniform_(0., 10.)\n",
    "print(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-6.7384e+34,  3.0880e-41, -9.0104e+35,  3.0880e-41,  8.9683e-44])\n"
     ]
    }
   ],
   "source": [
    "a = torch.Tensor(5)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=orange|green>\n",
    "\n",
    "**What is a static method?**\n",
    "\n",
    "Static methods, much like class methods, are methods that are bound to a class rather than its object.\n",
    "\n",
    "They do not require a class instance creation. So, are not dependent on the state of the object.\n",
    "\n",
    "The difference between a static method and a class method is:\n",
    "\n",
    "    Static method knows nothing about the class and just deals with the parameters.\n",
    "    Class method works with the class since its parameter is always the class itself.\n",
    "\n",
    "They can be called both by the class and its object.\n",
    "\n",
    "Class.staticmethodFunc()\n",
    "or even\n",
    "Class().staticmethodFunc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Costumized GCNConv based on Customized messagepassing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import inspect\n",
    "\n",
    "import torch\n",
    "from torch_geometric.utils import scatter_\n",
    "\n",
    "special_args = ['edge_index', 'edge_index_i', 'edge_index_j', 'size', 'size_i', 'size_j']\n",
    "\n",
    "__size_error_msg__ = ('All tensors which should get mapped to the same source '\n",
    "                      'or target nodes must be of same size in dimension 0.')\n",
    "\n",
    "is_python2 = sys.version_info[0] < 3\n",
    "getargspec = inspect.getargspec if is_python2 else inspect.getfullargspec\n",
    "\n",
    "\n",
    "class custom_MessagePassing(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, aggr='add', flow='source_to_target'):\n",
    "        super(custom_MessagePassing, self).__init__()\n",
    "\n",
    "        self.aggr = aggr\n",
    "        assert self.aggr in ['add', 'mean', 'max']\n",
    "\n",
    "        self.flow = flow\n",
    "        assert self.flow in ['source_to_target', 'target_to_source']\n",
    "\n",
    "        self.__message_args__ = getargspec(self.message)[0][1:]\n",
    "        self.__special_args__ = [(i, arg)\n",
    "                                 for i, arg in enumerate(self.__message_args__)\n",
    "                                 if arg in special_args]\n",
    "        self.__message_args__ = [\n",
    "            arg for arg in self.__message_args__ if arg not in special_args\n",
    "        ]\n",
    "        self.__update_args__ = getargspec(self.update)[0][2:]\n",
    "\n",
    "    def propagate(self, edge_index, size=None, **kwargs):\n",
    "        print('='*200)\n",
    "        print('Start output the info from the propagation function from messagepassing class (inherited by GCNConv):')\n",
    "        dim = 0\n",
    "        size = [None, None] if size is None else list(size)\n",
    "        assert len(size) == 2\n",
    "\n",
    "        i, j = (0, 1) if self.flow == 'target_to_source' else (1, 0)\n",
    "        ij = {\"_i\": i, \"_j\": j}\n",
    "\n",
    "        message_args = []\n",
    "        for arg in self.__message_args__:\n",
    "            if arg[-2:] in ij.keys():\n",
    "                tmp = kwargs.get(arg[:-2], None)\n",
    "                if tmp is None:  # pragma: no cover\n",
    "                    message_args.append(tmp)\n",
    "                else:\n",
    "                    idx = ij[arg[-2:]]\n",
    "                    if isinstance(tmp, tuple) or isinstance(tmp, list):\n",
    "                        assert len(tmp) == 2\n",
    "                        if tmp[1 - idx] is not None:\n",
    "                            if size[1 - idx] is None:\n",
    "                                size[1 - idx] = tmp[1 - idx].size(dim)\n",
    "                            if size[1 - idx] != tmp[1 - idx].size(dim):\n",
    "                                raise ValueError(__size_error_msg__)\n",
    "                        tmp = tmp[idx]\n",
    "\n",
    "                    if tmp is None:\n",
    "                        message_args.append(tmp)\n",
    "                    else:\n",
    "                        if size[idx] is None:\n",
    "                            size[idx] = tmp.size(dim)\n",
    "                        if size[idx] != tmp.size(dim):\n",
    "                            raise ValueError(__size_error_msg__)\n",
    "\n",
    "                        tmp = torch.index_select(tmp, dim, edge_index[idx])\n",
    "                        message_args.append(tmp)\n",
    "            else:\n",
    "                message_args.append(kwargs.get(arg, None))\n",
    "\n",
    "        size[0] = size[1] if size[0] is None else size[0]\n",
    "        size[1] = size[0] if size[1] is None else size[1]\n",
    "\n",
    "        kwargs['edge_index'] = edge_index\n",
    "        kwargs['size'] = size\n",
    "\n",
    "        for (idx, arg) in self.__special_args__:\n",
    "            if arg[-2:] in ij.keys():\n",
    "                message_args.insert(idx, kwargs[arg[:-2]][ij[arg[-2:]]])\n",
    "            else:\n",
    "                message_args.insert(idx, kwargs[arg])\n",
    "\n",
    "        update_args = [kwargs[arg] for arg in self.__update_args__]\n",
    "        print('update_args: ', update_args)\n",
    "        print('kwargs include the args: ', kwargs.keys(), '\\n')\n",
    "        print('message_args during propagation for each convolution step contains: ')\n",
    "        # assume all the elements inside the message are tensor\n",
    "        for idx, val in enumerate(message_args):\n",
    "            print('Number ', idx, ' val type: ', type(message_args[idx]), ' val shape: ', message_args[idx].shape)\n",
    "        \n",
    "        \n",
    "        print('\\n call the message function inside the GCNConv (normalize the feature according to in- or out- degree of each nodes): ')\n",
    "        out = self.message(*message_args)\n",
    "        print('type and shape of the embedding after normalization based on in-dgree and out-degree of each node: ', type(out), out.shape, '\\n')\n",
    "        \n",
    "        print('\\n Step-4: call the scatter_ function (aggregates the feature values of source nodes into target nodes): ')\n",
    "        out = scatter_(self.aggr, out, edge_index[i], dim_size=size[i])\n",
    "        print('type and shape of embedding after scattering: ', type(out), out.shape, '\\n')\n",
    "        \n",
    "        print('call the update function (may add the bias for GCNConv, default all zeros): ')\n",
    "        out = self.update(out, *update_args)\n",
    "        print('type and shape of embedding after udpating (may add bias): ', type(out), out.shape, '\\n')\n",
    "        \n",
    "        print('\\n Step-5: return the new node embeddings: number_of_nodes by out_channels tensor. ')\n",
    "        print('End of the Info from the propagation function in messagepassing class ')\n",
    "        print('='*200)\n",
    "        return out\n",
    "\n",
    "    def message(self, x_j):  # pragma: no cover\n",
    "        return x_j\n",
    "\n",
    "    def update(self, aggr_out):  # pragma: no cover\n",
    "        return aggr_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Parameter\n",
    "from torch_scatter import scatter_add\n",
    "from torch_geometric.utils import add_remaining_self_loops\n",
    "import math\n",
    "\n",
    "def glorot(tensor):\n",
    "    if tensor is not None:\n",
    "        stdv = math.sqrt(6.0 / (tensor.size(-2) + tensor.size(-1)))\n",
    "        tensor.data.uniform_(-stdv, stdv)\n",
    "        \n",
    "def zeros(tensor):\n",
    "    if tensor is not None:\n",
    "        tensor.data.fill_(0)\n",
    "\n",
    "class custom_GCNConv(custom_MessagePassing):\n",
    "    '''\n",
    "        Here the __init__ will only be called once, when it is first created. Then each time we use the instance such as conv1,\n",
    "        we actually are calling the forward function. \n",
    "        Therefore, we are keep updating the self.weight and the self.bias , which are the two model parameters for each GCNConv layer\n",
    "    '''\n",
    "    def __init__(self, in_channels, out_channels, improved=False, cached=False,\n",
    "                 bias=True, **kwargs):\n",
    "        super().__init__(aggr='add', **kwargs)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.improved = improved\n",
    "        self.cached = cached\n",
    "\n",
    "        self.weight = Parameter(torch.Tensor(in_channels, out_channels))\n",
    "\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        glorot(self.weight)\n",
    "        zeros(self.bias)\n",
    "        self.cached_result = None\n",
    "        self.cached_num_edges = None\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def norm(edge_index, num_nodes, edge_weight=None, improved=False, dtype=None):\n",
    "        print('Inside the norm function: ')\n",
    "        \n",
    "        if edge_weight is None:\n",
    "            edge_weight = torch.ones((edge_index.size(1), ), dtype=dtype, device=edge_index.device)\n",
    "        print('type and shape of edge_weights (default to be all 1): ', type(edge_weight), edge_weight.shape)\n",
    "        \n",
    "        fill_value = 1 if not improved else 2\n",
    "        \n",
    "        print('\\n Step-2: Add remaining self loops to the edge_index:')\n",
    "        edge_index, edge_weight = add_remaining_self_loops(\n",
    "            edge_index, edge_weight, fill_value, num_nodes)\n",
    "        print('type and shape of updated edge_index', type(edge_index), edge_index.shape)\n",
    "        print('type and shape of updated edge weights', type(edge_weight), edge_weight.shape, '\\n')\n",
    "        \n",
    "        row, col = edge_index\n",
    "        deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "        print('type and shape of out-degree of each node:', type(deg[row]), deg[row].shape)\n",
    "        print('type and shape of in-degree of each node:', type(deg[col]), deg[col].shape, '\\n')\n",
    "        \n",
    "        return edge_index, deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        \"\"\"\"\"\"\n",
    "        print('start calling the forward of the GCNConv: ')\n",
    "        print('type and shape of the node feature matrix: ', type(x), x.shape)\n",
    "        print('type and shape of the edge_index matrix in COO format: ', type(x), x.shape)\n",
    "        x = torch.matmul(x, self.weight)\n",
    "        \n",
    "        print('\\n Step-1: Initialize the weights (continuous uniform distribution) and use it to linearly transform feature matrix: ')\n",
    "        print('type and shape of the node feature matrix after linear transformation', type(x), x.shape, '\\n')\n",
    "        \n",
    "        if self.cached and self.cached_result is not None:\n",
    "            if edge_index.size(1) != self.cached_num_edges:\n",
    "                raise RuntimeError(\n",
    "                    'Cached {} number of edges, but found {}. Please '\n",
    "                    'disable the caching behavior of this layer by removing '\n",
    "                    'the `cached=True` argument in its constructor.'.format(\n",
    "                        self.cached_num_edges, edge_index.size(1)))\n",
    "        \n",
    "        print('%' * 200)\n",
    "        print('Call the norm function inside forward (add self loops (i, i) edges to the edge_index and compute nomalization constants): ')\n",
    "        if not self.cached or self.cached_result is None:\n",
    "            self.cached_num_edges = edge_index.size(1)\n",
    "            edge_index, norm = self.norm(edge_index, x.size(0), edge_weight,\n",
    "                                         self.improved, x.dtype)\n",
    "            self.cached_result = edge_index, norm\n",
    "\n",
    "        edge_index, norm = self.cached_result\n",
    "        print('End of calling the norm function')\n",
    "        print('%' * 200)\n",
    "        print()\n",
    "        \n",
    "        print('Start calling the propagationg function inside the GCNConv forward func: ')\n",
    "        return self.propagate(edge_index, x=x, norm=norm)\n",
    "\n",
    "    def message(self, x_j, norm):\n",
    "        print('\\n Step-3: normalize the node feature, normalization constants are sqrt(in-dgree)*sqrt(out-dgree) of each node (inside the message function)')\n",
    "        return norm.view(-1, 1) * x_j\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        if self.bias is not None:\n",
    "            aggr_out = aggr_out + self.bias\n",
    "        return aggr_out\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}({}, {})'.format(self.__class__.__name__, self.in_channels,\n",
    "                                   self.out_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the dataset for input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info (attributes) of a single data instance\n",
      "Data(edge_index=[2, 88648], test_mask=[19717], train_mask=[19717], val_mask=[19717], x=[19717, 500], y=[19717]) \n",
      " number of nodes:  19717 \n",
      " number of edges:  88648 \n",
      " number of features per ndoe:  500 \n",
      " number of edge features:  0 \n",
      " all the attributes of data:  ['x', 'edge_index', 'y', 'train_mask', 'val_mask', 'test_mask']\n"
     ]
    }
   ],
   "source": [
    "'''Allow additional attributes of the instance to be set by defining: __setitem__ function '''\n",
    "from torch_geometric.datasets import Planetoid\n",
    "dataset = Planetoid(root='~/tmp/Planetoid/PubMed', name='PubMed')\n",
    "# print(len(dataset), dataset.num_classes, dataset.num_node_features)\n",
    "data = dataset[0]\n",
    "print('Info (attributes) of a single data instance')\n",
    "print(data, '\\n number of nodes: ', data.num_nodes, '\\n number of edges: ', data.num_edges, \\\n",
    "      '\\n number of features per ndoe: ', data.num_node_features, '\\n number of edge features: ', data.num_edge_features, \\\n",
    "      '\\n all the attributes of data: ', data.keys)\n",
    "# print(data.train_mask.shape)   # 1-D attributes\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = 'cpu'\n",
    "data = data.to(device)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "# calling one single GCNConv layer\n",
    "conv1 = custom_GCNConv(dataset.num_node_features, 2).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the customized GCNConv layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before the GCN layer, node feature matrix type and shape:  <class 'torch.Tensor'> torch.Size([19717, 500])\n",
      "before the GCN layer, edge_index matrix type and shape:  <class 'torch.Tensor'> torch.Size([2, 88648])\n",
      "Start calling the GCNConv \n",
      "\n",
      "********************************************************************************************************************************************************************************************************\n",
      "start calling the forward of the GCNConv: \n",
      "type and shape of the node feature matrix:  <class 'torch.Tensor'> torch.Size([19717, 500])\n",
      "type and shape of the edge_index matrix in COO format:  <class 'torch.Tensor'> torch.Size([19717, 500])\n",
      "\n",
      " Step-1: Initialize the weights (continuous uniform distribution) and use it to linearly transform feature matrix: \n",
      "type and shape of the node feature matrix after linear transformation <class 'torch.Tensor'> torch.Size([19717, 2]) \n",
      "\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "Call the norm function inside forward (add self loops (i, i) edges to the edge_index and compute nomalization constants): \n",
      "Inside the norm function: \n",
      "type and shape of edge_weights (default to be all 1):  <class 'torch.Tensor'> torch.Size([88648])\n",
      "\n",
      " Step-2: Add remaining self loops to the edge_index:\n",
      "type and shape of updated edge_index <class 'torch.Tensor'> torch.Size([2, 108365])\n",
      "type and shape of updated edge weights <class 'torch.Tensor'> torch.Size([108365]) \n",
      "\n",
      "type and shape of out-degree of each node: <class 'torch.Tensor'> torch.Size([108365])\n",
      "type and shape of in-degree of each node: <class 'torch.Tensor'> torch.Size([108365]) \n",
      "\n",
      "End of calling the norm function\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "\n",
      "Start calling the propagationg function inside the GCNConv forward func: \n",
      "========================================================================================================================================================================================================\n",
      "Start output the info from the propagation function from messagepassing class (inherited by GCNConv):\n",
      "update_args:  []\n",
      "kwargs include the args:  dict_keys(['x', 'norm', 'edge_index', 'size']) \n",
      "\n",
      "message_args during propagation for each convolution step contains: \n",
      "Number  0  val type:  <class 'torch.Tensor'>  val shape:  torch.Size([108365, 2])\n",
      "Number  1  val type:  <class 'torch.Tensor'>  val shape:  torch.Size([108365])\n",
      "\n",
      " call the message function inside the GCNConv (normalize the feature according to in- or out- degree of each nodes): \n",
      "\n",
      " Step-3: normalize the node feature, normalization constants are sqrt(in-dgree)*sqrt(out-dgree) of each node (inside the message function)\n",
      "type and shape of the embedding after normalization based on in-dgree and out-degree of each node:  <class 'torch.Tensor'> torch.Size([108365, 2]) \n",
      "\n",
      "\n",
      " Step-4: call the scatter_ function (aggregates the feature values of source nodes into target nodes): \n",
      "type and shape of embedding after scattering:  <class 'torch.Tensor'> torch.Size([19717, 2]) \n",
      "\n",
      "call the update function (may add the bias for GCNConv, default all zeros): \n",
      "type and shape of embedding after udpating (may add bias):  <class 'torch.Tensor'> torch.Size([19717, 2]) \n",
      "\n",
      "\n",
      " Step-5: return the new node embeddings: number_of_nodes by out_channels tensor. \n",
      "End of the Info from the propagation function in messagepassing class \n",
      "========================================================================================================================================================================================================\n",
      "********************************************************************************************************************************************************************************************************\n",
      "End of calling the GCNConv \n",
      "\n",
      "after the GCN layer, node feature matrix type and shape:  <class 'torch.Tensor'> torch.Size([19717, 2])\n"
     ]
    }
   ],
   "source": [
    "x, edge_index = data.x.cuda(), data.edge_index.cuda()\n",
    "# each row inside the x is the feature vector of a single node inside the graph\n",
    "print('before the GCN layer, node feature matrix type and shape: ', type(x), x.shape)\n",
    "print('before the GCN layer, edge_index matrix type and shape: ', type(edge_index), edge_index.shape)\n",
    "print('Start calling the GCNConv \\n')\n",
    "print('*' * 200)\n",
    "x = conv1(x, edge_index)\n",
    "print('*' * 200)\n",
    "print('End of calling the GCNConv \\n')\n",
    "\n",
    "print('after the GCN layer, node feature matrix type and shape: ', type(x), x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    0,     0,     0,  ..., 19714, 19715, 19716], device='cuda:0') tensor([ 1378,  1544,  6092,  ..., 12278,  4284, 16030], device='cuda:0')\n",
      "88648\n",
      "88648\n",
      "0 44324\n"
     ]
    }
   ],
   "source": [
    "# to check that in the COO format of the current undirected graph, each edge has been recorded twice\n",
    "ref = {}\n",
    "\n",
    "x, y = data.edge_index\n",
    "print(x, y)\n",
    "x, y = list(x.data.cpu().numpy()), list(y.data.cpu().numpy())\n",
    "res = list(zip(x, y))\n",
    "print(len(res))\n",
    "print(len(set(res)))\n",
    "for a, b in zip(x, y):\n",
    "    if (b, a) in ref:\n",
    "        ref[(b, a)] += 1\n",
    "    else:\n",
    "        ref[(a, b)] = 1\n",
    "res1 = [key for key, val in ref.items() if val == 1]\n",
    "res2 = [key for key, val in ref.items() if val == 2]\n",
    "print(len(res1), len(res2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse the models constructed by GCNConvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, 16)\n",
    "        self.conv2 = GCNConv(16, dataset.num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        print('start calling the forward of the GCNConv, before any layers: ')\n",
    "        print('type and shape of the node feature matrix: ', type(x), x.shape)\n",
    "        print('type and shape of the edge_index matrix in COO format: ', type(x), x.shape)\n",
    "        \n",
    "        x = self.conv1(x, edge_index)\n",
    "        print('After the first GCNConv layer, type and shape of the node feature matrix: ', type(x), x.shape)\n",
    "        \n",
    "#         # here we introduce the non-linearity\n",
    "#         x = F.relu(x)\n",
    "#         print('After the first relu, type and shape of the node feature matrix: ', type(x), x.shape)\n",
    "        \n",
    "#         x = F.dropout(x, training=self.training)\n",
    "#         print('After the first drop out layer, type and shape of the node feature matrix: ', type(x), x.shape)\n",
    "        \n",
    "#         x = self.conv2(x, edge_index)\n",
    "#         print('After the second GCNConv layer, type and shape of the node feature matrix: ', type(x), x.shape)\n",
    "                \n",
    "        return F.log_softmax(x, dim=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info (attributes) of a single data instance\n",
      "Data(edge_index=[2, 88648], test_mask=[19717], train_mask=[19717], val_mask=[19717], x=[19717, 500], y=[19717]) \n",
      " number of nodes:  19717 \n",
      " number of edges:  88648 \n",
      " number of features per ndoe:  500 \n",
      " number of edge features:  0 \n",
      " all the attributes of data:  ['x', 'edge_index', 'y', 'train_mask', 'val_mask', 'test_mask']\n"
     ]
    }
   ],
   "source": [
    "'''Allow additional attributes of the instance to be set by defining: __setitem__ function '''\n",
    "from torch_geometric.datasets import Planetoid\n",
    "dataset = Planetoid(root='~/tmp/Planetoid/PubMed', name='PubMed')\n",
    "# print(len(dataset), dataset.num_classes, dataset.num_node_features)\n",
    "data = dataset[0]\n",
    "print('Info (attributes) of a single data instance')\n",
    "print(data, '\\n number of nodes: ', data.num_nodes, '\\n number of edges: ', data.num_edges, \\\n",
    "      '\\n number of features per ndoe: ', data.num_node_features, '\\n number of edge features: ', data.num_edge_features, \\\n",
    "      '\\n all the attributes of data: ', data.keys)\n",
    "# print(data.train_mask.shape)   # 1-D attributes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.parameters at 0x7fc89e8d8b10> <class 'generator'>\n",
      "<class 'torch.Tensor'> torch.Size([500, 16])\n",
      "<class 'torch.Tensor'> torch.Size([16])\n",
      "<class 'torch.Tensor'> torch.Size([16, 3])\n",
      "<class 'torch.Tensor'> torch.Size([3])\n",
      "**************************************************************************************************** \n",
      "start the train:\n",
      "\n",
      "start calling the forward of the GCNConv, before any layers: \n",
      "type and shape of the node feature matrix:  <class 'torch.Tensor'> torch.Size([19717, 500])\n",
      "type and shape of the edge_index matrix in COO format:  <class 'torch.Tensor'> torch.Size([19717, 500])\n",
      "After the first GCNConv layer, type and shape of the node feature matrix:  <class 'torch.Tensor'> torch.Size([19717, 16])\n",
      "\n",
      "after the GCN based model forward, classification result matrix of probability type and shape:  <class 'torch.Tensor'> torch.Size([19717, 16])\n",
      "tensor(2.7716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "after the GCN based model forward, loss type and shape:  <class 'torch.Tensor'> torch.Size([]) 2.771639347076416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/conda-bld/pytorch_1565272271120/work/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.\n",
      "/opt/conda/conda-bld/pytorch_1565272271120/work/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.\n",
      "/opt/conda/conda-bld/pytorch_1565272271120/work/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): GCNConv(500, 16)\n",
       "  (conv2): GCNConv(16, 3)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Net().to(device)\n",
    "data = dataset[0].to(device)\n",
    "# Parameter is a generator associated with a model instance (of torch.nn.Module)\n",
    "# here is the weight and bias for each GCNConv layers\n",
    "print(model.parameters(), type(model.parameters()))\n",
    "for param in model.parameters():\n",
    "    print(type(param.data), param.size())\n",
    "# specify which parameters need to be updated\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "\n",
    "model.train()\n",
    "print('*'*100, '\\nstart the train:\\n')\n",
    "for epoch in range(1):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    print('\\nafter the GCN based model forward, classification result matrix of probability type and shape: ', type(out), out.shape)\n",
    "    print('compare the predict and label: ')\n",
    "    print('predict shape: ',out[data.train_mask].shape, 'label shape: ', data.y[data.train_mask].shape)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "    print(loss)\n",
    "    print('after the GCN based model forward, loss type and shape: ', type(loss), loss.shape, loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    \n",
    "model.eval()\n",
    "### this prediction will be the second call of the model instance\n",
    "# _, pred = model(data).max(dim=1)\n",
    "# correct = float (pred[data.test_mask].eq(data.y[data.test_mask]).sum().item())\n",
    "# acc = correct / data.test_mask.sum().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What does the backward() function do:**\n",
    "\n",
    "loss.backward() computes dloss/dx for every parameter x which has requires_grad=True. These are accumulated into x.grad for every parameter x. In pseudo-code:\n",
    "\n",
    "x.grad += dloss/dx\n",
    "\n",
    "optimizer.step updates the value of x using the gradient x.grad. For example, the SGD optimizer performs:\n",
    "\n",
    "x += -lr * x.grad\n",
    "\n",
    "https://discuss.pytorch.org/t/what-does-the-backward-function-do/9944\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_geometric]",
   "language": "python",
   "name": "conda-env-pytorch_geometric-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
