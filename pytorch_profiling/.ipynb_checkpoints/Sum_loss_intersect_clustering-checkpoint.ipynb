{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import metis\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dig into the networkx for graph partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orginal graph G : \n",
      "<class 'networkx.classes.graph.Graph'> [0, 1, 2, 3] [(0, 1), (1, 2), (2, 3)]\n",
      "subgraph H from G: \n",
      "<class 'networkx.classes.graph.Graph'> <class 'networkx.classes.reportviews.NodeView'> [0, 1, 2] [(0, 1), (1, 2)]\n"
     ]
    }
   ],
   "source": [
    "G = nx.path_graph(4)  # or DiGraph, MultiGraph, MultiDiGraph, etc\n",
    "print('orginal graph G : ')\n",
    "print(type(G), list(G.nodes()), list(G.edges))\n",
    "H = G.subgraph([0, 1, 2])\n",
    "print('subgraph H from G: ')\n",
    "print(type(H), type(H.nodes()), list(H.nodes()), list(H.edges))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partition GCN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1) First use this customized GCNConv single layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import inspect\n",
    "\n",
    "import torch\n",
    "from torch_geometric.utils import scatter_\n",
    "\n",
    "special_args = ['edge_index', 'edge_index_i', 'edge_index_j', 'size', 'size_i', 'size_j']\n",
    "\n",
    "__size_error_msg__ = ('All tensors which should get mapped to the same source '\n",
    "                      'or target nodes must be of same size in dimension 0.')\n",
    "\n",
    "is_python2 = sys.version_info[0] < 3\n",
    "getargspec = inspect.getargspec if is_python2 else inspect.getfullargspec\n",
    "\n",
    "\n",
    "class custom_MessagePassing(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, aggr='add', flow='source_to_target'):\n",
    "        super(custom_MessagePassing, self).__init__()\n",
    "\n",
    "        self.aggr = aggr\n",
    "        assert self.aggr in ['add', 'mean', 'max']\n",
    "\n",
    "        self.flow = flow\n",
    "        assert self.flow in ['source_to_target', 'target_to_source']\n",
    "\n",
    "        self.__message_args__ = getargspec(self.message)[0][1:]\n",
    "        self.__special_args__ = [(i, arg)\n",
    "                                 for i, arg in enumerate(self.__message_args__)\n",
    "                                 if arg in special_args]\n",
    "        self.__message_args__ = [\n",
    "            arg for arg in self.__message_args__ if arg not in special_args\n",
    "        ]\n",
    "        self.__update_args__ = getargspec(self.update)[0][2:]\n",
    "\n",
    "    def propagate(self, edge_index, size=None, **kwargs):\n",
    "        print('='*200)\n",
    "        print('Start output the info from the propagation function from messagepassing class (inherited by GCNConv):')\n",
    "        dim = 0\n",
    "        size = [None, None] if size is None else list(size)\n",
    "        assert len(size) == 2\n",
    "\n",
    "        i, j = (0, 1) if self.flow == 'target_to_source' else (1, 0)\n",
    "        ij = {\"_i\": i, \"_j\": j}\n",
    "\n",
    "        message_args = []\n",
    "        for arg in self.__message_args__:\n",
    "            if arg[-2:] in ij.keys():\n",
    "                tmp = kwargs.get(arg[:-2], None)\n",
    "                if tmp is None:  # pragma: no cover\n",
    "                    message_args.append(tmp)\n",
    "                else:\n",
    "                    idx = ij[arg[-2:]]\n",
    "                    if isinstance(tmp, tuple) or isinstance(tmp, list):\n",
    "                        assert len(tmp) == 2\n",
    "                        if tmp[1 - idx] is not None:\n",
    "                            if size[1 - idx] is None:\n",
    "                                size[1 - idx] = tmp[1 - idx].size(dim)\n",
    "                            if size[1 - idx] != tmp[1 - idx].size(dim):\n",
    "                                raise ValueError(__size_error_msg__)\n",
    "                        tmp = tmp[idx]\n",
    "\n",
    "                    if tmp is None:\n",
    "                        message_args.append(tmp)\n",
    "                    else:\n",
    "                        if size[idx] is None:\n",
    "                            size[idx] = tmp.size(dim)\n",
    "                        if size[idx] != tmp.size(dim):\n",
    "                            raise ValueError(__size_error_msg__)\n",
    "\n",
    "                        tmp = torch.index_select(tmp, dim, edge_index[idx])\n",
    "                        message_args.append(tmp)\n",
    "            else:\n",
    "                message_args.append(kwargs.get(arg, None))\n",
    "\n",
    "        size[0] = size[1] if size[0] is None else size[0]\n",
    "        size[1] = size[0] if size[1] is None else size[1]\n",
    "\n",
    "        kwargs['edge_index'] = edge_index\n",
    "        kwargs['size'] = size\n",
    "\n",
    "        for (idx, arg) in self.__special_args__:\n",
    "            if arg[-2:] in ij.keys():\n",
    "                message_args.insert(idx, kwargs[arg[:-2]][ij[arg[-2:]]])\n",
    "            else:\n",
    "                message_args.insert(idx, kwargs[arg])\n",
    "\n",
    "        update_args = [kwargs[arg] for arg in self.__update_args__]\n",
    "        print('update_args: ', update_args)\n",
    "        print('kwargs include the args: ', kwargs.keys(), '\\n')\n",
    "        print('message_args during propagation for each convolution step contains: ')\n",
    "        # assume all the elements inside the message are tensor\n",
    "        for idx, val in enumerate(message_args):\n",
    "            print('Number ', idx, ' val type: ', type(message_args[idx]), ' val shape: ', message_args[idx].shape)\n",
    "        \n",
    "        \n",
    "        print('\\n call the message function inside the GCNConv (normalize the feature according to in- or out- degree of each nodes): ')\n",
    "        out = self.message(*message_args)\n",
    "        print('type and shape of the embedding after normalization based on in-dgree and out-degree of each node: ', type(out), out.shape, '\\n')\n",
    "        \n",
    "        print('\\n Step-4: call the scatter_ function (aggregates the feature values of source nodes into target nodes): ')\n",
    "        out = scatter_(self.aggr, out, edge_index[i], dim_size=size[i])\n",
    "        print('type and shape of embedding after scattering: ', type(out), out.shape, '\\n')\n",
    "        \n",
    "        print('call the update function (may add the bias for GCNConv, default all zeros): ')\n",
    "        out = self.update(out, *update_args)\n",
    "        print('type and shape of embedding after udpating (may add bias): ', type(out), out.shape, '\\n')\n",
    "        \n",
    "        print('\\n Step-5: return the new node embeddings: number_of_nodes by out_channels tensor. ')\n",
    "        print('End of the Info from the propagation function in messagepassing class ')\n",
    "        print('='*200)\n",
    "        return out\n",
    "\n",
    "    def message(self, x_j):  # pragma: no cover\n",
    "        return x_j\n",
    "\n",
    "    def update(self, aggr_out):  # pragma: no cover\n",
    "        return aggr_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Parameter\n",
    "from torch_scatter import scatter_add\n",
    "# from torch_geometric.nn import MessagePassing\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import add_remaining_self_loops\n",
    "import math\n",
    "\n",
    "def glorot(tensor):\n",
    "    if tensor is not None:\n",
    "        stdv = math.sqrt(6.0 / (tensor.size(-2) + tensor.size(-1)))\n",
    "#         tensor.data.uniform_(-stdv, stdv)\n",
    "        tensor.data.fill_(1.0)   # trivial example\n",
    "        \n",
    "def zeros(tensor):\n",
    "    if tensor is not None:\n",
    "        tensor.data.fill_(0)\n",
    "\n",
    "class custom_GCNConv(custom_MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels, improved=False, cached=False,\n",
    "                 bias=True, **kwargs):\n",
    "        super().__init__(aggr='add', **kwargs)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.improved = improved\n",
    "        self.cached = cached\n",
    "\n",
    "        self.weight = Parameter(torch.Tensor(in_channels, out_channels))\n",
    "\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "        print('display the initial weight: ', self.weight)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        glorot(self.weight)\n",
    "        zeros(self.bias)\n",
    "        self.cached_result = None\n",
    "        self.cached_num_edges = None\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def norm(edge_index, num_nodes, edge_weight=None, improved=False, dtype=None):\n",
    "        \n",
    "        if edge_weight is None:\n",
    "            edge_weight = torch.ones((edge_index.size(1), ), dtype=dtype, device=edge_index.device)\n",
    "        \n",
    "        fill_value = 1 if not improved else 2\n",
    "        \n",
    "        edge_index, edge_weight = add_remaining_self_loops(\n",
    "            edge_index, edge_weight, fill_value, num_nodes)\n",
    "        \n",
    "        row, col = edge_index\n",
    "        deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "        \n",
    "        return edge_index, deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        \"\"\"\"\"\"\n",
    "        print('\\n Inside the GCNConv forward: \\n display current weights: ', self.weight)\n",
    "        x = torch.matmul(x, self.weight)\n",
    "        \n",
    "        \n",
    "        if self.cached and self.cached_result is not None:\n",
    "            if edge_index.size(1) != self.cached_num_edges:\n",
    "                raise RuntimeError(\n",
    "                    'Cached {} number of edges, but found {}. Please '\n",
    "                    'disable the caching behavior of this layer by removing '\n",
    "                    'the `cached=True` argument in its constructor.'.format(\n",
    "                        self.cached_num_edges, edge_index.size(1)))\n",
    "        \n",
    "        if not self.cached or self.cached_result is None:\n",
    "            self.cached_num_edges = edge_index.size(1)\n",
    "            edge_index, norm = self.norm(edge_index, x.size(0), edge_weight,\n",
    "                                         self.improved, x.dtype)\n",
    "            self.cached_result = edge_index, norm\n",
    "\n",
    "        edge_index, norm = self.cached_result\n",
    "        print('\\n *********May use the topology info: \\n')\n",
    "        print('shape of edge_index: ', edge_index.shape)\n",
    "        print('shapes of normalized of the edge_weight (edge_weight)', norm.shape)\n",
    "        \n",
    "        res = self.propagate(edge_index, x=x, norm=norm)\n",
    "        print('\\n display the forward result from a single GCNConv, aggregated features ')\n",
    "        print('print type shape and values: ', type(res), res.shape, res)\n",
    "        print('\\n End of the GCNCOnv foward')\n",
    "        \n",
    "        return res\n",
    "\n",
    "    def message(self, x_j, norm):\n",
    "        return norm.view(-1, 1) * x_j\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        if self.bias is not None:\n",
    "            aggr_out = aggr_out + self.bias\n",
    "        return aggr_out\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}({}, {})'.format(self.__class__.__name__, self.in_channels,\n",
    "                                   self.out_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2) Establish a simple model based on a customized single GCNConv layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Net, self).__init__()\n",
    "        # one trivial example\n",
    "        self.conv1 = custom_GCNConv(in_channels, out_channels)\n",
    "#         self.conv2 = GCNConv(16, dataset.num_classes)\n",
    "\n",
    "    def forward(self, edge_index, features):\n",
    "        print('\\n Inside the foward of the Net model, before call the conv1: ')\n",
    "        print('used type and shape of edge_index: ', type(edge_index), edge_index.shape)\n",
    "        print('used type and shape of features: ', type(features), features.shape)\n",
    "        features = self.conv1(features, edge_index)\n",
    "#         print('after call the conv1: ')\n",
    "#         print('aggregated features , type, shape, values : ', type(features), features.shape, features)\n",
    "        \n",
    "        predictions = F.log_softmax(features, dim=1)\n",
    "#         print('calculated predictions , type, shape, values : ', type(predictions), predictions.shape, predictions)\n",
    "        print('End of calling the Net model foward \\n')\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Partition method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import metis\n",
    "import random\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class ClusteringMachine(object):\n",
    "    \"\"\"\n",
    "    Clustering the graph, feature set and label. Performed on the CPU side\n",
    "    \"\"\"\n",
    "    def __init__(self, edge_index, features, label, partition_num = 2):\n",
    "        \"\"\"\n",
    "        :param edge_index: COO format of the edge indices.\n",
    "        :param features: Feature matrix (ndarray).\n",
    "        :param label: label vector (ndarray).\n",
    "        \"\"\"\n",
    "        tmp = edge_index.t().numpy().tolist()\n",
    "#         tmp = edge_index.t().cpu().numpy().tolist()\n",
    "        self.graph = nx.from_edgelist(tmp)\n",
    "        self.edge_index = edge_index\n",
    "        self.features = features\n",
    "        self.label = label\n",
    "        self.partition_num = partition_num\n",
    "        self._set_sizes()\n",
    "\n",
    "    def _set_sizes(self):\n",
    "        \"\"\"\n",
    "        Setting the feature and class count.\n",
    "        \"\"\"\n",
    "        self.node_count = self.features.shape[0]\n",
    "        self.feature_count = self.features.shape[1]    # features all always in the columns\n",
    "        self.label_count = torch.max(self.label)+1\n",
    "\n",
    "    def decompose(self, test_ratio):\n",
    "        \"\"\"\n",
    "        Decomposing the graph, partitioning the features and label, creating Torch arrays.\n",
    "        \"\"\"\n",
    "#         self.metis_clustering()\n",
    "        print(\"\\nRandom graph clustering started.\\n\")\n",
    "        self.random_clustering()\n",
    "        \n",
    "        self.general_data_partitioning(test_ratio)\n",
    "        self.general_data_intersect(test_ratio)\n",
    "        \n",
    "        self.transfer_edges_and_nodes()\n",
    "\n",
    "    # just allocate each node to arandom cluster, store the membership inside each dict\n",
    "    def random_clustering(self):\n",
    "        \"\"\"\n",
    "        Random clustering the nodes.\n",
    "        \"\"\"\n",
    "        self.clusters = [cluster for cluster in range(self.partition_num)]\n",
    "        # randomly divide into two clusters\n",
    "#         self.cluster_membership = {node: random.choice(self.clusters) for node in self.graph.nodes()}\n",
    "        # for trial case : half-half devision, single part one or two partitions\n",
    "        self.cluster_membership = {node: 0 for node in range(self.node_count // self.partition_num)}\n",
    "        if self.partition_num > 1:\n",
    "            self.cluster_membership.update({node: 1 for node in range(self.node_count // self.partition_num, self.node_count)})\n",
    "        \n",
    "        # independent of the clustering method:\n",
    "        self.intersect_cluster = []\n",
    "        for i in range(1, self.partition_num):\n",
    "            tmp = [(m, n) for m, n in zip(self.clusters, self.clusters[i:])]\n",
    "            self.intersect_cluster.extend(tmp)\n",
    "            \n",
    "        self.macro_inter_edges = set(self.graph.edges())\n",
    "\n",
    "    def metis_clustering(self):\n",
    "        \"\"\"\n",
    "        Clustering the graph with Metis. For details see:\n",
    "        \"\"\"\n",
    "        (st, parts) = metis.part_graph(self.graph, self.partition_num)\n",
    "        self.clusters = list(set(parts))\n",
    "        self.cluster_membership = {node: membership for node, membership in enumerate(parts)}\n",
    "\n",
    "\n",
    "    def general_data_partitioning(self, test_ratio):\n",
    "        \"\"\"\n",
    "        Creating data partitions and train-test splits.\n",
    "        \"\"\"\n",
    "        self.sg_nodes = {}\n",
    "        self.sg_edges = {}\n",
    "        \n",
    "        self.sg_train_nodes = {}\n",
    "        self.sg_test_nodes = {}\n",
    "        \n",
    "        self.sg_features = {}\n",
    "        self.sg_labels = {}\n",
    "        \n",
    "        \n",
    "        # for each cluster we have six dicts to separate overall attributes into different clusters\n",
    "        for cluster in self.clusters:\n",
    "            # Returns a SubGraph view of the subgraph induced on nodes.\n",
    "            # The induced subgraph of the graph contains the nodes in nodes and the edges between those nodes.\n",
    "\n",
    "            subgraph = self.graph.subgraph([node for node in sorted(self.graph.nodes()) if self.cluster_membership[node] == cluster])\n",
    "            self.sg_nodes[cluster] = [node for node in sorted(subgraph.nodes())]\n",
    "            \n",
    "            # what about the edges outside? Currently ignore those in-between clusters edges\n",
    "\n",
    "            # map each node into it's index inside its own cluster\n",
    "            # create own indices of each node inside its cluster, can we still use the global index ? yes\n",
    "            # we sort the nodes is to make sure it is consistent with the features and labels, nothing to do with edges\n",
    "            mapper = {node: i for i, node in enumerate(sorted(self.sg_nodes[cluster]))}\n",
    "\n",
    "            # the edges inside its own partition, from two directions since it is an undirected graph\n",
    "            # the edges order does not matter\n",
    "            self.sg_edges[cluster] = [[mapper[edge[0]], mapper[edge[1]]] for edge in subgraph.edges()] +  \\\n",
    "                                       [[mapper[edge[1]], mapper[edge[0]]] for edge in subgraph.edges()]\n",
    "            \n",
    "            # update the macro inter-sect edges:\n",
    "            # because graph.subgraph's edge index may not follow the same order as graph.edges()\n",
    "            self.macro_inter_edges -= set([(edge[0], edge[1]) for edge in subgraph.edges()] +  \\\n",
    "                                       [(edge[1], edge[0]) for edge in subgraph.edges()])\n",
    "            print('cluster: ', cluster, 'mcro_inter_edges: ', self.macro_inter_edges)\n",
    "            \n",
    "            # for each cluster divide into train/test groups:\n",
    "            self.sg_train_nodes[cluster], self.sg_test_nodes[cluster] = train_test_split(list(mapper.values()), test_size = test_ratio)\n",
    "            \n",
    "            self.sg_test_nodes[cluster] = sorted(self.sg_test_nodes[cluster])\n",
    "            self.sg_train_nodes[cluster] = sorted(self.sg_train_nodes[cluster])\n",
    "\n",
    "            # extract specific rows from the feature matrix\n",
    "            self.sg_features[cluster] = self.features[self.sg_nodes[cluster],:]\n",
    "            # labels are 1-D tensor with class labels\n",
    "            self.sg_labels[cluster] = self.label[self.sg_nodes[cluster]]\n",
    "\n",
    "    def general_data_intersect(self, test_ratio):\n",
    "        \"\"\"\n",
    "            create data intersection between different isolate partitions of data\n",
    "        \"\"\"\n",
    "        self.inter_nodes = defaultdict(set)\n",
    "        self.inter_edges = defaultdict(list)\n",
    "        \n",
    "        self.inter_train_nodes = {}\n",
    "        self.inter_test_nodes = {}\n",
    "        \n",
    "        self.inter_features = {}\n",
    "        self.inter_labels = {}\n",
    "        \n",
    "        # first we have to divide the remaining inter-cluster nodes and edges, in isolate cluster, we know that be graph.nodes() or graph.edges()\n",
    "        # here we only know the edges as stored inside the macro_inter_edges\n",
    "        for start, end in self.macro_inter_edges:\n",
    "            start_cluster_id = self.cluster_membership[start]\n",
    "            end_cluster_id = self.cluster_membership[end]\n",
    "            \n",
    "            # make sure the smaller is before the larger, to be consistent with the rule as indicated by the self.intersect_cluster\n",
    "            if start_cluster_id > end_cluster_id:\n",
    "                start_cluster_id, end_cluster_id = end_cluster_id, start_cluster_id\n",
    "            \n",
    "            self.inter_nodes[(start_cluster_id, end_cluster_id)] |= {start, end}\n",
    "            \n",
    "            # the edges inside its own partition, from two directions since it is an undirected graph\n",
    "            self.inter_edges[(start_cluster_id, end_cluster_id)].append([start, end])\n",
    "        \n",
    "        for inter_cluster in self.intersect_cluster:\n",
    "            self.inter_nodes[inter_cluster] = sorted(list(self.inter_nodes[inter_cluster]))\n",
    "            \n",
    "            # need to use the mapper re-index the edges\n",
    "            mapper = {node: i for i, node in enumerate(self.inter_nodes[inter_cluster])}\n",
    "            self.inter_edges[inter_cluster] = [[mapper[edge[0]], mapper[edge[1]]] for edge in self.inter_edges[inter_cluster]] +  \\\n",
    "                                       [[mapper[edge[1]], mapper[edge[0]]] for edge in self.inter_edges[inter_cluster]]\n",
    "            \n",
    "            # for each cluster divide into train/test groups:\n",
    "            self.inter_train_nodes[inter_cluster], self.inter_test_nodes[inter_cluster] = train_test_split(list(mapper.values()), test_size = test_ratio)\n",
    "            \n",
    "            self.inter_test_nodes[inter_cluster] = sorted(self.inter_test_nodes[inter_cluster])\n",
    "            self.inter_train_nodes[inter_cluster] = sorted(self.inter_train_nodes[inter_cluster])\n",
    "\n",
    "            # extract specific rows from the feature matrix\n",
    "            self.inter_features[inter_cluster] = self.features[self.inter_nodes[inter_cluster],:]\n",
    "            # labels are 1-D tensor with class labels\n",
    "            self.inter_labels[inter_cluster] = self.label[self.inter_nodes[inter_cluster]]\n",
    "        \n",
    "            \n",
    "    def transfer_edges_and_nodes(self):\n",
    "        \"\"\"\n",
    "        Transfering the data to PyTorch format.\n",
    "        \"\"\"\n",
    "        for cluster in self.clusters:\n",
    "            # for the isolated-cluster data\n",
    "            self.sg_nodes[cluster] = torch.LongTensor(self.sg_nodes[cluster])\n",
    "\n",
    "            self.sg_edges[cluster] = torch.LongTensor(self.sg_edges[cluster]).t()\n",
    "            # Expects input to be <= 2-D tensor and transposes dimensions 0 and 1.\n",
    "            self.sg_train_nodes[cluster] = torch.LongTensor(self.sg_train_nodes[cluster])\n",
    "            self.sg_test_nodes[cluster] = torch.LongTensor(self.sg_test_nodes[cluster])\n",
    "            self.sg_features[cluster] = torch.FloatTensor(self.sg_features[cluster])\n",
    "            self.sg_labels[cluster] = torch.LongTensor(self.sg_labels[cluster])\n",
    "        \n",
    "        for inter_cluster in self.intersect_cluster:\n",
    "            # for the inter-cluster data\n",
    "            self.inter_nodes[inter_cluster] = torch.LongTensor(self.inter_nodes[inter_cluster])\n",
    "\n",
    "            self.inter_edges[inter_cluster] = torch.LongTensor(self.inter_edges[inter_cluster]).t()\n",
    "            # Expects input to be <= 2-D tensor and transposes dimensions 0 and 1.\n",
    "            self.inter_train_nodes[inter_cluster] = torch.LongTensor(self.inter_train_nodes[inter_cluster])\n",
    "            self.inter_test_nodes[inter_cluster] = torch.LongTensor(self.inter_test_nodes[inter_cluster])\n",
    "            self.inter_features[inter_cluster] = torch.FloatTensor(self.inter_features[inter_cluster])\n",
    "            self.inter_labels[inter_cluster] = torch.LongTensor(self.inter_labels[inter_cluster])\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Partition Graph with trainiing and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class ClusterGCNTrainer(object):\n",
    "    \"\"\"\n",
    "    Training a ClusterGCN.\n",
    "    \"\"\"\n",
    "    def __init__(self, clustering_machine, in_channels, out_channels):\n",
    "        \"\"\"\n",
    "        :param ags: Arguments object.\n",
    "        :param clustering_machine:\n",
    "        \"\"\"  \n",
    "        self.clustering_machine = clustering_machine\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.create_model(in_channels, out_channels)\n",
    "\n",
    "    def create_model(self, in_channels, out_channels):\n",
    "        \"\"\"\n",
    "        Creating a StackedGCN and transferring to CPU/GPU.\n",
    "        \"\"\"\n",
    "        self.model = Net(in_channels, out_channels)\n",
    "#         self.model = StackedGCN(self.args, self.clustering_machine.feature_count, self.clustering_machine.class_count)\n",
    "        self.model = self.model.to(self.device)\n",
    "    \n",
    "    # call the forward function batch by batch\n",
    "    def do_forward_pass(self, cluster):\n",
    "        \"\"\"\n",
    "        Making a forward pass with data from a given partition.\n",
    "        :param cluster: Cluster index.\n",
    "        :return average_loss: Average loss on the cluster.\n",
    "        :return node_count: Number of nodes.\n",
    "        \"\"\"\n",
    "        # the edges inside each clustered have been re-mark with local indices inside the cluster\n",
    "        edges = self.clustering_machine.sg_edges[cluster].to(self.device)\n",
    "        macro_nodes = self.clustering_machine.sg_nodes[cluster].to(self.device)\n",
    "        \n",
    "        # already re-index each node inside its local cluster\n",
    "        train_nodes = self.clustering_machine.sg_train_nodes[cluster].to(self.device)\n",
    "        # features has the implicit index, for N1 by K matrix, it always implies node indices: [0, N1-1]\n",
    "        features = self.clustering_machine.sg_features[cluster].to(self.device)\n",
    "        # torch.squeeze()  removes all the dimension with value 1, change the target from 2-D  (N by 1) into 1-D N tensor\n",
    "        \n",
    "        target = self.clustering_machine.sg_labels[cluster].to(self.device)\n",
    "        \n",
    "        # calculate the probabilites from log_sofmax\n",
    "        predictions = self.model(edges, features)\n",
    "        print('info about predictions the isolated cluster batch number: #', cluster)\n",
    "        print(predictions)\n",
    "        # trial sum loss\n",
    "        sum_loss = torch.nn.functional.nll_loss(predictions, target, reduction = 'sum')\n",
    "        \n",
    "        print('info about sum loss of the batch number: #', cluster)\n",
    "        print(sum_loss)\n",
    "        print('after the GCN based model forward, loss type and shape: ', type(sum_loss), sum_loss.shape, sum_loss.item())\n",
    "        \n",
    "        node_count = train_nodes.shape[0]\n",
    "\n",
    "        # for each cluster keep track of the counts of the nodes\n",
    "        return sum_loss, node_count\n",
    "    def do_forward_pass_inter(self, inter_cluster):\n",
    "        \"\"\"\n",
    "        Making a forward pass with data from a given partition.\n",
    "        :param inter_cluster: inter_inter_cluster index.\n",
    "        :return sum_loss: total loss on the inter inter_cluster.\n",
    "        :return node_count: Number of nodes.\n",
    "        \"\"\"\n",
    "        edges = self.clustering_machine.inter_edges[inter_cluster].to(self.device)\n",
    "        macro_nodes = self.clustering_machine.inter_nodes[inter_cluster].to(self.device)\n",
    "        \n",
    "        train_nodes = self.clustering_machine.inter_train_nodes[inter_cluster].to(self.device)\n",
    "        features = self.clustering_machine.inter_features[inter_cluster].to(self.device)\n",
    "        target = self.clustering_machine.inter_labels[inter_cluster].to(self.device)\n",
    "        \n",
    "        predictions = self.model(edges, features)\n",
    "        print('info about predictions the inter inter_cluster batch number: #', inter_cluster)\n",
    "        print(predictions)\n",
    "        sum_loss = torch.nn.functional.nll_loss(predictions, target, reduction = 'sum')\n",
    "        \n",
    "        print('info about sum loss of the inter inter_cluster batch number: #', inter_cluster)\n",
    "        print(sum_loss)\n",
    "        print('after the GCN based model forward, inter inter_cluster loss type and shape: ', type(sum_loss), sum_loss.shape, sum_loss.item())\n",
    "        node_count = train_nodes.shape[0]\n",
    "\n",
    "        return sum_loss, node_count\n",
    "\n",
    "\n",
    "    def update_average_loss(self, batch_average_loss, node_count):\n",
    "        \"\"\"\n",
    "        Updating the average loss in the epoch.\n",
    "        :param batch_average_loss: Loss of the cluster. \n",
    "        :param node_count: Number of nodes in currently processed cluster.\n",
    "        :return average_loss: Average loss in the epoch.\n",
    "        \"\"\"\n",
    "        self.accumulated_training_loss = self.accumulated_training_loss + batch_average_loss.item()*node_count\n",
    "        self.node_count_seen = self.node_count_seen + node_count\n",
    "        average_loss = self.accumulated_training_loss / self.node_count_seen\n",
    "        return average_loss\n",
    "\n",
    "    def do_prediction(self, cluster):\n",
    "        \"\"\"\n",
    "        Scoring a cluster.\n",
    "        :param cluster: Cluster index.\n",
    "        :return prediction: Prediction matrix with probabilities.\n",
    "        :return target: Target vector.\n",
    "        \"\"\"\n",
    "        edges = self.clustering_machine.sg_edges[cluster].to(self.device)\n",
    "        macro_nodes = self.clustering_machine.sg_nodes[cluster].to(self.device)\n",
    "        test_nodes = self.clustering_machine.sg_test_nodes[cluster].to(self.device)\n",
    "        features = self.clustering_machine.sg_features[cluster].to(self.device)\n",
    "\n",
    "        target = self.clustering_machine.sg_labels[cluster].to(self.device).squeeze()\n",
    "        target = target[test_nodes]\n",
    "        # when we do the model forward, we still use all the samples in one cluster, indludng both test and train\n",
    "        prediction = self.model(edges, features)\n",
    "        prediction = prediction[test_nodes,:]\n",
    "        return prediction, target\n",
    "\n",
    "    # iterate through epoch and also the clusters\n",
    "    def train(self, epoch_num=10, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Training a model.\n",
    "        \"\"\"\n",
    "        print(\"Training started.\\n\")\n",
    "\n",
    "#         epochs = trange(epoch_num, desc = \"Train Loss\")\n",
    "        epochs = tqdm(range(epoch_num), desc = \"Train Loss\")\n",
    "        # A shortcut for tqdm(xrange(args), *kwargs). On Python3+ range is used instead of xrange.\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        self.model.train()\n",
    "\n",
    "        for epoch in epochs:\n",
    "            random.shuffle(self.clustering_machine.clusters)\n",
    "#             self.node_count_seen = 0\n",
    "            self.accumulated_training_loss = 0\n",
    "            # calculate each isolated cluster:\n",
    "            print('\\nstart training the isolated cluster:')\n",
    "            for cluster in self.clustering_machine.clusters:\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                batch_sum_loss, node_count = self.do_forward_pass(cluster)\n",
    "                batch_sum_loss.backward()\n",
    "#                 self.optimizer.step()\n",
    "                self.accumulated_training_loss += batch_sum_loss.item()\n",
    "            \n",
    "            print(\"\\nTrain sum Loss of isolated clusters: %g\" % round(self.accumulated_training_loss,4))\n",
    "            # calculate inter-cluster:\n",
    "            print('\\nstart training the inter-cluster:')\n",
    "            for inter_cluster in self.clustering_machine.intersect_cluster:\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                batch_sum_loss, node_count = self.do_forward_pass_inter(inter_cluster)\n",
    "                batch_sum_loss.backward()\n",
    "#                 self.optimizer.step()\n",
    "                self.accumulated_training_loss += batch_sum_loss.item()\n",
    "            \n",
    "                    \n",
    "            epochs.set_description(\"Total Train sum Loss: %g\" % round(self.accumulated_training_loss,4))\n",
    "\n",
    "    def test(self):\n",
    "        \"\"\"\n",
    "        Scoring the test and printing the F-1 score.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        self.predictions = []\n",
    "        self.targets = []\n",
    "        for cluster in self.clustering_machine.clusters:\n",
    "            prediction, target = self.do_prediction(cluster)\n",
    "\n",
    "            self.predictions.append(prediction.cpu().detach().numpy())\n",
    "            self.targets.append(target.cpu().detach().numpy())\n",
    "        \n",
    "        self.targets = np.concatenate(self.targets)\n",
    "\n",
    "        self.predictions = np.concatenate(self.predictions).argmax(1)  # return the indices of maximum probability \n",
    "\n",
    "        \n",
    "        score = f1_score(self.targets, self.predictions, average=\"micro\")\n",
    "        print(\"\\nF-1 score: {:.4f}\".format(score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 0.],\n",
      "        [1., 1.],\n",
      "        [0., 1.]]) torch.Size([10, 2])\n"
     ]
    }
   ],
   "source": [
    "edge_index = torch.tensor([[0, 1, 0, 8, 1, 3, 1, 2, 4, 2, 4, 6, 6, 7, 7, 9, 2, 5, 5, 9, 9, 8], \n",
    "                           [1, 0, 8, 0, 3, 1, 2, 1, 2, 4, 6, 4, 7, 6, 9, 7, 5, 2, 9, 5, 8, 9]])\n",
    "# features = torch.rand(10, 3)\n",
    "features = torch.tensor([[0, 1], [1, 0], [0, 1], [1, 0], [1, 1],  \n",
    "                           [1, 1], [0, 1], [0, 0], [1, 1], [0, 1]], dtype = torch.float)\n",
    "label = torch.tensor([0, 0, 1, 1, 1, 0, 1, 0, 1, 1])\n",
    "\n",
    "print(features, features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# draw the draph\n",
    "def custom_clustering(graph, partition_num):\n",
    "    clusters = [cluster for cluster in range(partition_num)]\n",
    "    node_count = len(graph.nodes())\n",
    "    cluster_membership = {node: 0 for node in range(node_count // partition_num)}\n",
    "    if partition_num > 1:\n",
    "        cluster_membership.update({node: 1 for node in range(node_count // partition_num, node_count)})\n",
    "    subgraphs = [graph.subgraph([node for node in sorted(graph.nodes()) if cluster_membership[node] == cluster]) for cluster in clusters]\n",
    "    return subgraphs\n",
    "\n",
    "# 1) the whole graph\n",
    "tmp = edge_index.t().numpy().tolist()\n",
    "whole_graph = nx.from_edgelist(tmp)\n",
    "# print(len(whole_graph.nodes()) )\n",
    "subgraphs = custom_clustering(whole_graph, 2)\n",
    "print(len(subgraphs))\n",
    "\n",
    "plt.subplot(131)\n",
    "nx.draw(whole_graph, with_labels=True, font_weight='bold')\n",
    "plt.subplot(132)\n",
    "nx.draw(subgraphs[0], with_labels=True, font_weight='bold')\n",
    "plt.subplot(133)\n",
    "nx.draw(subgraphs[1], with_labels=True, font_weight='bold')\n",
    "# 2) the two halves of the graph\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!(nvidia-smi | grep 'python' | awk '{ print $3 }' | xargs -n1 kill -9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random graph clustering started.\n",
      "\n",
      "cluster:  0 mcro_inter_edges:  {(6, 7), (4, 6), (8, 9), (9, 5), (2, 5), (0, 8), (7, 9)}\n",
      "cluster:  1 mcro_inter_edges:  {(4, 6), (2, 5), (0, 8)}\n"
     ]
    }
   ],
   "source": [
    "# first use two batches\n",
    "clustering_machine = ClusteringMachine(edge_index, features, label, partition_num = 2)\n",
    "clustering_machine.decompose(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1}\n",
      "\n",
      "{0: tensor([0, 1, 2, 3, 4]), 1: tensor([5, 6, 7, 8, 9])}\n",
      "{0: tensor([[0, 1, 1, 2, 1, 3, 2, 4],\n",
      "        [1, 3, 2, 4, 0, 1, 1, 2]]), 1: tensor([[3, 1, 2, 4, 4, 2, 4, 0],\n",
      "        [4, 2, 4, 0, 3, 1, 2, 4]])}\n",
      "[(0, 1)]\n",
      "{(4, 6), (2, 5), (0, 8)}\n",
      "[(0, 1), (0, 8), (1, 3), (1, 2), (8, 9), (2, 4), (2, 5), (4, 6), (6, 7), (7, 9), (9, 5)]\n",
      "defaultdict(<class 'set'>, {(0, 1): tensor([0, 2, 4, 5, 6, 8])})\n",
      "defaultdict(<class 'list'>, {(0, 1): tensor([[2, 1, 0, 4, 3, 5],\n",
      "        [4, 3, 5, 2, 1, 0]])})\n",
      "display the initial weight:  Parameter containing:\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-b61d49435727>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclustering_machine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minter_edges\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mgcn_trainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClusterGCNTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclustering_machine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mgcn_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Sum of total training loss of two batches by nll_loss function: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgcn_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulated_training_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-059995b8824c>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, clustering_machine, in_channels, out_channels)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclustering_machine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclustering_machine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-059995b8824c>\u001b[0m in \u001b[0;36mcreate_model\u001b[0;34m(self, in_channels, out_channels)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m#         self.model = StackedGCN(self.args, self.clustering_machine.feature_count, self.clustering_machine.class_count)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# call the forward function batch by batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    430\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_backward_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    228\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory"
     ]
    }
   ],
   "source": [
    "print(clustering_machine.node_count)\n",
    "print(clustering_machine.cluster_membership)\n",
    "print()\n",
    "print(clustering_machine.sg_nodes)\n",
    "print(clustering_machine.sg_edges)\n",
    "\n",
    "\n",
    "print(clustering_machine.intersect_cluster)\n",
    "\n",
    "print(clustering_machine.macro_inter_edges)\n",
    "\n",
    "print(clustering_machine.graph.edges())\n",
    "\n",
    "print(clustering_machine.inter_nodes)\n",
    "print(clustering_machine.inter_edges)\n",
    "\n",
    "gcn_trainer = ClusterGCNTrainer(clustering_machine, 2, 2)\n",
    "gcn_trainer.train(1)\n",
    "print('Sum of total training loss of two batches by nll_loss function: ', gcn_trainer.accumulated_training_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random graph clustering started.\n",
      "\n",
      "cluster:  0 mcro_inter_edges:  set()\n"
     ]
    }
   ],
   "source": [
    "# second use a single batch\n",
    "clustering_machine_1 = ClusteringMachine(edge_index, features, label, partition_num = 1)\n",
    "clustering_machine_1.decompose(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0}\n",
      "\n",
      " isolated cluster info: \n",
      "{0: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])}\n",
      "{0: tensor([[0, 0, 1, 1, 8, 2, 2, 4, 6, 7, 9, 1, 8, 3, 2, 9, 4, 5, 6, 7, 9, 5],\n",
      "        [1, 8, 3, 2, 9, 4, 5, 6, 7, 9, 5, 0, 0, 1, 1, 8, 2, 2, 4, 6, 7, 9]])}\n",
      "\n",
      " inter cluster info: \n",
      "[]\n",
      "set()\n",
      "[(0, 1), (0, 8), (1, 3), (1, 2), (8, 9), (2, 4), (2, 5), (4, 6), (6, 7), (7, 9), (9, 5)]\n",
      "defaultdict(<class 'set'>, {})\n",
      "defaultdict(<class 'list'>, {})\n",
      "\n",
      " Call the trainer: \n",
      "display the initial weight:  Parameter containing:\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "Training started.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a989292eac0432c9c9939c50534ac66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Train Loss', max=1, style=ProgressStyle(description_width='in"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "start training the isolated cluster:\n",
      "\n",
      " Inside the foward of the Net model, before call the conv1: \n",
      "used type and shape of edge_index:  <class 'torch.Tensor'> torch.Size([2, 22])\n",
      "used type and shape of features:  <class 'torch.Tensor'> torch.Size([10, 2])\n",
      "\n",
      " Inside the GCNConv forward: \n",
      " display current weights:  Parameter containing:\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]], device='cuda:0', requires_grad=True)\n",
      "\n",
      " *********May use the topology info: \n",
      "\n",
      "shape of edge_index:  torch.Size([2, 32])\n",
      "shapes of normalized of the edge_weight (edge_weight) torch.Size([32])\n",
      "========================================================================================================================================================================================================\n",
      "Start output the info from the propagation function from messagepassing class (inherited by GCNConv):\n",
      "update_args:  []\n",
      "kwargs include the args:  dict_keys(['x', 'norm', 'edge_index', 'size']) \n",
      "\n",
      "message_args during propagation for each convolution step contains: \n",
      "Number  0  val type:  <class 'torch.Tensor'>  val shape:  torch.Size([32, 2])\n",
      "Number  1  val type:  <class 'torch.Tensor'>  val shape:  torch.Size([32])\n",
      "\n",
      " call the message function inside the GCNConv (normalize the feature according to in- or out- degree of each nodes): \n",
      "type and shape of the embedding after normalization based on in-dgree and out-degree of each node:  <class 'torch.Tensor'> torch.Size([32, 2]) \n",
      "\n",
      "\n",
      " Step-4: call the scatter_ function (aggregates the feature values of source nodes into target nodes): \n",
      "type and shape of embedding after scattering:  <class 'torch.Tensor'> torch.Size([10, 2]) \n",
      "\n",
      "call the update function (may add the bias for GCNConv, default all zeros): \n",
      "type and shape of embedding after udpating (may add bias):  <class 'torch.Tensor'> torch.Size([10, 2]) \n",
      "\n",
      "\n",
      " Step-5: return the new node embeddings: number_of_nodes by out_channels tensor. \n",
      "End of the Info from the propagation function in messagepassing class \n",
      "========================================================================================================================================================================================================\n",
      "\n",
      " display the forward result from a single GCNConv, aggregated features \n",
      "print type shape and values:  <class 'torch.Tensor'> torch.Size([10, 2]) tensor([[1.2887, 1.2887],\n",
      "        [1.1422, 1.1422],\n",
      "        [1.6547, 1.6547],\n",
      "        [0.8536, 0.8536],\n",
      "        [1.2887, 1.2887],\n",
      "        [1.2440, 1.2440],\n",
      "        [1.0000, 1.0000],\n",
      "        [0.6220, 0.6220],\n",
      "        [1.2887, 1.2887],\n",
      "        [1.4047, 1.4047]], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      " End of the GCNCOnv foward\n",
      "End of calling the Net model foward \n",
      "\n",
      "info about predictions the isolated cluster batch number: # 0\n",
      "tensor([[-0.6931, -0.6931],\n",
      "        [-0.6931, -0.6931],\n",
      "        [-0.6931, -0.6931],\n",
      "        [-0.6931, -0.6931],\n",
      "        [-0.6931, -0.6931],\n",
      "        [-0.6931, -0.6931],\n",
      "        [-0.6931, -0.6931],\n",
      "        [-0.6931, -0.6931],\n",
      "        [-0.6931, -0.6931],\n",
      "        [-0.6931, -0.6931]], device='cuda:0', grad_fn=<LogSoftmaxBackward>)\n",
      "info about sum loss of the batch number: # 0\n",
      "tensor(6.9315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "after the GCN based model forward, loss type and shape:  <class 'torch.Tensor'> torch.Size([]) 6.931471824645996\n",
      "\n",
      "Train sum Loss of isolated clusters: 6.9315\n",
      "\n",
      "start training the inter-cluster:\n",
      "\n",
      "Sum of total training loss of two batches by nll_loss function:  6.931471824645996\n"
     ]
    }
   ],
   "source": [
    "print(clustering_machine_1.node_count)\n",
    "print(clustering_machine_1.cluster_membership)\n",
    "print('\\n isolated cluster info: ')\n",
    "print(clustering_machine_1.sg_nodes)\n",
    "print(clustering_machine_1.sg_edges)\n",
    "\n",
    "print('\\n inter cluster info: ')\n",
    "print(clustering_machine_1.intersect_cluster)\n",
    "print(clustering_machine_1.macro_inter_edges)\n",
    "print(clustering_machine_1.graph.edges())\n",
    "print(clustering_machine_1.inter_nodes)\n",
    "print(clustering_machine_1.inter_edges)\n",
    "\n",
    "print('\\n Call the trainer: ')\n",
    "gcn_trainer_1 = ClusterGCNTrainer(clustering_machine_1, 2, 2)\n",
    "gcn_trainer_1.train(1)\n",
    "print('Sum of total training loss of two batches by nll_loss function: ', gcn_trainer_1.accumulated_training_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use data from pytorch geometric datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 3 500\n",
      "Info (attributes) of a single data instance\n",
      "Data(edge_index=[2, 88648], test_mask=[19717], train_mask=[19717], val_mask=[19717], x=[19717, 500], y=[19717]) \n",
      " number of nodes:  19717 \n",
      " number of edges:  88648 \n",
      " number of features per ndoe:  500 \n",
      " number of edge features:  0 \n",
      " all the attributes of data:  ['x', 'edge_index', 'y', 'train_mask', 'val_mask', 'test_mask']\n"
     ]
    }
   ],
   "source": [
    "# this data is also used the in the trivial example of the cluster-GCN paper\n",
    "from torch_geometric.datasets import Planetoid\n",
    "dataset = Planetoid(root='~/tmp/Planetoid/PubMed', name='PubMed')\n",
    "print(len(dataset), dataset.num_classes, dataset.num_node_features)\n",
    "data = dataset[0]\n",
    "print('Info (attributes) of a single data instance')\n",
    "print(data, '\\n number of nodes: ', data.num_nodes, '\\n number of edges: ', data.num_edges, \\\n",
    "      '\\n number of features per ndoe: ', data.num_node_features, '\\n number of edge features: ', data.num_edge_features, \\\n",
    "      '\\n all the attributes of data: ', data.keys)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# data = data.to(device)\n",
    "\n",
    "# x, edge_index, y = data.x.cuda(), data.edge_index.cuda(), data.y.cuda()\n",
    "# print(edge_index[:,0])\n",
    "# tmp = edge_index.t().cpu().numpy().tolist()\n",
    "# print(tmp[1])\n",
    "\n",
    "# conv1 = custom_GCNConv(dataset.num_node_features, 2).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute the GCN partitioning graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random graph clustering started.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clustering_machine = ClusteringMachine(data.edge_index, data.x, data.y, partition_num = 2)\n",
    "clustering_machine.decompose(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: tensor([    0,     3,     4,  ..., 19712, 19714, 19715]), 1: tensor([    1,     2,     5,  ..., 19710, 19713, 19716])}\n",
      "{0: tensor([[   0,    0,    0,  ..., 9846, 6933, 9685],\n",
      "        [3049, 3809, 7242,  ..., 6582, 6748, 9044]]), 1: tensor([[   0,    0,    1,  ..., 9786, 9508, 9584],\n",
      "        [1495, 4174, 5252,  ..., 9470, 9507, 9515]])}\n"
     ]
    }
   ],
   "source": [
    "print(clustering_machine.sg_nodes)\n",
    "print(clustering_machine.sg_edges)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn_trainer = ClusterGCNTrainer(clustering_machine, dataset.num_node_features, dataset.num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25d4ba9319694916a77536cb114a6834",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Train Loss', max=2, style=ProgressStyle(description_width='in"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [5 x 3], m2: [500 x 3] at /opt/conda/conda-bld/pytorch_1565272271120/work/aten/src/THC/generic/THCTensorMathBlas.cu:273",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-342-f1c6f9d16e9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgcn_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-340-9a9c5303dc7c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epoch_num, learning_rate)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m                 \u001b[0mbatch_average_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_forward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;31m#                 batch_average_loss.requres_grad = True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0mbatch_average_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-340-9a9c5303dc7c>\u001b[0m in \u001b[0;36mdo_forward_pass\u001b[0;34m(self, cluster)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m# calculate the probabilites from log_sofmax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medges\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0;31m# calculate the loss scalar from a cluster, only use the train nodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m#         average_loss = torch.nn.functional.nll_loss(predictions[train_nodes], target[train_nodes])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-284-af05aa1d007a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, edge_index, features)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-283-1ddad23a1378>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;34m\"\"\"\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [5 x 3], m2: [500 x 3] at /opt/conda/conda-bld/pytorch_1565272271120/work/aten/src/THC/generic/THCTensorMathBlas.cu:273"
     ]
    }
   ],
   "source": [
    "gcn_trainer.train(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 22])\n",
      "<class 'networkx.classes.graph.Graph'>\n",
      "<class 'networkx.classes.reportviews.NodeView'> [0, 1, 8, 3, 2, 4, 6, 7, 9, 5]\n",
      "[0, 1]\n",
      "{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1}\n",
      "<class 'networkx.classes.graph.Graph'>\n",
      "[(8, 9), (6, 7), (7, 9), (9, 5)]\n",
      "11\n",
      "[5, 6, 7, 8, 9]\n",
      "{5: 0, 6: 1, 7: 2, 8: 3, 9: 4}\n",
      "[[3, 4], [1, 2], [2, 4], [4, 0], [4, 3], [2, 1], [4, 2], [0, 4]]\n",
      "[[8, 9], [6, 7], [7, 9], [9, 5], [9, 8], [7, 6], [9, 7], [5, 9]]\n",
      "\n",
      "investigate the point of mapper:\n",
      "[1, 0, 3, 4] [2]\n",
      "valid the set use of the graph/subgraph edges:\n",
      "{(0, 1), (1, 2), (1, 3), (6, 7), (4, 6), (8, 9), (9, 5), (2, 5), (0, 8), (2, 4), (7, 9)}\n",
      "{(8, 9), (9, 5), (6, 7), (7, 9)}\n",
      "print the graph difference: \n",
      "{(0, 1), (1, 2), (1, 3), (4, 6), (2, 5), (0, 8), (2, 4)}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# trial on the subgraph\n",
    "\n",
    "edge_index = torch.tensor([[0, 1, 0, 8, 1, 3, 1, 2, 4, 2, 4, 6, 6, 7, 7, 9, 2, 5, 5, 9, 9, 8], \n",
    "                           [1, 0, 8, 0, 3, 1, 2, 1, 2, 4, 6, 4, 7, 6, 9, 7, 5, 2, 9, 5, 8, 9]])\n",
    "\n",
    "print(edge_index.shape)\n",
    "tmp = edge_index.t().numpy().tolist()\n",
    "\n",
    "graph = nx.from_edgelist(tmp)\n",
    "# or  G=nx.Graph(edgelist) # use Graph constructor\n",
    "# here Graph class is for undirected graph, for directed: use DiGraph class instead\n",
    "print(type(graph))\n",
    "# begin to partition\n",
    "partition_num = 2\n",
    "clusters = [cluster for cluster in range(partition_num)]\n",
    "print(type(graph.nodes()), graph.nodes())\n",
    "# randomdized \n",
    "# cluster_membership = {node: random.choice(clusters) for node in graph.nodes()}\n",
    "\n",
    "# trivial for test case\n",
    "node_count = 10\n",
    "cluster_membership = {node: 0 for node in range(node_count//partition_num)}\n",
    "cluster_membership.update({node: 1 for node in range(node_count//partition_num, node_count)})\n",
    "\n",
    "print(clusters)\n",
    "print(cluster_membership)\n",
    "\n",
    "# for subfgraph\n",
    "cluster = 1\n",
    "# print(graph.nodes())\n",
    "\n",
    "subgraph = graph.subgraph([node for node in sorted(graph.nodes()) if cluster_membership[node] == cluster])\n",
    "print(type(subgraph))\n",
    "# for undirected graph, it is only stored once in networkx class\n",
    "print(subgraph.edges())\n",
    "print(len(graph.edges()))\n",
    "\n",
    "sg_nodes = {}\n",
    "sg_edges = {}\n",
    "\n",
    "sg_nodes[cluster] = [node for node in sorted(subgraph.nodes())]\n",
    "print(sg_nodes[cluster])  # just convert it into a list of nodes and store inside the dict\n",
    "mapper = {node: i for i, node in enumerate(sorted(sg_nodes[cluster]))}\n",
    "print(mapper)\n",
    "mapper2 = {node: node for i, node in enumerate(sorted(sg_nodes[cluster]))}\n",
    "\n",
    "# the edges inside its own partition, from two directions since it is an undirected graph\n",
    "sg_edges[cluster] = [[mapper[edge[0]], mapper[edge[1]]] for edge in subgraph.edges()] +  \\\n",
    "                           [[mapper[edge[1]], mapper[edge[0]]] for edge in subgraph.edges()]\n",
    "# there will be no key errors since all is inside subgraph\n",
    "original = [[mapper2[edge[0]], mapper2[edge[1]]] for edge in subgraph.edges()] +  \\\n",
    "                           [[mapper2[edge[1]], mapper2[edge[0]]] for edge in subgraph.edges()]\n",
    "\n",
    "print(sg_edges[cluster])\n",
    "print(original)\n",
    "# investigate the point of mapper:\n",
    "print()\n",
    "print('investigate the point of mapper:')\n",
    "sg_train_nodes = {}\n",
    "sg_test_nodes = {}\n",
    "sg_train_nodes[cluster], sg_test_nodes[cluster] = train_test_split(list(mapper.values()), test_size = 0.1)\n",
    "\n",
    "print(sg_train_nodes[cluster], sg_test_nodes[cluster])\n",
    "\n",
    "\n",
    "# validiate the graph set\n",
    "print('valid the set use of the graph/subgraph edges:')\n",
    "print(set(graph.edges()) )\n",
    "print(set(subgraph.edges()))\n",
    "print('print the graph difference: ')\n",
    "print(set(graph.edges()) - set(subgraph.edges()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'set'>, {1: {'a'}, 2: [5]})\n"
     ]
    }
   ],
   "source": [
    "a = defaultdict(set)\n",
    "a[1].add('a')\n",
    "\n",
    "a[2] = [5]\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_geometric]",
   "language": "python",
   "name": "conda-env-pytorch_geometric-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
