{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## isolated clustering including recombination of mini-cluster for hpc run \n",
    "\n",
    "Comments:\n",
    "\n",
    "By using the read weighted edge list from a csv file, it saves much space on self.graph\n",
    "\n",
    "This will for specific batch number and hop-layer number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import csv\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pickle\n",
    "import shutil\n",
    "\n",
    "from utils import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import torch\n",
    "from torch_geometric.utils import scatter_\n",
    "\n",
    "import math\n",
    "import random\n",
    "\n",
    "from torch.nn import Parameter\n",
    "from torch_scatter import scatter_add\n",
    "# from torch_geometric.nn import MessagePassing\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import add_remaining_self_loops\n",
    "\n",
    "### ====================== Establish a GCN based model ========================\n",
    "class ListModule(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Abstract list layer class.\n",
    "    \"\"\"\n",
    "    def __init__(self, *args):\n",
    "        \"\"\"\n",
    "        Module initializing.\n",
    "        \"\"\"\n",
    "        super(ListModule, self).__init__()\n",
    "        idx = 0\n",
    "        for module in args:\n",
    "            self.add_module(str(idx), module)\n",
    "            idx += 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Getting the indexed layer.\n",
    "        \"\"\"\n",
    "        if idx < 0 or idx >= len(self._modules):\n",
    "            raise IndexError('index {} is out of range'.format(idx))\n",
    "        it = iter(self._modules.values())\n",
    "        for i in range(idx):\n",
    "            next(it)\n",
    "        return next(it)\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Iterating on the layers.\n",
    "        \"\"\"\n",
    "        return iter(self._modules.values())\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Number of layers.\n",
    "        \"\"\"\n",
    "        return len(self._modules)\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, input_layers = [16, 16], dropout=0.3):\n",
    "        \"\"\"\n",
    "        input layers: list of integers\n",
    "        dropout: probability of droping out \n",
    "        \"\"\"\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.input_layers = input_layers\n",
    "        self.dropout = dropout\n",
    "        self.setup_layers()\n",
    "\n",
    "    def setup_layers(self):\n",
    "        \"\"\"\n",
    "        Creating the layes based on the args.\n",
    "        \"\"\"\n",
    "        self.layers = []\n",
    "        self.input_layers = [self.in_channels] + self.input_layers + [self.out_channels]\n",
    "        for i, _ in enumerate(self.input_layers[:-1]):\n",
    "            self.layers.append(GCNConv(self.input_layers[i],self.input_layers[i+1]))\n",
    "        self.layers = ListModule(*self.layers)\n",
    "\n",
    "    # change the dropout positions: \n",
    "    def forward(self, edge_index, features):\n",
    "        if len(self.layers) > 1:\n",
    "            for i in range(len(self.layers)-1):\n",
    "                features = F.relu(self.layers[i](features, edge_index))\n",
    "#                 if i>0:\n",
    "                features = F.dropout(features, p = self.dropout, training = self.training)\n",
    "                    \n",
    "            features = self.layers[len(self.layers)-1](features, edge_index)\n",
    "        else:\n",
    "            features = self.layers[0](features, edge_index)    # for a single layer case\n",
    "\n",
    "        predictions = F.log_softmax(features, dim=1)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partition GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import metis\n",
    "import random\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import chain\n",
    "\n",
    "class ClusteringMachine(object):\n",
    "    \"\"\"\n",
    "    Clustering the graph, feature set and label. Performed on the CPU side\n",
    "    \"\"\"\n",
    "    def __init__(self, edge_index, features, label, tmp_folder = './tmp/', info_folder = './info/'):\n",
    "        \"\"\"\n",
    "        :param edge_index: COO format of the edge indices.\n",
    "        :param features: Feature matrix (ndarray).\n",
    "        :param label: label vector (ndarray).\n",
    "        :tmp_folder(string): the path of the folder to contain all the clustering information files\n",
    "        \"\"\"\n",
    "        self.features = features\n",
    "        self.label = label\n",
    "        self._set_sizes()\n",
    "        self.edge_index = edge_index\n",
    "        # store the information folder for memory tracing\n",
    "        self.tmp_folder = tmp_folder\n",
    "        self.info_folder = info_folder\n",
    "        \n",
    "        tmp = edge_index.t().numpy().tolist()\n",
    "        self.graph = nx.from_edgelist(tmp)\n",
    "        \n",
    "    def _set_sizes(self):\n",
    "        \"\"\"\n",
    "        Setting the feature and class count.\n",
    "        \"\"\"\n",
    "        self.node_count = self.features.shape[0]\n",
    "        self.feature_count = self.features.shape[1]    # features all always in the columns\n",
    "        self.label_count = len(np.unique(self.label.numpy()) )\n",
    "    \n",
    "    # 1) first use different clustering method, then split each cluster into train, test and validation nodes, split edges\n",
    "    def split_cluster_nodes_edges(self, test_ratio, validation_ratio, partition_num = 4, batch_num = 2, round_num = 2):\n",
    "        \"\"\"\n",
    "            1) decompose the whole graph into parition_num small mini-clusters, all the mini-cluster relevant use local variables\n",
    "            2) recombine the mini-clusters into batch_num batches (self.sg_nodes_global)\n",
    "        \"\"\"\n",
    "        mini_cluster_nodes_global = self.metis_clustering(self.graph, partition_num)\n",
    "        mini_cluster_id = list(mini_cluster_nodes_global.keys())\n",
    "        \n",
    "        relative_test_ratio = (test_ratio) / (1 - validation_ratio)\n",
    "        \n",
    "        mini_cluster_validation_nodes_global = {}\n",
    "        mini_cluster_train_nodes_global = {}\n",
    "        mini_cluster_test_nodes_global = {}\n",
    "        \n",
    "        for cluster in mini_cluster_id:\n",
    "            mini_cluster_model_nodes_global, mini_cluster_validation_nodes_global[cluster] = \\\n",
    "                    train_test_split(mini_cluster_nodes_global[cluster], test_size = validation_ratio)\n",
    "            mini_cluster_train_nodes_global[cluster], mini_cluster_test_nodes_global[cluster] = \\\n",
    "                    train_test_split(mini_cluster_model_nodes_global, test_size = relative_test_ratio)\n",
    "            \n",
    "        #recombine_mini_cluster_for_batch:\n",
    "        self.sg_nodes_global = {}\n",
    "        self.sg_validation_nodes_global = {}\n",
    "        self.sg_train_nodes_global = {}\n",
    "        self.sg_test_nodes_global = {}\n",
    "        # keep the info of each cluster:\n",
    "        self.info_isolate_cluster_size = {}\n",
    "        self.info_validation_cluster_size = {}\n",
    "        self.info_train_cluster_size = {}\n",
    "        self.info_test_cluster_size = {}\n",
    "        # compute how many elements is inside each batch\n",
    "        chunck_size = partition_num // batch_num\n",
    "        for round_id in range(round_num):\n",
    "            # first shuffle all the mini-cluster ids:\n",
    "            mini_cluster_order = mini_cluster_id\n",
    "            random.shuffle(mini_cluster_order)\n",
    "            combine_group = [mini_cluster_order[i * chunck_size : (i + 1) * chunck_size] for i in range((len(mini_cluster_order) + chunck_size - 1) // chunck_size )]  \n",
    "            for local_batch_id, group in enumerate(combine_group):\n",
    "                global_batch_id = round_id * batch_num + local_batch_id\n",
    "                self.sg_nodes_global[global_batch_id] = list(chain.from_iterable(mini_cluster_nodes_global[cluster_id] for cluster_id in group))\n",
    "                self.sg_validation_nodes_global[global_batch_id] = list(chain.from_iterable(mini_cluster_validation_nodes_global[cluster_id] for cluster_id in group))\n",
    "                self.sg_train_nodes_global[global_batch_id] = list(chain.from_iterable(mini_cluster_train_nodes_global[cluster_id] for cluster_id in group))\n",
    "                self.sg_test_nodes_global[global_batch_id] = list(chain.from_iterable(mini_cluster_test_nodes_global[cluster_id] for cluster_id in group))\n",
    "        \n",
    "        for batch in self.sg_nodes_global.keys():\n",
    "            # record the information of each recombined batch:\n",
    "            self.info_isolate_cluster_size[batch] = len(self.sg_nodes_global[batch])\n",
    "            self.info_validation_cluster_size[batch] = len(self.sg_validation_nodes_global[batch])\n",
    "            self.info_train_cluster_size[batch] = len(self.sg_train_nodes_global[batch])\n",
    "            self.info_test_cluster_size[batch] = len(self.sg_test_nodes_global[batch])\n",
    "    \n",
    "    # just allocate each node to arandom cluster, store the membership inside each dict\n",
    "    def random_clustering(self, target_nodes, partition_num):\n",
    "        \"\"\"\n",
    "            Random clustering the nodes.\n",
    "            Input: \n",
    "                1) target_nodes: list of node \n",
    "                2) partition_num: number of partition to be generated\n",
    "            Output: \n",
    "                1) membership of each node\n",
    "        \"\"\"\n",
    "        # randomly divide into two clusters\n",
    "        nodes_order = [node for node in target_nodes]\n",
    "        random.shuffle(nodes_order)\n",
    "        n = (len(nodes_order) + partition_num - 1) // partition_num\n",
    "        partition_list = [nodes_order[i * n:(i + 1) * n] for i in range(partition_num)]\n",
    "#         cluster_membership = {node : i for i, node_list in enumerate(partition_list) for node in node_list}\n",
    "        cluster_nodes_global = {i : node_list for i, node_list in enumerate(partition_list)}\n",
    "        \n",
    "        return cluster_nodes_global\n",
    "\n",
    "    def metis_clustering(self, target_graph, partition_num):\n",
    "        \"\"\"\n",
    "            Random clustering the nodes.\n",
    "            Input: \n",
    "                1) target_nodes: list of node \n",
    "                2) partition_num: number of partition to be generated\n",
    "            Output: \n",
    "                1) membership of each node\n",
    "        \"\"\"\n",
    "        (st, parts) = metis.part_graph(target_graph, partition_num)\n",
    "        clusters = list(set(parts))\n",
    "        cluster_nodes_global = defaultdict(list)\n",
    "        for node, cluster_id in enumerate(parts):\n",
    "            cluster_nodes_global[cluster_id].append(node)\n",
    "        return cluster_nodes_global\n",
    "        \n",
    "    def mini_batch_generate(self, batch_file_folder, target_seed):\n",
    "        \"\"\"\n",
    "            create the mini-batch focused on the train nodes only, include a total of k layers of neighbors of the original training nodes\n",
    "            k: number of layers of neighbors for each training node\n",
    "            fraction: fraction of neighbor nodes in each layer to be considered\n",
    "            Input:\n",
    "                1) target_seed: global ids of the nodes for seed to generate the batch\n",
    "                    usually one of (train_global, test_global_, validation_global)\n",
    "            Output: all tensors which are gonna be used in the train, forward procedure\n",
    "                local:\n",
    "                    1) sg_mini_edges_local\n",
    "                    2) self.sg_mini_train_edge_weight_local\n",
    "                    3) self.sg_mini_train_nodes_local\n",
    "                    4) self.sg_mini_train_features\n",
    "                    5) self.sg_mini_train_labels\n",
    "            \n",
    "        \"\"\"\n",
    "        info_batch_node_size = {}\n",
    "        info_batch_edge_size = {}\n",
    "                \n",
    "        for cluster in target_seed.keys():\n",
    "            batch_subgraph = self.graph.subgraph(self.sg_nodes_global[cluster])\n",
    "            \n",
    "             # first select all the overlapping nodes of the train nodes\n",
    "            mini_nodes_global = sorted(node for node in batch_subgraph.nodes())\n",
    "            \n",
    "            # store the global edges\n",
    "            mini_edges_global = {edge for edge in batch_subgraph.edges()}\n",
    "            \n",
    "            # map nodes from global index to local index\n",
    "            mini_mapper = {node: i for i, node in enumerate(mini_nodes_global)}\n",
    "            \n",
    "            # store local index of batch nodes\n",
    "            mini_nodes_local = [ mini_mapper[global_idx] for global_idx in target_seed[cluster] ]\n",
    "            \n",
    "            # store local index of batch edges\n",
    "            mini_edges_local = \\\n",
    "                           [ [ mini_mapper[edge[0]], mini_mapper[edge[1]] ] for edge in mini_edges_global ] + \\\n",
    "                           [ [ mini_mapper[edge[1]], mini_mapper[edge[0]] ] for edge in mini_edges_global ]\n",
    "            \n",
    "            # store local features and lables\n",
    "            mini_features = self.features[mini_nodes_global,:]\n",
    "            mini_labels = self.label[mini_nodes_global]\n",
    "            \n",
    "            # record information \n",
    "            info_batch_node_size[cluster] = len(mini_nodes_global)\n",
    "            info_batch_edge_size[cluster] = len(mini_edges_local)\n",
    "            \n",
    "            mini_nodes_local = torch.LongTensor(mini_nodes_local)\n",
    "            mini_edges_local = torch.LongTensor(mini_edges_local).t()\n",
    "            mini_features = torch.FloatTensor(mini_features)\n",
    "            mini_labels = torch.LongTensor(mini_labels)\n",
    "            \n",
    "            minibatch_data = [mini_nodes_local, mini_edges_local, mini_features, mini_labels]\n",
    "            \n",
    "            batch_file_name = batch_file_folder + 'batch_' + str(cluster)\n",
    "            \n",
    "            # store the batch files\n",
    "            t0 = time.time()\n",
    "            with open(batch_file_name, \"wb\") as fp:\n",
    "                pickle.dump(minibatch_data, fp)\n",
    "            store_time = ((time.time() - t0) * 1000)\n",
    "#             print('*** Generate batch file for # {0:3d} batch, writing the batch file costed {1:.2f} ms ***'.format(cluster, store_time) )\n",
    "#             print('writing to the path: ', batch_file_name)\n",
    "            \n",
    "        return info_batch_node_size, info_batch_edge_size\n",
    "    \n",
    "    def save_info_dict(self, data, file_name, target_folder, header = 'key, value'):\n",
    "        # output the batch size information as the csv file\n",
    "        os.makedirs(os.path.dirname(target_folder), exist_ok=True)\n",
    "        target_file = target_folder + file_name\n",
    "        \n",
    "        with open(target_file, 'a', newline='\\n') as fp:\n",
    "            wr = csv.writer(fp, delimiter = ',')\n",
    "            fp.write('\\n')\n",
    "            wr.writerow(header.split(','))\n",
    "            for key, val in data.items():\n",
    "                wr.writerow([key+1, val])\n",
    "    \n",
    "    def mini_batch_train_clustering(self, batch_folder, train_batch_num = 2):\n",
    "        data_type = 'train'\n",
    "        batch_file_folder = batch_folder + data_type + '/'\n",
    "        check_folder_exist(batch_file_folder)\n",
    "        os.makedirs(os.path.dirname(batch_file_folder), exist_ok=True)\n",
    "        \n",
    "        self.info_train_batch_node_size, self.info_train_batch_edge_size  = self.mini_batch_generate(batch_file_folder, self.sg_train_nodes_global)\n",
    "        self.info_train_seed_size = {key : len(val) for key, val in self.sg_train_nodes_global.items()}\n",
    "        \n",
    "        self.save_info_dict(self.info_train_batch_node_size, 'batch_size_info.csv', self.info_folder, header = 'train_batch_node_id, train_batch_node_size')\n",
    "        self.save_info_dict(self.info_train_batch_edge_size, 'batch_size_info.csv', self.info_folder, header = 'train_batch_edge_id, train_batch_edge_size')\n",
    "        self.save_info_dict(self.info_train_seed_size, 'batch_size_info.csv', self.info_folder, header = 'train_seed_node_id, train_seed_node_size')\n",
    "        \n",
    "    def mini_batch_validation_clustering(self, batch_folder, valid_batch_num = 2):\n",
    "        data_type = 'validation'\n",
    "        batch_file_folder = batch_folder + data_type + '/'\n",
    "        check_folder_exist(batch_file_folder)\n",
    "        os.makedirs(os.path.dirname(batch_file_folder), exist_ok=True)\n",
    "\n",
    "        self.info_validation_batch_node_size, self.info_validation_batch_edge_size = self.mini_batch_generate(batch_file_folder, self.sg_validation_nodes_global)\n",
    "        self.info_validation_seed_size = {key : len(val) for key, val in self.sg_validation_nodes_global.items()}\n",
    "        \n",
    "        self.save_info_dict(self.info_validation_batch_node_size, 'batch_size_info.csv', self.info_folder, header = 'validation_batch_node_id, validation_batch_node_size')\n",
    "        self.save_info_dict(self.info_validation_batch_edge_size, 'batch_size_info.csv', self.info_folder, header = 'validation_batch_edge_id, validation_batch_edge_size')\n",
    "        self.save_info_dict(self.info_validation_seed_size, 'batch_size_info.csv', self.info_folder, header = 'validation_seed_node_id, validation_seed_node_size')\n",
    "        \n",
    "    def mini_batch_test_clustering(self, batch_folder, test_batch_num = 2):\n",
    "        data_type = 'test'\n",
    "        batch_file_folder = batch_folder + data_type + '/'\n",
    "        check_folder_exist(batch_file_folder)\n",
    "        os.makedirs(os.path.dirname(batch_file_folder), exist_ok=True)\n",
    "        \n",
    "        self.info_test_batch_node_size, self.info_test_batch_edge_size = self.mini_batch_generate(batch_file_folder, self.sg_test_nodes_global)\n",
    "        self.info_test_seed_size = {key : len(val) for key, val in self.sg_test_nodes_global.items()}\n",
    "        self.save_info_dict(self.info_test_batch_node_size, 'batch_size_info.csv', self.info_folder, header = 'test_batch_node_id, test_batch_node_size')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Partition Graph with trainiing and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Custom_GCN_layer import Net\n",
    "import time\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class ClusterGCNTrainer_mini_Train(object):\n",
    "    \"\"\"\n",
    "    Training a ClusterGCN.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_folder, in_channels, out_channels, input_layers = [32, 16], dropout=0.3):\n",
    "        \"\"\"\n",
    "        :param in_channels, out_channels: input and output feature dimension\n",
    "        :param clustering_machine:\n",
    "        \"\"\"  \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.data_folder = data_folder\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.input_layers = input_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.create_model()\n",
    "\n",
    "    def create_model(self):\n",
    "        \"\"\"\n",
    "        Creating a StackedGCN and transferring to CPU/GPU.\n",
    "        \"\"\"\n",
    "#         print('used layers are: ', str(self.input_layers))\n",
    "        self.model = Net(self.in_channels, self.out_channels, input_layers = self.input_layers, dropout = self.dropout)\n",
    "        self.model = self.model.to(self.device)\n",
    "    \n",
    "    # call the forward function batch by batch\n",
    "    def do_forward_pass(self, tr_train_nodes, tr_edges, tr_features, tr_target):\n",
    "        \"\"\"\n",
    "        Making a forward pass with data from a given partition.\n",
    "        :param cluster: Cluster index.\n",
    "        :return average_loss: Average loss on the cluster.\n",
    "        :return node_count: Number of nodes.\n",
    "        \"\"\"\n",
    "        \n",
    "        '''Target and features are one-one mapping'''\n",
    "        # calculate the probabilites from log_sofmax\n",
    "        predictions = self.model(tr_edges, tr_features)\n",
    "        \n",
    "        ave_loss = torch.nn.functional.nll_loss(predictions[tr_train_nodes], tr_target[tr_train_nodes])\n",
    "        node_count = tr_train_nodes.shape[0]\n",
    "\n",
    "        # for each cluster keep track of the counts of the nodes\n",
    "        return ave_loss, node_count\n",
    "\n",
    "    def update_average_loss(self, batch_average_loss, node_count, isolate = True):\n",
    "        \"\"\"\n",
    "        Updating the average loss in the epoch.\n",
    "        :param batch_average_loss: Loss of the cluster. \n",
    "        :param node_count: Number of nodes in currently processed cluster.\n",
    "        :return average_loss: Average loss in the epoch.\n",
    "        \"\"\"\n",
    "        self.accumulated_training_loss = self.accumulated_training_loss + batch_average_loss.item() * node_count\n",
    "        if isolate:\n",
    "            self.node_count_seen = self.node_count_seen + node_count\n",
    "        average_loss = self.accumulated_training_loss / self.node_count_seen\n",
    "        return average_loss\n",
    "\n",
    "    # iterate through epoch and also the clusters\n",
    "    def train_investigate_F1(self, epoch_num=10, learning_rate=0.01, weight_decay = 0.01, mini_epoch_num = 1, output_period = 10, train_batch_num = 2, valid_batch_num = 2):\n",
    "        \"\"\"\n",
    "            *** Periodically output the F1 score during training. After certain number of epochs ***\n",
    "            epoch_num:  number of total training epoch number\n",
    "            learning rate: learning rate during training\n",
    "            weight_decay:  decay coefficients for the regularization\n",
    "            mini_epoch_num:  number of epochs of repeating training after loading data on the GPU\n",
    "            output_period:  number of epochs after which output the F1 and accuray to investigate the model refining process\n",
    "        \"\"\"\n",
    "        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        self.model.train()   #   set into train mode, only effective for certain modules such as dropout and batchNorm\n",
    "        self.record_ave_training_loss = []\n",
    "        self.time_train_load_data = 0\n",
    "        \n",
    "        epoch_partition = epoch_num // mini_epoch_num\n",
    "        investigate_f1 = {}\n",
    "        investigate_accuracy = {}\n",
    "        \n",
    "        t0 = time.time()\n",
    "        train_clusters = list(range(train_batch_num))\n",
    "        for epoch_part in range(epoch_partition):\n",
    "#             For test purpose, we let the clusters to follow specific order\n",
    "            random.shuffle(train_clusters)\n",
    "            self.node_count_seen = 0\n",
    "            self.accumulated_training_loss = 0\n",
    "            for cluster in train_clusters:\n",
    "                # for each batch, we load once and train it for multiple epochs:\n",
    "                # read in the train data from the pickle files\n",
    "                batch_file_name = self.data_folder + 'train/batch_' + str(cluster)\n",
    "                \n",
    "                t2 = time.time()\n",
    "                with open(batch_file_name, \"rb\") as fp:\n",
    "                    minibatch_data_train = pickle.load(fp)\n",
    "                read_time = (time.time() - t2) * 1000\n",
    "#                 print('*** During training for # {0:3d} batch, reading batch file costed {1:.2f} ms ***'.format(cluster, read_time) )\n",
    "                \n",
    "                tr_train_nodes, tr_edges, tr_features, tr_target = minibatch_data_train\n",
    "                \n",
    "                t1 = time.time()\n",
    "                tr_train_nodes = tr_train_nodes.to(self.device)\n",
    "                tr_edges = tr_edges.to(self.device)\n",
    "                tr_features = tr_features.to(self.device)\n",
    "                tr_target = tr_target.to(self.device)\n",
    "                \n",
    "                self.time_train_load_data += (time.time() - t1) * 1000\n",
    "                # train each batch for multiple epochs\n",
    "                for mini_epoch in range(mini_epoch_num):\n",
    "                    # record the current overall epoch index:\n",
    "                    real_epoch_num = 1 + mini_epoch + mini_epoch_num * epoch_part # real_epoch_num starts from 0, therefore we add 1\n",
    "                    \n",
    "                    self.optimizer.zero_grad()\n",
    "                    batch_ave_loss, node_count = self.do_forward_pass(tr_train_nodes, tr_edges, tr_features, tr_target)\n",
    "                    batch_ave_loss.backward()\n",
    "                    self.optimizer.step()\n",
    "                    ave_loss = self.update_average_loss(batch_ave_loss, node_count)\n",
    "                    \n",
    "                    # at this point finish a single train duration: update the parameter and calcualte the loss function\n",
    "                    # periodically output the F1-score in the middle of the training process\n",
    "                    if real_epoch_num % output_period == 0:\n",
    "                        investigate_f1[real_epoch_num], investigate_accuracy[real_epoch_num] = self.batch_validate(valid_batch_num = valid_batch_num)\n",
    "                        self.model.train()    # reset to the train mode\n",
    "            \n",
    "            self.record_ave_training_loss.append(ave_loss)\n",
    "        # convert to ms\n",
    "        self.time_train_total = ((time.time() - t0) * 1000)\n",
    "        return investigate_f1, investigate_accuracy\n",
    "\n",
    "    # iterate through epoch and also the clusters\n",
    "    def train(self, epoch_num=10, learning_rate=0.01, weight_decay = 0.01, mini_epoch_num = 1, train_batch_num = 2):\n",
    "        \"\"\"\n",
    "            *** Training a model. ***\n",
    "            epoch_num:  number of total training epoch number\n",
    "            learning rate: learning rate during training\n",
    "            weight_decay:  decay coefficients for the regularization\n",
    "            mini_epoch_num:  number of epochs of repeating training after loading data on the GPU\n",
    "        \"\"\"\n",
    "        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        self.model.train()\n",
    "        self.record_ave_training_loss = []\n",
    "        self.time_train_load_data = 0\n",
    "        \n",
    "        epoch_partition = epoch_num // mini_epoch_num\n",
    "        t0 = time.time()\n",
    "        train_clusters = list(range(train_batch_num))\n",
    "        for epoch in range(epoch_partition):\n",
    "#             For test purpose, we let the clusters to follow specific order\n",
    "            random.shuffle(train_clusters)\n",
    "            self.node_count_seen = 0\n",
    "            self.accumulated_training_loss = 0\n",
    "            \n",
    "            for cluster in train_clusters:\n",
    "                # read in the train data from the pickle files\n",
    "                batch_file_name = self.data_folder + 'train/batch_' + str(cluster)\n",
    "                \n",
    "                t2 = time.time()\n",
    "                with open(batch_file_name, \"rb\") as fp:\n",
    "                    minibatch_data_train = pickle.load(fp)\n",
    "                read_time = (time.time() - t2) * 1000\n",
    "#                 print('*** During training for # {0:3d} batch, reading batch file costed {1:.2f} ms ***'.format(cluster, read_time) )\n",
    "                \n",
    "                tr_train_nodes, tr_edges, tr_features, tr_target = minibatch_data_train\n",
    "                \n",
    "                # for each cluster, we load once and train it for multiple epochs:\n",
    "                t1 = time.time()\n",
    "                tr_train_nodes = tr_train_nodes.to(self.device)\n",
    "                tr_edges = tr_edges.to(self.device)\n",
    "                tr_features = tr_features.to(self.device)\n",
    "                tr_target = tr_target.to(self.device)\n",
    "                \n",
    "                self.time_train_load_data += (time.time() - t1) * 1000\n",
    "                # train each batch for multiple epochs\n",
    "                for mini_epoch in range(mini_epoch_num):\n",
    "                    self.optimizer.zero_grad()\n",
    "                    batch_ave_loss, node_count = self.do_forward_pass(tr_train_nodes, tr_edges, tr_features, tr_target)\n",
    "                    batch_ave_loss.backward()\n",
    "                    self.optimizer.step()\n",
    "                    ave_loss = self.update_average_loss(batch_ave_loss, node_count)\n",
    "            \n",
    "            self.record_ave_training_loss.append(ave_loss)\n",
    "        # convert to ms\n",
    "        self.time_train_total = ((time.time() - t0) * 1000)\n",
    "    \n",
    "\n",
    "    def do_batch_validation_prediction(self, valid_validation_nodes, valid_edges, valid_features, valid_target):\n",
    "        \"\"\"\n",
    "        Scoring a cluster.\n",
    "        :param cluster: Cluster index.\n",
    "        :return prediction: Prediction matrix with probabilities.\n",
    "        :return target: Target vector.\n",
    "        \"\"\"\n",
    "        predictions = self.model(valid_edges, valid_features)\n",
    "        return predictions[valid_validation_nodes], valid_target[valid_validation_nodes]\n",
    "\n",
    "    def batch_validate(self, valid_batch_num = 2):\n",
    "        \"\"\"\n",
    "        Scoring the test and printing the F-1 score.\n",
    "        \"\"\"\n",
    "        self.model.eval()   # set into test mode, only effective for certain modules such as dropout and batchNorm\n",
    "        \n",
    "        predictions = []\n",
    "        targets = []\n",
    "        valid_clusters = list(range(valid_batch_num))\n",
    "        for cluster in valid_clusters:\n",
    "            # read in the train data from the pickle files\n",
    "            batch_file_name = self.data_folder + 'validation/batch_' + str(cluster)\n",
    "            \n",
    "            t2 = time.time()\n",
    "            with open(batch_file_name, \"rb\") as fp:\n",
    "                minibatch_data_validation = pickle.load(fp)\n",
    "            read_time = (time.time() - t2) * 1000\n",
    "#             print('*** During validation for # {0:3d} batch, reading batch file costed {1:.2f} ms ***'.format(cluster, read_time) )\n",
    "\n",
    "            valid_validation_nodes, valid_edges, valid_features, valid_target = minibatch_data_validation\n",
    "            \n",
    "            valid_validation_nodes = valid_validation_nodes.to(self.device)\n",
    "            valid_edges = valid_edges.to(self.device)\n",
    "            valid_features = valid_features.to(self.device)\n",
    "            valid_target = valid_target.to(self.device)\n",
    "            \n",
    "            \n",
    "            prediction, target = self.do_batch_validation_prediction(valid_validation_nodes, valid_edges, valid_features, valid_target)\n",
    "\n",
    "            predictions.append(prediction.cpu().detach().numpy())\n",
    "            targets.append(target.cpu().detach().numpy())\n",
    "        \n",
    "        # concatenate all the ndarrays inside this list\n",
    "        targets = np.concatenate(targets)\n",
    "        # along axis:    axis == 1\n",
    "        predictions = np.concatenate(predictions).argmax(1)  # return the indices of maximum probability \n",
    "#         print('shape of the targets and predictions are: ', self.targets.shape, self.predictions.shape)\n",
    "        \n",
    "        f1 = f1_score(targets, predictions, average=\"micro\")\n",
    "        accuracy = accuracy_score(targets, predictions)\n",
    "#         print(\"\\nTest F-1 score: {:.4f}\".format(score))\n",
    "        return (f1, accuracy)\n",
    "\n",
    "# for cross-validation purpose: \n",
    "    def do_prediction(self, cluster):\n",
    "        \"\"\"\n",
    "        Scoring a cluster.\n",
    "        :param cluster: Cluster index.\n",
    "        :return prediction: Prediction matrix with probabilities.\n",
    "        :return target: Target vector.\n",
    "        \"\"\"\n",
    "        test_nodes = self.clustering_machine.sg_test_nodes_global[cluster].to(self.device)\n",
    "        prediction = self.model(self.edges, self.features, self.edge_weights)\n",
    "        \n",
    "        return prediction[test_nodes], self.label[test_nodes]\n",
    "    \n",
    "    def test(self):\n",
    "        \"\"\"\n",
    "        Scoring the test and printing the F-1 score.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        self.predictions = []\n",
    "        self.targets = []\n",
    "        \n",
    "        self.edges = self.clustering_machine.edge_index_global_self_loops.to(self.device)\n",
    "        self.features = self.clustering_machine.features.to(self.device)\n",
    "        self.edge_weights = self.clustering_machine.edge_weight_global.to(self.device)\n",
    "        self.label = self.clustering_machine.label.to(self.device)\n",
    "        \n",
    "        for cluster in self.clustering_machine.test_clusters:\n",
    "            prediction, target = self.do_prediction(cluster)\n",
    "\n",
    "            self.predictions.append(prediction.cpu().detach().numpy())\n",
    "            self.targets.append(target.cpu().detach().numpy())\n",
    "        \n",
    "        # concatenate all the ndarrays inside this list\n",
    "        self.targets = np.concatenate(self.targets)\n",
    "        # along axis:    axis == 1\n",
    "        self.predictions = np.concatenate(self.predictions).argmax(1)  # return the indices of maximum probability \n",
    "#         print('shape of the targets and predictions are: ', self.targets.shape, self.predictions.shape)\n",
    "        \n",
    "        f1 = f1_score(self.targets, self.predictions, average=\"micro\")\n",
    "        accuracy = accuracy_score(self.targets, self.predictions)\n",
    "#         print(\"\\nTest F-1 score: {:.4f}\".format(score))\n",
    "        return (f1, accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Trivial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 2.],\n",
      "        [0., 3.],\n",
      "        [0., 4.],\n",
      "        [0., 5.],\n",
      "        [0., 6.],\n",
      "        [0., 7.],\n",
      "        [0., 8.],\n",
      "        [0., 9.]]) torch.Size([10, 2])\n"
     ]
    }
   ],
   "source": [
    "'''Trivial data'''\n",
    "edge_index = torch.tensor([[0, 1, 1, 3, 1, 2, 4, 2, 4, 6, 6, 7, 7, 9, 2, 5, 9, 8], \n",
    "                           [1, 0, 3, 1, 2, 1, 2, 4, 6, 4, 7, 6, 9, 7, 5, 2, 8, 9]])\n",
    "# features = torch.rand(10, 3)\n",
    "features = torch.tensor([[0, 0], [0, 1], [0, 2], [0, 3], [0, 4],  \n",
    "                           [0, 5], [0, 6], [0, 7], [0, 8], [0, 9]], dtype = torch.float)\n",
    "# label = torch.tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "label = torch.tensor([0, 1, 1, 0, 1, 1, 1, 0, 0, 0])\n",
    "print(features, features.shape)\n",
    "\n",
    "# set the tmp folder\n",
    "# tmp_folder = './tmp/'\n",
    "# check_folder_exist(tmp_folder)\n",
    "# os.makedirs(os.path.dirname(tmp_folder), exist_ok=True)\n",
    "# set the store clustering path\n",
    "clustering_folder = './res_save_batch/clustering/'\n",
    "check_folder_exist(clustering_folder)\n",
    "clustering_file_name = clustering_folder + 'check_clustering_machine.txt'\n",
    "os.makedirs(os.path.dirname(clustering_file_name), exist_ok=True)\n",
    "\n",
    "node_count = features.shape[0]\n",
    "clustering_machine = ClusteringMachine(edge_index, features, label)\n",
    "clustering_machine.split_cluster_nodes_edges(0.4, 0.4, partition_num = 2)\n",
    "\n",
    "with open(clustering_file_name, \"wb\") as fp:\n",
    "    pickle.dump(clustering_machine, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### minibatch train nodes and batch validatioin "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5, 0.5)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "mini_batch_folder = './res_save_batch/mini_batch_files/'\n",
    "check_folder_exist(mini_batch_folder)\n",
    "\n",
    "with open(clustering_file_name, \"rb\") as fp:\n",
    "    clustering_machine = pickle.load(fp)\n",
    "\n",
    "# generate the batches for train and validation\n",
    "clustering_machine.mini_batch_train_clustering(mini_batch_folder, train_batch_num = 2) # include number of layers\n",
    "\n",
    "clustering_machine.mini_batch_validation_clustering(mini_batch_folder, valid_batch_num = 2)\n",
    "\n",
    "# construct the batch trainer\n",
    "gcn_trainer_batch = ClusterGCNTrainer_mini_Train(mini_batch_folder, 2, 2, input_layers = [16], dropout=0.3)\n",
    "\n",
    "gcn_trainer_batch.train(1, 0.0001, 0.1, train_batch_num = 2)\n",
    "gcn_trainer_batch.batch_validate(valid_batch_num = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the train loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_train_loss_converge(mini_batch_folder, data_name, dataset, image_path,  comments, input_layer = [32, 16], epoch_num = 300, \\\n",
    "                              dropout = 0.3, lr = 0.0001, weight_decay = 0.01, mini_epoch_num = 5, \\\n",
    "                               valid_part_num = 2, train_part_num = 2, test_part_num = 1):\n",
    "    # mini-batch, but valid also in batches\n",
    "    Trainer_folder = mini_batch_folder + 'GCNtrainer/'\n",
    "    check_folder_exist(Trainer_folder)\n",
    "    os.makedirs(os.path.dirname(Trainer_folder), exist_ok=True)\n",
    "    \n",
    "    trainer_id = 0\n",
    "    Cluster_train_batch_run(trainer_id, mini_batch_folder, data_name, dataset, image_path, input_layer = input_layer, epochs=epoch_num, \\\n",
    "                                                                               dropout = dropout, lr = lr, weight_decay = weight_decay, mini_epoch_num = mini_epoch_num, \\\n",
    "                                                                               train_part_num = train_part_num, test_part_num = test_part_num)\n",
    "    trainer_file_name = mini_batch_folder + 'GCNtrainer/GCN_trainer_' + str(trainer_id)\n",
    "    with open(trainer_file_name, \"rb\") as fp:\n",
    "        Cluster_train_batch_trainer = pickle.load(fp)\n",
    "    \n",
    "    draw_Cluster_train_valid_batch = draw_trainer_info(data_name, Cluster_train_batch_trainer, image_path, 'train_valid_batch_' + comments)\n",
    "    draw_Cluster_train_valid_batch.draw_ave_loss_per_node()\n",
    "    \n",
    "\n",
    "''' Draw the information about the GCN calculating batch size '''\n",
    "def draw_cluster_info(clustering_machine, data_name, img_path, comments = '_cluster_node_distr'):\n",
    "    \"\"\"\n",
    "        Won't call this for mini-batch with no clustering \n",
    "    \"\"\"\n",
    "    cluster_id = clustering_machine.train_clusters    # a list of cluster indices\n",
    "    cluster_datapoints = {'cluster_id': cluster_id,  \\\n",
    "                          'train_batch' : [clustering_machine.info_train_batch_size[idx] for idx in cluster_id], \\\n",
    "                          'cluster_size' : [clustering_machine.info_isolate_cluster_size[idx] for idx in cluster_id], \\\n",
    "                         }\n",
    "                         \n",
    "    df = pd.DataFrame(data=cluster_datapoints, dtype=np.int32)\n",
    "    # print(df)\n",
    "    df_reshape = df.melt('cluster_id', var_name = 'clusters', value_name = 'node_num')\n",
    "    \n",
    "    plt.clf()\n",
    "    plt.figure()\n",
    "    sns.set(style='whitegrid')\n",
    "    g = sns.catplot(x=\"cluster_id\", y=\"node_num\", hue='clusters', kind='bar', data=df_reshape)\n",
    "    g.despine(left=True)\n",
    "    g.fig.suptitle(data_name + comments)\n",
    "    g.set_xlabels(\"Cluster ID\")\n",
    "    g.set_ylabels(\"Number of nodes\")\n",
    "    \n",
    "    img_name = img_path + data_name + comments\n",
    "    os.makedirs(os.path.dirname(img_name), exist_ok=True)\n",
    "    g.savefig(img_name, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specific model run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Execute the testing program '''\n",
    "def set_clustering_machine(data, image_path, intermediate_data_folder, test_ratio = 0.05, validation_ratio = 0.85, train_part_num = 2, mini_cluster_num = 16, round_num = 2):\n",
    "    \"\"\"\n",
    "        Set the batch machine plus generate the training batches\n",
    "            1) data: the target dataset data\n",
    "            2) intermediate_data_folder: path to store the intermediate generated data\n",
    "            3) test_ratio, validation_ratio: data split ratio\n",
    "            4) neigh_layer: number of hops (layers) for the neighbor nodes \n",
    "            5) train_frac: each time including fraction of the neigbor nodes in each layer\n",
    "            6) valid_part_num, train_part_num, test_part_num :  batch number for validation, train and test data correspondingly\n",
    "    \"\"\"\n",
    "    # set the tmp file for garbage tmp files, just collect the info:\n",
    "    tmp_folder = './tmp/'\n",
    "    check_folder_exist(tmp_folder)\n",
    "    os.makedirs(os.path.dirname(tmp_folder), exist_ok=True)\n",
    "    \n",
    "    # Set the clustering information storing path\n",
    "    clustering_file_folder = intermediate_data_folder + 'clustering/'\n",
    "    check_folder_exist(clustering_file_folder)  # if exist then delete\n",
    "    clustering_file_name = clustering_file_folder + 'clustering_machine.txt'\n",
    "    os.makedirs(os.path.dirname(clustering_file_folder), exist_ok=True)\n",
    "    \n",
    "    # if we use the random assignment of the code, then filtering out the isolated data may not be necessary\n",
    "#     connect_edge_index, connect_features, connect_label = filter_out_isolate(data.edge_index, data.x, data.y)\n",
    "#     clustering_machine = ClusteringMachine(connect_edge_index, connect_features, connect_label)\n",
    "    print('\\n' + '=' * 100)\n",
    "    print('Start to generate the clustering machine:')\n",
    "    t0 = time.time()\n",
    "    clustering_machine = ClusteringMachine(data.edge_index, data.x, data.y, tmp_folder, info_folder = image_path)\n",
    "    batch_machine_create = time.time() - t0\n",
    "    print('Batch machine creation costs a total of {0:.4f} seconds!'.format(batch_machine_create))\n",
    "    \n",
    "    # at last output the information inside the folder:\n",
    "    print_dir_content_info(tmp_folder)\n",
    "    \n",
    "#     clustering_machine.split_cluster_nodes_edges(test_ratio, validation_ratio, partition_num = train_part_num)\n",
    "    # mini-batch only: split to train test valid before clustering\n",
    "    print('Start to split data into train, test, validation:')\n",
    "    t1 = time.time()\n",
    "    clustering_machine.split_cluster_nodes_edges(test_ratio, validation_ratio, partition_num = mini_cluster_num, batch_num = train_part_num, round_num = round_num)\n",
    "    data_split_time = time.time() - t1\n",
    "    print('Data splitting costs a total of {0:.4f} seconds!'.format(data_split_time))\n",
    "    \n",
    "    print('Start to store the batch machine file:')\n",
    "    t3 = time.time()\n",
    "    with open(clustering_file_name, \"wb\") as fp:\n",
    "        pickle.dump(clustering_machine, fp)\n",
    "    batch_machine_store_time = time.time() - t3\n",
    "    print('Storing batch machine after training batches generation costs a total of {0:.4f} seconds!'.format(batch_machine_store_time))\n",
    "    print('\\n' + '=' * 100)\n",
    "    # output the memory usage information\n",
    "    output_GPU_memory_usage('Memory_use_setting_cluster.txt', image_path, comment ='after setting clustering machine: ')\n",
    "    \n",
    "def set_clustering_machine_train_batch(image_path, intermediate_data_folder, train_part_num = 2):\n",
    "    \"\"\"\n",
    "        Generate the train batches\n",
    "    \"\"\"\n",
    "    clustering_file_folder = intermediate_data_folder + 'clustering/'\n",
    "    clustering_file_name = clustering_file_folder + 'clustering_machine.txt'\n",
    "    print('\\n' + '=' * 100)\n",
    "    \n",
    "    t0 = time.time()\n",
    "    with open(clustering_file_name, \"rb\") as fp:\n",
    "        clustering_machine = pickle.load(fp)\n",
    "    batch_machine_read = time.time() - t0\n",
    "    print('Batch machine reading costs a total of {0:.4f} seconds!'.format(batch_machine_read))\n",
    "    \n",
    "    mini_batch_folder = intermediate_data_folder\n",
    "#     check_folder_exist(mini_batch_folder)  # if exist then delete\n",
    "    print('Start to generate the training batches:')\n",
    "    t2 = time.time()\n",
    "    clustering_machine.mini_batch_train_clustering(mini_batch_folder, train_batch_num = train_part_num)\n",
    "    train_batch_production_time = time.time() - t2\n",
    "    print('Train batches production costs a total of {0:.4f} seconds!'.format(train_batch_production_time))\n",
    "    print_dir_content_info(mini_batch_folder + 'train/')\n",
    "    print('=' * 100)\n",
    "\n",
    "def set_clustering_machine_validation_batch(image_path, intermediate_data_folder, valid_part_num = 2):\n",
    "    \"\"\"\n",
    "        Generate the validation batches\n",
    "    \"\"\"\n",
    "    clustering_file_folder = intermediate_data_folder + 'clustering/'\n",
    "    clustering_file_name = clustering_file_folder + 'clustering_machine.txt'\n",
    "    print('\\n' + '=' * 100)\n",
    "    \n",
    "    t0 = time.time()\n",
    "    with open(clustering_file_name, \"rb\") as fp:\n",
    "        clustering_machine = pickle.load(fp)\n",
    "    batch_machine_read = time.time() - t0\n",
    "    print('Batch machine reading costs a total of {0:.4f} seconds!'.format(batch_machine_read))\n",
    "    \n",
    "    print('Start to generate the validation batches:')\n",
    "    mini_batch_folder = intermediate_data_folder\n",
    "    t1 = time.time()\n",
    "    # for validation , fraction has to be 1.0 so that to include the information form original graph\n",
    "    clustering_machine.mini_batch_validation_clustering(mini_batch_folder, valid_batch_num = valid_part_num)\n",
    "    validation_batch_production_time = time.time() - t1\n",
    "    print('Validation batches production costs a total of {0:.4f} seconds!'.format(validation_batch_production_time))\n",
    "    print_dir_content_info(mini_batch_folder + 'validation/')\n",
    "    print('=' * 100)\n",
    "    # output the memory usage information\n",
    "    output_GPU_memory_usage('Memory_use_setting_cluster.txt', image_path, comment ='after generating validation batches: ')\n",
    "\n",
    "def Cluster_train_batch_run(trainer_id, mini_batch_folder, data_name, dataset, image_path, input_layer = [16, 16], epochs=300, \\\n",
    "                           dropout = 0.3, lr = 0.01, weight_decay = 0.01, mini_epoch_num = 5, \\\n",
    "                                 train_part_num = 2, test_part_num = 1):\n",
    "    \"\"\"\n",
    "    # Run the mini-batch model (train and validate both in batches)\n",
    "    Tuning parameters:  dropout, lr (learning rate), weight_decay: l2 regularization\n",
    "    return: validation accuracy value, validation F-1 value, time_training (ms), time_data_load (ms)\n",
    "    \"\"\"\n",
    "#     print('\\n' + '=' * 100)\n",
    "#     print('Start generate the trainer:')\n",
    "    t0 = time.time()\n",
    "    gcn_trainer = ClusterGCNTrainer_mini_Train(mini_batch_folder, dataset.num_node_features, dataset.num_classes, input_layers = input_layer, dropout = dropout)\n",
    "    train_create = time.time() - t0\n",
    "#     print('Trainer creation costs a total of {0:.4f} seconds!'.format(train_create))\n",
    "    \n",
    "#     print('Start train the model:')\n",
    "    t1 = time.time()\n",
    "    gcn_trainer.train(epoch_num=epochs, learning_rate=lr, weight_decay=weight_decay, mini_epoch_num = mini_epoch_num, train_batch_num = train_part_num)\n",
    "    train_period = time.time() - t1\n",
    "#     print('Training costs a total of {0:.4f} seconds!'.format(train_period))\n",
    "    \n",
    "#     print('Start to save the GCN trainer model (parameters: weights, bias):')\n",
    "    trainer_file_name = mini_batch_folder + 'GCNtrainer/GCN_trainer_' + str(trainer_id)\n",
    "    t2 = time.time()\n",
    "    with open(trainer_file_name, \"wb\") as fp:\n",
    "        pickle.dump(gcn_trainer, fp)\n",
    "    store_trainer = time.time() - t2\n",
    "#     print('Storing the trainer costs a total of {0:.4f} seconds!'.format(store_trainer))\n",
    "#     print('-' * 80)\n",
    "    output_GPU_memory_usage('Memory_use_batch_train.txt', image_path, comment ='after generating trainer and train minibatches: ')\n",
    "\n",
    "def Cluster_valid_batch_run(trainer_id, mini_batch_folder, data_name, dataset, image_path, input_layer = [16, 16], epochs=300, \\\n",
    "                           dropout = 0.3, lr = 0.01, weight_decay = 0.01, mini_epoch_num = 5, \\\n",
    "                                 valid_part_num = 2):\n",
    "#     print('Start to read the GCN trainer model (parameters: weights, bias):')\n",
    "    trainer_file_name = mini_batch_folder + 'GCNtrainer/GCN_trainer_' + str(trainer_id)\n",
    "    t1 = time.time()\n",
    "    with open(trainer_file_name, \"rb\") as fp:\n",
    "        gcn_trainer = pickle.load(fp)\n",
    "    read_trainer = (time.time() - t1) * 1000\n",
    "#     print('Reading the trainer costs a total of {0:.4f} seconds!'.format(read_trainer))\n",
    "    \n",
    "#     print('Start validate the model:')\n",
    "    t2 = time.time()\n",
    "    validation_F1, validation_accuracy = gcn_trainer.batch_validate(valid_batch_num = valid_part_num)\n",
    "    validation_period = time.time() - t2\n",
    "#     print('Validatoin costs a total of {0:.4f} seconds!'.format(validation_period))\n",
    "#     print('Finish train and validate the model:')\n",
    "#     print('=' * 100)\n",
    "    time_train_total = gcn_trainer.time_train_total\n",
    "    time_data_load = gcn_trainer.time_train_load_data\n",
    "    \n",
    "    output_GPU_memory_usage('Memory_use_batch_validation.txt', image_path, comment ='after validating minibatches: ')\n",
    "    \n",
    "    return validation_accuracy, validation_F1, time_train_total, time_data_load\n",
    "\n",
    "\n",
    "def Cluster_train_valid_batch_investigate(mini_batch_folder, data_name, dataset, image_path, input_layer = [16, 16], epochs=300, \\\n",
    "                           dropout = 0.3, lr = 0.01, weight_decay = 0.01, mini_epoch_num = 5, output_period = 10, \n",
    "                                         valid_part_num = 2, train_part_num = 2, test_part_num = 1):\n",
    "    \"\"\"\n",
    "        *** dynamically investigate the F1 score in the middle of the training after certain period ***\n",
    "        output: two dict containing F1-score and accuracy of a certain epoch index\n",
    "    \"\"\"\n",
    "#     print('\\n' + '=' * 100)\n",
    "#     print('Start generate the trainer:')\n",
    "    t0 = time.time()\n",
    "    gcn_trainer = ClusterGCNTrainer_mini_Train(mini_batch_folder, dataset.num_node_features, dataset.num_classes, input_layers = input_layer, dropout = dropout)\n",
    "    train_create = time.time() - t0\n",
    "#     print('Trainer creation costs a total of {0:.4f} seconds!'.format(train_create))\n",
    "    \n",
    "#     print('Start train the model:')\n",
    "    t1 = time.time()\n",
    "    Train_period_F1, Train_period_accuracy = gcn_trainer.train_investigate_F1(epoch_num=epochs, learning_rate=lr, weight_decay=weight_decay, mini_epoch_num = mini_epoch_num, \\\n",
    "                                                            output_period = output_period, train_batch_num = train_part_num, valid_batch_num = valid_part_num)\n",
    "    train_period = time.time() - t1\n",
    "    print('In-process Training costs a total of {0:.4f} seconds!'.format(train_period))\n",
    "#     print('=' * 100)\n",
    "    output_GPU_memory_usage('Memory_use_investigate_batch_train_valid.txt', image_path, comment ='after train_validation investigate batches  minibatches: ')\n",
    "    return Train_period_F1, Train_period_accuracy\n",
    "\n",
    "# for the purpose for tuning \n",
    "def Cluster_train_valid_batch_run(mini_batch_folder, data_name, dataset, image_path, input_layer = [16, 16], epochs=300, \\\n",
    "                           dropout = 0.3, lr = 0.01, weight_decay = 0.01, mini_epoch_num = 5, \\\n",
    "                                 valid_part_num = 2, train_part_num = 2, test_part_num = 1):\n",
    "    \"\"\"\n",
    "    # Run the mini-batch model (train and validate both in batches)\n",
    "    Tuning parameters:  dropout, lr (learning rate), weight_decay: l2 regularization\n",
    "    return: validation accuracy value, validation F-1 value, time_training (ms), time_data_load (ms)\n",
    "    \"\"\"\n",
    "#     gcn_trainer_batch = ClusterGCNTrainer_mini_Train(mini_batch_folder, 2, 2, 2, 2, 2, input_layers = [16], dropout=0.3)\n",
    "#     print('\\n' + '=' * 100)\n",
    "#     print('Start generate the trainer:')\n",
    "    t0 = time.time()\n",
    "    gcn_trainer = ClusterGCNTrainer_mini_Train(mini_batch_folder, dataset.num_node_features, dataset.num_classes, input_layers = input_layer, dropout = dropout)\n",
    "    train_create = time.time() - t0\n",
    "#     print('Trainer creation costs a total of {0:.4f} seconds!'.format(train_create))\n",
    "    \n",
    "#     print('Start train the model:')\n",
    "    t1 = time.time()\n",
    "    gcn_trainer.train(epoch_num=epochs, learning_rate=lr, weight_decay=weight_decay, mini_epoch_num = mini_epoch_num, train_batch_num = train_part_num)\n",
    "    train_period = time.time() - t1\n",
    "#     print('Training costs a total of {0:.4f} seconds!'.format(train_period))\n",
    "#     print('-' * 80)\n",
    "    \n",
    "#     print('Start validate the model:')\n",
    "    t2 = time.time()\n",
    "    validation_F1, validation_accuracy = gcn_trainer.batch_validate(valid_batch_num = valid_part_num)\n",
    "    validation_period = time.time() - t2\n",
    "#     print('Validatoin costs a total of {0:.4f} seconds!'.format(validation_period))\n",
    "#     print('=' * 100)\n",
    "    time_train_total = gcn_trainer.time_train_total\n",
    "    time_data_load = gcn_trainer.time_train_load_data\n",
    "    output_GPU_memory_usage('Memory_use_train_validation_together.txt', image_path, comment ='after train_validation batches  minibatches together: ')\n",
    "    return validation_accuracy, validation_F1, time_train_total, time_data_load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test and compare different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_one_train(mini_batch_folder, image_path, repeate_time = 5, input_layer = [32], epoch_num = 300, \\\n",
    "                dropout = 0.3, lr = 0.0001, weight_decay = 0.01, mini_epoch_num = 5, \\\n",
    "                train_part_num = 2, test_part_num = 1):\n",
    "    \"\"\"\n",
    "        Perform one train and store the results for all trainer\n",
    "    \"\"\"\n",
    "    Trainer_folder = mini_batch_folder + 'GCNtrainer/'\n",
    "    check_folder_exist(Trainer_folder)\n",
    "    os.makedirs(os.path.dirname(Trainer_folder), exist_ok=True)\n",
    "#     graph_model = ['batch_valid', 'train_batch', 'whole_graph', 'isolate']\n",
    "    for trainer_id in range(repeate_time):\n",
    "        model_res = []\n",
    "        \n",
    "        Cluster_train_batch_run(trainer_id, mini_batch_folder, data_name, dataset, image_path, input_layer = input_layer, epochs=epoch_num, \\\n",
    "                                                         dropout = dropout, lr = lr, weight_decay = weight_decay, mini_epoch_num = mini_epoch_num, \\\n",
    "                                                         train_part_num = train_part_num, test_part_num = test_part_num)\n",
    "        \n",
    "def execute_one_validation(mini_batch_folder, image_path, repeate_time = 5, input_layer = [32], epoch_num = 300, \\\n",
    "                dropout = 0.3, lr = 0.0001, weight_decay = 0.01, mini_epoch_num = 5, \\\n",
    "                valid_part_num = 2):\n",
    "    \"\"\"\n",
    "        return all test-F1 and validation-F1 for all four models\n",
    "    \"\"\"\n",
    "    validation_accuracy = {}\n",
    "    validation_f1 = {}\n",
    "    time_total_train = {}\n",
    "    time_data_load = {}\n",
    "    \n",
    "    # Each graph model corresponds to one function below\n",
    "#     graph_model = ['batch_valid', 'train_batch', 'whole_graph', 'isolate']\n",
    "    graph_model = ['batch_valid']\n",
    "    for trainer_id in range(repeate_time):\n",
    "        model_res = []\n",
    "        model_res.append(Cluster_valid_batch_run(trainer_id, mini_batch_folder, data_name, dataset, image_path, input_layer = input_layer, epochs=epoch_num, \\\n",
    "                                                         dropout = dropout, lr = lr, weight_decay = weight_decay, mini_epoch_num = mini_epoch_num, \\\n",
    "                                                      valid_part_num = valid_part_num)[:4])\n",
    "        \n",
    "        validation_accuracy[trainer_id], validation_f1[trainer_id], time_total_train[trainer_id], time_data_load[trainer_id] = zip(*model_res)\n",
    "    return graph_model, validation_accuracy, validation_f1, time_total_train, time_data_load\n",
    "\n",
    "def store_data_multi_tests(f1_data, data_name, graph_model, img_path, comments):\n",
    "    run_id = sorted(f1_data.keys())\n",
    "    run_data = {'run_id': run_id}\n",
    "    \n",
    "    run_data.update({model_name : [f1_data[key][idx] for key in run_id] for idx, model_name in enumerate(graph_model)})\n",
    "    \n",
    "    pickle_filename = img_path + data_name + '_' + comments + '.pkl'\n",
    "    os.makedirs(os.path.dirname(pickle_filename), exist_ok=True)\n",
    "    df = pd.DataFrame(data=run_data, dtype=np.int32)\n",
    "    df.to_pickle(pickle_filename)\n",
    "    return pickle_filename\n",
    "\n",
    "def draw_data_multi_tests(pickle_filename, data_name, comments, xlabel, ylabel):\n",
    "    df = pd.read_pickle(pickle_filename)\n",
    "    df_reshape = df.melt('run_id', var_name = 'model', value_name = ylabel)\n",
    "\n",
    "    plt.clf()\n",
    "    plt.figure()\n",
    "    sns.set(style='whitegrid')\n",
    "    g = sns.catplot(x=\"model\", y=ylabel, kind='box', data=df_reshape)\n",
    "    g.despine(left=True)\n",
    "    g.fig.suptitle(data_name + ' ' + ylabel + ' ' + comments)\n",
    "    g.set_xlabels(xlabel)\n",
    "    g.set_ylabels(ylabel)\n",
    "\n",
    "    img_name = pickle_filename[:-4] + '_img'\n",
    "    os.makedirs(os.path.dirname(img_name), exist_ok=True)\n",
    "    plt.savefig(img_name, bbox_inches='tight')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate performance in the middle of the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_investigate(mini_batch_folder, image_path, repeate_time = 5, input_layer = [32], epoch_num = 300, \\\n",
    "                        dropout = 0.3, lr = 0.0001, weight_decay = 0.01, mini_epoch_num = 5, output_period = 10, \\\n",
    "                         valid_part_num = 2, train_part_num = 2, test_part_num = 1):\n",
    "    \"\"\"\n",
    "        return all test-F1 and validation-F1 for all four models\n",
    "    \"\"\"\n",
    "    \n",
    "    Train_peroid_f1 = {}\n",
    "    Train_peroid_accuracy = {}\n",
    "    \n",
    "    for i in range(repeate_time):\n",
    "        Train_peroid_f1[i], Train_peroid_accuracy[i] = Cluster_train_valid_batch_investigate(mini_batch_folder, data_name, dataset, image_path, input_layer = input_layer, epochs=epoch_num, \\\n",
    "                                            dropout = dropout, lr = lr, weight_decay = weight_decay, mini_epoch_num = mini_epoch_num, output_period = output_period, \\\n",
    "                                                                    valid_part_num = valid_part_num, train_part_num = train_part_num, test_part_num = test_part_num)\n",
    "        \n",
    "    return Train_peroid_f1, Train_peroid_accuracy\n",
    "\n",
    "def store_data_multi_investigate(investigate_res, data_name, res_name, img_path, comments):\n",
    "    \"\"\"\n",
    "        investigate_res: currently either F1-score or accuracy a dict {epoch num : value}\n",
    "    \"\"\"\n",
    "    run_id = sorted(investigate_res.keys())\n",
    "    run_data = {'run_id': run_id}\n",
    "    \n",
    "    epoch_num_range = sorted(investigate_res[0].keys())  # at least one entry exists inside the dictionary and the epoch range is fixed\n",
    "    run_data.update({epoch_num : [investigate_res[key][epoch_num] for key in run_id] for epoch_num in epoch_num_range})\n",
    "    \n",
    "    pickle_filename = img_path + data_name + '_' + res_name + '_' + comments + '.pkl'\n",
    "    os.makedirs(os.path.dirname(pickle_filename), exist_ok=True)\n",
    "    df = pd.DataFrame(data=run_data, dtype=np.int32)\n",
    "    df.to_pickle(pickle_filename)\n",
    "    return pickle_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"To test one single model for different parameter values\"\"\"\n",
    "def execute_tuning(tune_params, mini_batch_folder, image_path, repeate_time = 7, input_layer = [32], epoch_num = 400, \\\n",
    "                  dropout = 0.1, lr = 0.0001, weight_decay = 0.1, mini_epoch_num = 20, \\\n",
    "                  valid_part_num = 2, train_part_num = 2, test_part_num = 1):\n",
    "    \"\"\"\n",
    "        Tune all the hyperparameters\n",
    "        1) learning rate\n",
    "        2) dropout\n",
    "        3) layer unit number\n",
    "        4) weight decay\n",
    "    \"\"\"\n",
    "    validation_accuracy = {}\n",
    "    validation_f1 = {}\n",
    "    time_total_train = {}\n",
    "    time_data_load = {}\n",
    "    \n",
    "    res = [{tune_val : Cluster_train_valid_batch_run(mini_batch_folder, data_name, dataset, image_path, input_layer = input_layer, epochs=epoch_num, \\\n",
    "            dropout = dropout, lr = lr, weight_decay = weight_decay, mini_epoch_num = tune_val, \\\n",
    "            valid_part_num = valid_part_num, train_part_num = train_part_num, test_part_num = test_part_num) for tune_val in tune_params} for i in range(repeate_time)]\n",
    "    \n",
    "    for i, ref in enumerate(res):\n",
    "        validation_accuracy[i] = {tune_val : res_lst[0] for tune_val, res_lst in ref.items()}\n",
    "        validation_f1[i] = {tune_val : res_lst[1] for tune_val, res_lst in ref.items()}\n",
    "        time_total_train[i] = {tune_val : res_lst[2] for tune_val, res_lst in ref.items()}\n",
    "        time_data_load[i] = {tune_val : res_lst[3] for tune_val, res_lst in ref.items()}\n",
    "        \n",
    "    return validation_accuracy, validation_f1, time_total_train, time_data_load\n",
    "\n",
    "def store_data_multi_tuning(tune_params, target, data_name, img_path, comments):\n",
    "    \"\"\"\n",
    "        tune_params: is the tuning parameter list\n",
    "        target: is the result, here should be F1-score, accuraycy, load time, train time\n",
    "    \"\"\"\n",
    "    run_ids = sorted(target.keys())   # key is the run_id\n",
    "    run_data = {'run_id': run_ids}\n",
    "    # the key can be converted to string or not: i.e. str(tune_val)\n",
    "    # here we keep it as integer such that we want it to follow order\n",
    "    tmp = {tune_val : [target[run_id][tune_val] for run_id in run_ids] for tune_val in tune_params}  # the value is list\n",
    "    run_data.update(tmp)\n",
    "    \n",
    "    pickle_filename = img_path + data_name + '_' + comments + '.pkl'\n",
    "    os.makedirs(os.path.dirname(pickle_filename), exist_ok=True)\n",
    "    df = pd.DataFrame(data=run_data, dtype=np.int32)\n",
    "    df.to_pickle(pickle_filename)\n",
    "    return pickle_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-test execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_train_loss(data, data_name, dataset, image_data_path, intermediate_data_path, partition_nums, layers, \\\n",
    "                      dropout = 0.1, lr = 0.0001, weight_decay = 0.1, mini_epoch_num = 20, \\\n",
    "                      valid_part_num = 2, mini_cluster_num = 16, round_num = 2):\n",
    "    for partn in partition_nums:\n",
    "        for GCN_layer in layers:\n",
    "            net_layer = len(GCN_layer) + 1\n",
    "            hop_layer = net_layer\n",
    "            print('Start checking train loss for partition num: ' + str(partn) + ' hop layer: ' + str(hop_layer))\n",
    "            img_path = image_data_path + 'cluster_num_' + str(partn) + '/' + 'net_layer_' + str(net_layer) + '_hop_layer_' + str(hop_layer) + '/'\n",
    "            img_path += 'output_train_loss/'  # further subfolder for different task\n",
    "            intermediate_data_folder = intermediate_data_path\n",
    "            \n",
    "            # set the batch for validation and train\n",
    "            mini_batch_folder = intermediate_data_folder\n",
    "            set_clustering_machine(data, img_path, intermediate_data_folder, test_ratio = 0.05, validation_ratio = 0.85, train_part_num = partn, mini_cluster_num = mini_cluster_num, round_num = round_num)\n",
    "            set_clustering_machine_train_batch(img_path, intermediate_data_folder, train_part_num = partn)\n",
    "            set_clustering_machine_validation_batch(img_path, intermediate_data_folder, valid_part_num = partn)\n",
    "            \n",
    "            check_train_loss_converge(mini_batch_folder, data_name, dataset, img_path, 'part_num_' + str(partn), input_layer = GCN_layer, epoch_num = 400, \\\n",
    "                                     dropout = dropout, lr = lr, weight_decay = weight_decay, mini_epoch_num = mini_epoch_num, \n",
    "                                     valid_part_num = partn, train_part_num = partn, test_part_num = 1)\n",
    "            \n",
    "#             # for the large dataset and split first case, the cluster info cannot be generated\n",
    "#             clustering_machine.mini_batch_train_clustering(hop_layer)\n",
    "#             draw_cluster_info(clustering_machine, data_name, img_path, comments = '_cluster_node_distr_' + str(hop_layer) + '_hops')\n",
    "            \n",
    "def output_F1_score(data, data_name, dataset, image_data_path, intermediate_data_path, partition_nums, layers, \\\n",
    "                    dropout = 0.1, lr = 0.0001, weight_decay = 0.1, mini_epoch_num = 20, \\\n",
    "                    valid_part_num = 2, mini_cluster_num = 16, round_num = 2):            \n",
    "    for partn in partition_nums:\n",
    "        for GCN_layer in layers:\n",
    "            net_layer = len(GCN_layer) + 1\n",
    "            hop_layer = net_layer\n",
    "            \n",
    "            # set the save path\n",
    "            print('Start running for partition num: ' + str(partn) + ' hop layer ' + str(hop_layer))\n",
    "            img_path = image_data_path + 'cluster_num_' + str(partn) + '/' + 'net_layer_' + str(net_layer) + '_hop_layer_' + str(hop_layer) + '/'\n",
    "            img_path += 'output_F1_score/'  # further subfolder for different task\n",
    "            intermediate_data_folder = intermediate_data_path\n",
    "            \n",
    "            # set the batch for validation and train\n",
    "            mini_batch_folder = intermediate_data_folder\n",
    "            set_clustering_machine(data, img_path, intermediate_data_folder, test_ratio = 0.05, validation_ratio = 0.85, train_part_num = partn, mini_cluster_num = mini_cluster_num, round_num = round_num)\n",
    "            set_clustering_machine_train_batch(img_path, intermediate_data_folder, train_part_num = partn)\n",
    "            set_clustering_machine_validation_batch(img_path, intermediate_data_folder, valid_part_num = partn)\n",
    "            \n",
    "            # start to run the model, train and validation \n",
    "            execute_one_train(mini_batch_folder, img_path, repeate_time = 7, input_layer = GCN_layer, epoch_num = 400, \n",
    "                                            dropout = dropout, lr = lr, weight_decay = weight_decay, mini_epoch_num = mini_epoch_num, \\\n",
    "                                             train_part_num = partn, test_part_num = 1)\n",
    "            \n",
    "            graph_model, validation_accuracy, validation_f1, time_total_train, time_data_load = \\\n",
    "                execute_one_validation(mini_batch_folder, img_path, repeate_time = 7, input_layer = GCN_layer, epoch_num = 400, \n",
    "                                            dropout = dropout, lr = lr, weight_decay = weight_decay, mini_epoch_num = mini_epoch_num, \\\n",
    "                                             valid_part_num = partn)\n",
    "            \n",
    "            \n",
    "            validation_accuracy = store_data_multi_tests(validation_accuracy, data_name, graph_model, img_path, 'test_cluster_num_' + str(partn) + '_hops_' + str(hop_layer))\n",
    "            draw_data_multi_tests(validation_accuracy, data_name, 'vali_cluster_num_' + str(partn) + '_hop_' + str(hop_layer), 'models', 'Accuracy')\n",
    "\n",
    "            validation_f1 = store_data_multi_tests(validation_f1, data_name, graph_model, img_path, 'validation_cluster_num_' + str(partn) + '_hops_' + str(hop_layer))\n",
    "            draw_data_multi_tests(validation_f1, data_name, 'vali_cluster_num_' + str(partn) + '_hop_' + str(hop_layer), 'models', 'F1 score')\n",
    "\n",
    "            time_train = store_data_multi_tests(time_total_train, data_name, graph_model, img_path, 'train_time_cluster_num_' + str(partn) + '_hops_' + str(hop_layer))\n",
    "            draw_data_multi_tests(time_train, data_name, 'train_time_cluster_num_' + str(partn) + '_hop_' + str(hop_layer), 'models', 'Train Time (ms)')\n",
    "\n",
    "            time_load = store_data_multi_tests(time_data_load, data_name, graph_model, img_path, 'load_time_cluster_num_' + str(partn) + '_hops_' + str(hop_layer))\n",
    "            draw_data_multi_tests(time_load, data_name, 'load_time_cluster_num_' + str(partn) + '_hop_' + str(hop_layer), 'models', 'Load Time (ms)')\n",
    "\n",
    "def output_train_investigate(data, data_name, dataset, image_data_path, intermediate_data_path, partition_nums, layers, \\\n",
    "                             dropout = 0.1, lr = 0.0001, weight_decay = 0.1, mini_epoch_num = 20, output_period = 40, \\\n",
    "                             valid_part_num = 2, mini_cluster_num = 16, round_num = 2):            \n",
    "    for partn in partition_nums:\n",
    "        for GCN_layer in layers:\n",
    "            net_layer = len(GCN_layer) + 1\n",
    "            hop_layer = net_layer\n",
    "            # set the save path\n",
    "            print('Start running for partition num: ' + str(partn) + ' hop layer ' + str(hop_layer))\n",
    "            img_path = image_data_path + 'cluster_num_' + str(partn) + '/' + 'net_layer_' + str(net_layer) + '_hop_layer_' + str(hop_layer) + '/'\n",
    "            img_path += 'train_investigate/'  # further subfolder for different task\n",
    "            intermediate_data_folder = intermediate_data_path\n",
    "            \n",
    "            # set the batch for validation and train\n",
    "            mini_batch_folder = intermediate_data_folder\n",
    "            set_clustering_machine(data, img_path, intermediate_data_folder, test_ratio = 0.05, validation_ratio = 0.85, train_part_num = partn, mini_cluster_num = mini_cluster_num, round_num = round_num)\n",
    "            set_clustering_machine_train_batch(img_path, intermediate_data_folder, train_part_num = partn)\n",
    "            set_clustering_machine_validation_batch(img_path, intermediate_data_folder, valid_part_num = partn)\n",
    "\n",
    "            Train_peroid_f1, Train_peroid_accuracy = execute_investigate(mini_batch_folder, img_path, repeate_time = 7, input_layer = GCN_layer, epoch_num = 400, \\\n",
    "                                            dropout = dropout, lr = lr, weight_decay = weight_decay, mini_epoch_num = mini_epoch_num, output_period = output_period, \\\n",
    "                                            valid_part_num = partn, train_part_num = partn, test_part_num = 1)\n",
    "            \n",
    "            Train_peroid_f1 = store_data_multi_investigate(Train_peroid_f1, data_name, 'F1_score', img_path, 'invest_batch_num_' + str(partn) + '_hops_' + str(hop_layer))\n",
    "            draw_data_multi_tests(Train_peroid_f1, data_name, 'Train_process_batch_num_' + str(partn) + '_hop_' + str(hop_layer), 'epoch number', 'F1 score')\n",
    "\n",
    "            Train_peroid_accuracy = store_data_multi_investigate(Train_peroid_accuracy, data_name, 'Accuracy', img_path, 'invest_batch_num_' + str(partn) + '_hops_' + str(hop_layer))\n",
    "            draw_data_multi_tests(Train_peroid_accuracy, data_name, 'Train_process_batch_num_' + str(partn) + '_hop_' + str(hop_layer), 'epoch number', 'Accuracy')\n",
    "            \n",
    "            \n",
    "            \n",
    "def output_tune_param(data, data_name, dataset, image_data_path, intermediate_data_path, partition_nums, layers, \\\n",
    "                      dropout = 0.1, lr = 0.0001, weight_decay = 0.1, mini_epoch_num = 20, \\\n",
    "                     valid_part_num = 2, mini_cluster_num = 16, round_num = 2):\n",
    "    for partn in partition_nums:\n",
    "        for GCN_layer in layers:\n",
    "            net_layer = len(GCN_layer) + 1\n",
    "            hop_layer = net_layer\n",
    "            # Set the tune parameters and name\n",
    "            tune_name = 'batch_epoch_num'\n",
    "            tune_params = [400, 200, 100, 50, 20, 10, 5, 1]\n",
    "#             tune_name = 'weight_decay'\n",
    "#             tune_params = [0.01, 0.1, 0.3, 0.5]\n",
    "\n",
    "            img_path = image_data_path + 'cluster_num_' + str(partn) + '/' + 'net_layer_' + str(net_layer) + '_hop_layer_' + str(hop_layer) + '/'\n",
    "            img_path += 'tune_' + tune_name + '/'  # further subfolder for different task\n",
    "            intermediate_data_folder = intermediate_data_path\n",
    "            print('Start tuning for tuning param: ' + tune_name + ' partition num: ' + str(partn) + ' hop layer ' + str(hop_layer))\n",
    "            \n",
    "            # set the batch for validation and train\n",
    "            mini_batch_folder = intermediate_data_folder\n",
    "            set_clustering_machine(data, img_path, intermediate_data_folder, test_ratio = 0.05, validation_ratio = 0.85, train_part_num = partn, mini_cluster_num = mini_cluster_num, round_num = round_num)\n",
    "            set_clustering_machine_train_batch(img_path, intermediate_data_folder, train_part_num = partn)\n",
    "            set_clustering_machine_validation_batch(img_path, intermediate_data_folder, valid_part_num = partn)\n",
    "\n",
    "            validation_accuracy, validation_f1, time_total_train, time_data_load = execute_tuning(tune_params, mini_batch_folder, img_path, repeate_time = 7, \\\n",
    "                                                input_layer = GCN_layer, epoch_num = 400, dropout = dropout, lr = lr, weight_decay = weight_decay, mini_epoch_num = mini_epoch_num, \\\n",
    "                                                valid_part_num = partn, train_part_num = partn, test_part_num = 1)\n",
    "\n",
    "            validation_accuracy = store_data_multi_tuning(tune_params,validation_accuracy, data_name, img_path, 'accuracy_cluster_num_' + str(partn) + '_hops_' + str(hop_layer))\n",
    "            draw_data_multi_tests(validation_accuracy, data_name, 'vali_cluster_num_' + str(partn) + '_hop_' + str(hop_layer), 'epochs_per_batch', 'Accuracy')\n",
    "\n",
    "            validation_f1 = store_data_multi_tuning(tune_params, validation_f1, data_name, img_path, 'validation_cluster_num_' + str(partn) + '_hops_' + str(hop_layer))\n",
    "            draw_data_multi_tests(validation_f1, data_name, 'vali_cluster_num_' + str(partn) + '_hop_' + str(hop_layer), 'epochs_per_batch', 'F1 score')\n",
    "\n",
    "            time_train = store_data_multi_tuning(tune_params, time_total_train, data_name, img_path, 'train_time_cluster_num_' + str(partn) + '_hops_' + str(hop_layer))\n",
    "            draw_data_multi_tests(time_train, data_name, 'train_time_cluster_num_' + str(partn) + '_hop_' + str(hop_layer), 'epochs_per_batch', 'Train Time (ms)')\n",
    "\n",
    "            time_load = store_data_multi_tuning(tune_params, time_data_load, data_name, img_path, 'load_time_cluster_num_' + str(partn) + '_hops_' + str(hop_layer))\n",
    "            draw_data_multi_tests(time_load, data_name, 'load_time_cluster_num_' + str(partn) + '_hop_' + str(hop_layer), 'epochs_per_batch', 'Load Time (ms)')\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use data from pytorch geometric datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_data_root = '/media/xiangli/storage1/projects/tmpdata/'\n",
    "test_folder_name = 'recombine_isolate_cluster/train_10%_mini_cluster_16_round_2/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cora dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "data_name = 'Cora'\n",
    "dataset = Planetoid(root = local_data_root + 'Planetoid/Cora', name=data_name)\n",
    "data = dataset[0]\n",
    "image_data_path = './results/' + data_name + '/' + test_folder_name\n",
    "# set the current folder as the intermediate data folder so that we can easily copy either clustering \n",
    "intermediate_data_folder = './'\n",
    "\n",
    "partition_nums = [2, 4, 8]\n",
    "layers = [[], [32], [32, 32]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PubMed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "data_name = 'PubMed'\n",
    "dataset = Planetoid(root = local_data_root + 'Planetoid/PubMed', name=data_name)\n",
    "data = dataset[0]\n",
    "image_data_path = './results/' + data_name + '/' + test_folder_name\n",
    "intermediate_data_folder = './'\n",
    "partition_nums = [2, 4, 8]\n",
    "layers = [[], [64], [64, 64]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CoraFull Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of data:  1\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import CoraFull\n",
    "data_name = 'CoraFull'\n",
    "dataset = CoraFull(root = local_data_root + 'CoralFull')\n",
    "print('number of data: ', len(dataset))\n",
    "data = dataset[0]\n",
    "\n",
    "image_data_path = './results/' + data_name + '/' + test_folder_name\n",
    "intermediate_data_folder = './'\n",
    "partition_nums = [2, 4, 8]\n",
    "layers = [[], [128], [128, 128]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tune hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start tuning for tuning param: batch_epoch_num partition num: 2 hop layer 1\n",
      "\n",
      "====================================================================================================\n",
      "Start to generate the clustering machine:\n",
      "Batch machine creation costs a total of 0.1663 seconds!\n",
      "\n",
      " Information about the content of ./tmp/\n",
      "\n",
      "Start to split data into train, test, validation:\n",
      "Data splitting costs a total of 0.1739 seconds!\n",
      "Start to store the batch machine file:\n",
      "Storing batch machine after training batches generation costs a total of 0.0412 seconds!\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0534 seconds!\n",
      "Start to generate the training batches:\n",
      "Train batches production costs a total of 0.2785 seconds!\n",
      "\n",
      " Information about the content of ./train/\n",
      "File name: [ batch_1 ]; with size: 19695.7998046875 KB\n",
      "File name: [ batch_3 ]; with size: 19806.2216796875 KB\n",
      "File name: [ batch_0 ]; with size: 19678.5107421875 KB\n",
      "File name: [ batch_2 ]; with size: 19561.5576171875 KB\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0747 seconds!\n",
      "Start to generate the validation batches:\n",
      "Validation batches production costs a total of 0.2715 seconds!\n",
      "\n",
      " Information about the content of ./validation/\n",
      "File name: [ batch_1 ]; with size: 19753.6279296875 KB\n",
      "File name: [ batch_3 ]; with size: 19864.4404296875 KB\n",
      "File name: [ batch_0 ]; with size: 19736.3466796875 KB\n",
      "File name: [ batch_2 ]; with size: 19619.0029296875 KB\n",
      "\n",
      "====================================================================================================\n",
      "Start tuning for tuning param: batch_epoch_num partition num: 2 hop layer 2\n",
      "\n",
      "====================================================================================================\n",
      "Start to generate the clustering machine:\n",
      "Batch machine creation costs a total of 0.1648 seconds!\n",
      "\n",
      " Information about the content of ./tmp/\n",
      "\n",
      "Start to split data into train, test, validation:\n",
      "Data splitting costs a total of 0.1691 seconds!\n",
      "Start to store the batch machine file:\n",
      "Storing batch machine after training batches generation costs a total of 0.0314 seconds!\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0382 seconds!\n",
      "Start to generate the training batches:\n",
      "Train batches production costs a total of 0.2757 seconds!\n",
      "\n",
      " Information about the content of ./train/\n",
      "File name: [ batch_1 ]; with size: 19722.5107421875 KB\n",
      "File name: [ batch_3 ]; with size: 19670.0810546875 KB\n",
      "File name: [ batch_0 ]; with size: 19647.2685546875 KB\n",
      "File name: [ batch_2 ]; with size: 19706.6357421875 KB\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0746 seconds!\n",
      "Start to generate the validation batches:\n",
      "Validation batches production costs a total of 0.2685 seconds!\n",
      "\n",
      " Information about the content of ./validation/\n",
      "File name: [ batch_1 ]; with size: 19780.4638671875 KB\n",
      "File name: [ batch_3 ]; with size: 19727.8388671875 KB\n",
      "File name: [ batch_0 ]; with size: 19704.9794921875 KB\n",
      "File name: [ batch_2 ]; with size: 19764.5419921875 KB\n",
      "\n",
      "====================================================================================================\n",
      "Start tuning for tuning param: batch_epoch_num partition num: 2 hop layer 3\n",
      "\n",
      "====================================================================================================\n",
      "Start to generate the clustering machine:\n",
      "Batch machine creation costs a total of 0.1767 seconds!\n",
      "\n",
      " Information about the content of ./tmp/\n",
      "\n",
      "Start to split data into train, test, validation:\n",
      "Data splitting costs a total of 0.1774 seconds!\n",
      "Start to store the batch machine file:\n",
      "Storing batch machine after training batches generation costs a total of 0.0328 seconds!\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0385 seconds!\n",
      "Start to generate the training batches:\n",
      "Train batches production costs a total of 0.2696 seconds!\n",
      "\n",
      " Information about the content of ./train/\n",
      "File name: [ batch_1 ]; with size: 19693.8310546875 KB\n",
      "File name: [ batch_3 ]; with size: 19686.4638671875 KB\n",
      "File name: [ batch_0 ]; with size: 19681.6982421875 KB\n",
      "File name: [ batch_2 ]; with size: 19686.6904296875 KB\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0384 seconds!\n",
      "Start to generate the validation batches:\n",
      "Validation batches production costs a total of 0.3150 seconds!\n",
      "\n",
      " Information about the content of ./validation/\n",
      "File name: [ batch_1 ]; with size: 19751.6435546875 KB\n",
      "File name: [ batch_3 ]; with size: 19744.2763671875 KB\n",
      "File name: [ batch_0 ]; with size: 19739.5498046875 KB\n",
      "File name: [ batch_2 ]; with size: 19744.5419921875 KB\n",
      "\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:58: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:58: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start tuning for tuning param: batch_epoch_num partition num: 4 hop layer 1\n",
      "\n",
      "====================================================================================================\n",
      "Start to generate the clustering machine:\n",
      "Batch machine creation costs a total of 0.1308 seconds!\n",
      "\n",
      " Information about the content of ./tmp/\n",
      "\n",
      "Start to split data into train, test, validation:\n",
      "Data splitting costs a total of 0.2171 seconds!\n",
      "Start to store the batch machine file:\n",
      "Storing batch machine after training batches generation costs a total of 0.0329 seconds!\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0387 seconds!\n",
      "Start to generate the training batches:\n",
      "Train batches production costs a total of 0.2015 seconds!\n",
      "\n",
      " Information about the content of ./train/\n",
      "File name: [ batch_1 ]; with size: 9721.3544921875 KB\n",
      "File name: [ batch_7 ]; with size: 9728.1513671875 KB\n",
      "File name: [ batch_3 ]; with size: 9791.4716796875 KB\n",
      "File name: [ batch_4 ]; with size: 9814.0263671875 KB\n",
      "File name: [ batch_6 ]; with size: 9820.2763671875 KB\n",
      "File name: [ batch_0 ]; with size: 9767.1279296875 KB\n",
      "File name: [ batch_5 ]; with size: 9663.9091796875 KB\n",
      "File name: [ batch_2 ]; with size: 9742.7529296875 KB\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0753 seconds!\n",
      "Start to generate the validation batches:\n",
      "Validation batches production costs a total of 0.2037 seconds!\n",
      "\n",
      " Information about the content of ./validation/\n",
      "File name: [ batch_1 ]; with size: 9750.1748046875 KB\n",
      "File name: [ batch_7 ]; with size: 9756.9638671875 KB\n",
      "File name: [ batch_3 ]; with size: 9820.4794921875 KB\n",
      "File name: [ batch_4 ]; with size: 9843.1201171875 KB\n",
      "File name: [ batch_6 ]; with size: 9849.3935546875 KB\n",
      "File name: [ batch_0 ]; with size: 9796.0810546875 KB\n",
      "File name: [ batch_5 ]; with size: 9692.5498046875 KB\n",
      "File name: [ batch_2 ]; with size: 9771.6357421875 KB\n",
      "\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:58: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:58: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:58: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:58: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start tuning for tuning param: batch_epoch_num partition num: 4 hop layer 2\n",
      "\n",
      "====================================================================================================\n",
      "Start to generate the clustering machine:\n",
      "Batch machine creation costs a total of 0.1297 seconds!\n",
      "\n",
      " Information about the content of ./tmp/\n",
      "\n",
      "Start to split data into train, test, validation:\n",
      "Data splitting costs a total of 0.1707 seconds!\n",
      "Start to store the batch machine file:\n",
      "Storing batch machine after training batches generation costs a total of 0.0327 seconds!\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0377 seconds!\n",
      "Start to generate the training batches:\n",
      "Train batches production costs a total of 0.1983 seconds!\n",
      "\n",
      " Information about the content of ./train/\n",
      "File name: [ batch_1 ]; with size: 9733.7451171875 KB\n",
      "File name: [ batch_7 ]; with size: 9692.8935546875 KB\n",
      "File name: [ batch_3 ]; with size: 9776.2763671875 KB\n",
      "File name: [ batch_4 ]; with size: 9792.9482421875 KB\n",
      "File name: [ batch_6 ]; with size: 9782.3076171875 KB\n",
      "File name: [ batch_0 ]; with size: 9778.5185546875 KB\n",
      "File name: [ batch_5 ]; with size: 9762.3076171875 KB\n",
      "File name: [ batch_2 ]; with size: 9738.1357421875 KB\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0734 seconds!\n",
      "Start to generate the validation batches:\n",
      "Validation batches production costs a total of 0.1963 seconds!\n",
      "\n",
      " Information about the content of ./validation/\n",
      "File name: [ batch_1 ]; with size: 9762.6044921875 KB\n",
      "File name: [ batch_7 ]; with size: 9721.6279296875 KB\n",
      "File name: [ batch_3 ]; with size: 9805.2451171875 KB\n",
      "File name: [ batch_4 ]; with size: 9821.9482421875 KB\n",
      "File name: [ batch_6 ]; with size: 9811.2998046875 KB\n",
      "File name: [ batch_0 ]; with size: 9807.4873046875 KB\n",
      "File name: [ batch_5 ]; with size: 9791.2451171875 KB\n",
      "File name: [ batch_2 ]; with size: 9767.0029296875 KB\n",
      "\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:58: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:58: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:58: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:58: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start tuning for tuning param: batch_epoch_num partition num: 4 hop layer 3\n",
      "\n",
      "====================================================================================================\n",
      "Start to generate the clustering machine:\n",
      "Batch machine creation costs a total of 0.1330 seconds!\n",
      "\n",
      " Information about the content of ./tmp/\n",
      "\n",
      "Start to split data into train, test, validation:\n",
      "Data splitting costs a total of 0.1687 seconds!\n",
      "Start to store the batch machine file:\n",
      "Storing batch machine after training batches generation costs a total of 0.0322 seconds!\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0371 seconds!\n",
      "Start to generate the training batches:\n",
      "Train batches production costs a total of 0.2388 seconds!\n",
      "\n",
      " Information about the content of ./train/\n",
      "File name: [ batch_1 ]; with size: 9699.5654296875 KB\n",
      "File name: [ batch_7 ]; with size: 9789.5107421875 KB\n",
      "File name: [ batch_3 ]; with size: 9879.4951171875 KB\n",
      "File name: [ batch_4 ]; with size: 9808.8779296875 KB\n",
      "File name: [ batch_6 ]; with size: 9768.4013671875 KB\n",
      "File name: [ batch_0 ]; with size: 9779.9638671875 KB\n",
      "File name: [ batch_5 ]; with size: 9663.6669921875 KB\n",
      "File name: [ batch_2 ]; with size: 9668.8076171875 KB\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0386 seconds!\n",
      "Start to generate the validation batches:\n",
      "Validation batches production costs a total of 0.2017 seconds!\n",
      "\n",
      " Information about the content of ./validation/\n",
      "File name: [ batch_1 ]; with size: 9728.3232421875 KB\n",
      "File name: [ batch_7 ]; with size: 9818.5185546875 KB\n",
      "File name: [ batch_3 ]; with size: 9908.7685546875 KB\n",
      "File name: [ batch_4 ]; with size: 9837.9326171875 KB\n",
      "File name: [ batch_6 ]; with size: 9797.3701171875 KB\n",
      "File name: [ batch_0 ]; with size: 9808.9169921875 KB\n",
      "File name: [ batch_5 ]; with size: 9692.2998046875 KB\n",
      "File name: [ batch_2 ]; with size: 9697.4873046875 KB\n",
      "\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:58: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:58: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:58: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:58: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start tuning for tuning param: batch_epoch_num partition num: 8 hop layer 1\n",
      "\n",
      "====================================================================================================\n",
      "Start to generate the clustering machine:\n",
      "Batch machine creation costs a total of 0.1354 seconds!\n",
      "\n",
      " Information about the content of ./tmp/\n",
      "\n",
      "Start to split data into train, test, validation:\n",
      "Data splitting costs a total of 0.1717 seconds!\n",
      "Start to store the batch machine file:\n",
      "Storing batch machine after training batches generation costs a total of 0.0330 seconds!\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0751 seconds!\n",
      "Start to generate the training batches:\n",
      "Train batches production costs a total of 0.1924 seconds!\n",
      "\n",
      " Information about the content of ./train/\n",
      "File name: [ batch_9 ]; with size: 4879.1181640625 KB\n",
      "File name: [ batch_1 ]; with size: 4851.0087890625 KB\n",
      "File name: [ batch_15 ]; with size: 4892.2353515625 KB\n",
      "File name: [ batch_8 ]; with size: 4816.9775390625 KB\n",
      "File name: [ batch_13 ]; with size: 4889.2041015625 KB\n",
      "File name: [ batch_11 ]; with size: 4823.3916015625 KB\n",
      "File name: [ batch_7 ]; with size: 4884.5634765625 KB\n",
      "File name: [ batch_3 ]; with size: 4847.4541015625 KB\n",
      "File name: [ batch_4 ]; with size: 4911.0087890625 KB\n",
      "File name: [ batch_6 ]; with size: 4840.9228515625 KB\n",
      "File name: [ batch_0 ]; with size: 4850.2275390625 KB\n",
      "File name: [ batch_5 ]; with size: 4847.5791015625 KB\n",
      "File name: [ batch_14 ]; with size: 4911.1962890625 KB\n",
      "File name: [ batch_10 ]; with size: 4820.7978515625 KB\n",
      "File name: [ batch_2 ]; with size: 4826.8447265625 KB\n",
      "File name: [ batch_12 ]; with size: 4825.4072265625 KB\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0393 seconds!\n",
      "Start to generate the validation batches:\n",
      "Validation batches production costs a total of 0.1899 seconds!\n",
      "\n",
      " Information about the content of ./validation/\n",
      "File name: [ batch_9 ]; with size: 4893.6513671875 KB\n",
      "File name: [ batch_1 ]; with size: 4865.4404296875 KB\n",
      "File name: [ batch_15 ]; with size: 4906.7998046875 KB\n",
      "File name: [ batch_8 ]; with size: 4831.3154296875 KB\n",
      "File name: [ batch_13 ]; with size: 4903.7607421875 KB\n",
      "File name: [ batch_11 ]; with size: 4837.7451171875 KB\n",
      "File name: [ batch_7 ]; with size: 4899.1123046875 KB\n",
      "File name: [ batch_3 ]; with size: 4861.8857421875 KB\n",
      "File name: [ batch_4 ]; with size: 4925.6279296875 KB\n",
      "File name: [ batch_6 ]; with size: 4855.3232421875 KB\n",
      "File name: [ batch_0 ]; with size: 4864.6669921875 KB\n",
      "File name: [ batch_5 ]; with size: 4862.0185546875 KB\n",
      "File name: [ batch_14 ]; with size: 4925.8154296875 KB\n",
      "File name: [ batch_10 ]; with size: 4835.1513671875 KB\n",
      "File name: [ batch_2 ]; with size: 4841.2138671875 KB\n",
      "File name: [ batch_12 ]; with size: 4839.7685546875 KB\n",
      "\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:58: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:58: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:58: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:58: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start tuning for tuning param: batch_epoch_num partition num: 8 hop layer 2\n",
      "\n",
      "====================================================================================================\n",
      "Start to generate the clustering machine:\n",
      "Batch machine creation costs a total of 0.1313 seconds!\n",
      "\n",
      " Information about the content of ./tmp/\n",
      "\n",
      "Start to split data into train, test, validation:\n",
      "Data splitting costs a total of 0.1714 seconds!\n",
      "Start to store the batch machine file:\n",
      "Storing batch machine after training batches generation costs a total of 0.0330 seconds!\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0374 seconds!\n",
      "Start to generate the training batches:\n",
      "Train batches production costs a total of 0.1939 seconds!\n",
      "\n",
      " Information about the content of ./train/\n",
      "File name: [ batch_9 ]; with size: 4825.4072265625 KB\n",
      "File name: [ batch_1 ]; with size: 4898.3759765625 KB\n",
      "File name: [ batch_15 ]; with size: 4858.2822265625 KB\n",
      "File name: [ batch_8 ]; with size: 4823.3916015625 KB\n",
      "File name: [ batch_13 ]; with size: 4847.9931640625 KB\n",
      "File name: [ batch_11 ]; with size: 4820.7978515625 KB\n",
      "File name: [ batch_7 ]; with size: 4848.2587890625 KB\n",
      "File name: [ batch_3 ]; with size: 4874.6025390625 KB\n",
      "File name: [ batch_4 ]; with size: 4830.8056640625 KB\n",
      "File name: [ batch_6 ]; with size: 4898.2119140625 KB\n",
      "File name: [ batch_0 ]; with size: 4779.6728515625 KB\n",
      "File name: [ batch_5 ]; with size: 4836.0322265625 KB\n",
      "File name: [ batch_14 ]; with size: 4862.4697265625 KB\n",
      "File name: [ batch_10 ]; with size: 4917.3369140625 KB\n",
      "File name: [ batch_2 ]; with size: 4897.9306640625 KB\n",
      "File name: [ batch_12 ]; with size: 4904.8369140625 KB\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0738 seconds!\n",
      "Start to generate the validation batches:\n",
      "Validation batches production costs a total of 0.1929 seconds!\n",
      "\n",
      " Information about the content of ./validation/\n",
      "File name: [ batch_9 ]; with size: 4839.7685546875 KB\n",
      "File name: [ batch_1 ]; with size: 4912.9482421875 KB\n",
      "File name: [ batch_15 ]; with size: 4872.7529296875 KB\n",
      "File name: [ batch_8 ]; with size: 4837.7451171875 KB\n",
      "File name: [ batch_13 ]; with size: 4862.4169921875 KB\n",
      "File name: [ batch_11 ]; with size: 4835.1513671875 KB\n",
      "File name: [ batch_7 ]; with size: 4862.6982421875 KB\n",
      "File name: [ batch_3 ]; with size: 4889.1201171875 KB\n",
      "File name: [ batch_4 ]; with size: 4845.1826171875 KB\n",
      "File name: [ batch_6 ]; with size: 4912.7919921875 KB\n",
      "File name: [ batch_0 ]; with size: 4793.9013671875 KB\n",
      "File name: [ batch_5 ]; with size: 4850.4169921875 KB\n",
      "File name: [ batch_14 ]; with size: 4876.9404296875 KB\n",
      "File name: [ batch_10 ]; with size: 4931.9716796875 KB\n",
      "File name: [ batch_2 ]; with size: 4912.5107421875 KB\n",
      "File name: [ batch_12 ]; with size: 4919.4482421875 KB\n",
      "\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:58: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:58: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:58: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:58: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start tuning for tuning param: batch_epoch_num partition num: 8 hop layer 3\n",
      "\n",
      "====================================================================================================\n",
      "Start to generate the clustering machine:\n",
      "Batch machine creation costs a total of 0.1761 seconds!\n",
      "\n",
      " Information about the content of ./tmp/\n",
      "\n",
      "Start to split data into train, test, validation:\n",
      "Data splitting costs a total of 0.1730 seconds!\n",
      "Start to store the batch machine file:\n",
      "Storing batch machine after training batches generation costs a total of 0.0332 seconds!\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0367 seconds!\n",
      "Start to generate the training batches:\n",
      "Train batches production costs a total of 0.1918 seconds!\n",
      "\n",
      " Information about the content of ./train/\n",
      "File name: [ batch_9 ]; with size: 4779.6728515625 KB\n",
      "File name: [ batch_1 ]; with size: 4823.8837890625 KB\n",
      "File name: [ batch_15 ]; with size: 4884.0087890625 KB\n",
      "File name: [ batch_8 ]; with size: 4841.3525390625 KB\n",
      "File name: [ batch_13 ]; with size: 4878.9072265625 KB\n",
      "File name: [ batch_11 ]; with size: 4881.3603515625 KB\n",
      "File name: [ batch_7 ]; with size: 4851.0087890625 KB\n",
      "File name: [ batch_3 ]; with size: 4832.6259765625 KB\n",
      "File name: [ batch_4 ]; with size: 4884.5634765625 KB\n",
      "File name: [ batch_6 ]; with size: 4840.9228515625 KB\n",
      "File name: [ batch_0 ]; with size: 4811.3056640625 KB\n",
      "File name: [ batch_5 ]; with size: 4974.0009765625 KB\n",
      "File name: [ batch_14 ]; with size: 4948.5322265625 KB\n",
      "File name: [ batch_10 ]; with size: 4803.8603515625 KB\n",
      "File name: [ batch_2 ]; with size: 4841.6103515625 KB\n",
      "File name: [ batch_12 ]; with size: 4842.6962890625 KB\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0386 seconds!\n",
      "Start to generate the validation batches:\n",
      "Validation batches production costs a total of 0.1916 seconds!\n",
      "\n",
      " Information about the content of ./validation/\n",
      "File name: [ batch_9 ]; with size: 4793.9013671875 KB\n",
      "File name: [ batch_1 ]; with size: 4838.2451171875 KB\n",
      "File name: [ batch_15 ]; with size: 4898.5576171875 KB\n",
      "File name: [ batch_8 ]; with size: 4855.7607421875 KB\n",
      "File name: [ batch_13 ]; with size: 4893.4326171875 KB\n",
      "File name: [ batch_11 ]; with size: 4895.8935546875 KB\n",
      "File name: [ batch_7 ]; with size: 4865.4404296875 KB\n",
      "File name: [ batch_3 ]; with size: 4847.0263671875 KB\n",
      "File name: [ batch_4 ]; with size: 4899.1123046875 KB\n",
      "File name: [ batch_6 ]; with size: 4855.3232421875 KB\n",
      "File name: [ batch_0 ]; with size: 4825.6279296875 KB\n",
      "File name: [ batch_5 ]; with size: 4988.8076171875 KB\n",
      "File name: [ batch_14 ]; with size: 4963.2607421875 KB\n",
      "File name: [ batch_10 ]; with size: 4818.1591796875 KB\n",
      "File name: [ batch_2 ]; with size: 4856.0185546875 KB\n",
      "File name: [ batch_12 ]; with size: 4857.1044921875 KB\n",
      "\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:58: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:58: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:58: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:58: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tuning the mini_epoch\n",
    "output_tune_param(data, data_name, dataset, image_data_path, intermediate_data_folder, partition_nums, layers, \\\n",
    "                  dropout = 0.3, lr = 0.0001, weight_decay = 0.001, mini_epoch_num = 20, valid_part_num = 2, mini_cluster_num = 16, round_num = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start checking train loss for partition num: 2 hop layer: 1\n",
      "\n",
      "====================================================================================================\n",
      "Start to generate the clustering machine:\n",
      "Batch machine creation costs a total of 0.1375 seconds!\n",
      "\n",
      " Information about the content of ./tmp/\n",
      "\n",
      "Start to split data into train, test, validation:\n",
      "Data splitting costs a total of 0.2193 seconds!\n",
      "Start to store the batch machine file:\n",
      "Storing batch machine after training batches generation costs a total of 0.0316 seconds!\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0377 seconds!\n",
      "Start to generate the training batches:\n",
      "Train batches production costs a total of 0.2791 seconds!\n",
      "\n",
      " Information about the content of ./train/\n",
      "File name: [ batch_1 ]; with size: 19725.1982421875 KB\n",
      "File name: [ batch_3 ]; with size: 19663.1279296875 KB\n",
      "File name: [ batch_0 ]; with size: 19650.9248046875 KB\n",
      "File name: [ batch_2 ]; with size: 19716.8076171875 KB\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0898 seconds!\n",
      "Start to generate the validation batches:\n",
      "Validation batches production costs a total of 0.2783 seconds!\n",
      "\n",
      " Information about the content of ./validation/\n",
      "File name: [ batch_1 ]; with size: 19783.1435546875 KB\n",
      "File name: [ batch_3 ]; with size: 19720.8779296875 KB\n",
      "File name: [ batch_0 ]; with size: 19708.6435546875 KB\n",
      "File name: [ batch_2 ]; with size: 19774.7216796875 KB\n",
      "\n",
      "====================================================================================================\n",
      "Start checking train loss for partition num: 2 hop layer: 2\n",
      "\n",
      "====================================================================================================\n",
      "Start to generate the clustering machine:\n",
      "Batch machine creation costs a total of 0.1658 seconds!\n",
      "\n",
      " Information about the content of ./tmp/\n",
      "\n",
      "Start to split data into train, test, validation:\n",
      "Data splitting costs a total of 0.1735 seconds!\n",
      "Start to store the batch machine file:\n",
      "Storing batch machine after training batches generation costs a total of 0.0415 seconds!\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0522 seconds!\n",
      "Start to generate the training batches:\n",
      "Train batches production costs a total of 0.2714 seconds!\n",
      "\n",
      " Information about the content of ./train/\n",
      "File name: [ batch_1 ]; with size: 19656.9091796875 KB\n",
      "File name: [ batch_3 ]; with size: 19565.0341796875 KB\n",
      "File name: [ batch_0 ]; with size: 19717.5576171875 KB\n",
      "File name: [ batch_2 ]; with size: 19808.6201171875 KB\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0739 seconds!\n",
      "Start to generate the validation batches:\n",
      "Validation batches production costs a total of 0.2822 seconds!\n",
      "\n",
      " Information about the content of ./validation/\n",
      "File name: [ batch_1 ]; with size: 19714.6904296875 KB\n",
      "File name: [ batch_3 ]; with size: 19622.5263671875 KB\n",
      "File name: [ batch_0 ]; with size: 19775.4404296875 KB\n",
      "File name: [ batch_2 ]; with size: 19866.7919921875 KB\n",
      "\n",
      "====================================================================================================\n",
      "Start checking train loss for partition num: 2 hop layer: 3\n",
      "\n",
      "====================================================================================================\n",
      "Start to generate the clustering machine:\n",
      "Batch machine creation costs a total of 0.1681 seconds!\n",
      "\n",
      " Information about the content of ./tmp/\n",
      "\n",
      "Start to split data into train, test, validation:\n",
      "Data splitting costs a total of 0.1744 seconds!\n",
      "Start to store the batch machine file:\n",
      "Storing batch machine after training batches generation costs a total of 0.0426 seconds!\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0537 seconds!\n",
      "Start to generate the training batches:\n",
      "Train batches production costs a total of 0.2833 seconds!\n",
      "\n",
      " Information about the content of ./train/\n",
      "File name: [ batch_1 ]; with size: 19732.0419921875 KB\n",
      "File name: [ batch_3 ]; with size: 19645.5810546875 KB\n",
      "File name: [ batch_0 ]; with size: 19642.4873046875 KB\n",
      "File name: [ batch_2 ]; with size: 19727.1044921875 KB\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0734 seconds!\n",
      "Start to generate the validation batches:\n",
      "Validation batches production costs a total of 0.2893 seconds!\n",
      "\n",
      " Information about the content of ./validation/\n",
      "File name: [ batch_1 ]; with size: 19790.0263671875 KB\n",
      "File name: [ batch_3 ]; with size: 19703.2607421875 KB\n",
      "File name: [ batch_0 ]; with size: 19700.1669921875 KB\n",
      "File name: [ batch_2 ]; with size: 19785.0888671875 KB\n",
      "\n",
      "====================================================================================================\n",
      "Start checking train loss for partition num: 4 hop layer: 1\n",
      "\n",
      "====================================================================================================\n",
      "Start to generate the clustering machine:\n",
      "Batch machine creation costs a total of 0.1619 seconds!\n",
      "\n",
      " Information about the content of ./tmp/\n",
      "\n",
      "Start to split data into train, test, validation:\n",
      "Data splitting costs a total of 0.1736 seconds!\n",
      "Start to store the batch machine file:\n",
      "Storing batch machine after training batches generation costs a total of 0.0424 seconds!\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0528 seconds!\n",
      "Start to generate the training batches:\n",
      "Train batches production costs a total of 0.2075 seconds!\n",
      "\n",
      " Information about the content of ./train/\n",
      "File name: [ batch_1 ]; with size: 9927.4091796875 KB\n",
      "File name: [ batch_7 ]; with size: 9743.5419921875 KB\n",
      "File name: [ batch_3 ]; with size: 9688.2060546875 KB\n",
      "File name: [ batch_4 ]; with size: 9756.9794921875 KB\n",
      "File name: [ batch_6 ]; with size: 9724.3154296875 KB\n",
      "File name: [ batch_0 ]; with size: 9767.9716796875 KB\n",
      "File name: [ batch_5 ]; with size: 9801.2763671875 KB\n",
      "File name: [ batch_2 ]; with size: 9649.9326171875 KB\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0539 seconds!\n",
      "Start to generate the validation batches:\n",
      "Validation batches production costs a total of 0.2425 seconds!\n",
      "\n",
      " Information about the content of ./validation/\n",
      "File name: [ batch_1 ]; with size: 9956.8310546875 KB\n",
      "File name: [ batch_7 ]; with size: 9772.4404296875 KB\n",
      "File name: [ batch_3 ]; with size: 9716.9091796875 KB\n",
      "File name: [ batch_4 ]; with size: 9785.9091796875 KB\n",
      "File name: [ batch_6 ]; with size: 9753.1123046875 KB\n",
      "File name: [ batch_0 ]; with size: 9796.9013671875 KB\n",
      "File name: [ batch_5 ]; with size: 9830.3154296875 KB\n",
      "File name: [ batch_2 ]; with size: 9678.5419921875 KB\n",
      "\n",
      "====================================================================================================\n",
      "Start checking train loss for partition num: 4 hop layer: 2\n",
      "\n",
      "====================================================================================================\n",
      "Start to generate the clustering machine:\n",
      "Batch machine creation costs a total of 0.1666 seconds!\n",
      "\n",
      " Information about the content of ./tmp/\n",
      "\n",
      "Start to split data into train, test, validation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data splitting costs a total of 0.1725 seconds!\n",
      "Start to store the batch machine file:\n",
      "Storing batch machine after training batches generation costs a total of 0.0410 seconds!\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0511 seconds!\n",
      "Start to generate the training batches:\n",
      "Train batches production costs a total of 0.1993 seconds!\n",
      "\n",
      " Information about the content of ./train/\n",
      "File name: [ batch_1 ]; with size: 9728.0263671875 KB\n",
      "File name: [ batch_7 ]; with size: 9748.1826171875 KB\n",
      "File name: [ batch_3 ]; with size: 9770.3232421875 KB\n",
      "File name: [ batch_4 ]; with size: 9764.9169921875 KB\n",
      "File name: [ batch_6 ]; with size: 9793.9404296875 KB\n",
      "File name: [ batch_0 ]; with size: 9759.6201171875 KB\n",
      "File name: [ batch_5 ]; with size: 9719.7919921875 KB\n",
      "File name: [ batch_2 ]; with size: 9769.1748046875 KB\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0873 seconds!\n",
      "Start to generate the validation batches:\n",
      "Validation batches production costs a total of 0.2051 seconds!\n",
      "\n",
      " Information about the content of ./validation/\n",
      "File name: [ batch_1 ]; with size: 9756.8701171875 KB\n",
      "File name: [ batch_7 ]; with size: 9777.0966796875 KB\n",
      "File name: [ batch_3 ]; with size: 9799.2685546875 KB\n",
      "File name: [ batch_4 ]; with size: 9793.8623046875 KB\n",
      "File name: [ batch_6 ]; with size: 9822.9482421875 KB\n",
      "File name: [ batch_0 ]; with size: 9788.5498046875 KB\n",
      "File name: [ batch_5 ]; with size: 9748.5888671875 KB\n",
      "File name: [ batch_2 ]; with size: 9798.1201171875 KB\n",
      "\n",
      "====================================================================================================\n",
      "Start checking train loss for partition num: 4 hop layer: 3\n",
      "\n",
      "====================================================================================================\n",
      "Start to generate the clustering machine:\n",
      "Batch machine creation costs a total of 0.1653 seconds!\n",
      "\n",
      " Information about the content of ./tmp/\n",
      "\n",
      "Start to split data into train, test, validation:\n",
      "Data splitting costs a total of 0.1739 seconds!\n",
      "Start to store the batch machine file:\n",
      "Storing batch machine after training batches generation costs a total of 0.0431 seconds!\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0526 seconds!\n",
      "Start to generate the training batches:\n",
      "Train batches production costs a total of 0.2042 seconds!\n",
      "\n",
      " Information about the content of ./train/\n",
      "File name: [ batch_1 ]; with size: 9633.5966796875 KB\n",
      "File name: [ batch_7 ]; with size: 9713.9873046875 KB\n",
      "File name: [ batch_3 ]; with size: 9807.9091796875 KB\n",
      "File name: [ batch_4 ]; with size: 9714.4091796875 KB\n",
      "File name: [ batch_6 ]; with size: 9753.4873046875 KB\n",
      "File name: [ batch_0 ]; with size: 9839.9169921875 KB\n",
      "File name: [ batch_5 ]; with size: 9847.1357421875 KB\n",
      "File name: [ batch_2 ]; with size: 9747.8466796875 KB\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0907 seconds!\n",
      "Start to generate the validation batches:\n",
      "Validation batches production costs a total of 0.2047 seconds!\n",
      "\n",
      " Information about the content of ./validation/\n",
      "File name: [ batch_1 ]; with size: 9662.1435546875 KB\n",
      "File name: [ batch_7 ]; with size: 9742.7763671875 KB\n",
      "File name: [ batch_3 ]; with size: 9836.9638671875 KB\n",
      "File name: [ batch_4 ]; with size: 9743.1826171875 KB\n",
      "File name: [ batch_6 ]; with size: 9782.4013671875 KB\n",
      "File name: [ batch_0 ]; with size: 9869.0732421875 KB\n",
      "File name: [ batch_5 ]; with size: 9876.3232421875 KB\n",
      "File name: [ batch_2 ]; with size: 9776.7529296875 KB\n",
      "\n",
      "====================================================================================================\n",
      "Start checking train loss for partition num: 8 hop layer: 1\n",
      "\n",
      "====================================================================================================\n",
      "Start to generate the clustering machine:\n",
      "Batch machine creation costs a total of 0.1647 seconds!\n",
      "\n",
      " Information about the content of ./tmp/\n",
      "\n",
      "Start to split data into train, test, validation:\n",
      "Data splitting costs a total of 0.1734 seconds!\n",
      "Start to store the batch machine file:\n",
      "Storing batch machine after training batches generation costs a total of 0.0417 seconds!\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0522 seconds!\n",
      "Start to generate the training batches:\n",
      "Train batches production costs a total of 0.1893 seconds!\n",
      "\n",
      " Information about the content of ./train/\n",
      "File name: [ batch_9 ]; with size: 4928.3134765625 KB\n",
      "File name: [ batch_1 ]; with size: 4866.5478515625 KB\n",
      "File name: [ batch_15 ]; with size: 4840.9228515625 KB\n",
      "File name: [ batch_8 ]; with size: 4867.4228515625 KB\n",
      "File name: [ batch_13 ]; with size: 4804.8212890625 KB\n",
      "File name: [ batch_11 ]; with size: 4819.2822265625 KB\n",
      "File name: [ batch_7 ]; with size: 4881.3603515625 KB\n",
      "File name: [ batch_3 ]; with size: 4772.9384765625 KB\n",
      "File name: [ batch_4 ]; with size: 4889.2041015625 KB\n",
      "File name: [ batch_6 ]; with size: 4884.0087890625 KB\n",
      "File name: [ batch_0 ]; with size: 4867.8291015625 KB\n",
      "File name: [ batch_5 ]; with size: 4852.4384765625 KB\n",
      "File name: [ batch_14 ]; with size: 4888.4697265625 KB\n",
      "File name: [ batch_10 ]; with size: 4870.8369140625 KB\n",
      "File name: [ batch_2 ]; with size: 4842.4384765625 KB\n",
      "File name: [ batch_12 ]; with size: 4841.3525390625 KB\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0526 seconds!\n",
      "Start to generate the validation batches:\n",
      "Validation batches production costs a total of 0.1964 seconds!\n",
      "\n",
      " Information about the content of ./validation/\n",
      "File name: [ batch_9 ]; with size: 4942.9951171875 KB\n",
      "File name: [ batch_1 ]; with size: 4881.0263671875 KB\n",
      "File name: [ batch_15 ]; with size: 4855.3232421875 KB\n",
      "File name: [ batch_8 ]; with size: 4881.9091796875 KB\n",
      "File name: [ batch_13 ]; with size: 4819.1279296875 KB\n",
      "File name: [ batch_11 ]; with size: 4833.6201171875 KB\n",
      "File name: [ batch_7 ]; with size: 4895.8935546875 KB\n",
      "File name: [ batch_3 ]; with size: 4787.1513671875 KB\n",
      "File name: [ batch_4 ]; with size: 4903.7607421875 KB\n",
      "File name: [ batch_6 ]; with size: 4898.5576171875 KB\n",
      "File name: [ batch_0 ]; with size: 4882.3076171875 KB\n",
      "File name: [ batch_5 ]; with size: 4866.8857421875 KB\n",
      "File name: [ batch_14 ]; with size: 4903.0185546875 KB\n",
      "File name: [ batch_10 ]; with size: 4885.3466796875 KB\n",
      "File name: [ batch_2 ]; with size: 4856.8623046875 KB\n",
      "File name: [ batch_12 ]; with size: 4855.7607421875 KB\n",
      "\n",
      "====================================================================================================\n",
      "Start checking train loss for partition num: 8 hop layer: 2\n",
      "\n",
      "====================================================================================================\n",
      "Start to generate the clustering machine:\n",
      "Batch machine creation costs a total of 0.1593 seconds!\n",
      "\n",
      " Information about the content of ./tmp/\n",
      "\n",
      "Start to split data into train, test, validation:\n",
      "Data splitting costs a total of 0.1722 seconds!\n",
      "Start to store the batch machine file:\n",
      "Storing batch machine after training batches generation costs a total of 0.0420 seconds!\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0516 seconds!\n",
      "Start to generate the training batches:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches production costs a total of 0.1914 seconds!\n",
      "\n",
      " Information about the content of ./train/\n",
      "File name: [ batch_9 ]; with size: 4798.5947265625 KB\n",
      "File name: [ batch_1 ]; with size: 4816.2822265625 KB\n",
      "File name: [ batch_15 ]; with size: 4922.5322265625 KB\n",
      "File name: [ batch_8 ]; with size: 4830.8056640625 KB\n",
      "File name: [ batch_13 ]; with size: 4894.0166015625 KB\n",
      "File name: [ batch_11 ]; with size: 4820.5478515625 KB\n",
      "File name: [ batch_7 ]; with size: 4874.3291015625 KB\n",
      "File name: [ batch_3 ]; with size: 4852.4384765625 KB\n",
      "File name: [ batch_4 ]; with size: 4820.8212890625 KB\n",
      "File name: [ batch_6 ]; with size: 4857.3994140625 KB\n",
      "File name: [ batch_0 ]; with size: 4888.4697265625 KB\n",
      "File name: [ batch_5 ]; with size: 4862.3525390625 KB\n",
      "File name: [ batch_14 ]; with size: 4848.5869140625 KB\n",
      "File name: [ batch_10 ]; with size: 4904.8369140625 KB\n",
      "File name: [ batch_2 ]; with size: 4890.8603515625 KB\n",
      "File name: [ batch_12 ]; with size: 4840.9072265625 KB\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0523 seconds!\n",
      "Start to generate the validation batches:\n",
      "Validation batches production costs a total of 0.2318 seconds!\n",
      "\n",
      " Information about the content of ./validation/\n",
      "File name: [ batch_9 ]; with size: 4812.8779296875 KB\n",
      "File name: [ batch_1 ]; with size: 4830.6201171875 KB\n",
      "File name: [ batch_15 ]; with size: 4937.1826171875 KB\n",
      "File name: [ batch_8 ]; with size: 4845.1826171875 KB\n",
      "File name: [ batch_13 ]; with size: 4908.5732421875 KB\n",
      "File name: [ batch_11 ]; with size: 4834.9013671875 KB\n",
      "File name: [ batch_7 ]; with size: 4888.8388671875 KB\n",
      "File name: [ batch_3 ]; with size: 4866.8857421875 KB\n",
      "File name: [ batch_4 ]; with size: 4835.1826171875 KB\n",
      "File name: [ batch_6 ]; with size: 4871.8466796875 KB\n",
      "File name: [ batch_0 ]; with size: 4903.0185546875 KB\n",
      "File name: [ batch_5 ]; with size: 4876.8154296875 KB\n",
      "File name: [ batch_14 ]; with size: 4863.0107421875 KB\n",
      "File name: [ batch_10 ]; with size: 4919.4482421875 KB\n",
      "File name: [ batch_2 ]; with size: 4905.4248046875 KB\n",
      "File name: [ batch_12 ]; with size: 4855.3310546875 KB\n",
      "\n",
      "====================================================================================================\n",
      "Start checking train loss for partition num: 8 hop layer: 3\n",
      "\n",
      "====================================================================================================\n",
      "Start to generate the clustering machine:\n",
      "Batch machine creation costs a total of 0.1267 seconds!\n",
      "\n",
      " Information about the content of ./tmp/\n",
      "\n",
      "Start to split data into train, test, validation:\n",
      "Data splitting costs a total of 0.1725 seconds!\n",
      "Start to store the batch machine file:\n",
      "Storing batch machine after training batches generation costs a total of 0.0419 seconds!\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0882 seconds!\n",
      "Start to generate the training batches:\n",
      "Train batches production costs a total of 0.2017 seconds!\n",
      "\n",
      " Information about the content of ./train/\n",
      "File name: [ batch_9 ]; with size: 4974.0009765625 KB\n",
      "File name: [ batch_1 ]; with size: 4897.9306640625 KB\n",
      "File name: [ batch_15 ]; with size: 4915.9853515625 KB\n",
      "File name: [ batch_8 ]; with size: 4779.6728515625 KB\n",
      "File name: [ batch_13 ]; with size: 4824.5712890625 KB\n",
      "File name: [ batch_11 ]; with size: 4811.3056640625 KB\n",
      "File name: [ batch_7 ]; with size: 4831.1103515625 KB\n",
      "File name: [ batch_3 ]; with size: 4873.4306640625 KB\n",
      "File name: [ batch_4 ]; with size: 4823.3916015625 KB\n",
      "File name: [ batch_6 ]; with size: 4836.0322265625 KB\n",
      "File name: [ batch_0 ]; with size: 4884.0087890625 KB\n",
      "File name: [ batch_5 ]; with size: 4904.8369140625 KB\n",
      "File name: [ batch_14 ]; with size: 4848.2587890625 KB\n",
      "File name: [ batch_10 ]; with size: 4840.9228515625 KB\n",
      "File name: [ batch_2 ]; with size: 4811.1181640625 KB\n",
      "File name: [ batch_12 ]; with size: 4867.8291015625 KB\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0525 seconds!\n",
      "Start to generate the validation batches:\n",
      "Validation batches production costs a total of 0.1956 seconds!\n",
      "\n",
      " Information about the content of ./validation/\n",
      "File name: [ batch_9 ]; with size: 4988.8076171875 KB\n",
      "File name: [ batch_1 ]; with size: 4912.5107421875 KB\n",
      "File name: [ batch_15 ]; with size: 4930.6279296875 KB\n",
      "File name: [ batch_8 ]; with size: 4793.9013671875 KB\n",
      "File name: [ batch_13 ]; with size: 4838.9326171875 KB\n",
      "File name: [ batch_11 ]; with size: 4825.6279296875 KB\n",
      "File name: [ batch_7 ]; with size: 4845.4873046875 KB\n",
      "File name: [ batch_3 ]; with size: 4887.9326171875 KB\n",
      "File name: [ batch_4 ]; with size: 4837.7451171875 KB\n",
      "File name: [ batch_6 ]; with size: 4850.4169921875 KB\n",
      "File name: [ batch_0 ]; with size: 4898.5576171875 KB\n",
      "File name: [ batch_5 ]; with size: 4919.4482421875 KB\n",
      "File name: [ batch_14 ]; with size: 4862.6982421875 KB\n",
      "File name: [ batch_10 ]; with size: 4855.3232421875 KB\n",
      "File name: [ batch_2 ]; with size: 4825.4404296875 KB\n",
      "File name: [ batch_12 ]; with size: 4882.3076171875 KB\n",
      "\n",
      "====================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check training loss\n",
    "output_train_loss(data, data_name, dataset, image_data_path, intermediate_data_folder, partition_nums, layers, \\\n",
    "                  dropout = 0.3, lr = 0.0001, weight_decay = 0.001, mini_epoch_num = 20, valid_part_num = 2, mini_cluster_num = 16, round_num = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Check in_train performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start running for partition num: 2 hop layer 1\n",
      "\n",
      "====================================================================================================\n",
      "Start to generate the clustering machine:\n",
      "Batch machine creation costs a total of 0.1688 seconds!\n",
      "\n",
      " Information about the content of ./tmp/\n",
      "\n",
      "Start to split data into train, test, validation:\n",
      "Data splitting costs a total of 0.1724 seconds!\n",
      "Start to store the batch machine file:\n",
      "Storing batch machine after training batches generation costs a total of 0.0408 seconds!\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0509 seconds!\n",
      "Start to generate the training batches:\n",
      "Train batches production costs a total of 0.2789 seconds!\n",
      "\n",
      " Information about the content of ./train/\n",
      "File name: [ batch_1 ]; with size: 19690.0966796875 KB\n",
      "File name: [ batch_3 ]; with size: 19648.7373046875 KB\n",
      "File name: [ batch_0 ]; with size: 19679.9638671875 KB\n",
      "File name: [ batch_2 ]; with size: 19730.8544921875 KB\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0735 seconds!\n",
      "Start to generate the validation batches:\n",
      "Validation batches production costs a total of 0.2870 seconds!\n",
      "\n",
      " Information about the content of ./validation/\n",
      "File name: [ batch_1 ]; with size: 19747.9794921875 KB\n",
      "File name: [ batch_3 ]; with size: 19706.4248046875 KB\n",
      "File name: [ batch_0 ]; with size: 19737.7451171875 KB\n",
      "File name: [ batch_2 ]; with size: 19788.8310546875 KB\n",
      "\n",
      "====================================================================================================\n",
      "In-process Training costs a total of 2.0410 seconds!\n",
      "In-process Training costs a total of 2.0144 seconds!\n",
      "In-process Training costs a total of 2.0369 seconds!\n",
      "In-process Training costs a total of 2.0139 seconds!\n",
      "In-process Training costs a total of 2.0942 seconds!\n",
      "In-process Training costs a total of 2.0606 seconds!\n",
      "In-process Training costs a total of 2.0624 seconds!\n",
      "Start running for partition num: 2 hop layer 2\n",
      "\n",
      "====================================================================================================\n",
      "Start to generate the clustering machine:\n",
      "Batch machine creation costs a total of 0.1723 seconds!\n",
      "\n",
      " Information about the content of ./tmp/\n",
      "\n",
      "Start to split data into train, test, validation:\n",
      "Data splitting costs a total of 0.1735 seconds!\n",
      "Start to store the batch machine file:\n",
      "Storing batch machine after training batches generation costs a total of 0.0422 seconds!\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0525 seconds!\n",
      "Start to generate the training batches:\n",
      "Train batches production costs a total of 0.2908 seconds!\n",
      "\n",
      " Information about the content of ./train/\n",
      "File name: [ batch_1 ]; with size: 19513.2841796875 KB\n",
      "File name: [ batch_3 ]; with size: 19701.8857421875 KB\n",
      "File name: [ batch_0 ]; with size: 19859.3076171875 KB\n",
      "File name: [ batch_2 ]; with size: 19677.0498046875 KB\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0795 seconds!\n",
      "Start to generate the validation batches:\n",
      "Validation batches production costs a total of 0.3042 seconds!\n",
      "\n",
      " Information about the content of ./validation/\n",
      "File name: [ batch_1 ]; with size: 19570.6123046875 KB\n",
      "File name: [ batch_3 ]; with size: 19759.7607421875 KB\n",
      "File name: [ batch_0 ]; with size: 19917.6435546875 KB\n",
      "File name: [ batch_2 ]; with size: 19734.8388671875 KB\n",
      "\n",
      "====================================================================================================\n",
      "In-process Training costs a total of 3.0420 seconds!\n",
      "In-process Training costs a total of 2.9983 seconds!\n",
      "In-process Training costs a total of 3.0537 seconds!\n",
      "In-process Training costs a total of 3.0011 seconds!\n",
      "In-process Training costs a total of 3.0424 seconds!\n",
      "In-process Training costs a total of 3.0178 seconds!\n",
      "In-process Training costs a total of 2.9994 seconds!\n",
      "Start running for partition num: 2 hop layer 3\n",
      "\n",
      "====================================================================================================\n",
      "Start to generate the clustering machine:\n",
      "Batch machine creation costs a total of 0.1701 seconds!\n",
      "\n",
      " Information about the content of ./tmp/\n",
      "\n",
      "Start to split data into train, test, validation:\n",
      "Data splitting costs a total of 0.1726 seconds!\n",
      "Start to store the batch machine file:\n",
      "Storing batch machine after training batches generation costs a total of 0.0420 seconds!\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0522 seconds!\n",
      "Start to generate the training batches:\n",
      "Train batches production costs a total of 0.2815 seconds!\n",
      "\n",
      " Information about the content of ./train/\n",
      "File name: [ batch_1 ]; with size: 19800.6591796875 KB\n",
      "File name: [ batch_3 ]; with size: 19664.7138671875 KB\n",
      "File name: [ batch_0 ]; with size: 19570.8076171875 KB\n",
      "File name: [ batch_2 ]; with size: 19707.2216796875 KB\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0382 seconds!\n",
      "Start to generate the validation batches:\n",
      "Validation batches production costs a total of 0.3264 seconds!\n",
      "\n",
      " Information about the content of ./validation/\n",
      "File name: [ batch_1 ]; with size: 19858.8544921875 KB\n",
      "File name: [ batch_3 ]; with size: 19722.5185546875 KB\n",
      "File name: [ batch_0 ]; with size: 19628.2763671875 KB\n",
      "File name: [ batch_2 ]; with size: 19765.0810546875 KB\n",
      "\n",
      "====================================================================================================\n",
      "In-process Training costs a total of 4.0298 seconds!\n",
      "In-process Training costs a total of 3.9412 seconds!\n",
      "In-process Training costs a total of 4.0212 seconds!\n",
      "In-process Training costs a total of 3.9900 seconds!\n",
      "In-process Training costs a total of 4.0543 seconds!\n",
      "In-process Training costs a total of 3.9930 seconds!\n",
      "In-process Training costs a total of 3.9513 seconds!\n",
      "Start running for partition num: 4 hop layer 1\n",
      "\n",
      "====================================================================================================\n",
      "Start to generate the clustering machine:\n",
      "Batch machine creation costs a total of 0.1710 seconds!\n",
      "\n",
      " Information about the content of ./tmp/\n",
      "\n",
      "Start to split data into train, test, validation:\n",
      "Data splitting costs a total of 0.1718 seconds!\n",
      "Start to store the batch machine file:\n",
      "Storing batch machine after training batches generation costs a total of 0.0422 seconds!\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0522 seconds!\n",
      "Start to generate the training batches:\n",
      "Train batches production costs a total of 0.2047 seconds!\n",
      "\n",
      " Information about the content of ./train/\n",
      "File name: [ batch_1 ]; with size: 9729.4091796875 KB\n",
      "File name: [ batch_7 ]; with size: 9829.8857421875 KB\n",
      "File name: [ batch_3 ]; with size: 9723.7685546875 KB\n",
      "File name: [ batch_4 ]; with size: 9697.3623046875 KB\n",
      "File name: [ batch_6 ]; with size: 9734.8310546875 KB\n",
      "File name: [ batch_0 ]; with size: 9842.4326171875 KB\n",
      "File name: [ batch_5 ]; with size: 9767.5966796875 KB\n",
      "File name: [ batch_2 ]; with size: 9734.7216796875 KB\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0909 seconds!\n",
      "Start to generate the validation batches:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation batches production costs a total of 0.2047 seconds!\n",
      "\n",
      " Information about the content of ./validation/\n",
      "File name: [ batch_1 ]; with size: 9758.2685546875 KB\n",
      "File name: [ batch_7 ]; with size: 9859.0107421875 KB\n",
      "File name: [ batch_3 ]; with size: 9752.5810546875 KB\n",
      "File name: [ batch_4 ]; with size: 9726.1044921875 KB\n",
      "File name: [ batch_6 ]; with size: 9763.6748046875 KB\n",
      "File name: [ batch_0 ]; with size: 9871.5732421875 KB\n",
      "File name: [ batch_5 ]; with size: 9796.5498046875 KB\n",
      "File name: [ batch_2 ]; with size: 9763.5732421875 KB\n",
      "\n",
      "====================================================================================================\n",
      "In-process Training costs a total of 3.4271 seconds!\n",
      "In-process Training costs a total of 3.4667 seconds!\n",
      "In-process Training costs a total of 3.4751 seconds!\n",
      "In-process Training costs a total of 3.4419 seconds!\n",
      "In-process Training costs a total of 3.4311 seconds!\n",
      "In-process Training costs a total of 3.4395 seconds!\n",
      "In-process Training costs a total of 3.4216 seconds!\n",
      "Start running for partition num: 4 hop layer 2\n",
      "\n",
      "====================================================================================================\n",
      "Start to generate the clustering machine:\n",
      "Batch machine creation costs a total of 0.1677 seconds!\n",
      "\n",
      " Information about the content of ./tmp/\n",
      "\n",
      "Start to split data into train, test, validation:\n",
      "Data splitting costs a total of 0.1686 seconds!\n",
      "Start to store the batch machine file:\n",
      "Storing batch machine after training batches generation costs a total of 0.0416 seconds!\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0522 seconds!\n",
      "Start to generate the training batches:\n",
      "Train batches production costs a total of 0.2029 seconds!\n",
      "\n",
      " Information about the content of ./train/\n",
      "File name: [ batch_1 ]; with size: 9744.1513671875 KB\n",
      "File name: [ batch_7 ]; with size: 9824.9873046875 KB\n",
      "File name: [ batch_3 ]; with size: 9839.9248046875 KB\n",
      "File name: [ batch_4 ]; with size: 9690.6748046875 KB\n",
      "File name: [ batch_6 ]; with size: 9780.6748046875 KB\n",
      "File name: [ batch_0 ]; with size: 9746.8935546875 KB\n",
      "File name: [ batch_5 ]; with size: 9730.9638671875 KB\n",
      "File name: [ batch_2 ]; with size: 9701.6123046875 KB\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0920 seconds!\n",
      "Start to generate the validation batches:\n",
      "Validation batches production costs a total of 0.2046 seconds!\n",
      "\n",
      " Information about the content of ./validation/\n",
      "File name: [ batch_1 ]; with size: 9773.0107421875 KB\n",
      "File name: [ batch_7 ]; with size: 9854.0810546875 KB\n",
      "File name: [ batch_3 ]; with size: 9869.0810546875 KB\n",
      "File name: [ batch_4 ]; with size: 9719.3857421875 KB\n",
      "File name: [ batch_6 ]; with size: 9809.6669921875 KB\n",
      "File name: [ batch_0 ]; with size: 9775.7841796875 KB\n",
      "File name: [ batch_5 ]; with size: 9759.8310546875 KB\n",
      "File name: [ batch_2 ]; with size: 9730.3701171875 KB\n",
      "\n",
      "====================================================================================================\n",
      "In-process Training costs a total of 4.8429 seconds!\n",
      "In-process Training costs a total of 4.8207 seconds!\n",
      "In-process Training costs a total of 4.8023 seconds!\n",
      "In-process Training costs a total of 4.7791 seconds!\n",
      "In-process Training costs a total of 4.8167 seconds!\n",
      "In-process Training costs a total of 4.8250 seconds!\n",
      "In-process Training costs a total of 4.7925 seconds!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start running for partition num: 4 hop layer 3\n",
      "\n",
      "====================================================================================================\n",
      "Start to generate the clustering machine:\n",
      "Batch machine creation costs a total of 0.2004 seconds!\n",
      "\n",
      " Information about the content of ./tmp/\n",
      "\n",
      "Start to split data into train, test, validation:\n",
      "Data splitting costs a total of 0.1748 seconds!\n",
      "Start to store the batch machine file:\n",
      "Storing batch machine after training batches generation costs a total of 0.0424 seconds!\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0524 seconds!\n",
      "Start to generate the training batches:\n",
      "Train batches production costs a total of 0.3463 seconds!\n",
      "\n",
      " Information about the content of ./train/\n",
      "File name: [ batch_1 ]; with size: 9801.6201171875 KB\n",
      "File name: [ batch_7 ]; with size: 9839.9248046875 KB\n",
      "File name: [ batch_3 ]; with size: 9725.0419921875 KB\n",
      "File name: [ batch_4 ]; with size: 9706.6591796875 KB\n",
      "File name: [ batch_6 ]; with size: 9785.5029296875 KB\n",
      "File name: [ batch_0 ]; with size: 9764.4013671875 KB\n",
      "File name: [ batch_5 ]; with size: 9700.6201171875 KB\n",
      "File name: [ batch_2 ]; with size: 9735.9560546875 KB\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0906 seconds!\n",
      "Start to generate the validation batches:\n",
      "Validation batches production costs a total of 0.3062 seconds!\n",
      "\n",
      " Information about the content of ./validation/\n",
      "File name: [ batch_1 ]; with size: 9830.6591796875 KB\n",
      "File name: [ batch_7 ]; with size: 9869.0810546875 KB\n",
      "File name: [ batch_3 ]; with size: 9753.8701171875 KB\n",
      "File name: [ batch_4 ]; with size: 9735.4248046875 KB\n",
      "File name: [ batch_6 ]; with size: 9814.5185546875 KB\n",
      "File name: [ batch_0 ]; with size: 9793.3623046875 KB\n",
      "File name: [ batch_5 ]; with size: 9729.3466796875 KB\n",
      "File name: [ batch_2 ]; with size: 9764.7919921875 KB\n",
      "\n",
      "====================================================================================================\n",
      "In-process Training costs a total of 6.2916 seconds!\n",
      "In-process Training costs a total of 6.2695 seconds!\n",
      "In-process Training costs a total of 6.2494 seconds!\n",
      "In-process Training costs a total of 6.2740 seconds!\n",
      "In-process Training costs a total of 6.2307 seconds!\n",
      "In-process Training costs a total of 6.2849 seconds!\n",
      "In-process Training costs a total of 6.2717 seconds!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:58: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:58: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start running for partition num: 8 hop layer 1\n",
      "\n",
      "====================================================================================================\n",
      "Start to generate the clustering machine:\n",
      "Batch machine creation costs a total of 0.1708 seconds!\n",
      "\n",
      " Information about the content of ./tmp/\n",
      "\n",
      "Start to split data into train, test, validation:\n",
      "Data splitting costs a total of 0.1734 seconds!\n",
      "Start to store the batch machine file:\n",
      "Storing batch machine after training batches generation costs a total of 0.0420 seconds!\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0513 seconds!\n",
      "Start to generate the training batches:\n",
      "Train batches production costs a total of 0.1896 seconds!\n",
      "\n",
      " Information about the content of ./train/\n",
      "File name: [ batch_9 ]; with size: 4874.6025390625 KB\n",
      "File name: [ batch_1 ]; with size: 4797.9462890625 KB\n",
      "File name: [ batch_15 ]; with size: 4840.9228515625 KB\n",
      "File name: [ batch_8 ]; with size: 4948.5322265625 KB\n",
      "File name: [ batch_13 ]; with size: 4832.6259765625 KB\n",
      "File name: [ batch_11 ]; with size: 4919.2978515625 KB\n",
      "File name: [ batch_7 ]; with size: 4867.3212890625 KB\n",
      "File name: [ batch_3 ]; with size: 4874.4541015625 KB\n",
      "File name: [ batch_4 ]; with size: 4852.5712890625 KB\n",
      "File name: [ batch_6 ]; with size: 4884.0087890625 KB\n",
      "File name: [ batch_0 ]; with size: 4881.3603515625 KB\n",
      "File name: [ batch_5 ]; with size: 4911.0087890625 KB\n",
      "File name: [ batch_14 ]; with size: 4816.2587890625 KB\n",
      "File name: [ batch_10 ]; with size: 4831.1103515625 KB\n",
      "File name: [ batch_2 ]; with size: 4789.2509765625 KB\n",
      "File name: [ batch_12 ]; with size: 4797.9462890625 KB\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0526 seconds!\n",
      "Start to generate the validation batches:\n",
      "Validation batches production costs a total of 0.2329 seconds!\n",
      "\n",
      " Information about the content of ./validation/\n",
      "File name: [ batch_9 ]; with size: 4889.1201171875 KB\n",
      "File name: [ batch_1 ]; with size: 4812.2294921875 KB\n",
      "File name: [ batch_15 ]; with size: 4855.3232421875 KB\n",
      "File name: [ batch_8 ]; with size: 4963.2607421875 KB\n",
      "File name: [ batch_13 ]; with size: 4847.0263671875 KB\n",
      "File name: [ batch_11 ]; with size: 4933.9326171875 KB\n",
      "File name: [ batch_7 ]; with size: 4881.8076171875 KB\n",
      "File name: [ batch_3 ]; with size: 4888.9638671875 KB\n",
      "File name: [ batch_4 ]; with size: 4867.0107421875 KB\n",
      "File name: [ batch_6 ]; with size: 4898.5576171875 KB\n",
      "File name: [ batch_0 ]; with size: 4895.8935546875 KB\n",
      "File name: [ batch_5 ]; with size: 4925.6279296875 KB\n",
      "File name: [ batch_14 ]; with size: 4830.5966796875 KB\n",
      "File name: [ batch_10 ]; with size: 4845.4873046875 KB\n",
      "File name: [ batch_2 ]; with size: 4803.5107421875 KB\n",
      "File name: [ batch_12 ]; with size: 4812.2294921875 KB\n",
      "\n",
      "====================================================================================================\n",
      "In-process Training costs a total of 6.6229 seconds!\n",
      "In-process Training costs a total of 6.6385 seconds!\n",
      "In-process Training costs a total of 6.6061 seconds!\n",
      "In-process Training costs a total of 6.6460 seconds!\n",
      "In-process Training costs a total of 6.6091 seconds!\n",
      "In-process Training costs a total of 6.6057 seconds!\n",
      "In-process Training costs a total of 6.5993 seconds!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:58: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:58: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start running for partition num: 8 hop layer 2\n",
      "\n",
      "====================================================================================================\n",
      "Start to generate the clustering machine:\n",
      "Batch machine creation costs a total of 0.1324 seconds!\n",
      "\n",
      " Information about the content of ./tmp/\n",
      "\n",
      "Start to split data into train, test, validation:\n",
      "Data splitting costs a total of 0.2228 seconds!\n",
      "Start to store the batch machine file:\n",
      "Storing batch machine after training batches generation costs a total of 0.0417 seconds!\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0515 seconds!\n",
      "Start to generate the training batches:\n",
      "Train batches production costs a total of 0.1947 seconds!\n",
      "\n",
      " Information about the content of ./train/\n",
      "File name: [ batch_9 ]; with size: 4847.5791015625 KB\n",
      "File name: [ batch_1 ]; with size: 4864.7275390625 KB\n",
      "File name: [ batch_15 ]; with size: 4840.6337890625 KB\n",
      "File name: [ batch_8 ]; with size: 4925.0009765625 KB\n",
      "File name: [ batch_13 ]; with size: 4897.9306640625 KB\n",
      "File name: [ batch_11 ]; with size: 4826.8447265625 KB\n",
      "File name: [ batch_7 ]; with size: 4861.1650390625 KB\n",
      "File name: [ batch_3 ]; with size: 4820.8681640625 KB\n",
      "File name: [ batch_4 ]; with size: 4943.1181640625 KB\n",
      "File name: [ batch_6 ]; with size: 4841.3525390625 KB\n",
      "File name: [ batch_0 ]; with size: 4820.8212890625 KB\n",
      "File name: [ batch_5 ]; with size: 4850.2275390625 KB\n",
      "File name: [ batch_14 ]; with size: 4880.2119140625 KB\n",
      "File name: [ batch_10 ]; with size: 4819.2822265625 KB\n",
      "File name: [ batch_2 ]; with size: 4858.4541015625 KB\n",
      "File name: [ batch_12 ]; with size: 4825.4072265625 KB\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0528 seconds!\n",
      "Start to generate the validation batches:\n",
      "Validation batches production costs a total of 0.1938 seconds!\n",
      "\n",
      " Information about the content of ./validation/\n",
      "File name: [ batch_9 ]; with size: 4862.0185546875 KB\n",
      "File name: [ batch_1 ]; with size: 4879.2216796875 KB\n",
      "File name: [ batch_15 ]; with size: 4855.0419921875 KB\n",
      "File name: [ batch_8 ]; with size: 4939.6513671875 KB\n",
      "File name: [ batch_13 ]; with size: 4912.5107421875 KB\n",
      "File name: [ batch_11 ]; with size: 4841.2138671875 KB\n",
      "File name: [ batch_7 ]; with size: 4875.6279296875 KB\n",
      "File name: [ batch_3 ]; with size: 4835.2216796875 KB\n",
      "File name: [ batch_4 ]; with size: 4957.8232421875 KB\n",
      "File name: [ batch_6 ]; with size: 4855.7607421875 KB\n",
      "File name: [ batch_0 ]; with size: 4835.1826171875 KB\n",
      "File name: [ batch_5 ]; with size: 4864.6669921875 KB\n",
      "File name: [ batch_14 ]; with size: 4894.7451171875 KB\n",
      "File name: [ batch_10 ]; with size: 4833.6201171875 KB\n",
      "File name: [ batch_2 ]; with size: 4872.9091796875 KB\n",
      "File name: [ batch_12 ]; with size: 4839.7685546875 KB\n",
      "\n",
      "====================================================================================================\n",
      "In-process Training costs a total of 9.7949 seconds!\n",
      "In-process Training costs a total of 9.8287 seconds!\n",
      "In-process Training costs a total of 9.8123 seconds!\n",
      "In-process Training costs a total of 9.7837 seconds!\n",
      "In-process Training costs a total of 9.8443 seconds!\n",
      "In-process Training costs a total of 9.8229 seconds!\n",
      "In-process Training costs a total of 9.8102 seconds!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:58: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:58: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start running for partition num: 8 hop layer 3\n",
      "\n",
      "====================================================================================================\n",
      "Start to generate the clustering machine:\n",
      "Batch machine creation costs a total of 0.1667 seconds!\n",
      "\n",
      " Information about the content of ./tmp/\n",
      "\n",
      "Start to split data into train, test, validation:\n",
      "Data splitting costs a total of 0.1726 seconds!\n",
      "Start to store the batch machine file:\n",
      "Storing batch machine after training batches generation costs a total of 0.0413 seconds!\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0512 seconds!\n",
      "Start to generate the training batches:\n",
      "Train batches production costs a total of 0.1974 seconds!\n",
      "\n",
      " Information about the content of ./train/\n",
      "File name: [ batch_9 ]; with size: 4820.5478515625 KB\n",
      "File name: [ batch_1 ]; with size: 4805.9462890625 KB\n",
      "File name: [ batch_15 ]; with size: 4862.4697265625 KB\n",
      "File name: [ batch_8 ]; with size: 4789.2509765625 KB\n",
      "File name: [ batch_13 ]; with size: 4840.9072265625 KB\n",
      "File name: [ batch_11 ]; with size: 4824.5712890625 KB\n",
      "File name: [ batch_7 ]; with size: 4852.4384765625 KB\n",
      "File name: [ batch_3 ]; with size: 4897.9306640625 KB\n",
      "File name: [ batch_4 ]; with size: 4881.3603515625 KB\n",
      "File name: [ batch_6 ]; with size: 4861.1650390625 KB\n",
      "File name: [ batch_0 ]; with size: 4911.1962890625 KB\n",
      "File name: [ batch_5 ]; with size: 4809.8369140625 KB\n",
      "File name: [ batch_14 ]; with size: 4943.1181640625 KB\n",
      "File name: [ batch_10 ]; with size: 4872.0947265625 KB\n",
      "File name: [ batch_2 ]; with size: 4841.6103515625 KB\n",
      "File name: [ batch_12 ]; with size: 4904.8369140625 KB\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.0897 seconds!\n",
      "Start to generate the validation batches:\n",
      "Validation batches production costs a total of 0.1947 seconds!\n",
      "\n",
      " Information about the content of ./validation/\n",
      "File name: [ batch_9 ]; with size: 4834.9013671875 KB\n",
      "File name: [ batch_1 ]; with size: 4820.2607421875 KB\n",
      "File name: [ batch_15 ]; with size: 4876.9404296875 KB\n",
      "File name: [ batch_8 ]; with size: 4803.5107421875 KB\n",
      "File name: [ batch_13 ]; with size: 4855.3310546875 KB\n",
      "File name: [ batch_11 ]; with size: 4838.9326171875 KB\n",
      "File name: [ batch_7 ]; with size: 4866.8857421875 KB\n",
      "File name: [ batch_3 ]; with size: 4912.5107421875 KB\n",
      "File name: [ batch_4 ]; with size: 4895.8935546875 KB\n",
      "File name: [ batch_6 ]; with size: 4875.6279296875 KB\n",
      "File name: [ batch_0 ]; with size: 4925.8154296875 KB\n",
      "File name: [ batch_5 ]; with size: 4824.1513671875 KB\n",
      "File name: [ batch_14 ]; with size: 4957.8232421875 KB\n",
      "File name: [ batch_10 ]; with size: 4886.5888671875 KB\n",
      "File name: [ batch_2 ]; with size: 4856.0185546875 KB\n",
      "File name: [ batch_12 ]; with size: 4919.4482421875 KB\n",
      "\n",
      "====================================================================================================\n",
      "In-process Training costs a total of 12.9240 seconds!\n",
      "In-process Training costs a total of 12.9246 seconds!\n",
      "In-process Training costs a total of 12.8466 seconds!\n",
      "In-process Training costs a total of 12.8700 seconds!\n",
      "In-process Training costs a total of 12.8690 seconds!\n",
      "In-process Training costs a total of 12.9519 seconds!\n",
      "In-process Training costs a total of 12.8950 seconds!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:58: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:58: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "output_train_investigate(data, data_name, dataset, image_data_path, intermediate_data_folder, partition_nums, layers, \\\n",
    "                         dropout = 0.1, lr = 0.0001, weight_decay = 0.001, mini_epoch_num = 20, output_period = 40, \\\n",
    "                         valid_part_num = 2, mini_cluster_num = 16, round_num = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output_F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start running for partition num: 2 hop layer 1\n",
      "\n",
      "====================================================================================================\n",
      "Start to generate the clustering machine:\n",
      "Batch machine creation costs a total of 0.2043 seconds!\n",
      "\n",
      " Information about the content of ./tmp/\n",
      "\n",
      "Start to split data into train, test, validation:\n",
      "Data splitting costs a total of 0.2018 seconds!\n",
      "Start to store the batch machine file:\n",
      "Storing batch machine after training batches generation costs a total of 0.4779 seconds!\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.6452 seconds!\n",
      "Start to generate the training batches:\n",
      "Train batches production costs a total of 9.3198 seconds!\n",
      "\n",
      " Information about the content of ./train/\n",
      "File name: [ batch_1 ]; with size: 340926.7939453125 KB\n",
      "File name: [ batch_3 ]; with size: 333775.9501953125 KB\n",
      "File name: [ batch_0 ]; with size: 333725.5419921875 KB\n",
      "File name: [ batch_2 ]; with size: 340877.2626953125 KB\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.6858 seconds!\n",
      "Start to generate the validation batches:\n",
      "Validation batches production costs a total of 13.5641 seconds!\n",
      "\n",
      " Information about the content of ./validation/\n",
      "File name: [ batch_1 ]; with size: 340985.4423828125 KB\n",
      "File name: [ batch_3 ]; with size: 333833.3720703125 KB\n",
      "File name: [ batch_0 ]; with size: 333782.9638671875 KB\n",
      "File name: [ batch_2 ]; with size: 340935.9111328125 KB\n",
      "\n",
      "====================================================================================================\n",
      "Start running for partition num: 2 hop layer 2\n",
      "\n",
      "====================================================================================================\n",
      "Start to generate the clustering machine:\n",
      "Batch machine creation costs a total of 0.2025 seconds!\n",
      "\n",
      " Information about the content of ./tmp/\n",
      "\n",
      "Start to split data into train, test, validation:\n",
      "Data splitting costs a total of 0.2012 seconds!\n",
      "Start to store the batch machine file:\n",
      "Storing batch machine after training batches generation costs a total of 0.4768 seconds!\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.6457 seconds!\n",
      "Start to generate the training batches:\n",
      "Train batches production costs a total of 9.5806 seconds!\n",
      "\n",
      " Information about the content of ./train/\n",
      "File name: [ batch_1 ]; with size: 335632.8408203125 KB\n",
      "File name: [ batch_3 ]; with size: 335911.0908203125 KB\n",
      "File name: [ batch_0 ]; with size: 339026.9345703125 KB\n",
      "File name: [ batch_2 ]; with size: 338731.3408203125 KB\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.6504 seconds!\n",
      "Start to generate the validation batches:\n",
      "Validation batches production costs a total of 13.4376 seconds!\n",
      "\n",
      " Information about the content of ./validation/\n",
      "File name: [ batch_1 ]; with size: 335690.5751953125 KB\n",
      "File name: [ batch_3 ]; with size: 335968.8798828125 KB\n",
      "File name: [ batch_0 ]; with size: 339085.2705078125 KB\n",
      "File name: [ batch_2 ]; with size: 338789.6220703125 KB\n",
      "\n",
      "====================================================================================================\n",
      "Start running for partition num: 2 hop layer 3\n",
      "\n",
      "====================================================================================================\n",
      "Start to generate the clustering machine:\n",
      "Batch machine creation costs a total of 0.1956 seconds!\n",
      "\n",
      " Information about the content of ./tmp/\n",
      "\n",
      "Start to split data into train, test, validation:\n",
      "Data splitting costs a total of 0.2494 seconds!\n",
      "Start to store the batch machine file:\n",
      "Storing batch machine after training batches generation costs a total of 0.4795 seconds!\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.6413 seconds!\n",
      "Start to generate the training batches:\n",
      "Train batches production costs a total of 9.6979 seconds!\n",
      "\n",
      " Information about the content of ./train/\n",
      "File name: [ batch_1 ]; with size: 338239.6533203125 KB\n",
      "File name: [ batch_3 ]; with size: 338214.5517578125 KB\n",
      "File name: [ batch_0 ]; with size: 336414.4345703125 KB\n",
      "File name: [ batch_2 ]; with size: 336435.4404296875 KB\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.6464 seconds!\n",
      "Start to generate the validation batches:\n",
      "Validation batches production costs a total of 13.7462 seconds!\n",
      "\n",
      " Information about the content of ./validation/\n",
      "File name: [ batch_1 ]; with size: 338297.8251953125 KB\n",
      "File name: [ batch_3 ]; with size: 338272.7314453125 KB\n",
      "File name: [ batch_0 ]; with size: 336472.3330078125 KB\n",
      "File name: [ batch_2 ]; with size: 336493.3310546875 KB\n",
      "\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:58: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:58: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start running for partition num: 4 hop layer 1\n",
      "\n",
      "====================================================================================================\n",
      "Start to generate the clustering machine:\n",
      "Batch machine creation costs a total of 0.1977 seconds!\n",
      "\n",
      " Information about the content of ./tmp/\n",
      "\n",
      "Start to split data into train, test, validation:\n",
      "Data splitting costs a total of 0.1990 seconds!\n",
      "Start to store the batch machine file:\n",
      "Storing batch machine after training batches generation costs a total of 0.4736 seconds!\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.6358 seconds!\n",
      "Start to generate the training batches:\n",
      "Train batches production costs a total of 9.2232 seconds!\n",
      "\n",
      " Information about the content of ./train/\n",
      "File name: [ batch_1 ]; with size: 169564.0498046875 KB\n",
      "File name: [ batch_7 ]; with size: 166857.8232421875 KB\n",
      "File name: [ batch_3 ]; with size: 168966.3388671875 KB\n",
      "File name: [ batch_4 ]; with size: 170308.2763671875 KB\n",
      "File name: [ batch_6 ]; with size: 166525.6279296875 KB\n",
      "File name: [ batch_0 ]; with size: 169245.2216796875 KB\n",
      "File name: [ batch_5 ]; with size: 170498.5029296875 KB\n",
      "File name: [ batch_2 ]; with size: 166417.4638671875 KB\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.6473 seconds!\n",
      "Start to generate the validation batches:\n",
      "Validation batches production costs a total of 13.6175 seconds!\n",
      "\n",
      " Information about the content of ./validation/\n",
      "File name: [ batch_1 ]; with size: 169593.2294921875 KB\n",
      "File name: [ batch_7 ]; with size: 166886.5576171875 KB\n",
      "File name: [ batch_3 ]; with size: 168995.4326171875 KB\n",
      "File name: [ batch_4 ]; with size: 170337.5966796875 KB\n",
      "File name: [ batch_6 ]; with size: 166554.2998046875 KB\n",
      "File name: [ batch_0 ]; with size: 169274.3544921875 KB\n",
      "File name: [ batch_5 ]; with size: 170527.8466796875 KB\n",
      "File name: [ batch_2 ]; with size: 166446.1279296875 KB\n",
      "\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:58: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:58: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:58: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:58: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start running for partition num: 4 hop layer 2\n",
      "\n",
      "====================================================================================================\n",
      "Start to generate the clustering machine:\n",
      "Batch machine creation costs a total of 0.1941 seconds!\n",
      "\n",
      " Information about the content of ./tmp/\n",
      "\n",
      "Start to split data into train, test, validation:\n",
      "Data splitting costs a total of 0.2521 seconds!\n",
      "Start to store the batch machine file:\n",
      "Storing batch machine after training batches generation costs a total of 0.4782 seconds!\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.6453 seconds!\n",
      "Start to generate the training batches:\n",
      "Train batches production costs a total of 9.1729 seconds!\n",
      "\n",
      " Information about the content of ./train/\n",
      "File name: [ batch_1 ]; with size: 164102.7685546875 KB\n",
      "File name: [ batch_7 ]; with size: 167215.8310546875 KB\n",
      "File name: [ batch_3 ]; with size: 169806.1435546875 KB\n",
      "File name: [ batch_4 ]; with size: 167249.1748046875 KB\n",
      "File name: [ batch_6 ]; with size: 169521.3623046875 KB\n",
      "File name: [ batch_0 ]; with size: 169393.1279296875 KB\n",
      "File name: [ batch_5 ]; with size: 170202.6748046875 KB\n",
      "File name: [ batch_2 ]; with size: 170903.5341796875 KB\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.6498 seconds!\n",
      "Start to generate the validation batches:\n",
      "Validation batches production costs a total of 13.9359 seconds!\n",
      "\n",
      " Information about the content of ./validation/\n",
      "File name: [ batch_1 ]; with size: 164131.0107421875 KB\n",
      "File name: [ batch_7 ]; with size: 167244.6201171875 KB\n",
      "File name: [ batch_3 ]; with size: 169835.3701171875 KB\n",
      "File name: [ batch_4 ]; with size: 167277.9716796875 KB\n",
      "File name: [ batch_6 ]; with size: 169550.5341796875 KB\n",
      "File name: [ batch_0 ]; with size: 169422.2998046875 KB\n",
      "File name: [ batch_5 ]; with size: 170231.9873046875 KB\n",
      "File name: [ batch_2 ]; with size: 170932.9638671875 KB\n",
      "\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:58: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:58: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:58: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:58: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start running for partition num: 4 hop layer 3\n",
      "\n",
      "====================================================================================================\n",
      "Start to generate the clustering machine:\n",
      "Batch machine creation costs a total of 0.2038 seconds!\n",
      "\n",
      " Information about the content of ./tmp/\n",
      "\n",
      "Start to split data into train, test, validation:\n",
      "Data splitting costs a total of 0.1992 seconds!\n",
      "Start to store the batch machine file:\n",
      "Storing batch machine after training batches generation costs a total of 0.4771 seconds!\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.6451 seconds!\n",
      "Start to generate the training batches:\n",
      "Train batches production costs a total of 9.3861 seconds!\n",
      "\n",
      " Information about the content of ./train/\n",
      "File name: [ batch_1 ]; with size: 167441.2763671875 KB\n",
      "File name: [ batch_7 ]; with size: 167286.0185546875 KB\n",
      "File name: [ batch_3 ]; with size: 167418.0654296875 KB\n",
      "File name: [ batch_4 ]; with size: 166409.0263671875 KB\n",
      "File name: [ batch_6 ]; with size: 173090.0888671875 KB\n",
      "File name: [ batch_0 ]; with size: 171512.2373046875 KB\n",
      "File name: [ batch_5 ]; with size: 167405.6279296875 KB\n",
      "File name: [ batch_2 ]; with size: 167829.2763671875 KB\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.6551 seconds!\n",
      "Start to generate the validation batches:\n",
      "Validation batches production costs a total of 14.1426 seconds!\n",
      "\n",
      " Information about the content of ./validation/\n",
      "File name: [ batch_1 ]; with size: 167470.1044921875 KB\n",
      "File name: [ batch_7 ]; with size: 167314.8232421875 KB\n",
      "File name: [ batch_3 ]; with size: 167446.8701171875 KB\n",
      "File name: [ batch_4 ]; with size: 166437.6904296875 KB\n",
      "File name: [ batch_6 ]; with size: 173119.8857421875 KB\n",
      "File name: [ batch_0 ]; with size: 171541.7685546875 KB\n",
      "File name: [ batch_5 ]; with size: 167434.4326171875 KB\n",
      "File name: [ batch_2 ]; with size: 167858.1826171875 KB\n",
      "\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:58: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:58: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:58: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:58: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start running for partition num: 8 hop layer 1\n",
      "\n",
      "====================================================================================================\n",
      "Start to generate the clustering machine:\n",
      "Batch machine creation costs a total of 0.2065 seconds!\n",
      "\n",
      " Information about the content of ./tmp/\n",
      "\n",
      "Start to split data into train, test, validation:\n",
      "Data splitting costs a total of 0.1968 seconds!\n",
      "Start to store the batch machine file:\n",
      "Storing batch machine after training batches generation costs a total of 0.4691 seconds!\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.6365 seconds!\n",
      "Start to generate the training batches:\n",
      "Train batches production costs a total of 9.3090 seconds!\n",
      "\n",
      " Information about the content of ./train/\n",
      "File name: [ batch_9 ]; with size: 86009.0322265625 KB\n",
      "File name: [ batch_1 ]; with size: 84894.5087890625 KB\n",
      "File name: [ batch_15 ]; with size: 84686.9697265625 KB\n",
      "File name: [ batch_8 ]; with size: 83118.1650390625 KB\n",
      "File name: [ batch_13 ]; with size: 83862.3994140625 KB\n",
      "File name: [ batch_11 ]; with size: 84824.6337890625 KB\n",
      "File name: [ batch_7 ]; with size: 81784.6650390625 KB\n",
      "File name: [ batch_3 ]; with size: 84098.3056640625 KB\n",
      "File name: [ batch_4 ]; with size: 86390.9541015625 KB\n",
      "File name: [ batch_6 ]; with size: 82878.5869140625 KB\n",
      "File name: [ batch_0 ]; with size: 84247.9384765625 KB\n",
      "File name: [ batch_5 ]; with size: 85606.1259765625 KB\n",
      "File name: [ batch_14 ]; with size: 84821.8212890625 KB\n",
      "File name: [ batch_10 ]; with size: 84278.7197265625 KB\n",
      "File name: [ batch_2 ]; with size: 84062.8916015625 KB\n",
      "File name: [ batch_12 ]; with size: 82362.6103515625 KB\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.6502 seconds!\n",
      "Start to generate the validation batches:\n",
      "Validation batches production costs a total of 14.7618 seconds!\n",
      "\n",
      " Information about the content of ./validation/\n",
      "File name: [ batch_9 ]; with size: 86023.8544921875 KB\n",
      "File name: [ batch_1 ]; with size: 84909.1357421875 KB\n",
      "File name: [ batch_15 ]; with size: 84701.5654296875 KB\n",
      "File name: [ batch_8 ]; with size: 83132.4716796875 KB\n",
      "File name: [ batch_13 ]; with size: 83876.8388671875 KB\n",
      "File name: [ batch_11 ]; with size: 84839.2451171875 KB\n",
      "File name: [ batch_7 ]; with size: 81798.7451171875 KB\n",
      "File name: [ batch_3 ]; with size: 84112.7919921875 KB\n",
      "File name: [ batch_4 ]; with size: 86405.8310546875 KB\n",
      "File name: [ batch_6 ]; with size: 82892.8701171875 KB\n",
      "File name: [ batch_0 ]; with size: 84262.4404296875 KB\n",
      "File name: [ batch_5 ]; with size: 85620.8623046875 KB\n",
      "File name: [ batch_14 ]; with size: 84836.4326171875 KB\n",
      "File name: [ batch_10 ]; with size: 84293.2294921875 KB\n",
      "File name: [ batch_2 ]; with size: 84077.3857421875 KB\n",
      "File name: [ batch_12 ]; with size: 82376.7998046875 KB\n",
      "\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:58: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:58: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:58: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/ipykernel_launcher.py:58: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/home/xiangli/anaconda3/envs/pytorch_geometric/lib/python3.7/site-packages/seaborn/axisgrid.py:311: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start running for partition num: 8 hop layer 2\n",
      "\n",
      "====================================================================================================\n",
      "Start to generate the clustering machine:\n",
      "Batch machine creation costs a total of 0.1595 seconds!\n",
      "\n",
      " Information about the content of ./tmp/\n",
      "\n",
      "Start to split data into train, test, validation:\n",
      "Data splitting costs a total of 0.2472 seconds!\n",
      "Start to store the batch machine file:\n",
      "Storing batch machine after training batches generation costs a total of 0.4716 seconds!\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.6380 seconds!\n",
      "Start to generate the training batches:\n",
      "Train batches production costs a total of 9.1947 seconds!\n",
      "\n",
      " Information about the content of ./train/\n",
      "File name: [ batch_9 ]; with size: 84756.6572265625 KB\n",
      "File name: [ batch_1 ]; with size: 83520.9150390625 KB\n",
      "File name: [ batch_15 ]; with size: 86767.8369140625 KB\n",
      "File name: [ batch_8 ]; with size: 84237.5634765625 KB\n",
      "File name: [ batch_13 ]; with size: 83520.9150390625 KB\n",
      "File name: [ batch_11 ]; with size: 82297.0791015625 KB\n",
      "File name: [ batch_7 ]; with size: 86771.8056640625 KB\n",
      "File name: [ batch_3 ]; with size: 84278.7197265625 KB\n",
      "File name: [ batch_4 ]; with size: 82878.5869140625 KB\n",
      "File name: [ batch_6 ]; with size: 85477.3681640625 KB\n",
      "File name: [ batch_0 ]; with size: 84824.6337890625 KB\n",
      "File name: [ batch_5 ]; with size: 83862.3994140625 KB\n",
      "File name: [ batch_14 ]; with size: 82872.5869140625 KB\n",
      "File name: [ batch_10 ]; with size: 85233.7119140625 KB\n",
      "File name: [ batch_2 ]; with size: 82362.6103515625 KB\n",
      "File name: [ batch_12 ]; with size: 84288.0634765625 KB\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Batch machine reading costs a total of 0.6574 seconds!\n",
      "Start to generate the validation batches:\n",
      "Validation batches production costs a total of 14.6769 seconds!\n",
      "\n",
      " Information about the content of ./validation/\n",
      "File name: [ batch_9 ]; with size: 84771.2685546875 KB\n",
      "File name: [ batch_1 ]; with size: 83535.3076171875 KB\n",
      "File name: [ batch_15 ]; with size: 86782.7763671875 KB\n",
      "File name: [ batch_8 ]; with size: 84252.0654296875 KB\n",
      "File name: [ batch_13 ]; with size: 83535.3076171875 KB\n",
      "File name: [ batch_11 ]; with size: 82311.2529296875 KB\n",
      "File name: [ batch_7 ]; with size: 86786.7451171875 KB\n",
      "File name: [ batch_3 ]; with size: 84293.2294921875 KB\n",
      "File name: [ batch_4 ]; with size: 82892.8701171875 KB\n",
      "File name: [ batch_6 ]; with size: 85492.0888671875 KB\n",
      "File name: [ batch_0 ]; with size: 84839.2451171875 KB\n",
      "File name: [ batch_5 ]; with size: 83876.8388671875 KB\n",
      "File name: [ batch_14 ]; with size: 82886.8701171875 KB\n",
      "File name: [ batch_10 ]; with size: 85248.3857421875 KB\n",
      "File name: [ batch_2 ]; with size: 82376.7998046875 KB\n",
      "File name: [ batch_12 ]; with size: 84302.5732421875 KB\n",
      "\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# check F1-score\n",
    "output_F1_score(data, data_name, dataset, image_data_path, intermediate_data_folder, partition_nums, layers, \\\n",
    "                dropout = 0.1, lr = 0.0001, weight_decay = 0.1, mini_epoch_num = 20, \\\n",
    "                valid_part_num = 2, mini_cluster_num = 16, round_num = 2)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CoraFull dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import CoraFull\n",
    "data_name = 'CoraFull'\n",
    "dataset = CoraFull(root = local_data_root + 'CoralFull')\n",
    "print('number of data: ', len(dataset))\n",
    "data = dataset[0]\n",
    "\n",
    "image_data_path = './results/' + data_name + '/' + test_folder_name\n",
    "intermediate_data_folder = './intermediate_data/' + data_name + '/' + test_folder_name\n",
    "\n",
    "partition_nums = [4]\n",
    "layers = [[128, 128]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune the parameter\n",
    "\n",
    "output_tune_param(data, data_name, dataset, image_data_path, intermediate_data_folder, partition_nums, layers, \\\n",
    "                  dropout = 0.5, lr = 0.0001, weight_decay = 0.001, mini_epoch_num = 20, valid_part_num = 2)\n",
    "\n",
    "# in-train process\n",
    "# output_train_investigate(data, data_name, dataset, image_data_path, intermediate_data_folder, partition_nums, layers, \\\n",
    "#                          dropout = 0.5, lr = 0.0001, weight_decay = 0.001, mini_epoch_num = 20, output_period = 40, valid_part_num = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check F1-score\n",
    "# output_F1_score(data, data_name, dataset, image_data_path, intermediate_data_folder, partition_nums, layers, \\\n",
    "#                 dropout = 0.1, lr = 0.0001, weight_decay = 0.1, mini_epoch_num = 20, valid_part_num = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CiteSeer Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "data_name = 'CiteSeer'\n",
    "dataset = Planetoid(root = local_data_root + 'Planetoid/CiteSeer', name=data_name)\n",
    "data = dataset[0]\n",
    "image_data_path = './results/' + data_name + '/' + test_folder_name\n",
    "\n",
    "partition_nums = [2, 4, 8]\n",
    "layers = [[], [16], [16, 16]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning the epoch number per batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking train loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PubMed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "data_name = 'PubMed'\n",
    "dataset = Planetoid(root = local_data_root + 'Planetoid/PubMed', name=data_name)\n",
    "data = dataset[0]\n",
    "image_data_path = './results/' + data_name + '/' + test_folder_name\n",
    "\n",
    "partition_nums = [2, 4, 8]\n",
    "layers = [[], [64], [64, 64], [64, 64, 64]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tune epoch number per batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the train error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free GPU memory\n",
    "# !(nvidia-smi | grep 'python' | awk '{ print $3 }' | xargs -n1 kill -9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_geometric]",
   "language": "python",
   "name": "conda-env-pytorch_geometric-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
